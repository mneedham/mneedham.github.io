<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2013s on Mark Needham</title>
    <link>http://mneedham.github.io/2013/</link>
    <description>Recent content in 2013s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Dec 2013 21:46:42 +0000</lastBuildDate>
    
	<atom:link href="http://mneedham.github.io/2013/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>RxJava: From Future to Observable</title>
      <link>http://mneedham.github.io/2013/12/28/rxjava-from-future-to-observable/</link>
      <pubDate>Sat, 28 Dec 2013 21:46:42 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/12/28/rxjava-from-future-to-observable/</guid>
      <description>I first came across Reactive Extensions about 4 years ago on Matthew Podwysocki&#39;s blog but then haven&#39;t heard much about it until I saw Matthew give a talk at Code Mesh a few weeks ago.
It seems to have grown in popularity recently and I noticed that&#39;s there&#39;s now a Java version called RxJava written by Netflix.
I thought I&#39;d give it a try by changing some code I wrote while exploring cypher&#39;s MERGE function to expose an Observable instead of Futures.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Using MERGE with schema indexes/constraints</title>
      <link>http://mneedham.github.io/2013/12/23/neo4j-cypher-using-merge-with-schema-indexesconstraints/</link>
      <pubDate>Mon, 23 Dec 2013 13:30:38 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/12/23/neo4j-cypher-using-merge-with-schema-indexesconstraints/</guid>
      <description>A couple of weeks about I wrote about cypher&#39;s MERGE function and over the last few days I&#39;ve been exploring how it works when used with schema indexes and unique constraints.
A common use case with Neo4j is to model users and events where an event could be a tweet, Facebook post or Pinterest pin. The model might look like this:
 We&#39;d have a stream of (user, event) pairs and a cypher statement like the following to get the data into Neo4j:</description>
    </item>
    
    <item>
      <title>Supporting production code: Start with the simple things</title>
      <link>http://mneedham.github.io/2013/12/20/supporting-production-code-start-with-the-simple-things/</link>
      <pubDate>Fri, 20 Dec 2013 18:07:36 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/12/20/supporting-production-code-start-with-the-simple-things/</guid>
      <description>A few months ago I wrote about my experiences supporting production code while working at uSwitch. Since then I&#39;ve been working on support for Neo4j customers and I&#39;ve realised that there are a couple of other things to keep in mind while debugging production problems that I missed from the initial list.
Keep a clear head / Hold back your assumptions The first is that it&#39;s very helpful to completely clear your head of any assumptions when looking at a problem.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Getting the hang of MERGE</title>
      <link>http://mneedham.github.io/2013/12/10/neo4j-cypher-getting-the-hang-of-merge/</link>
      <pubDate>Tue, 10 Dec 2013 23:46:46 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/12/10/neo4j-cypher-getting-the-hang-of-merge/</guid>
      <description>I&#39;ve been trying to get the hang of cypher&#39;s MERGE function and started out by writing a small file to import some people with random properties using the java-faker library.
public class Merge { private static Label PERSON = DynamicLabel.label(&amp;quot;Person&amp;quot;); public static void main(String[] args) throws IOException { File dbFile = new File(&amp;quot;/tmp/test-db&amp;quot;); FileUtils.deleteRecursively(dbFile); Faker faker = new Faker(); Random random = new Random(); GraphDatabaseService db = new GraphDatabaseFactory().newEmbeddedDatabase(dbFile.getPath()); Transaction tx = db.</description>
    </item>
    
    <item>
      <title>Neo4j: What is a node?</title>
      <link>http://mneedham.github.io/2013/11/29/neo4j-what-is-a-node/</link>
      <pubDate>Fri, 29 Nov 2013 19:50:53 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/29/neo4j-what-is-a-node/</guid>
      <description>One of the first things I needed to learn when I started using Neo4j was how to model my domain using nodes and relationships and it wasn&#39;t initially obvious to me what things should be nodes.
Luckily Ian Robinson showed me a mini-algorithm which I found helpful for getting started. The steps are as follows:
 Write out the questions you want to ask Highlight/underline the nouns Those are your nodes!</description>
    </item>
    
    <item>
      <title>Neo4j: The case of neo4j-shell and the invisible text ft. Windows and the neo4j-desktop </title>
      <link>http://mneedham.github.io/2013/11/29/neo4j-the-case-of-windows-neo4j-desktop-and-the-invisible-text/</link>
      <pubDate>Fri, 29 Nov 2013 17:08:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/29/neo4j-the-case-of-windows-neo4j-desktop-and-the-invisible-text/</guid>
      <description>I&#39;ve been playing around with Neo4j on a Windows VM recently and I wanted to launch neo4j-shell to run a few queries.
The neo4j-shell script isn&#39;t shipped with Neo4j desktop which I used to install Neo4j on my VM but we can still launch it from the Windows Command Prompt with the following command:cd &#34;C:\Program Files\Neo4j Community&#34; C:\Program Files\Neo4j Communityjre\bin\java -cp bin\neo4j-desktop-2.0.0-RC1.jar org.neo4j.shell.StartClient Welcome to the Neo4j Shell! Enter &#39;help&#39; for a list of commands NOTE: Remote Neo4j graph database service &#39;shell&#39; at port 1337 Want bash-like features?</description>
    </item>
    
    <item>
      <title>Neo4j: Modelling &#39;series&#39; of events</title>
      <link>http://mneedham.github.io/2013/11/29/neo4j-modelling-series-of-events/</link>
      <pubDate>Fri, 29 Nov 2013 00:51:25 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/29/neo4j-modelling-series-of-events/</guid>
      <description>One of the things I&#39;ve never worked out how to model in my football graph is series of matches so that I could answer questions like the following:
 How many goals has Robin Van Persie scored in his last 10 matches in the Barclays Premier League?  A brute force approach would be to get all the matches featuring Robin Van Persie in a certain competition, order them by date and take the top ten which would work but doesn&#39;t feel very graph.</description>
    </item>
    
    <item>
      <title>Neo4j: The &#39;thinking in graphs&#39; curve</title>
      <link>http://mneedham.github.io/2013/11/27/neo4j-the-thinking-in-graphs-curve/</link>
      <pubDate>Wed, 27 Nov 2013 23:09:31 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/27/neo4j-the-thinking-in-graphs-curve/</guid>
      <description>In a couple of Neo4j talks I&#39;ve done recently I&#39;ve been asked how long it takes to get used to modelling data in graphs and whether I felt it&#39;s simpler than alternative approaches.
My experience of &#39;thinking in graphs&#39;™ closely mirrors what I believe is a fairly common curve when learning technologies which change the way you think:
 There is an initial stage where it seems really hard because it&#39;s different to what we&#39;re used to and at this stage we might want to go back to what we&#39;re used to.</description>
    </item>
    
    <item>
      <title>Neo4j: Using aliases to handle messy data</title>
      <link>http://mneedham.github.io/2013/11/26/neo4j-using-aliases-to-handle-messy-data/</link>
      <pubDate>Tue, 26 Nov 2013 00:12:56 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/26/neo4j-using-aliases-to-handle-messy-data/</guid>
      <description>One of the common problems when building data heavy applications is that names of things in the domain are often named differently depending on which system you get the data from.
This means that we&#39;ll typically end up running the data from different sources through a normalisation process to ensure that we have consistent naming in the database:
 I&#39;ve recently started linking the football stadium a match was played in to the match in my football graph but unfortunately different match compilers use different spellings or even names for the same stadium.</description>
    </item>
    
    <item>
      <title>Neo4j 2.0.0-M06 -&gt; 2.0.0-RC1: Optional relationships with OPTIONAL MATCH</title>
      <link>http://mneedham.github.io/2013/11/23/neo4j-2-0-0-m06-2-0-0-rc1-optional-relationships-with-optional-match/</link>
      <pubDate>Sat, 23 Nov 2013 22:54:58 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/23/neo4j-2-0-0-m06-2-0-0-rc1-optional-relationships-with-optional-match/</guid>
      <description>One of the breaking changes in Neo4j 2.0.0-RC1 compared to previous versions is that the -[?]- syntax for matching optional relationships has been retired and replaced with the OPTIONAL MATCH construct.
An example where we might want to match an optional relationship could be if we want to find colleagues that we haven&#39;t worked with given the following model:
 Suppose we have the following data set:
CREATE (steve:Person {name: &amp;quot;Steve&amp;quot;}) CREATE (john:Person {name: &amp;quot;John&amp;quot;}) CREATE (david:Person {name: &amp;quot;David&amp;quot;}) CREATE (paul:Person {name: &amp;quot;Paul&amp;quot;}) CREATE (sam:Person {name: &amp;quot;Sam&amp;quot;}) CREATE (londonOffice:Office {name: &amp;quot;London Office&amp;quot;}) CREATE UNIQUE (steve)-[:WORKS_IN]-&amp;gt;(londonOffice) CREATE UNIQUE (john)-[:WORKS_IN]-&amp;gt;(londonOffice) CREATE UNIQUE (david)-[:WORKS_IN]-&amp;gt;(londonOffice) CREATE UNIQUE (paul)-[:WORKS_IN]-&amp;gt;(londonOffice) CREATE UNIQUE (sam)-[:WORKS_IN]-&amp;gt;(londonOffice) CREATE UNIQUE (steve)-[:COLLEAGUES_WITH]-&amp;gt;(john) CREATE UNIQUE (steve)-[:COLLEAGUES_WITH]-&amp;gt;(david)  We might write the following query to find people from the same office as Steve but that he hasn&#39;t worked with:</description>
    </item>
    
    <item>
      <title>Neo4j 2.0.0-M06 -&gt; 2.0.0-RC1: Working with path expressions</title>
      <link>http://mneedham.github.io/2013/11/23/neo4j-2-0-0-m06-2-0-0-rc1-working-with-path-expressions/</link>
      <pubDate>Sat, 23 Nov 2013 10:30:41 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/23/neo4j-2-0-0-m06-2-0-0-rc1-working-with-path-expressions/</guid>
      <description>We recently released Neo4j 2.0.0-RC1 and since there were some breaking changes from Neo4j 2.0.0-M06 I decided to check if I needed to update any of my football graph queries.
On query which no longer worked as I expected was the following one which calculated the top goal scorers for televised games:
MATCH (player:Player)-[:played|subbed_on]-&amp;gt;stats WITH stats.goals AS goals, player, stats-[:in]-&amp;gt;()-[:on_tv]-() as onTv RETURN player.name, SUM(CASE WHEN onTv = FALSE THEN goals ELSE 0 END) as nonTvGoals, SUM(CASE WHEN onTv = TRUE THEN goals ELSE 0 END) as tvGoals, SUM(goals) as allGoals ORDER BY tvGoals DESC LIMIT 10  This is what that section of the graph looks like visually:</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Creating relationships between nodes from adjacent rows in a query</title>
      <link>http://mneedham.github.io/2013/11/22/neo4j-cypher-creating-relationships-between-nodes-from-adjacent-rows-in-a-query/</link>
      <pubDate>Fri, 22 Nov 2013 22:45:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/22/neo4j-cypher-creating-relationships-between-nodes-from-adjacent-rows-in-a-query/</guid>
      <description>I want to introduce the concept of a season into my graph so I can have import matches for multiple years and then vary the time period which queries take into account.
I started by creating season nodes like this:
CREATE (:Season {name: &amp;quot;2013/2014&amp;quot;, timestamp: 1375315200}) CREATE (:Season {name: &amp;quot;2012/2013&amp;quot;, timestamp: 1343779200}) CREATE (:Season {name: &amp;quot;2011/2012&amp;quot;, timestamp: 1312156800}) CREATE (:Season {name: &amp;quot;2010/2011&amp;quot;, timestamp: 1280620800}) CREATE (:Season {name: &amp;quot;2009/2010&amp;quot;, timestamp: 1249084800})  I wanted to add a &#39;NEXT&#39; relationship between the seasons so that I could have an in graph season index which would allow me to write queries like the following:</description>
    </item>
    
    <item>
      <title>Java: Schedule a job to run on a time interval</title>
      <link>http://mneedham.github.io/2013/11/17/java-schedule-a-job-to-run-on-a-time-interval/</link>
      <pubDate>Sun, 17 Nov 2013 22:58:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/17/java-schedule-a-job-to-run-on-a-time-interval/</guid>
      <description>Recently I&#39;ve spent some time building a set of tests around rolling upgrades between Neo4j versions and as part of that I wanted to log the state of the cluster as the upgrade was happening.
The main thread of the test blocks waiting until the upgrade is done so I wanted to log on another thread every few seconds. Alistair pointed me at the ScheduledExecutorService which worked quite nicely.
I ended up with a test which looked roughly like this:</description>
    </item>
    
    <item>
      <title>Git: Viewing the last commit on all the tags</title>
      <link>http://mneedham.github.io/2013/11/16/git-viewing-the-last-commit-on-all-the-tags/</link>
      <pubDate>Sat, 16 Nov 2013 21:58:08 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/16/git-viewing-the-last-commit-on-all-the-tags/</guid>
      <description>A couple of days ago I was curious when different versions of Neo4j had been released and although the release notes page was helpful I thought I&#39;d find more detailed information if I looked up the git tags.
Assuming that we&#39;ve already got a clone of the repository on our machine:
$ git clone git@github.com:neo4j/neo4j.git  We can pull down the latest tags by calling git fetch --tags or git fetch -t</description>
    </item>
    
    <item>
      <title>Python: Making scikit-learn and pandas play nice</title>
      <link>http://mneedham.github.io/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</link>
      <pubDate>Sat, 09 Nov 2013 13:58:54 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</guid>
      <description>In the last post I wrote about Nathan and my attempts at the Kaggle Titanic Problem I mentioned that we our next step was to try out scikit-learn so I thought I should summarise where we&#39;ve got up to.
We needed to write a classification algorithm to work out whether a person onboard the Titanic survived and luckily scikit-learn has extensive documentation on each of the algorithms.
Unfortunately almost all those examples use numpy data structures and we&#39;d loaded the data using pandas and didn&#39;t particularly want to switch back!</description>
    </item>
    
    <item>
      <title>Python: Scoping variables to use with timeit</title>
      <link>http://mneedham.github.io/2013/11/09/python-scoping-variables-to-use-with-timeit/</link>
      <pubDate>Sat, 09 Nov 2013 11:01:08 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/09/python-scoping-variables-to-use-with-timeit/</guid>
      <description>I&#39;ve been playing around with Python&#39;s timeit library to help benchmark some Neo4j cypher queries but I ran into some problems when trying to give it accessible to variables in my program.
I had the following python script which I would call from the terminal using python top-away-scorers.py:
import query_profiler as qp attempts = [ {&amp;quot;query&amp;quot;: &#39;&#39;&#39;MATCH (player:Player)-[:played]-&amp;gt;stats-[:in]-&amp;gt;game, stats-[:for]-&amp;gt;team WHERE game&amp;lt;-[:away_team]-team RETURN player.name, SUM(stats.goals) AS goals ORDER BY goals DESC LIMIT 10&#39;&#39;&#39;} ] qp.</description>
    </item>
    
    <item>
      <title>Neo4j 2.0.0-M06: Applying Wes Freeman&#39;s Cypher Optimisation tricks</title>
      <link>http://mneedham.github.io/2013/11/08/neo4j-2-0-0-m06-applying-wes-freemans-cypher-optimisation-tricks/</link>
      <pubDate>Fri, 08 Nov 2013 09:40:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/08/neo4j-2-0-0-m06-applying-wes-freemans-cypher-optimisation-tricks/</guid>
      <description>Wes has been teaching me some of his tricks for tuning Neo4j cypher queries over the last few weeks so I thought I should write up a few examples of the master&#39;s advice in action.
I&#39;ve created a mini benchmarking tool using Python&#39;s timeit and numpy to run different queries multiple times and return the mean, min, max and 95th percentile times.
I&#39;ve made my football data set available in case you want to follow along and we&#39;ll start with a query to find the top goal scorers away from home.</description>
    </item>
    
    <item>
      <title>Python: Generate all combinations of a list</title>
      <link>http://mneedham.github.io/2013/11/06/python-generate-all-combinations-of-a-list/</link>
      <pubDate>Wed, 06 Nov 2013 07:25:24 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/06/python-generate-all-combinations-of-a-list/</guid>
      <description>Nathan and I have been playing around with different scikit-learn machine learning classifiers and we wanted to run different combinations of features through each one and work out which gave the best result.
We started with a list of features:
all_columns = [&amp;quot;Fare&amp;quot;, &amp;quot;Sex&amp;quot;, &amp;quot;Pclass&amp;quot;, &#39;Embarked&#39;]  itertools#combinations allows us to create combinations with a length of our choice:
&amp;gt;&amp;gt;&amp;gt; import itertools as it &amp;gt;&amp;gt;&amp;gt; list(it.combinations(all_columns, 3)) [(&#39;Fare&#39;, &#39;Sex&#39;, &#39;Pclass&#39;), (&#39;Fare&#39;, &#39;Sex&#39;, &#39;Embarked&#39;), (&#39;Fare&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;), (&#39;Sex&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;)]  We wanted to create combinations of arbitrary length so we wanted to combine a few invocations of that functions like this:</description>
    </item>
    
    <item>
      <title>Python: matplotlib -  Import error ft2font Symbol not found: _FT_Attach_File (Mac OS X 10.8.3/Mountain Lion)</title>
      <link>http://mneedham.github.io/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</link>
      <pubDate>Sun, 03 Nov 2013 11:14:48 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</guid>
      <description>As I mentioned at the end of my last post about the Titanic Kaggle problem our next step was to do some proper machine learning&amp;trade; using scikit-learn so I started by looking at the Decision Tree example.
Unfortunately I ended up on the mother of all yak shaving missions while trying to execute the code which draws a chart using matplotlib.
I ran the following line from the tutorial:
import pylab as pl  which lead to this exception:</description>
    </item>
    
    <item>
      <title>Neo4j: A first attempt at retail product substitution</title>
      <link>http://mneedham.github.io/2013/11/01/neo4j-a-first-attempt-at-retail-product-substitution/</link>
      <pubDate>Fri, 01 Nov 2013 20:41:18 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/11/01/neo4j-a-first-attempt-at-retail-product-substitution/</guid>
      <description>One of the interesting problems in the world of online shopping from the perspective of the retailer is working out whether there is a suitable substitute product if an ordered item isn&#39;t currently in stock.
Since this problem brings together three types of data - order history, stock levels and products - it seems like it should be a nice fit for Neo4j so I &#39;graphed up&#39; a quick example.</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>http://mneedham.github.io/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>Nathan and I have been looking at Kaggle&#39;s Titanic problem and while working through the Python tutorial Nathan pointed out that we could greatly simplify the code if we used pandas instead.
The problem we had with numpy is that you use integers to reference columns. We spent a lot of time being thoroughly confused as to why something wasn&#39;t working only to realise we were using the wrong column.</description>
    </item>
    
    <item>
      <title>pandas: Adding a column to a DataFrame (based on another DataFrame)</title>
      <link>http://mneedham.github.io/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</link>
      <pubDate>Wed, 30 Oct 2013 06:12:08 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</guid>
      <description>Nathan and I have been working on the Titanic Kaggle problem using the pandas data analysis library and one thing we wanted to do was add a column to a DataFrame indicating if someone survived.
We had the following (simplified) DataFrame containing some information about customers on board the Titanic:
def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) customers = pd.DataFrame(columns=[&#39;PassengerId&#39;,&#39;Pclass&#39;,&#39;Name&#39;,&#39;Sex&#39;,&#39;Fare&#39;]) customers = addrow(customers, [dict(PassengerId=892, Pclass=3, Name=&amp;quot;Kelly, Mr. James&amp;quot;, Sex=&amp;quot;male&amp;quot;, Fare=7.8292)]) customers = addrow(customers, [dict(PassengerId=893, Pclass=3, Name=&amp;quot;Wilkes, Mrs.</description>
    </item>
    
    <item>
      <title>Thinking Fast and Slow - Daniel Kahneman: Book Review</title>
      <link>http://mneedham.github.io/2013/10/27/thinking-fast-and-slow-daniel-kahneman-book-review/</link>
      <pubDate>Sun, 27 Oct 2013 22:53:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/27/thinking-fast-and-slow-daniel-kahneman-book-review/</guid>
      <description>I picked up Daniel Kahneman&#39;s &#39;Thinking Fast and Slow&#39; after a recommendation by Mike Jones in early 2013 - it&#39;s taken me quite a while to get through it.
The book starts by describing our two styles of thinking...
 System 1 – operates automatically and quickly, with little or no effort and no sense of voluntary control.  System 2 – allocates attention to the effortful mental activities that demand it, including complex computations.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Profiling ORDER BY LIMIT vs LIMIT</title>
      <link>http://mneedham.github.io/2013/10/27/neo4j-cypher-profiling-order-by-limit-vs-limit/</link>
      <pubDate>Sun, 27 Oct 2013 00:33:54 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/27/neo4j-cypher-profiling-order-by-limit-vs-limit/</guid>
      <description>Something I&#39;ve seen people get confused by when writing queries using Neo4j&#39;s cypher query language is the sometimes significant difference in query execution time when using &#39;LIMIT&#39; on its own compared to using it in combination with &#39;ORDER BY&#39;.
The confusion is centred around the fact that at first glance it seems like the only thing different between these queries is the sorting of the rows but there&#39;s actually more to it.</description>
    </item>
    
    <item>
      <title>Neo4j: Making implicit relationships explicit &amp; bidirectional relationships</title>
      <link>http://mneedham.github.io/2013/10/25/neo4j-making-implicit-relationships-explicit-bidirectional-relationships/</link>
      <pubDate>Fri, 25 Oct 2013 16:03:48 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/25/neo4j-making-implicit-relationships-explicit-bidirectional-relationships/</guid>
      <description>I recently read Michal Bachman&#39;s post about bidirectional relationships in Neo4j in which he suggests that for some relationship types we&#39;re not that interested in the relationship&#39;s direction and can therefore ignore it when querying.
He uses the following example showing the partnership between Neo Technology and GraphAware:
 Both companies are partners with each other but since we can just as quickly find incoming and outgoing relationships we may as well just have one relationship between the two companies/nodes.</description>
    </item>
    
    <item>
      <title>Neo4j: Modelling hyper edges in a property graph</title>
      <link>http://mneedham.github.io/2013/10/22/neo4j-modelling-hyper-edges-in-a-property-graph/</link>
      <pubDate>Tue, 22 Oct 2013 22:02:14 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/22/neo4j-modelling-hyper-edges-in-a-property-graph/</guid>
      <description>At the Graph Database meet up in Antwerp last week we discussed how you would model a hyper edge in a property graph like Neo4j and I realised that I&#39;d done this in my football graph without realising.
A hyper edge is defined as follows:
 A hyperedge is a connection between two or more vertices, or nodes, of a hypergraph. A hypergraph is a graph in which generalized edges (called hyperedges) may connect more than two nodes with discrete properties.</description>
    </item>
    
    <item>
      <title>Neo4j 2.0: Labels, indexes and the like</title>
      <link>http://mneedham.github.io/2013/10/22/neo4j-2-0-labels-indexes-and-the-like/</link>
      <pubDate>Tue, 22 Oct 2013 20:20:30 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/22/neo4j-2-0-labels-indexes-and-the-like/</guid>
      <description>Last week I did a couple of talks about modelling with Neo4j meet ups in Amsterdam and Antwerp and there were a few questions about how indexing works with labels that are being introduced in Neo4j 2.0
As well as defining properties on nodes we can also assign them a label which can be used to categorise different groups of nodes.
For example in the football graph we might choose to tag player nodes with the label &#39;Player&#39;:</description>
    </item>
    
    <item>
      <title>Neo4j: Testing an unmanaged extension using CommunityServerBuilder</title>
      <link>http://mneedham.github.io/2013/10/20/neo4j-testing-an-unmanaged-extension-using-communitserverbuilder/</link>
      <pubDate>Sun, 20 Oct 2013 21:46:16 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/20/neo4j-testing-an-unmanaged-extension-using-communitserverbuilder/</guid>
      <description>I&#39;ve been playing around with Neo4j unmanaged extensions recently and I wanted to be able to check that it worked properly without having to deploy it to a real Neo4j server.
I&#39;d previously used ImpermanentGraphDatabase when using Neo4j embedded and Ian pointed me towards CommunityServerBuilder which allows us to do a similar thing in Neo4j server world.
I&#39;ve created an example of a dummy unmanaged extension and test showing this approach but it&#39;s reasonably simple.</description>
    </item>
    
    <item>
      <title>Neo4j: Accessing JMX beans via HTTP</title>
      <link>http://mneedham.github.io/2013/10/20/neo4j-accessing-jmx-beans-via-http/</link>
      <pubDate>Sun, 20 Oct 2013 11:13:54 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/20/neo4j-accessing-jmx-beans-via-http/</guid>
      <description>One of the additional features that Neo4j enterprise provides is access to various JMX properties which describe various aspects of the database.
These would typically be accessed by using jConsole or similar but some monitoring tools aren&#39;t able to use the JMX hook and a HTTP interface would work better.
Luckily Neo4j server does expose the JMX beans and we can get a list of URIs to query by hitting the following URI:</description>
    </item>
    
    <item>
      <title>Neo4j: Exploring new data sets with help from Neo4j browser</title>
      <link>http://mneedham.github.io/2013/10/18/neo4j-exploring-new-data-sets-with-help-from-neo4j-browser/</link>
      <pubDate>Fri, 18 Oct 2013 11:43:59 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/18/neo4j-exploring-new-data-sets-with-help-from-neo4j-browser/</guid>
      <description>One of the things that I&#39;ve found difficult when looking at a new Neo4j database is working out the structure of the data it contains.
I&#39;m used to relational databases where you can easily get a list of the table and the foreign keys that allow you to join them to each other.
 This has traditionally been difficult when using Neo4j but with the release of the Neo4j browser we can now easily get this type of overview by clicking on the Neo4j icon at the top left of the browser.</description>
    </item>
    
    <item>
      <title>neo4j: Setting query timeout</title>
      <link>http://mneedham.github.io/2013/10/17/neo4j-setting-query-timeout/</link>
      <pubDate>Thu, 17 Oct 2013 06:47:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/17/neo4j-setting-query-timeout/</guid>
      <description>Updated December 2015  When I initially wrote this post in 2013 this was an experimental feature that worked using the Neo4j 1.9 series but no longer does in more recent Neo4j versions (2.2, 2.3). The terminating a running transaction page in the docs describes the supported way of terminating queries. - - - - - - - - - - -
When I was first learning cypher, neo4j&#39;s query language, I frequently wrote queries which traversed the whole graph multiple times and &#39;hung&#39; for hours as they were evaluated.</description>
    </item>
    
    <item>
      <title>Java: Incrementally read/stream a CSV file</title>
      <link>http://mneedham.github.io/2013/10/14/java-incrementally-readstream-a-csv-file/</link>
      <pubDate>Mon, 14 Oct 2013 07:27:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/14/java-incrementally-readstream-a-csv-file/</guid>
      <description>I&#39;ve been doing some work which involves reading in CSV files, for which I&#39;ve been using OpenCSV, and my initial approach was to read through the file line by line, parse the contents and save it into a list of maps.
This works when the contents of the file fit into memory but is problematic for larger files where I needed to stream the file and process each line individually rather than all of them after the file was loaded.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Getting rid of an optional match</title>
      <link>http://mneedham.github.io/2013/10/13/neo4jcypher-getting-rid-of-an-optional-match/</link>
      <pubDate>Sun, 13 Oct 2013 21:59:51 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/13/neo4jcypher-getting-rid-of-an-optional-match/</guid>
      <description>I was looking back over some of the queries I wrote for my football data set and I came across one I&#39;d written to work out how many goals players scored in matches that were televised.
The data model looks like this:
 My initial query to work out the top 10 scorers in televised games was as follows:
MATCH (player:Player) WITH player MATCH player-[:played|subbed_on]-&amp;gt;stats-[:in]-&amp;gt;game-[t?:on_tv]-&amp;gt;channel WITH COLLECT({goals: stats.goals, type: TYPE(t)}) AS games, player RETURN player.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Converting queries from 1.9 to 2.0 -  &#39;Can&#39;t use optional patterns without explicit START clause&#39;</title>
      <link>http://mneedham.github.io/2013/10/03/neo4jcypher-converting-queries-from-1-9-to-2-0-cant-use-optional-patterns-without-explicit-start-clause/</link>
      <pubDate>Thu, 03 Oct 2013 16:16:02 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/10/03/neo4jcypher-converting-queries-from-1-9-to-2-0-cant-use-optional-patterns-without-explicit-start-clause/</guid>
      <description>I&#39;ve been playing around with the most recent Neo4j 2.0 milestone release - 2.0.0-M05 - and one of the first things I did was translate the queries from my football data set which were written against Neo4j 1.9.
The following query calculates the number of goals scored by players in matches that were shown on television, not on television and in total.
START player=node:players(&#39;name:*&#39;) MATCH player-[:played|subbed_on]-&amp;gt;stats-[:in]-&amp;gt;game-[t?:on_tv]-&amp;gt;channel WITH COLLECT([stats.goals, TYPE(t)]) AS games, player RETURN player.</description>
    </item>
    
    <item>
      <title>On Writing Well - William Zinsser: Book Review</title>
      <link>http://mneedham.github.io/2013/09/30/on-writing-well-william-zinsser-book-review/</link>
      <pubDate>Mon, 30 Sep 2013 22:48:13 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/30/on-writing-well-william-zinsser-book-review/</guid>
      <description>I first came across William Zinsser&#39;s &#39;On Writing Well&#39; about a year ago, but put it down having flicked through a couple of the chapters that I felt were relevant.
It came back onto my radar a month ago and this time I decided to read it cover to cover as I was sure there were some insights that I&#39;d missed due to my haphazard approach the first time around.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Translating 1.9 FILTER queries to use 2.0 list comprehensions</title>
      <link>http://mneedham.github.io/2013/09/30/neo4jcypher-translating-1-9-filter-queries-to-use-2-0-list-comprehensions/</link>
      <pubDate>Mon, 30 Sep 2013 21:34:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/30/neo4jcypher-translating-1-9-filter-queries-to-use-2-0-list-comprehensions/</guid>
      <description>I was looking back over some cypher queries I&#39;d written earlier in the year against my football data set to find some examples of where list comprehensions could be useful and I came across this query which is used to work out which teams were the most badly behaved in terms of accumulating red and yellow cards:
START team = node:teams(&#39;name:*&#39;) MATCH team&amp;lt;-[:for]-like_this&amp;lt;-[:started|as_sub]-player-[r?:sent_off_in|booked_in]-&amp;gt;game&amp;lt;-[:in]-like_this WITH team, COLLECT(r) AS cards WITH team, FILTER(x IN cards: TYPE(x) = &amp;quot;sent_off_in&amp;quot;) AS reds, FILTER(x IN cards: TYPE(x) = &amp;quot;booked_in&amp;quot;) AS yellows RETURN team.</description>
    </item>
    
    <item>
      <title>Elo Rating System: Ranking Champions League teams using Clojure Part 2</title>
      <link>http://mneedham.github.io/2013/09/30/elo-rating-system-ranking-champions-league-teams-using-clojure-part-2/</link>
      <pubDate>Mon, 30 Sep 2013 20:26:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/30/elo-rating-system-ranking-champions-league-teams-using-clojure-part-2/</guid>
      <description>A few weeks ago I wrote about ranking Champions League teams using the Elo Rating algorithm, and since I wrote that post I&#39;ve collated data for 10 years worth of matches so I thought an update was in order.
After extracting the details of all those matches I saved them to a JSON file so that I wouldn&#39;t have to parse the HTML pages every time I tweaked the algorithm. This should also make it easier for other people to play with the data.</description>
    </item>
    
    <item>
      <title>Clojure: Writing JSON to a file - &#34;Exception Don&#39;t know how to write JSON of class org.joda.time.DateTime&#34;</title>
      <link>http://mneedham.github.io/2013/09/26/clojure-writing-json-to-a-file-exception-dont-know-how-to-write-json-of-class-org-joda-time-datetime/</link>
      <pubDate>Thu, 26 Sep 2013 19:11:29 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/26/clojure-writing-json-to-a-file-exception-dont-know-how-to-write-json-of-class-org-joda-time-datetime/</guid>
      <description>As I mentioned in an earlier post I&#39;ve been transforming Clojure hash&#39;s into JSON strings using data.json but ran into trouble while trying to parse a hash which contained a Joda Time DateTime instance.
The date in question was constructed like this:
(ns json-date-example (:require [clj-time.format :as f]) (:require [clojure.data.json :as json])) (defn as-date [date-field] (f/parse (f/formatter &amp;quot;dd MMM YYYY&amp;quot;) date-field )) (def my-date (as-date &amp;quot;18 Mar 2012&amp;quot;))  And when I tried to convert a hash containing that object into a string I got the following exception:</description>
    </item>
    
    <item>
      <title>Clojure: Writing JSON to a file/reading JSON from a file</title>
      <link>http://mneedham.github.io/2013/09/26/clojure-writing-json-to-a-filereading-json-from-a-file/</link>
      <pubDate>Thu, 26 Sep 2013 07:47:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/26/clojure-writing-json-to-a-filereading-json-from-a-file/</guid>
      <description>A few weeks ago I described how I&#39;d scraped football matches using Clojure&#39;s Enlive, and the next step after translating the HTML representation into a Clojure map was to save it as a JSON document.
I decided to follow a two step process to achieve this:
 Convert hash to JSON string Write JSON string to file  I imagine there&#39;s probably a way to convert the hash to a stream and pipe that into a file but my JSON document isn&#39;t very large so I think this way is ok for now.</description>
    </item>
    
    <item>
      <title>cURL: POST/Upload multi part form</title>
      <link>http://mneedham.github.io/2013/09/23/curl-postupload-multi-part-form/</link>
      <pubDate>Mon, 23 Sep 2013 22:16:29 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/23/curl-postupload-multi-part-form/</guid>
      <description>I&#39;ve been doing some work which involved uploading a couple of files from a HTML form and I wanted to check that the server side code was working by executing a cURL command rather than using the browser.
The form looks like this:
&amp;lt;form action=&amp;quot;http://foobar.com&amp;quot; method=&amp;quot;POST&amp;quot; enctype=&amp;quot;multipart/form-data&amp;quot;&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&amp;quot;nodes&amp;quot;&amp;gt;File 1:&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;file1&amp;quot; id=&amp;quot;file1&amp;quot;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&amp;quot;relationships&amp;quot;&amp;gt;File 2:&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;file2&amp;quot; id=&amp;quot;file2&amp;quot;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;input type=&amp;quot;submit&amp;quot; name=&amp;quot;submit&amp;quot; value=&amp;quot;Submit&amp;quot;&amp;gt; &amp;lt;/form&amp;gt;  If we convert the POST request from the browser into a cURL equivalent we end up with the following:</description>
    </item>
    
    <item>
      <title>Clojure: Anonymous functions using short notation and the &#39;ArityException Wrong number of args (0) passed to: PersistentVector&#39;</title>
      <link>http://mneedham.github.io/2013/09/23/clojure-anonymous-functions-using-short-notation-and-the-arityexception-wrong-number-of-args-0-passed-to-persistentvector/</link>
      <pubDate>Mon, 23 Sep 2013 21:42:12 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/23/clojure-anonymous-functions-using-short-notation-and-the-arityexception-wrong-number-of-args-0-passed-to-persistentvector/</guid>
      <description>In the time I&#39;ve spent playing around with Clojure one thing I&#39;ve always got confused by is the error message you get when trying to return a vector using the anonymous function shorthand.
For example, if we want function which creates a vector with the values 1, 2, and the argument passed into the function we could write the following:
&amp;gt; ((fn [x] [1 2 x]) 6) [1 2 6]  However, when I tried to convert it to the shorthand &#39;#()&#39; syntax I got the following exception:</description>
    </item>
    
    <item>
      <title>Clojure/Emacs/nrepl: Stacktrace-less error messages</title>
      <link>http://mneedham.github.io/2013/09/22/clojureemacsnrepl-stacktrace-less-error-messages/</link>
      <pubDate>Sun, 22 Sep 2013 23:07:04 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/22/clojureemacsnrepl-stacktrace-less-error-messages/</guid>
      <description>Ever since I started using the Emacs + nrepl combination to play around with Clojure I&#39;ve been getting fairly non descript error messages whenever I pass the wrong parameters to a function.
For example if I try to update a non existent key in a form I get a Null Pointer Exception:
&amp;gt; (update-in {} [:mark] inc) NullPointerException clojure.lang.Numbers.ops (Numbers.java:942)  In this case it&#39;s clear that the hash doesn&#39;t have a key &#39;:mark&#39; so the function blows up.</description>
    </item>
    
    <item>
      <title>Clojure/Emacs/nrepl: Ctrl X &#43; Ctrl E leads to &#39;FileNotFoundException Could not locate […] on classpath&#39;</title>
      <link>http://mneedham.github.io/2013/09/22/clojureemacsnrepl-ctrl-x-ctrl-e-leads-to-filenotfoundexception-could-not-locate-on-classpath/</link>
      <pubDate>Sun, 22 Sep 2013 21:23:25 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/22/clojureemacsnrepl-ctrl-x-ctrl-e-leads-to-filenotfoundexception-could-not-locate-on-classpath/</guid>
      <description>I&#39;ve been playing around with Clojure using Emacs and nrepl recently and my normal work flow is to write some code in Emacs and then have it evaluated in nrepl by typing Ctrl X + Ctrl E at the end of the function.
I tried this once recently and got the following exception instead of a successful evaluation:
FileNotFoundException Could not locate ranking_algorithms/ranking__init.class or ranking_algorithms/ranking.clj on classpath: clojure.lang.RT.load (RT.java:432)  I was a bit surprised because I had nrepl running already (via (Meta + X) + Enter + nrepl-jack-in) and I&#39;d only ever seen that exception refer to dependencies which weren&#39;t in my project.</description>
    </item>
    
    <item>
      <title>Clojure: Stripping all the whitespace</title>
      <link>http://mneedham.github.io/2013/09/22/clojure-stripping-all-the-whitespace/</link>
      <pubDate>Sun, 22 Sep 2013 18:54:47 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/22/clojure-stripping-all-the-whitespace/</guid>
      <description>When putting together data sets to play around with, one of the more boring tasks is stripping out characters that you&#39;re not interested in and more often than not those characters are white spaces.
Since I&#39;ve been building data sets using Clojure I wanted to write a function that would do this for me.
I started out with the following string:
(def word &amp;quot; with a little bit of space we can make it through the night &amp;quot;)  which I wanted to format in such a way that there would be a maximum of one space between each word.</description>
    </item>
    
    <item>
      <title>Clojure: Converting an array/set into a hash map</title>
      <link>http://mneedham.github.io/2013/09/20/clojure-converting-an-arrayset-into-a-hash-map/</link>
      <pubDate>Fri, 20 Sep 2013 21:13:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/20/clojure-converting-an-arrayset-into-a-hash-map/</guid>
      <description>When I was implementing the Elo Rating algorithm a few weeks ago one thing I needed to do was come up with a base ranking for each team.
I started out with a set of teams that looked like this:
(def teams #{ &amp;quot;Man Utd&amp;quot; &amp;quot;Man City&amp;quot; &amp;quot;Arsenal&amp;quot; &amp;quot;Chelsea&amp;quot;})  and I wanted to transform that into a map from the team to their ranking e.g.
Man Utd -&amp;gt; {:points 1200} Man City -&amp;gt; {:points 1200} Arsenal -&amp;gt; {:points 1200} Chelsea -&amp;gt; {:points 1200}  I had read the documentation of array-map, a function which can be used to transform a collection of pairs into a map, and it seemed like it might do the trick.</description>
    </item>
    
    <item>
      <title>Clojure: Converting a string to a date</title>
      <link>http://mneedham.github.io/2013/09/20/clojure-converting-a-string-to-a-date/</link>
      <pubDate>Fri, 20 Sep 2013 07:00:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/20/clojure-converting-a-string-to-a-date/</guid>
      <description>I wanted to do some date manipulation in Clojure recently and figured that since clj-time is a wrapper around Joda Time it&#39;d probably do the trick.
The first thing we need to do is add the dependency to our project file and then run lein reps to pull down the appropriate JARs. The project file should look something like this:
project.clj
(defproject ranking-algorithms &amp;quot;0.1.0-SNAPSHOT&amp;quot; :license {:name &amp;quot;Eclipse Public License&amp;quot; :url &amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>Clojure: See every step of a reduce</title>
      <link>http://mneedham.github.io/2013/09/19/clojure-see-every-step-of-a-reduce/</link>
      <pubDate>Thu, 19 Sep 2013 23:57:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/19/clojure-see-every-step-of-a-reduce/</guid>
      <description>Last year I wrote about a Haskell function called scanl which returned the intermediate steps of a fold over a collection and last week I realised that I needed a similar function in Clojure to analyse a reduce I&#39;d written.
A simple reduce which adds together the numbers 1-10 would look like this:
&amp;gt; (reduce + 0 (range 1 11)) 55  If we want to see the intermediate values of this function called then instead of using reduce there&#39;s a function called reductions which gives us exactly what we want:</description>
    </item>
    
    <item>
      <title>Data Science: Don&#39;t build a crawler (if you can avoid it!)</title>
      <link>http://mneedham.github.io/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</link>
      <pubDate>Thu, 19 Sep 2013 06:55:19 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</guid>
      <description>On Tuesday I spoke at the Data Science London meetup about football data and I started out by covering some lessons I&#39;ve learnt about building data sets for personal use when open data isn&#39;t available.
When that&#39;s the case you often end up scraping HTML pages to extract the data that you&#39;re interested in and then storing that in files or in a database if you want to be more fancy.</description>
    </item>
    
    <item>
      <title>Clojure: Merge two maps but only keep the keys of one of them</title>
      <link>http://mneedham.github.io/2013/09/17/clojure-merge-two-maps-but-only-keep-the-keys-of-one-of-them/</link>
      <pubDate>Tue, 17 Sep 2013 01:03:37 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/17/clojure-merge-two-maps-but-only-keep-the-keys-of-one-of-them/</guid>
      <description>I&#39;ve been playing around with Clojure maps recently and I wanted to merge two maps of rankings where the rankings in the second map overrode those in the first while only keeping the teams from the first map.
The merge function overrides keys in earlier maps but also adds keys that only appear in later maps. For example, if we merge the following maps:
&amp;gt; (merge {&amp;quot;Man. United&amp;quot; 1500 &amp;quot;Man. City&amp;quot; 1400} {&amp;quot;Man.</description>
    </item>
    
    <item>
      <title>Clojure: Updating keys in a map</title>
      <link>http://mneedham.github.io/2013/09/17/clojure-updating-keys-in-a-map/</link>
      <pubDate>Tue, 17 Sep 2013 00:24:48 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/17/clojure-updating-keys-in-a-map/</guid>
      <description>I&#39;ve been playing with Clojure over the last few weeks and as a result I&#39;ve been using a lot of maps to represent the data.
For example if we have the following map of teams to Glicko ratings and ratings deviations:
(def teams { &amp;quot;Man. United&amp;quot; {:points 1500 :rd 350} &amp;quot;Man. City&amp;quot; {:points 1450 :rd 300} })  We might want to increase Man. United&#39;s points score by one for which we could use the update-in function:</description>
    </item>
    
    <item>
      <title>Glicko Rating System: A simple example using Clojure</title>
      <link>http://mneedham.github.io/2013/09/14/glicko-rating-system-a-simple-example-using-clojure/</link>
      <pubDate>Sat, 14 Sep 2013 21:02:30 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/14/glicko-rating-system-a-simple-example-using-clojure/</guid>
      <description>A couple of weeks ago I wrote about the Elo Rating system and when reading more about it I learnt that one of its weaknesses is that it doesn&#39;t take into account the reliability of a players&#39; rating.
For example, a player may not have played for a long time. When they next play a match we shouldn&#39;t assume that the accuracy of that rating is the same as for another player with the same rating but who plays regularly.</description>
    </item>
    
    <item>
      <title>Clojure: All things regex</title>
      <link>http://mneedham.github.io/2013/09/14/clojure-all-things-regex/</link>
      <pubDate>Sat, 14 Sep 2013 01:24:51 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/14/clojure-all-things-regex/</guid>
      <description>I&#39;ve been doing some scrapping of web pages recently using Clojure and Enlive and as part of that I&#39;ve had to write regular expressions to extract the data I&#39;m interested in.
On my travels I&#39;ve come across a few different functions and I&#39;m never sure which is the right one to use so I thought I&#39;d document what I&#39;ve tried for future me.
Check if regex matches The first regex I wrote was while scrapping the Champions League results from the Rec.</description>
    </item>
    
    <item>
      <title>jackson-core-asl - java.lang.AbstractMethodError: org.codehaus.jackson.JsonNode.getValueAsText()Ljava/lang/String;</title>
      <link>http://mneedham.github.io/2013/09/14/jackson-core-asl-java-lang-abstractmethoderror-org-codehaus-jackson-jsonnode-getvalueastextljavalangstring/</link>
      <pubDate>Sat, 14 Sep 2013 00:06:37 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/09/14/jackson-core-asl-java-lang-abstractmethoderror-org-codehaus-jackson-jsonnode-getvalueastextljavalangstring/</guid>
      <description>Ian and I were doing a bit of work on an internal application which processes JSON messages and interacts with AWS and we started seeing the following exception after doing an upgrade of jackson-mapper-asl from 1.8.9 to 1.9.13:
2013-09-13 11:01:50 +0000: Exception while handling {MessageId: 7e695fb3-549a-4b 40-b1cf-9dbc5e97a8df, ... } java.lang.AbstractMethodError: org.codehaus.jackson.JsonNode.getValueAsText()Lja va/lang/String; ... at com.amazonaws.services.sqs.AmazonSQSAsyncClient$20.call(AmazonSQSAsyn cClient.java:1200) at com.amazonaws.services.sqs.AmazonSQSAsyncClient$20.call(AmazonSQSAsyn cClient.java:1191) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor. java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor .java:615) at java.</description>
    </item>
    
    <item>
      <title>Elo Rating System: Ranking Champions League teams using Clojure</title>
      <link>http://mneedham.github.io/2013/08/31/elo-rating-system-ranking-champions-league-teams-using-clojure/</link>
      <pubDate>Sat, 31 Aug 2013 13:01:16 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/31/elo-rating-system-ranking-champions-league-teams-using-clojure/</guid>
      <description>As I mentioned in an earlier blog post I&#39;ve been learning about ranking systems and one of the first ones I came across was the Elo rating system which is most famously used to rank chess players.
The Elo rating system uses the following formula to work out a player/team&#39;s ranking after they&#39;ve participated in a match:
 R&#39; = R + K * (S - E)  R&#39; is the new rating R is the old rating K is a maximum value for increase or decrease of rating (16 or 32 for ELO) S is the score for a game E is the expected score for a game  I converted that formula into the following Clojure functions:</description>
    </item>
    
    <item>
      <title>Neo4j&#39;s Graph Café London - 28th August 2013</title>
      <link>http://mneedham.github.io/2013/08/31/neo4js-graph-cafe-london-28th-august-2013/</link>
      <pubDate>Sat, 31 Aug 2013 10:52:14 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/31/neo4js-graph-cafe-london-28th-august-2013/</guid>
      <description>On Wednesday evening I attended an interesting spin on the monthly Neo4j meetup, where instead of the usual &#39;talk then go to the pub afterwards&#39; format my colleagues Rik and Arturas organised Graph Café in the Doggetts Coat and Badge pub in Blackfriars.
The format was changed as well - the evening consisted of ~10 lightening talks which were spread out over about 3 hours, an approach Rik has used at similar events in Belgium and Holland earlier in the year.</description>
    </item>
    
    <item>
      <title>Clojure: Handling state by updating a vector inside an atom</title>
      <link>http://mneedham.github.io/2013/08/30/clojure-handling-state-by-updating-a-vector-inside-an-atom/</link>
      <pubDate>Fri, 30 Aug 2013 12:23:21 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/30/clojure-handling-state-by-updating-a-vector-inside-an-atom/</guid>
      <description>As I mentioned in a previous blog post, I&#39;ve been learning about ranking algorithms and I wanted to apply them to a series of football matches to see who the strongest team was.
Before that, however, I wanted to sketch out the functions that I&#39;d need to do this and I started with the following collections of matches and team rankings:
(def m [{:home &amp;quot;Manchester United&amp;quot;, :away &amp;quot;Manchester City&amp;quot;, :home_score 1, :away_score 0} {:home &amp;quot;Manchester United&amp;quot;, :away &amp;quot;Manchester City&amp;quot;, :home_score 2, :away_score 0}]) (def teams [ {:name &amp;quot;Manchester United&amp;quot; :points 1200} {:name &amp;quot;Manchester City&amp;quot; :points 1200} ])  I wanted to iterate over the matches and make the appropriate updates to the teams&#39; rankings depending on the result of the match.</description>
    </item>
    
    <item>
      <title>Clojure/Enlive: Screen scraping a HTML file from disk</title>
      <link>http://mneedham.github.io/2013/08/26/clojureenlive-screen-scraping-a-html-file-from-disk/</link>
      <pubDate>Mon, 26 Aug 2013 17:58:58 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/26/clojureenlive-screen-scraping-a-html-file-from-disk/</guid>
      <description>I wanted to play around with some Champions League data and I came across the Rec Sport Soccer Statistics Foundation which has collected results of all matches since the tournament started in 1955.
I wanted to get a list of all the matches for a specific season so I started out by downloading the file:
$ pwd /tmp/football $ wget http://www.rsssf.com/ec/ec200203det.html  The next step was to load that page and then run a CSS selector over it to extract the matches.</description>
    </item>
    
    <item>
      <title>Ranking Systems: What I&#39;ve learnt so far</title>
      <link>http://mneedham.github.io/2013/08/24/ranking-systems-what-ive-learnt-so-far/</link>
      <pubDate>Sat, 24 Aug 2013 11:05:58 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/24/ranking-systems-what-ive-learnt-so-far/</guid>
      <description>I often go off on massive tangents reading all about a new topic but don&#39;t record what I&#39;ve read so if I go back to the topic again in the future I have to start from scratch which is quite frustrating.
In this instance after playing around with calculating the eigenvector centrality of a sub graph I learnt that this algorithm can also be used in ranking systems.
I started off by reading a paper written by James Keener about the Perron-Frobenius Theorem and the ranking of American football teams.</description>
    </item>
    
    <item>
      <title>Unix: tar - Extracting, creating and viewing archives</title>
      <link>http://mneedham.github.io/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</link>
      <pubDate>Thu, 22 Aug 2013 22:56:23 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</guid>
      <description>I&#39;ve been playing around with the Unix tar command a bit this week and realised that I&#39;d memorised some of the flag combinations but didn&#39;t actually know what each of them meant.
For example, one of the most common things that I want to do is extract a gripped neo4j archive:
$ wget http://dist.neo4j.org/neo4j-community-1.9.2-unix.tar.gz $ tar -xvf neo4j-community-1.9.2-unix.tar.gz  where:
 -x means extract -v means produce verbose output i.e. print out the names of all the files as you unpack it -f means unpack the file which follows this flag i.</description>
    </item>
    
    <item>
      <title>Products &amp; Infinite configurability</title>
      <link>http://mneedham.github.io/2013/08/22/products-infinite-configurability/</link>
      <pubDate>Thu, 22 Aug 2013 22:11:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/22/products-infinite-configurability/</guid>
      <description>One of the common feature requests on the ThoughtWorks projects that I worked on was that the application we were working on should be almost infinitely configurable to cover potential future use cases.
My experience of attempting to do this was that you ended up with an extremely complicated code base and those future use cases often didn&#39;t come to fruition.
It therefore made more sense to solve the problem at hand and then make the code more configurable if/when the need arose.</description>
    </item>
    
    <item>
      <title>Model to answer your questions rather than modelling reality</title>
      <link>http://mneedham.github.io/2013/08/22/model-to-answer-your-questions-rather-than-modelling-reality/</link>
      <pubDate>Thu, 22 Aug 2013 21:26:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/22/model-to-answer-your-questions-rather-than-modelling-reality/</guid>
      <description>On the recommendation of Ian Robinson I&#39;ve been reading the 2nd edition of William&#39;s Kent&#39;s &#39;Data and Reality&#39; and the author makes an interesting observation at the end of the first chapter which resonated with me:
 Once more: we are not modelling reality, but the way information about reality is processed, by people.  It reminds me of similar advice in Eric Evans&#39; Domain Driven Design and it&#39;s advice which I believe is helpful when designing a model in a graph database.</description>
    </item>
    
    <item>
      <title>Coding: Hack then revert</title>
      <link>http://mneedham.github.io/2013/08/19/coding-hack-then-revert/</link>
      <pubDate>Mon, 19 Aug 2013 23:13:04 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/19/coding-hack-then-revert/</guid>
      <description>For a long while my default approach when I came across a new code base that I wanted to change was to read all the code and try and understand how it all fitted together by sketching out flow of control diagrams.
Only after I&#39;d done that would I start planning how I could make my changes.
This works reasonably well but it&#39;s quite time consuming and a couple of years ago a former colleague (I can&#39;t remember who!</description>
    </item>
    
    <item>
      <title>BT Internet: Non existent hosts mapping to 92.242.132.15</title>
      <link>http://mneedham.github.io/2013/08/17/bt-internet-non-existent-hosts-mapping-to-92-242-132-15/</link>
      <pubDate>Sat, 17 Aug 2013 21:13:27 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/17/bt-internet-non-existent-hosts-mapping-to-92-242-132-15/</guid>
      <description>We have a test in our code which checks for unresolvable hosts and it started failing for me because instead of throwing an UnknownHostException from the following call:
InetAddress.getByName( &amp;quot;host.that.is.invalid&amp;quot; )  I was getting back a valid although unreachable host. When I called ping it was easier to see what was going on:
$ ping host.that.is.invalid PING host.that.is.invalid (92.242.132.15): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2  As you can see, that hostname is resolving to &#39;92.</description>
    </item>
    
    <item>
      <title>Jersey Client: java.net.ProtocolException: Server redirected too many times/Setting cookies on request</title>
      <link>http://mneedham.github.io/2013/08/17/jersey-client-java-net-protocolexception-server-redirected-too-many-timessetting-cookies-on-request/</link>
      <pubDate>Sat, 17 Aug 2013 20:25:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/17/jersey-client-java-net-protocolexception-server-redirected-too-many-timessetting-cookies-on-request/</guid>
      <description>A couple of weeks ago I was trying to write a test around some OAuth code that we have on an internal application and I was using Jersey Client to send the various requests.
I initially started with the following code:
Client = Client.create(); ClientResponse response = client.resource( &amp;quot;http://localhost:59680&amp;quot; ).get( ClientResponse.class );  but when I ran the test I was getting the following exception:
com.sun.jersey.api.client.ClientHandlerException: java.net.ProtocolException: Server redirected too many times (20) at com.</description>
    </item>
    
    <item>
      <title>Python: for/list comprehensions and dictionaries</title>
      <link>http://mneedham.github.io/2013/08/13/python-forlist-comprehensions-and-dictionaries/</link>
      <pubDate>Tue, 13 Aug 2013 22:59:52 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/13/python-forlist-comprehensions-and-dictionaries/</guid>
      <description>I&#39;ve been working through Coursera&#39;s Linear Algebra course and since all of the exercises are in Python I&#39;ve been playing around with it again.
One interesting thing I learnt is that you can construct dictionaries using a list comprehension type syntax.
For example, if we start with the following dictionaries:
&amp;gt;&amp;gt;&amp;gt; x = { &amp;quot;a&amp;quot;: 1, &amp;quot;b&amp;quot;:2 } &amp;gt;&amp;gt;&amp;gt; y = {1: &amp;quot;mark&amp;quot;, 2: &amp;quot;will&amp;quot;} &amp;gt;&amp;gt;&amp;gt; x {&#39;a&#39;: 1, &#39;b&#39;: 2} &amp;gt;&amp;gt;&amp;gt; y {1: &#39;mark&#39;, 2: &#39;will&#39;}  We might want to create a new dictionary which links from the keys in x to the values in y.</description>
    </item>
    
    <item>
      <title>9 algorithms that changed the future - John MacCormick: Book Review</title>
      <link>http://mneedham.github.io/2013/08/13/9-algorithms-that-changed-the-future-john-maccormick-book-review/</link>
      <pubDate>Tue, 13 Aug 2013 20:00:45 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/13/9-algorithms-that-changed-the-future-john-maccormick-book-review/</guid>
      <description>The Book 9 algorithms that changed the future (the ingenious ideas that drive today&#39;s computers) by John MacCormick
My Thoughts I came across this book while idly browsing a book store and since I&#39;ve found most introduction to algorithms books very dry I thought it&#39;d be interesting to see what one aimed at the general public would be like.
Overall it was an enjoyable read and I quite like the pattern that the author used for each algorithm, which was:</description>
    </item>
    
    <item>
      <title>Jersey Client: com.sun.jersey.api.client.UniformInterfaceException</title>
      <link>http://mneedham.github.io/2013/08/11/jersey-client-com-sun-jersey-api-client-uniforminterfaceexception/</link>
      <pubDate>Sun, 11 Aug 2013 08:07:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/11/jersey-client-com-sun-jersey-api-client-uniforminterfaceexception/</guid>
      <description>As I mentioned in a post a couple of weeks ago we&#39;ve been doing some which involved calling the neo4j server&#39;s HA URI to determine whether a machine was slave or master.
We started off with the following code using jersey-client:
public class HaSpike { public static void main(String[] args) { String response = client() .resource(&amp;quot;http://localhost:7474/db/manage/server/ha/slave&amp;quot;) .accept(MediaType.TEXT_PLAIN) .get(String.class); System.out.println(&amp;quot;response = &amp;quot; + response); } private static Client client() { DefaultClientConfig defaultClientConfig = new DefaultClientConfig(); defaultClientConfig.</description>
    </item>
    
    <item>
      <title>neo4j: Extracting a subgraph as an adjacency matrix and calculating eigenvector centrality with JBLAS</title>
      <link>http://mneedham.github.io/2013/08/11/neo4j-extracting-a-subgraph-as-an-adjacency-matrix-and-calculating-eigenvector-centrality-with-jblas/</link>
      <pubDate>Sun, 11 Aug 2013 07:23:31 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/11/neo4j-extracting-a-subgraph-as-an-adjacency-matrix-and-calculating-eigenvector-centrality-with-jblas/</guid>
      <description>Earlier in the week I wrote a blog post showing how to calculate the eigenvector centrality of an adjacency matrix using JBLAS and the next step was to work out the eigenvector centrality of a neo4j sub graph.
There were 3 steps involved in doing this:
 Export the neo4j sub graph as an adjacency matrix Run JBLAS over it to get eigenvector centrality scores for each node Write those scores back into neo4j  I decided to make use of the Paul Revere data set from Kieran Healy&#39;s blog post which consists of people and groups that they had membership of.</description>
    </item>
    
    <item>
      <title>Java/JBLAS: Calculating eigenvector centrality of an adjacency matrix</title>
      <link>http://mneedham.github.io/2013/08/05/javajblas-calculating-eigenvector-centrality-of-an-adjacency-matrix/</link>
      <pubDate>Mon, 05 Aug 2013 22:12:37 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/08/05/javajblas-calculating-eigenvector-centrality-of-an-adjacency-matrix/</guid>
      <description>I recently came across a very interesting post by Kieran Healy where he runs through a bunch of graph algorithms to see whether he can detect the most influential people behind the American Revolution based on their membership of various organisations.
The first algorithm he looked at was betweenness centrality which I&#39;ve looked at previously and is used to determine the load and importance of a node in a graph.</description>
    </item>
    
    <item>
      <title>AWS: Attaching an EBS volume on an EC2 instance and making it available for use</title>
      <link>http://mneedham.github.io/2013/07/31/aws-attaching-an-ebs-volume-on-an-ec2-instance-and-making-it-available-for-use/</link>
      <pubDate>Wed, 31 Jul 2013 06:21:42 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/31/aws-attaching-an-ebs-volume-on-an-ec2-instance-and-making-it-available-for-use/</guid>
      <description>I recently wanted to attach an EBS volume to an existing EC2 instance that I had running and since it was for a one off tasks (famous last words) I decided to configure it manually.
I created the EBS volume through the AWS console and one thing that initially caught me out is that the EC2 instance and EBS volume need to be in the same region and zone.
Therefore if I create my EC2 instance in &#39;eu-west-1b&#39; then I need to create my EBS volume in &#39;eu-west-1b&#39; as well otherwise I won&#39;t be able to attach it to that instance.</description>
    </item>
    
    <item>
      <title>Getting started with screen</title>
      <link>http://mneedham.github.io/2013/07/31/getting-started-with-screen/</link>
      <pubDate>Wed, 31 Jul 2013 05:41:12 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/31/getting-started-with-screen/</guid>
      <description>Last week I had a ~10GB file I wanted to download to my machine but Chrome&#39;s initial estimate was that it would take 10+ hours to do so which meant I&#39;d have probably shutdown my machine before it had completed.
It seemed to make more sense to spin up an EC2 instance and download it onto there instead but I didn&#39;t want to have to keep an SSH session open to that machine either.</description>
    </item>
    
    <item>
      <title>s3cmd: put fails with “Connection reset by peer” for large files</title>
      <link>http://mneedham.github.io/2013/07/30/s3cmd-put-fails-with-connection-reset-by-peer-for-large-files/</link>
      <pubDate>Tue, 30 Jul 2013 16:20:16 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/30/s3cmd-put-fails-with-connection-reset-by-peer-for-large-files/</guid>
      <description>I recently wanted to copy some large files from an AWS instance into an S3 bucket using s3cmd but ended up with the following error when trying to use the &#39;put&#39; command:
$ s3cmd put /mnt/ebs/myfile.tar s3://mybucket.somewhere.com /mnt/ebs/myfile.tar -&amp;gt; s3://mybucket.somewhere.com/myfile.tar [1 of 1] 1077248 of 12185313280 0% in 1s 937.09 kB/s failed WARNING: Upload failed: /myfile.tar ([Errno 104] Connection reset by peer) WARNING: Retrying on lower speed (throttle=0.00) WARNING: Waiting 3 sec.</description>
    </item>
    
    <item>
      <title>netcat: Strange behaviour with UDP - only receives first packet sent</title>
      <link>http://mneedham.github.io/2013/07/30/netcat-strange-behaviour-with-udp-only-receives-first-packet-sent/</link>
      <pubDate>Tue, 30 Jul 2013 06:01:47 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/30/netcat-strange-behaviour-with-udp-only-receives-first-packet-sent/</guid>
      <description>I was playing around with netcat yesterday to create a client and server which would communicate via UDP packets and I rediscovered some &#34;weird&#34; behaviour which I&#39;d previously encountered but not explained.
I started up a netcat server listening for UDP packets on port 9000 of my machine:
$ nc -kluv localhost 9000  We can check with lsof what running that command has done:
$ lsof -Pni :9000 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nc 63289 markhneedham 5u IPv6 0xc99222a54b3975b5 0t0 UDP [::1]:9000  We can see that the netcat process is listening on port 9000 so let&#39;s send it a UDP packet, using another netcat process:</description>
    </item>
    
    <item>
      <title>Jersey Client: Testing external calls</title>
      <link>http://mneedham.github.io/2013/07/28/jersey-client-testing-external-calls/</link>
      <pubDate>Sun, 28 Jul 2013 20:43:24 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/28/jersey-client-testing-external-calls/</guid>
      <description>Jim and I have been doing a bit of work over the last week which involved calling neo4j&#39;s HA status URI to check whether or not an instance was a master/slave and we&#39;ve been using jersey-client.
The code looked roughly like this:
class Neo4jInstance { private Client httpClient; private URI hostname; public Neo4jInstance(Client httpClient, URI hostname) { this.httpClient = httpClient; this.hostname = hostname; } public Boolean isSlave() { String slaveURI = hostname.</description>
    </item>
    
    <item>
      <title>Product Documentation: The receiver decides if it&#39;s successful</title>
      <link>http://mneedham.github.io/2013/07/28/product-documentation-the-receiver-decides-if-its-successful/</link>
      <pubDate>Sun, 28 Jul 2013 16:18:38 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/28/product-documentation-the-receiver-decides-if-its-successful/</guid>
      <description>One of the things I remember being taught while growing up is that in an interaction where somebody is something to someone else it&#39;s their responsibility to do so in a way that the receiver can understand.
Even if you think you&#39;ve done a good job of explaining something, the receiver of the communication decides whether or not that&#39;s the case.
I&#39;d always assumed that this advice made most sense in the context of a one to one conversation but recently I&#39;ve realised that it also makes sense when thinking about product documentation.</description>
    </item>
    
    <item>
      <title>Graph Processing: Betweeness Centrality - neo4j&#39;s cypher vs graphstream</title>
      <link>http://mneedham.github.io/2013/07/27/graph-processing-betweeness-centrality-neo4js-cypher-vs-graphstream/</link>
      <pubDate>Sat, 27 Jul 2013 11:21:52 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/27/graph-processing-betweeness-centrality-neo4js-cypher-vs-graphstream/</guid>
      <description>Last week I wrote about the betweenness centrality algorithm and my attempts to understand it using graphstream and while reading the source I realised that I might be able to put something together using neo4j&#39;s all shortest paths algorithm.
To recap, the betweenness centrality algorithm is used to determine the load and importance of a node in a graph.
While talking about this with Jen she pointed out that calculating the betweenness centrality of nodes across the whole graph often doesn&#39;t make sense.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Getting the hang of query parameters</title>
      <link>http://mneedham.github.io/2013/07/27/neo4jcypher-getting-the-hang-of-query-parameters/</link>
      <pubDate>Sat, 27 Jul 2013 09:30:26 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/27/neo4jcypher-getting-the-hang-of-query-parameters/</guid>
      <description>For as long as I&#39;ve been using neo4j&#39;s cypher query language Michael has been telling me to use parameters in my queries but the performance of the queries was always acceptable so I didn&#39;t feel the need.
However, recently I was playing around with a data set and I created ~500 nodes using code similar to this:
require &#39;open-uri&#39; open(&amp;quot;data/people.cyp&amp;quot;, &#39;w&#39;) { |f| (1..500).each do |value| f.puts(&amp;quot;CREATE (p:Person{name: \&amp;quot;#{value}\&amp;quot;})&amp;quot;) end }  That creates a file of cypher statements that look like this:</description>
    </item>
    
    <item>
      <title>On &#34;The fear of blogging about technical topics&#34;</title>
      <link>http://mneedham.github.io/2013/07/22/on-the-fear-of-blogging-about-technical-topics/</link>
      <pubDate>Mon, 22 Jul 2013 23:47:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/22/on-the-fear-of-blogging-about-technical-topics/</guid>
      <description>My former colleague Anne Simmons recently wrote an interesting post in which she describes some of the reasons that she finds herself not wanting to write about technical topics..
I wrote a post at the end of 2012 in which I explained some of the reasons why I think writing about what you learn is a good idea but Anne brought up some things I hadn&#39;t thought of which I think are worth addressing.</description>
    </item>
    
    <item>
      <title>Lessons from supporting production code</title>
      <link>http://mneedham.github.io/2013/07/22/lessons-from-supporting-production-code/</link>
      <pubDate>Mon, 22 Jul 2013 22:37:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/22/lessons-from-supporting-production-code/</guid>
      <description>Until I started working on the uSwitch energy website around 8 months ago I had not really done any support of a production system so I learnt some interesting lessons in my time there.
Look at the new code first We had our application wired up to Airbrake so whenever a user did anything which resulted in an exception being thrown we received a report with the stack trace, environment variables and which page they were on.</description>
    </item>
    
    <item>
      <title>Jersey: Listing all resources, paths, verbs to build an entry point/index for an API</title>
      <link>http://mneedham.github.io/2013/07/21/jersey-listing-all-resources-paths-verbs-to-build-an-entry-pointindex-for-an-api/</link>
      <pubDate>Sun, 21 Jul 2013 11:07:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/21/jersey-listing-all-resources-paths-verbs-to-build-an-entry-pointindex-for-an-api/</guid>
      <description>I&#39;ve been playing around with Jersey over the past couple of days and one thing I wanted to do was create an entry point or index which listed all my resources, the available paths and the verbs they accepted.
Guido Simone explained a neat way of finding the paths and verbs for a specific resource using Jersey&#39;s IntrospectionModeller:
AbstractResource resource = IntrospectionModeller.createResource(JacksonResource.class); System.out.println(&amp;quot;Path is &amp;quot; + resource.getPath().getValue()); String uriPrefix = resource.</description>
    </item>
    
    <item>
      <title>Jersey Server: com.sun.jersey.api.MessageException: A message body writer for Java class org.codehaus.jackson.node.ObjectNode and MIME media type application/json was not found</title>
      <link>http://mneedham.github.io/2013/07/21/jersey-server-com-sun-jersey-api-messageexception-a-message-body-writer-for-java-class-org-codehaus-jackson-node-objectnode-and-mime-media-type-applicationjson-was-not-found/</link>
      <pubDate>Sun, 21 Jul 2013 10:37:45 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/21/jersey-server-com-sun-jersey-api-messageexception-a-message-body-writer-for-java-class-org-codehaus-jackson-node-objectnode-and-mime-media-type-applicationjson-was-not-found/</guid>
      <description>I&#39;ve been reacquainted with my good friend Jersey over the last couple of days and in getting up and running was reminded that things which seemed easy at the time aren&#39;t as easy when starting from scratch.
I eventually settled on using Sunny Gleason&#39;s j4-minimal repository which wires up Jersey with Jackson, Guice and Jetty which seemed like a good place to start.
I prefer building up JSON objects explicitly rather than setting up automatic mapping so the first thing I did was change the JacksonResource to have an end point that returned a manually built up JSON object:</description>
    </item>
    
    <item>
      <title>Graph Processing: Calculating betweenness centrality for an undirected graph using graphstream</title>
      <link>http://mneedham.github.io/2013/07/19/graph-processing-calculating-betweenness-centrality-for-an-undirected-graph-using-graphstream/</link>
      <pubDate>Fri, 19 Jul 2013 00:37:41 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/19/graph-processing-calculating-betweenness-centrality-for-an-undirected-graph-using-graphstream/</guid>
      <description>Since I now spend most of my time surrounded by graphs I thought it&#39;d be interesting to learn a bit more about graph processing, a topic my colleague Jim wrote about a couple of years ago.
I like to think of the types of queries you&#39;d do with a graph processing engine as being similar in style graph global queries where you take most of the nodes in a graph into account and do some sort of calculation.</description>
    </item>
    
    <item>
      <title>Git: Commit squashing made even easier using &#39;git branch --set-upstream&#39;</title>
      <link>http://mneedham.github.io/2013/07/16/git-commit-squashing-made-even-easier-using-git-branch-set-upstream/</link>
      <pubDate>Tue, 16 Jul 2013 08:13:02 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/16/git-commit-squashing-made-even-easier-using-git-branch-set-upstream/</guid>
      <description>A few days ago I wrote a blog post describing how I wanted to squash a series of commits into one bigger one before making a pull request and in the comments Rob Hunter showed me an even easier way to do so.
To recap, by the end of the post I had the following git config:
$ cat .git/config [remote &amp;quot;origin&amp;quot;] fetch = +refs/heads/*:refs/remotes/origin/* url = git@github.com:mneedham/neo4j-shell-tools.git [branch &amp;quot;master&amp;quot;] remote = origin merge = refs/heads/master [remote &amp;quot;base&amp;quot;] url = git@github.</description>
    </item>
    
    <item>
      <title>Java: Testing a socket is listening on all network interfaces/wildcard interface</title>
      <link>http://mneedham.github.io/2013/07/14/java-testing-a-socket-is-listening-on-all-network-interfaceswildcard-interface/</link>
      <pubDate>Sun, 14 Jul 2013 14:31:44 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/14/java-testing-a-socket-is-listening-on-all-network-interfaceswildcard-interface/</guid>
      <description>I previously wrote a blog post describing how I&#39;ve been trying to learn more about network sockets in which I created some server sockets and connected to them using netcat.
The next step was to do the same thing in Java and I started out by writing a server socket which echoed any messages sent by the client:
public class EchoServer { public static void main(String[] args) throws IOException { int port = 4444; ServerSocket serverSocket = new ServerSocket(port, 50, InetAddress.</description>
    </item>
    
    <item>
      <title>Learning more about network sockets</title>
      <link>http://mneedham.github.io/2013/07/14/learning-more-about-network-sockets/</link>
      <pubDate>Sun, 14 Jul 2013 09:52:17 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/14/learning-more-about-network-sockets/</guid>
      <description>While reading through some of the neo4j code a few weeks ago I realised that I didn&#39;t have a very good understanding about the mechanics behind network ports/sockets so I thought I&#39;d try to learn more.
In particular I&#39;d not considered what binding a socket to different network interfaces meant so I decided to setup a few examples using netcat to help me understand better.
To start with let&#39;s list the network interfaces that I have on my machine using ifconfig:</description>
    </item>
    
    <item>
      <title>Git/GitHub: Squashing all commits before sending a pull request</title>
      <link>http://mneedham.github.io/2013/07/13/gitgithub-squashing-all-commits-before-sending-a-pull-request/</link>
      <pubDate>Sat, 13 Jul 2013 18:47:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/13/gitgithub-squashing-all-commits-before-sending-a-pull-request/</guid>
      <description>My colleague Michael has been doing some work to make it easier for people to import data into neo4j and his latest attempt is neo4j-shell-tools which adds some additional commands to the neo4j-shell.
I&#39;ve spent a bit of time refactoring the readme which I&#39;d done on a branch of my fork of the repository and consisted of 46 commits, most changing 2 or 3 lines.
I wanted to send Michael a pull request on Github but first I needed to squash all my commits down into a single one.</description>
    </item>
    
    <item>
      <title>neo4j Unmanaged Extension: Creating gzipped streamed responses with Jetty</title>
      <link>http://mneedham.github.io/2013/07/08/neo4j-unmanaged-extension-creating-gzipped-streamed-responses-with-jetty/</link>
      <pubDate>Mon, 08 Jul 2013 23:48:23 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/08/neo4j-unmanaged-extension-creating-gzipped-streamed-responses-with-jetty/</guid>
      <description>I recently wrote a blog post describing how we created a streamed response and the next thing we wanted to do was gzip the response to shrink it&#39;s size a bit.
A bit of searching led to GZIPContentEncodingFilter popping up a lot of times but this is actually needed for a client processing a gripped response rather than helping us to gzip a response from the server.
I noticed that there was a question about this on the mailing list from about a year ago although Michael pointed out that the repository has now moved and the example is available here instead.</description>
    </item>
    
    <item>
      <title>JAX RS: Streaming a Response using StreamingOutput</title>
      <link>http://mneedham.github.io/2013/07/08/jax-rs-streaming-a-response-using-streamingoutput/</link>
      <pubDate>Mon, 08 Jul 2013 23:19:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/08/jax-rs-streaming-a-response-using-streamingoutput/</guid>
      <description>A couple of weeks ago Jim and I were building out a neo4j unmanaged extension from which we wanted to return the results of a traversal which had a lot of paths.
Our code initially looked a bit like this:
package com.markandjim @Path(&amp;quot;/subgraph&amp;quot;) public class ExtractSubGraphResource { private final GraphDatabaseService database; public ExtractSubGraphResource(@Context GraphDatabaseService database) { this.database = database; } @GET @Produces(MediaType.TEXT_PLAIN) @Path(&amp;quot;/{nodeId}/{depth}&amp;quot;) public Response hello(@PathParam(&amp;quot;nodeId&amp;quot;) long nodeId, @PathParam(&amp;quot;depth&amp;quot;) int depth) { Node node = database.</description>
    </item>
    
    <item>
      <title>Survivorship Bias and Product Development</title>
      <link>http://mneedham.github.io/2013/07/08/survivorship-bias-and-product-development/</link>
      <pubDate>Mon, 08 Jul 2013 22:14:38 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/07/08/survivorship-bias-and-product-development/</guid>
      <description>A couple of months ago I came across an interesting article by the author of &#39;You Are Not So Smart&#39; about a fallacy known as &#39;Survivorship Bias&#39; which Wikipedia defines as: The logical error of concentrating on the people or things that &#34;survived&#34; some process and inadvertently overlooking those that didn&#39;t because of their lack of visibility. I particularly liked the story describing how Abraham Wald helped the US military overcome an instance of this error when trying to work out where to place armour on their bomber planes:</description>
    </item>
    
    <item>
      <title>Ruby: Calculating the orthodromic distance using the Haversine formula</title>
      <link>http://mneedham.github.io/2013/06/30/ruby-calculating-the-orthodromic-distance-using-the-haversine-formula/</link>
      <pubDate>Sun, 30 Jun 2013 22:53:14 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/30/ruby-calculating-the-orthodromic-distance-using-the-haversine-formula/</guid>
      <description>As part of the UI I&#39;m building around my football stadiums data set I wanted to calculate the distance from a football stadium to a point on the map in Ruby since cypher doesn&#39;t currently return this value.
I had the following cypher query to return the football stadiums near Westminster along with their lat/long values:
lat, long, distance = [&amp;quot;51.55786291569685&amp;quot;, &amp;quot;0.144195556640625&amp;quot;, 10] query = &amp;quot; START node = node:geom(&#39;withinDistance:[#{lat}, #{long}, #{distance}]&#39;)&amp;quot; query &amp;lt;&amp;lt; &amp;quot; RETURN node.</description>
    </item>
    
    <item>
      <title>Leaflet JS: Resizing a map to keep a circle diameter inside it</title>
      <link>http://mneedham.github.io/2013/06/30/leaflet-js-resizing-a-map-to-keep-a-circle-diameter-inside-it/</link>
      <pubDate>Sun, 30 Jun 2013 22:23:50 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/30/leaflet-js-resizing-a-map-to-keep-a-circle-diameter-inside-it/</guid>
      <description>I&#39;ve been working on creating a UI to make searching for the football stadiums that I wrote about last week a bit easier and I thought I&#39;d give Leaflet JS a try.
Leaflet is a Javascript library which was recommended to me by Jason Neylon) and can be used as a wrapper around Open Street Map.
I started by creating a simple form where you could fill in a lat/long and distance and it would centre the map on that lat/long and show you a list of the stadiums within that diameter next to the map.</description>
    </item>
    
    <item>
      <title>Vagrant: Multi (virtual) machine with Puppet roles</title>
      <link>http://mneedham.github.io/2013/06/30/vagrant-multi-virtual-machine-with-puppet-roles/</link>
      <pubDate>Sun, 30 Jun 2013 13:13:14 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/30/vagrant-multi-virtual-machine-with-puppet-roles/</guid>
      <description>I&#39;ve been playing around with setting up a neo4j cluster using Vagrant and HAProxy and one thing I wanted to do was define two different roles for the HAProxy and neo4j machines.
When I was working at uSwitch Nathan had solved a similar problem, but with AWS VMs, by defining the role in an environment variable in the VM&#39;s spin up script.
In retrospect I think I might have been able to do that by using the shell provisioner and calling that before the puppet provisioner but Nathan, Gareth Rushgrove and Gregor Russbuelt suggested that using facter might be better.</description>
    </item>
    
    <item>
      <title>Vagrant 1.2.2: `[]&#39;: can&#39;t convert Symbol into Integer (TypeError)/The following settings don&#39;t exist</title>
      <link>http://mneedham.github.io/2013/06/29/vagrant-1-2-2-cant-convert-symbol-into-integer-typeerrorthe-following-settings-dont-exist/</link>
      <pubDate>Sat, 29 Jun 2013 08:44:00 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/29/vagrant-1-2-2-cant-convert-symbol-into-integer-typeerrorthe-following-settings-dont-exist/</guid>
      <description>As I mentioned in my previous post I&#39;ve been playing around with Vagrant for the past couple of days and I was trying to adapt a Vagrantfile that Nathan created a few months ago to do what I wanted.
I&#39;m using Vagrant 1.2.2 and I started out with the following Vagrantfile:
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.define :neo01 do |neo| neo.vm.network :hostonly, &amp;quot;192.168.33.101&amp;quot; neo.vm.forward_port 8080, 4569 end end  Unfortunately a &#39;vagrant up&#39; doesn&#39;t quite work as expected:</description>
    </item>
    
    <item>
      <title>Vagrant/Virtual Box: There was an error executing the following command with VBoxManage - Progress object failure: NS_ERROR_CALL_FAILED</title>
      <link>http://mneedham.github.io/2013/06/29/vagrantvirtual-box-there-was-an-error-executing-the-following-command-with-vboxmanage-progress-object-failure-ns_error_call_failed/</link>
      <pubDate>Sat, 29 Jun 2013 07:38:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/29/vagrantvirtual-box-there-was-an-error-executing-the-following-command-with-vboxmanage-progress-object-failure-ns_error_call_failed/</guid>
      <description>I&#39;ve been playing around with Vagrant a bit again lately and having installed it on a new machine was running into the following exception when I tried to run &#39;vagrant up&#39; on a new virtual machine:
ERROR vagrant: /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/plugins/providers/virtualbox/driver/base.rb:292:in `block in execute&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/util/retryable.rb:17:in `retryable&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/plugins/providers/virtualbox/driver/base.rb:282:in `execute&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/plugins/providers/virtualbox/driver/version_4_2.rb:165:in `import&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/plugins/providers/virtualbox/action/import.rb:15:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/warden.rb:34:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/builtin/handle_box_url.rb:38:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/warden.rb:34:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/plugins/providers/virtualbox/action/check_accessible.rb:18:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/warden.rb:34:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/runner.rb:61:in `block in run&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/util/busy.rb:19:in `busy&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/runner.rb:61:in `run&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.1.2/lib/vagrant/action/builtin/call.rb:51:in `call&#39; /Applications/Vagrant/embedded/gems/gems/vagrant-1.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Aggregating relationships within a path</title>
      <link>http://mneedham.github.io/2013/06/27/neo4jcypher-aggregating-relationships-within-a-path/</link>
      <pubDate>Thu, 27 Jun 2013 10:32:18 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/27/neo4jcypher-aggregating-relationships-within-a-path/</guid>
      <description>I recently came across an interesting use case of paths in a graph where we wanted to calculate the frequency of communication between two people by showing how frequently each emailed the other.
The model looked like this:
 which we can create with the following cypher statements:
CREATE (email1 { name: &#39;Email 1&#39;, title: &#39;Some stuff&#39; }) CREATE (email2 { name: &#39;Email 2&#39;, title: &amp;quot;Absolutely irrelevant&amp;quot; }) CREATE (email3 { name: &#39;Email 3&#39;, title: &amp;quot;Something else&amp;quot; }) CREATE (person1 { name: &#39;Mark&#39; }) CREATE (person2 { name: &#39;Jim&#39; }) CREATE (person3 { name: &#39;Alistair&#39; }) CREATE (person1)-[:SENT]-&amp;gt;(email1) CREATE (person2)-[:RECEIVED]-&amp;gt;(email1) CREATE (person3)-[:RECEIVED]-&amp;gt;(email1) CREATE (person1)-[:SENT]-&amp;gt;(email2) CREATE (person2)-[:RECEIVED]-&amp;gt;(email2) CREATE (person2)-[:SENT]-&amp;gt;(email3) CREATE (person1)-[:RECEIVED]-&amp;gt;(email3)  We want to return a list containing pairs of people and how many times they emailed each other, so in this case we want to return a table showing the following:</description>
    </item>
    
    <item>
      <title>Unix/awk: Extracting substring using a regular expression with capture groups</title>
      <link>http://mneedham.github.io/2013/06/26/unixawk-extracting-substring-using-a-regular-expression-with-capture-groups/</link>
      <pubDate>Wed, 26 Jun 2013 15:23:14 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/26/unixawk-extracting-substring-using-a-regular-expression-with-capture-groups/</guid>
      <description>A couple of years ago I wrote a blog post explaining how I&#39;d used GNU awk to extract story numbers from git commit messages and I wanted to do a similar thing today to extract some node ids from a file.
My eventual solution looked like this:
$ echo &amp;quot;mark #1000&amp;quot; | gawk &#39;{ match($0, /#([0-9]+)/, arr); if(arr[1] != &amp;quot;&amp;quot;) print arr[1] }&#39; 1000  But in the comments an alternative approach was suggested which used the Mac version of awk and the RSTART and RLENGTH global variables which get set when a match is found:</description>
    </item>
    
    <item>
      <title>neo4j Spatial: Indexing football stadiums using the REST API</title>
      <link>http://mneedham.github.io/2013/06/24/neo4j-spatial-indexing-football-stadiums-using-the-rest-api/</link>
      <pubDate>Mon, 24 Jun 2013 07:17:15 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/24/neo4j-spatial-indexing-football-stadiums-using-the-rest-api/</guid>
      <description>Late last week my colleague Peter wrote up some documentation about creating spatial indexes in neo4j via HTTP, something I hadn&#39;t realised was possible until then.
I previously wrote about indexing football stadiums using neo4j spatial but the annoying thing about the approach I described was that I was using neo4j in embedded mode which restricts you to using a JVM language.
The rest of my code is in Ruby so I thought I&#39;d translate that code.</description>
    </item>
    
    <item>
      <title>neo4j: A simple example using the JDBC driver</title>
      <link>http://mneedham.github.io/2013/06/20/neo4j-a-simple-example-using-the-jdbc-driver/</link>
      <pubDate>Thu, 20 Jun 2013 07:21:46 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/20/neo4j-a-simple-example-using-the-jdbc-driver/</guid>
      <description>Michael recently pointed me to the neo4j JDBC driver which he and Rickard have written so I thought I&#39;d try and port the code from my previous post to use that instead of the console.
To start with I added the following dependencies to my POM file:
&amp;lt;dependencies&amp;gt; ... &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.neo4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;neo4j-jdbc&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.9&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; &amp;lt;repositories&amp;gt; &amp;lt;repository&amp;gt; &amp;lt;id&amp;gt;neo4j-maven&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;neo4j maven&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;http://m2.neo4j.org&amp;lt;/url&amp;gt; &amp;lt;/repository&amp;gt; &amp;lt;/repositories&amp;gt;  I then tried to create a connection to a local neo4j server instance that I had running on port 7474:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: CREATE with optional properties</title>
      <link>http://mneedham.github.io/2013/06/20/neo4jcypher-create-with-optional-properties/</link>
      <pubDate>Thu, 20 Jun 2013 06:31:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/20/neo4jcypher-create-with-optional-properties/</guid>
      <description>I&#39;ve written before about using the cypher CREATE statement to add inferred information to a neo4j graph and sometimes we want to do that but have to deal with optional properties while creating our new relationships.
For example let&#39;s say we have the following people in our graph with the &#39;started&#39; and &#39;left&#39; properties representing their tenure at a company:
CREATE (person1 { personId: 1, started: 1361708546 }) CREATE (person2 { personId: 2, started: 1361708546, left: 1371708646 }) CREATE (company { companyId: 1 })  We want to create a &#39;TENURE&#39; link from them to the company including the &#39;started&#39; and &#39;left&#39; properties when applicable and might start with the following query:</description>
    </item>
    
    <item>
      <title>neo4j: WrappingNeoServerBootstrapper and the case of the /webadmin 404</title>
      <link>http://mneedham.github.io/2013/06/19/neo4j-wrappingneoserverbootstrapper-and-the-case-of-the-webadmin-404/</link>
      <pubDate>Wed, 19 Jun 2013 05:32:50 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/19/neo4j-wrappingneoserverbootstrapper-and-the-case-of-the-webadmin-404/</guid>
      <description>When people first use neo4j they frequently start out by embedding it in a Java application but eventually they want to explore the graph in a more visual way.
One simple way to do this is to start neo4j in server mode and use the web console.
Our initial code might read like this:
public class GraphMeUp { public static void main(String[] args) { GraphDatabaseService graphDb = new EmbeddedGraphDatabase(&amp;quot;/path/to/data/graph.db&amp;quot;); } }  or:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Finding single hop paths</title>
      <link>http://mneedham.github.io/2013/06/15/neo4jcypher-finding-single-hop-paths/</link>
      <pubDate>Sat, 15 Jun 2013 13:04:53 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/15/neo4jcypher-finding-single-hop-paths/</guid>
      <description>The neo4j docs have a few examples explaining how to to write cypher queries dealing with path ranges but an interesting variation that I came across recently is where we want to find the individual hops in a path.
I thought the managers that Chelsea have had since Roman Abramovich took over would serve as a useful data set to show how this works.
So we create all the managers and a &#39;SUCCEEDED_BY&#39; relationship between them as follows:</description>
    </item>
    
    <item>
      <title>Java: Finding/Setting JDK/$JAVA_HOME on Mac OS X</title>
      <link>http://mneedham.github.io/2013/06/15/java-findingsetting-jdkjava_home-on-mac-os-x/</link>
      <pubDate>Sat, 15 Jun 2013 10:28:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/15/java-findingsetting-jdkjava_home-on-mac-os-x/</guid>
      <description>As long as I&#39;ve been using a Mac I always understood that if you needed to set $JAVA_HOME for any program, it should be set to /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK.
On my machine this points to the 1.6 JDK:
$ ls -alh /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK -&amp;gt; /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents  This was a bit surprising to me since I&#39;ve actually got Java 7 installed on the machine as well so I&#39;d assumed the symlink would have been changed:</description>
    </item>
    
    <item>
      <title>neo4j/cypher/Lucene: Dealing with special characters</title>
      <link>http://mneedham.github.io/2013/06/15/neo4jcypherlucene-dealing-with-special-characters/</link>
      <pubDate>Sat, 15 Jun 2013 09:53:15 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/15/neo4jcypherlucene-dealing-with-special-characters/</guid>
      <description>neo4j uses Lucene to handle indexing of nodes and relationships in the graph but something that can be a bit confusing at first is how to handle special characters in Lucene queries.
For example let&#39;s say we set up a database with the following data:
CREATE ({name: &amp;quot;-one&amp;quot;}) CREATE ({name: &amp;quot;-two&amp;quot;}) CREATE ({name: &amp;quot;-three&amp;quot;}) CREATE ({name: &amp;quot;four&amp;quot;})  And for whatever reason we only wanted to return the nodes that begin with a hyphen.</description>
    </item>
    
    <item>
      <title>git: Having a branch/tag with the same name (error: dst refspec matches more than one.)</title>
      <link>http://mneedham.github.io/2013/06/13/git-having-a-branchtag-with-the-same-name-error-dst-refspec-matches-more-than-one/</link>
      <pubDate>Thu, 13 Jun 2013 22:18:31 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/13/git-having-a-branchtag-with-the-same-name-error-dst-refspec-matches-more-than-one/</guid>
      <description>Andres and I recently found ourselves wanting to delete a remote branch which had the same name as a tag and therefore the normal way of doing that wasn&#39;t worked out as well as we&#39;d hoped.
I created a dummy repository to recreate the state we&#39;d got ourselves into:
$ echo &amp;quot;mark&amp;quot; &amp;gt; README $ git commit -am &amp;quot;readme&amp;quot; $ echo &amp;quot;for the branch&amp;quot; &amp;gt;&amp;gt; README $ git commit -am &amp;quot;for the branch&amp;quot; $ git checkout -b same Switched to a new branch &#39;same&#39; $ git push origin same Counting objects: 5, done.</description>
    </item>
    
    <item>
      <title>Unix: find, xargs, zipinfo and the &#39;caution: filename not matched:&#39; error</title>
      <link>http://mneedham.github.io/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</link>
      <pubDate>Sun, 09 Jun 2013 23:10:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</guid>
      <description>As I mentioned in my previous post last week I needed to scan all the jar files included with the neo4j-enterprise gem and I started out by finding out where it&#39;s located on my machine:
$ bundle show neo4j-enterprise /Users/markhneedham/.rbenv/versions/jruby-1.7.1/lib/ruby/gems/shared/gems/neo4j-enterprise-1.8.2-java  I then thought I could get a list of all the jar files using find and pipe it into zipinfo via xargs to get all the file names and then search for HighlyAvailableGraphDatabaseFactory:</description>
    </item>
    
    <item>
      <title>neo4j.rb HA: NameError: cannot load Java class org.neo4j.graphdb.factory.HighlyAvailableGraphDatabaseFactory</title>
      <link>http://mneedham.github.io/2013/06/09/neo4j-rb-ha-nameerror-cannot-load-java-class-org-neo4j-graphdb-factory-highlyavailablegraphdatabasefactory/</link>
      <pubDate>Sun, 09 Jun 2013 16:57:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/09/neo4j-rb-ha-nameerror-cannot-load-java-class-org-neo4j-graphdb-factory-highlyavailablegraphdatabasefactory/</guid>
      <description>neo4.rb is a JRuby gem that allows you to create an embedded neo4j database and last week I was working out how to setup a neo4j 1.8.2 HA cluster using the gem.
There is an example showing how to create a HA cluster using neo4j.rb so I thought I could adapt that to do what I wanted.
I had the following Gemfile:
source &#39;http://rubygems.org&#39; gem &#39;neo4j&#39;, &#39;2.2.4&#39; gem &#39;neo4j-community&#39;, &#39;1.8.2&#39; gem &#39;neo4j-advanced&#39;, &#39;1.</description>
    </item>
    
    <item>
      <title>neo4j/cypher 2.0: The CASE statement</title>
      <link>http://mneedham.github.io/2013/06/09/neo4jcypher-2-0-the-case-statement/</link>
      <pubDate>Sun, 09 Jun 2013 14:02:27 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/09/neo4jcypher-2-0-the-case-statement/</guid>
      <description>I&#39;ve been playing around with how you might model Premier League managers tenures at different clubs in neo4j and eventually decided on the following model:
 The date modelling is based on an approach I first came across in a shutl presentation and is described in more detail in the docs. I created a dummy data set with some made up appointments and dismissals and then tried to write a query to show me who was the manager for a team on a specific date.</description>
    </item>
    
    <item>
      <title>The Affect Heuristic</title>
      <link>http://mneedham.github.io/2013/06/06/the-affect-heuristic/</link>
      <pubDate>Thu, 06 Jun 2013 22:36:04 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/06/the-affect-heuristic/</guid>
      <description>In my continued reading of Daniel Kahneman&#39;s Thinking Fast and Slow I&#39;ve reached the section which talks about the affect heuristic which seems particularly applicable to the technical decisions that we make.
The dominance of conclusions over arguments is most pronounced where emotions are involved. The psychologist Paul Slovic has proposed an affect heuristic in which people let their likes and dislikes determine their beliefs about the world. The way I&#39;ve seen this heuristic coming into play in the software world is when we do an &#39;objective&#39; overview of the technical tools/options that we could use to solve a particular problem.</description>
    </item>
    
    <item>
      <title>Ego Depletion</title>
      <link>http://mneedham.github.io/2013/06/04/ego-depletion/</link>
      <pubDate>Tue, 04 Jun 2013 23:16:29 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/06/04/ego-depletion/</guid>
      <description>On the recommendation of Mike Jones I&#39;ve been reading through Daniel Kahneman&#39;s Thinking Fast and Slow in which the first part of the book covers our two styles of thinking:
 System 1 - operates automatically and quickly, with little or no effort and no sense of voluntary control. System 2 - allocates attention to the effortful mental activities that demand it, including complex computations. The operations of System 2 are often associated with the subjective experience of agency, choice, and concentration.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: 400 response - Paths can&#39;t be created inside of foreach</title>
      <link>http://mneedham.github.io/2013/05/31/neo4jcypher-400-response-paths-cant-be-created-inside-of-foreach/</link>
      <pubDate>Fri, 31 May 2013 00:37:57 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/31/neo4jcypher-400-response-paths-cant-be-created-inside-of-foreach/</guid>
      <description>In the neo4j 1.9 milestone releases if we wanted to create multiple relationships from a node we could use the following cypher syntax:
require &#39;neography&#39; neo = Neography::Rest.new neo.execute_query(&amp;quot;create (me {name: &#39;Mark&#39;})&amp;quot;) query = &amp;quot; START n=node:node_auto_index(name={name})&amp;quot; query &amp;lt;&amp;lt; &amp;quot; FOREACH (friend in {friends} : CREATE f=friend, n-[:FRIEND]-&amp;gt;f)&amp;quot; neo.execute_query(query, {&amp;quot;name&amp;quot; =&amp;gt; &amp;quot;Mark&amp;quot;, &amp;quot;friends&amp;quot; =&amp;gt; [{ &amp;quot;name&amp;quot; =&amp;gt; &amp;quot;Will&amp;quot;}, {&amp;quot;name&amp;quot; =&amp;gt; &amp;quot;Paul&amp;quot;}]})  To check that the &#39;FRIEND&#39; relationships have been created we&#39;d write the following query:</description>
    </item>
    
    <item>
      <title>Viewing the contents of an archive</title>
      <link>http://mneedham.github.io/2013/05/29/viewing-the-contents-of-an-archive/</link>
      <pubDate>Wed, 29 May 2013 11:22:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/29/viewing-the-contents-of-an-archive/</guid>
      <description>Everyone now and then I want to check the contents of an archive without unpacking it and I tend to use unzip to do so:
$ unzip -l batch-import-jar-with-dependencies.jar | tail -n 10 1645 02-17-13 01:03 org/neo4j/batchimport/StdOutReport.class 3089 02-17-13 01:03 org/neo4j/batchimport/structs/NodeStruct.class 1244 02-17-13 01:03 org/neo4j/batchimport/structs/Property.class 1732 02-17-13 01:03 org/neo4j/batchimport/structs/PropertyHolder.class 1635 02-17-13 01:03 org/neo4j/batchimport/structs/Relationship.class 905 02-17-13 01:03 org/neo4j/batchimport/utils/Chunker.class 1884 02-17-13 01:03 org/neo4j/batchimport/utils/Params.class 4445 02-17-13 01:03 org/neo4j/batchimport/Utils.class -------- ------- 49947859 16447 files  It does the job although it does print out some information that we&#39;re not really interested in so I was intrigued to see that Alistair used zipinfo when he wanted to achieve a similar thing:</description>
    </item>
    
    <item>
      <title>Pomodoros: Just start the timer</title>
      <link>http://mneedham.github.io/2013/05/27/pomodoros-just-start-the-timer/</link>
      <pubDate>Mon, 27 May 2013 13:23:36 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/27/pomodoros-just-start-the-timer/</guid>
      <description>I wrote earlier in the year about my use of pomodoros to track what I&#39;m doing outside of work and having done this for 6 months I noticed that I&#39;m now procrastinating over picking something off the list to work on.
(I know…I am awesome!)
I&#39;m not sure whether this is because I don&#39;t have anything really appealing on the list of things to work on or whether having so many things listed (I have 8-10 items) is causing the paralysis.</description>
    </item>
    
    <item>
      <title>A/B Testing: Being pragmatic with statistical significance</title>
      <link>http://mneedham.github.io/2013/05/27/ab-testing-pragmatica-statistical-significance/</link>
      <pubDate>Mon, 27 May 2013 13:13:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/27/ab-testing-pragmatica-statistical-significance/</guid>
      <description>One of the first things that we did before starting any of the A/B tests that I&#39;ve previously written about was to work out how many users we needed to go through before we could be sure that the results we saw were statistically significant.
We used the prop.test function from R to do this and based on our traffic at the time worked out that we&#39;d need to run a test for 6 weeks to achieve statistical significance.</description>
    </item>
    
    <item>
      <title>Polyglot Persistence: Embrace the ETL</title>
      <link>http://mneedham.github.io/2013/05/27/polyglot-persistence-embrace-the-etl/</link>
      <pubDate>Mon, 27 May 2013 00:11:23 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/27/polyglot-persistence-embrace-the-etl/</guid>
      <description>Over the past few years I&#39;ve seen the emergence of polyglot persistence i.e. using different data storage technologies for different data and in most situations we work that out up front.
 For example we might use MongoDB to store data about a customer journey through our website but we might simultaneously write page view data through to something like Hadoop or Redshift:
This works reasonably well but sometimes it might not be immediately obvious how we want to query our data when we first start collecting it and our storage choice might not be the best for writing these queries.</description>
    </item>
    
    <item>
      <title>Polyglot Persistence: The &#39;boring&#39; relational option</title>
      <link>http://mneedham.github.io/2013/05/26/polyglot-persistence-the-boring-relational-option/</link>
      <pubDate>Sun, 26 May 2013 23:29:12 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/26/polyglot-persistence-the-boring-relational-option/</guid>
      <description>I was chatting with Brian Blignaut last week after the Equal Experts NoSQL event and he made an interesting observation that in this age of Polyglot Persistence we often rule out the relational database.
I think it&#39;s definitely better that we now have many different options for where we store our data - be it as key/value pairs, documents or as a network/graph.
Having these options forces us to think more about how we&#39;re going to read/write data in our application whereas previously our effort was focused around which tables we were going to pull out.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Properties or relationships? It&#39;s easy to switch</title>
      <link>http://mneedham.github.io/2013/05/25/neo4jcypher-properties-or-relationships-its-easy-to-switch/</link>
      <pubDate>Sat, 25 May 2013 12:21:55 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/25/neo4jcypher-properties-or-relationships-its-easy-to-switch/</guid>
      <description>I&#39;ve written previously about how I&#39;ve converted properties on nodes into relationships and over the past week there was an interesting discussion on the neo4j mailing list about where each is appropriate.
Jim gives quite a neat summary of the difference between the two on the thread:
 Properties are the data that an entity like a node [or relationship] holds. Relationships simply form the semantic glue (type, direction, cardinality) between nodes.</description>
    </item>
    
    <item>
      <title>Feedback: Reacting immediately</title>
      <link>http://mneedham.github.io/2013/05/23/feedback-reacting-immediately/</link>
      <pubDate>Thu, 23 May 2013 22:43:53 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/23/feedback-reacting-immediately/</guid>
      <description>I was recently reading an article written by Henry Winter where he mentioned some of the ideas that Sir ALex Ferguson has been covering in some interviews he&#39;s been doing at Harvard and one bit stood out for me:  In a series of interviews in Harvard, Ferguson debated dealing with “fragile” egos in the dressing room, the power of the two simple words “well done” in motivating individuals and the importance of criticising players’ mistakes immediately after the match and then moving on.</description>
    </item>
    
    <item>
      <title>Ruby/Python: Constructing a taxonomy from an array using zip</title>
      <link>http://mneedham.github.io/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</link>
      <pubDate>Sun, 19 May 2013 22:44:40 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</guid>
      <description>As I mentioned in my previous blog post I&#39;ve been hacking on a product taxonomy and I wanted to create a &#39;CHILD&#39; relationship between a collection of categories.
For example, I had the following array and I wanted to transform it into an array of &#39;SubCategory, Category&#39; pairs:
taxonomy = [&amp;quot;Cat&amp;quot;, &amp;quot;SubCat&amp;quot;, &amp;quot;SubSubCat&amp;quot;] # I wanted this to become [(&amp;quot;Cat&amp;quot;, &amp;quot;SubCat&amp;quot;), (&amp;quot;SubCat&amp;quot;, &amp;quot;SubSubCat&amp;quot;)  In order to do this we need to zip the first 2 items with the last which I found reasonably easy to do using Python:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Keep longest path when finding taxonomy</title>
      <link>http://mneedham.github.io/2013/05/19/neo4jcypher-keep-longest-path-when-finding-taxonomy/</link>
      <pubDate>Sun, 19 May 2013 22:15:06 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/19/neo4jcypher-keep-longest-path-when-finding-taxonomy/</guid>
      <description>I&#39;ve been playing around with modelling a product taxonomy and one thing that I wanted to do was find out the full path where a product sits under the tree.
I created a simple data set to show the problem:
CREATE (cat { name: &amp;quot;Cat&amp;quot; }) CREATE (subcat1 { name: &amp;quot;SubCat1&amp;quot; }) CREATE (subcat2 { name: &amp;quot;SubCat2&amp;quot; }) CREATE (subsubcat1 { name: &amp;quot;SubSubCat1&amp;quot; }) CREATE (product1 { name: &amp;quot;Product1&amp;quot; }) CREATE (cat)-[:CHILD]-subcat1-[:CHILD]-subsubcat1 CREATE (product1)-[:HAS_CATEGORY]-(subsubcat1)  I wanted to write a query which would return &#39;product1&#39; and the tree &#39;Cat - SubCat1 - SubSubCat1&#39; and initially wrote the following query:</description>
    </item>
    
    <item>
      <title>Unix: Working with parts of large files</title>
      <link>http://mneedham.github.io/2013/05/19/unix-working-with-parts-of-large-files/</link>
      <pubDate>Sun, 19 May 2013 21:44:03 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/19/unix-working-with-parts-of-large-files/</guid>
      <description>Chris and I were looking at the neo4j log files of a client earlier in the week and wanted to do some processing of the file so we could ask the client to send us some further information.
The log file was over 10,000 lines long but the bit of the file we were interesting in was only a few hundred lines.
I usually use Vim and the &#39;:set number&#39; when I want to refer to line numbers in a file but Chris showed me that we can achieve the same thing with e.</description>
    </item>
    
    <item>
      <title>A/B Testing: User Experience vs Conversion</title>
      <link>http://mneedham.github.io/2013/05/18/ab-testing-user-experience-vs-conversion/</link>
      <pubDate>Sat, 18 May 2013 20:18:50 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/18/ab-testing-user-experience-vs-conversion/</guid>
      <description>I&#39;ve written a couple of posts over the last few months about my experiences with A/B testing and one conversation we often used to have was around user experience vs conversion rate.
Once you start running an A/B test it encourages you to focus more on the conversion rate of users in different parts of the flow and your inclination is to make changes that increase that conversion rate.
Another one of our drivers is to provide the best user experience that we can to our customers and since sometimes this means that the best thing for them is not to switch it seems that these two must be in conflict.</description>
    </item>
    
    <item>
      <title>neo4j: When the web console returns nothing…use the data browser!</title>
      <link>http://mneedham.github.io/2013/05/17/neo4j-when-the-web-console-returns-nothinguse-the-data-browser/</link>
      <pubDate>Fri, 17 May 2013 00:00:16 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/17/neo4j-when-the-web-console-returns-nothinguse-the-data-browser/</guid>
      <description>In my time playing around with neo4j I&#39;ve run into a problem a few times where I executed a query using the web console (usually accessible @ http://localhost:7474/webadmin/#/console/) and have got absolutely no response.
I noticed a similar thing today when Rickard and I were having a look at why a Lucene index query wasn&#39;t behaving as we expected.
I setup some data in a neo4j database using neography with the following code:</description>
    </item>
    
    <item>
      <title>Book Review: The Signal and the Noise - Nate Silver</title>
      <link>http://mneedham.github.io/2013/05/14/book-review-the-signal-and-the-noise-nate-silver/</link>
      <pubDate>Tue, 14 May 2013 00:16:56 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/14/book-review-the-signal-and-the-noise-nate-silver/</guid>
      <description>Nate Silver is famous for having correctly predicted the winner of all 50 states in the 2012 United States elections and Sid recommended his book so I could learn more about statistics for the A/B tests that we were running.
I thought the book was a really good introduction to applied statistics and by using real life examples which most people would be able to relate to it makes a potentially dull subject interesting.</description>
    </item>
    
    <item>
      <title>Sublime: Overriding default file type/Assigning specific files to a file type</title>
      <link>http://mneedham.github.io/2013/05/05/sublime-overriding-default-file-typeassigning-specific-files-to-a-file-type/</link>
      <pubDate>Sun, 05 May 2013 00:03:17 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/05/05/sublime-overriding-default-file-typeassigning-specific-files-to-a-file-type/</guid>
      <description>I&#39;ve been using Sublime a bit recently and one thing I wanted to do was put neo4j cypher queries into files with arbitrary extensions and have them recognised as cypher files every time I open them.
I&#39;m using the cypher Sublime plugin to get the syntax highlighting but since I&#39;ve got my cypher in a .haml file it only remembers that it should have cypher highlighting as long as the file is open.</description>
    </item>
    
    <item>
      <title>Ruby 1.9.3 p0: Investigating weirdness with HTTP POST request in net/http</title>
      <link>http://mneedham.github.io/2013/04/30/ruby-1-9-3-p0-investigating-weirdness-with-http-post-request-in-nethttp/</link>
      <pubDate>Tue, 30 Apr 2013 21:37:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/30/ruby-1-9-3-p0-investigating-weirdness-with-http-post-request-in-nethttp/</guid>
      <description>Thibaut and I spent the best part of the last couple of days trying to diagnose a problem we were having trying to make a POST request using rest-client to one of our services.
We have nginx fronting the application server so the request passes through there first:
 The problem we were having was that the request was timing out on the client side before it had been processed and the request wasn&#39;t reaching the application server.</description>
    </item>
    
    <item>
      <title>Mac OS X: A couple of neat tools</title>
      <link>http://mneedham.github.io/2013/04/30/mac-os-x-a-couple-of-neat-tools/</link>
      <pubDate>Tue, 30 Apr 2013 20:07:57 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/30/mac-os-x-a-couple-of-neat-tools/</guid>
      <description>When I first started working at uSwitch Sid installed a couple of &#39;productivity applications&#39; on my Mac which I&#39;ve found pretty useful but from talking to others I realised they aren&#39;t known/being used by everyone.
Alfred Alfred is a Quick Silver replacement which allows you to quickly open applications, find files, search Google and more. Even though we&#39;re not using half of its features it&#39;s still proved to be useful.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Returning a row with zero count when no relationship exists</title>
      <link>http://mneedham.github.io/2013/04/30/neo4jcypher-returning-a-row-with-zero-count-when-no-relationship-exists/</link>
      <pubDate>Tue, 30 Apr 2013 07:02:09 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/30/neo4jcypher-returning-a-row-with-zero-count-when-no-relationship-exists/</guid>
      <description>I&#39;ve been trying to see if I can match some of the football stats that OptaJoe posts on twitter and one that I was looking at yesterday was around the number of red cards different teams have received.
 1 - Sunderland have picked up their first PL red card of the season. The only team without one now are Man Utd. Angels.  To refresh this is the sub graph that we&#39;ll need to look at to work it out:</description>
    </item>
    
    <item>
      <title>A/B Testing: Reporting</title>
      <link>http://mneedham.github.io/2013/04/28/ab-testing-reporting/</link>
      <pubDate>Sun, 28 Apr 2013 22:32:38 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/28/ab-testing-reporting/</guid>
      <description>A few months ago I wrote about my initial experiences with A/B testing and since then we&#39;ve been working on another one and learnt some things around reporting on these types of tests that I thought was interesting.
Reporting as a first class concern One thing we changed from our previous test after a suggestion by Mike was to start treating the reporting of data related to the test as a first class citizen.</description>
    </item>
    
    <item>
      <title>Treat servers as cattle: Spin them up, tear them down</title>
      <link>http://mneedham.github.io/2013/04/27/treat-servers-as-cattle-spin-them-up-tear-them-down/</link>
      <pubDate>Sat, 27 Apr 2013 14:22:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/27/treat-servers-as-cattle-spin-them-up-tear-them-down/</guid>
      <description>A few agos I wrote a post about treating servers as cattle, not as pets in which I described an approach to managing virtual machines at uSwitch whereby we frequently spin up new ones and delete the existing ones.
I&#39;ve worked on teams previously where we&#39;ve also talked about this mentality but ended up not doing it because it was difficult, usually for one of two reasons:
 Slow spin up - this might be due to the cloud providers infrastructure, doing too much on spin up or I&#39;m sure a variety of other reasons.</description>
    </item>
    
    <item>
      <title>Puppet: Package Versions - To pin or not to pin</title>
      <link>http://mneedham.github.io/2013/04/27/puppet-package-versions-to-pin-or-not-to-pin/</link>
      <pubDate>Sat, 27 Apr 2013 13:40:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/27/puppet-package-versions-to-pin-or-not-to-pin/</guid>
      <description>Over the last year or so I&#39;ve spent quite a bit of time working with puppet and one of the things that we had to decide when installing packages was whether or not to specify a particular version.
On the first project I worked on we didn&#39;t bother and just let the package manager chose the most recent version.
Therefore if we were installing nginx the puppet code would read like this:</description>
    </item>
    
    <item>
      <title>Unix: Checking for open sockets on nginx</title>
      <link>http://mneedham.github.io/2013/04/23/unix-checking-for-open-sockets-on-nginx/</link>
      <pubDate>Tue, 23 Apr 2013 23:59:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/23/unix-checking-for-open-sockets-on-nginx/</guid>
      <description>Tim and I were investigating a weird problem we were having with nginx where it was getting in a state where it had exceeded the number of open files allowed on the system and started rejecting requests.
We can find out the maximum number of open files that we&#39;re allowed on a system with the following command:
$ ulimit -n 1024  Our hypothesis was that some socket connections were never being closed and therefore the number of open files was climbing slowly upwards until it exceeded the limit.</description>
    </item>
    
    <item>
      <title>No downtime deploy with capistrano, Thin and nginx</title>
      <link>http://mneedham.github.io/2013/04/23/no-downtime-deploy-with-capistrano-thin-and-nginx/</link>
      <pubDate>Tue, 23 Apr 2013 23:25:15 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/23/no-downtime-deploy-with-capistrano-thin-and-nginx/</guid>
      <description>As I mentioned a couple of weeks ago I&#39;ve been working on a tutorial about thinking through problems in graphs and since it&#39;s a Sinatra application I thought thin would be a decent choice for web server.
In my initial setup I had the following nginx config file which was used to proxy requests on to thin:
/etc/nginx/sites-available/thinkingingraphs.conf
upstream thin { server 127.0.0.1:3000; } server { listen 80 default; server_name _; charset utf-8; rewrite ^\/status(.</description>
    </item>
    
    <item>
      <title>Puppet: Installing Oracle Java - oracle-license-v1-1 license could not be presented</title>
      <link>http://mneedham.github.io/2013/04/18/puppet-installing-oracle-java-oracle-license-v1-1-license-could-not-be-presented/</link>
      <pubDate>Thu, 18 Apr 2013 23:36:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/18/puppet-installing-oracle-java-oracle-license-v1-1-license-could-not-be-presented/</guid>
      <description>In order to run the neo4j server on my Ubuntu 12.04 Vagrant VM I needed to install the Oracle/Sun JDK which proved to be more difficult than I&#39;d expected.
I initially tried to install it via the OAB-Java script but was running into some dependency problems and eventually came across a post which specified a PPA that had an installer I could use.
I wrote a little puppet Java module to wrap the commands in:</description>
    </item>
    
    <item>
      <title>dpkg/apt-cache: Useful commands</title>
      <link>http://mneedham.github.io/2013/04/18/dpkgapt-cache-useful-commands/</link>
      <pubDate>Thu, 18 Apr 2013 21:54:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/18/dpkgapt-cache-useful-commands/</guid>
      <description>As I&#39;ve mentioned in a couple of previous posts I&#39;ve been playing around with creating a Vagrant VM that I can use for my neo4j hacking which has involved a lot of messing around with installing apt packages.
There are loads of different ways of working out what&#39;s going on when packages aren&#39;t installing as you&#39;d expect so I thought it&#39;d be good to document the ones I&#39;ve been using so I can find them more easily next time.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Redundant relationships</title>
      <link>http://mneedham.github.io/2013/04/16/neo4jcypher-redundant-relationships/</link>
      <pubDate>Tue, 16 Apr 2013 21:41:58 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/16/neo4jcypher-redundant-relationships/</guid>
      <description>Last week I was writing a query to find the top scorers in the Premier League so far this season alongside the number of games they&#39;ve played in which initially read like this:
START player = node:players(&#39;name:*&#39;) MATCH player-[:started|as_sub]-playedLike-[:in]-game-[r?:scored_in]-player WITH player, COUNT(DISTINCT game) AS games, COLLECT(r) AS allGoals RETURN player.name, games, LENGTH(allGoals) AS goals ORDER BY goals DESC LIMIT 5  +------------------------------------+ | player.name | games | goals | +------------------------------------+ | &amp;quot;Luis Suárez&amp;quot; | 30 | 22 | | &amp;quot;Robin Van Persie&amp;quot; | 30 | 19 | | &amp;quot;Gareth Bale&amp;quot; | 27 | 17 | | &amp;quot;Michu&amp;quot; | 29 | 16 | | &amp;quot;Demba Ba&amp;quot; | 28 | 15 | +------------------------------------+ 5 rows 1 ms  I modelled whether a player started a game or came on as a substitute with separate relationship types &#39;started&#39; and &#39;as_sub&#39; but in this query we&#39;re not interested in that, we just want to know whether they played.</description>
    </item>
    
    <item>
      <title>Puppet Debt</title>
      <link>http://mneedham.github.io/2013/04/16/puppet-debt/</link>
      <pubDate>Tue, 16 Apr 2013 20:57:53 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/16/puppet-debt/</guid>
      <description>I&#39;ve been playing around with a puppet configuration to run a neo4j server on an Ubuntu VM and one thing that has been quite tricky is getting the Sun/Oracle Java JDK to install repeatably.
I adapted Julian&#39;s Java module which uses OAB-Java and although it was certainly working cleanly at one stage I somehow ended up with it not working because of failed dependencies:
[2013-04-12 07:03:10] Notice: /Stage[main]/Java/Exec[install OAB repo]/returns: [x] Installing Java build requirements Ofailed [2013-04-12 07:03:10] Notice: /Stage[main]/Java/Exec[install OAB repo]/returns: ^[[m^O [i] Showing the last 5 lines from the logfile (/root/oab-java.</description>
    </item>
    
    <item>
      <title>Capistrano: Host key verification failed. ** [err] fatal: The remote end hung up unexpectedly</title>
      <link>http://mneedham.github.io/2013/04/14/capistrano-host-key-verification-failed-err-fatal-the-remote-end-hung-up-unexpectedly/</link>
      <pubDate>Sun, 14 Apr 2013 18:18:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/14/capistrano-host-key-verification-failed-err-fatal-the-remote-end-hung-up-unexpectedly/</guid>
      <description>As I mentioned in my previous post I&#39;ve been deploying a web application to a vagrant VM using Capistrano and my initial configuration was like so:
require &#39;capistrano/ext/multistage&#39; set :application, &amp;quot;thinkingingraphs&amp;quot; set :scm, :git set :repository, &amp;quot;git@bitbucket.org:markhneedham/thinkingingraphs.git&amp;quot; set :scm_passphrase, &amp;quot;&amp;quot; set :ssh_options, {:forward_agent =&amp;gt; true, :paranoid =&amp;gt; false, keys: [&#39;~/.vagrant.d/insecure_private_key&#39;]} set :stages, [&amp;quot;vagrant&amp;quot;] set :default_stage, &amp;quot;vagrant&amp;quot; set :user, &amp;quot;vagrant&amp;quot; server &amp;quot;192.168.33.101&amp;quot;, :app, :web, :db, :primary =&amp;gt; true set :deploy_to, &amp;quot;/var/www/thinkingingraphs&amp;quot;  When I ran &#39;cap deploy&#39; I ended up with the following error:</description>
    </item>
    
    <item>
      <title>Capistrano: Deploying to a Vagrant VM</title>
      <link>http://mneedham.github.io/2013/04/13/capistrano-deploying-to-a-vagrant-vm/</link>
      <pubDate>Sat, 13 Apr 2013 11:17:37 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/13/capistrano-deploying-to-a-vagrant-vm/</guid>
      <description>I&#39;ve been working on a tutorial around thinking through problems in graphs using my football graph and I wanted to deploy it on a local vagrant VM as a stepping stone to deploying it in a live environment.
My Vagrant file for the VM looks like this:
# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant::Config.run do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.define :neo01 do |neo| neo.vm.network :hostonly, &amp;quot;192.168.33.101&amp;quot; neo.</description>
    </item>
    
    <item>
      <title>awk: Parsing &#39;free -m&#39; output to get memory usage/consumption</title>
      <link>http://mneedham.github.io/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</link>
      <pubDate>Wed, 10 Apr 2013 07:03:15 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</guid>
      <description>Although I know this problem is already solved by collectd and New Relic I wanted to write a little shell script that showed me the memory usage on a bunch of VMs by parsing the output of free.
The output I was playing with looks like this:
$ free -m total used free shared buffers cached Mem: 365 360 5 0 59 97 -/+ buffers/cache: 203 161 Swap: 767 13 754  I wanted to find out what % of the memory on the machine was being used and as I understand it the numbers that we would use to calculate this are the &#39;total&#39; value on the &#39;Mem&#39; line and the &#39;used&#39; value on the &#39;buffers/cache&#39; line.</description>
    </item>
    
    <item>
      <title>Python: Reading a JSON file</title>
      <link>http://mneedham.github.io/2013/04/09/python-reading-a-json-file/</link>
      <pubDate>Tue, 09 Apr 2013 07:23:59 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/09/python-reading-a-json-file/</guid>
      <description>I&#39;ve been playing around with some code to spin up AWS instances using Fabric and Boto and one thing that I wanted to do was define a bunch of default properties in a JSON file and then load this into a script.
I found it harder to work out how to do this than I expected to so I thought I&#39;d document it for future me!
My JSON file looks like this:</description>
    </item>
    
    <item>
      <title>Treating servers as cattle, not as pets</title>
      <link>http://mneedham.github.io/2013/04/07/treating-servers-as-cattle-not-as-pets/</link>
      <pubDate>Sun, 07 Apr 2013 11:41:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/07/treating-servers-as-cattle-not-as-pets/</guid>
      <description>Although I didn&#39;t go to Dev Ops Days London earlier in the year I was following the hash tag on twitter and one of my favourites things that I read was the following:
 “Treating servers as cattle, not as pets” #DevOpsDays  I think this is particularly applicable now that a lot of the time we&#39;re using virtualised production environments via AWS, Rackspace or .
At uSwitch we use AWS and over the last week Sid and I spent some time investigating a memory leak by running our applications against two different versions of Ruby.</description>
    </item>
    
    <item>
      <title>Sublime: Getting Textmate&#39;s Reveal/Select in Side Bar (Cmd &#43; Ctrl &#43; R)</title>
      <link>http://mneedham.github.io/2013/04/07/sublime-getting-textmates-revealselect-in-side-bar-cmd-ctrl-r/</link>
      <pubDate>Sun, 07 Apr 2013 01:00:08 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/07/sublime-getting-textmates-revealselect-in-side-bar-cmd-ctrl-r/</guid>
      <description>After coming across this post about why you should use Sublime Text I decided to try using it a bit more and one of the things that I missed from Textmate was the way you can select the current file on the sidebar.
In Textmate the shortcut to do that is &#39;Cmd + Ctrl + R&#39; so I wanted to be able to do something similar or configure Sublime so it responded to the same shortcut.</description>
    </item>
    
    <item>
      <title>MySQL: Repairing broken tables/indices</title>
      <link>http://mneedham.github.io/2013/04/06/mysql-repairing-broken-tablesindices/</link>
      <pubDate>Sat, 06 Apr 2013 17:26:20 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/04/06/mysql-repairing-broken-tablesindices/</guid>
      <description>I part time administrate a football forum that I used to run when I was at university and one problem we had recently was that some of the tables/indices had got corrupted when MySQL crashed due to a lack of disc space.
We weren&#39;t seeing any visible sign of a problem in any of the logs but whenever you tried to query one of the topics it wasn&#39;t returning any posts.</description>
    </item>
    
    <item>
      <title>Embracing the logs</title>
      <link>http://mneedham.github.io/2013/03/31/embracing-the-logs/</link>
      <pubDate>Sun, 31 Mar 2013 21:44:19 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/31/embracing-the-logs/</guid>
      <description>Despite the fact that I&#39;ve been working full time in software for almost 8 years now every now and then I still need a reminder of how useful reading logs can be in helping solve problems.
I had a couple of such instances recently which I thought I&#39;d document.
The first was a couple of weeks ago when Tim and I were pairing on moving some applications from Passenger to Unicorn and were testing whether or not we&#39;d done so successfully.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Playing around with time</title>
      <link>http://mneedham.github.io/2013/03/31/neo4jcypher-playing-around-with-time/</link>
      <pubDate>Sun, 31 Mar 2013 21:08:22 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/31/neo4jcypher-playing-around-with-time/</guid>
      <description>I&#39;ve done a bit of modelling with years and months in neo4j graphs that I&#39;ve worked on previously but I haven&#39;t ever done anything with time so I thought it&#39;d be interesting to have a go with my football graph.
I came across this StackOverflow post on my travels which suggested that indexing nodes by time would be helpful and since I have a bunch of football matches with associated times I thought I&#39;d try it out.</description>
    </item>
    
    <item>
      <title>Editing config files on a server &amp; Ctrl-Z</title>
      <link>http://mneedham.github.io/2013/03/29/editing-config-files-on-a-server-ctrl-z/</link>
      <pubDate>Fri, 29 Mar 2013 10:51:37 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/29/editing-config-files-on-a-server-ctrl-z/</guid>
      <description>A couple of weeks ago Tim and I were spinning up a new service on a machine which wasn&#39;t quite working so we were manually making changes to the /etc/nginx/nginx.conf file and restarting nginx to try and sort it out.
This process is generally not that interesting - you open the file in vi, make some changes, close it, then restart nginx and see if it works. If not then you open the file again and repeat.</description>
    </item>
    
    <item>
      <title>Incrementally rolling out machines with a new puppet role</title>
      <link>http://mneedham.github.io/2013/03/24/incrementally-rolling-out-machines-with-a-new-puppet-role/</link>
      <pubDate>Sun, 24 Mar 2013 22:52:19 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/24/incrementally-rolling-out-machines-with-a-new-puppet-role/</guid>
      <description>Last week Jason and I with (a lot of) help from Tim have been working on moving several of our applications from Passenger to Unicorn and decided that the easiest way to do this was to create a new set of nodes with this setup.
The architecture we&#39;re working with looks like this at a VM level:
 The &#39;nginx LB&#39; nodes are responsible for routing all the requests to their appropriate application servers and the &#39;web&#39; nodes serve the different applications initially using Passenger.</description>
    </item>
    
    <item>
      <title>Best tool for the job/Learning new ways to do things</title>
      <link>http://mneedham.github.io/2013/03/24/best-tool-for-the-joblearning-new-ways-to-do-things/</link>
      <pubDate>Sun, 24 Mar 2013 22:01:12 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/24/best-tool-for-the-joblearning-new-ways-to-do-things/</guid>
      <description>I recently came across an interesting post written by Randy Luecke titled &#39;I&#39;m done with the web&#39; in which he expresses his surprise that people often aren&#39;t willing to take the time out to learn something new.
In this context he&#39;s referring to javascript libraries but I think his thinking is generally applicable.
Having worked for a few years now I&#39;ve played around with a reasonable number of programming languages/text editors/databases/etc to the point that I have favourites when it comes to solving certain problems.</description>
    </item>
    
    <item>
      <title>When nokogiri fails with &#39;Nokogiri::XML::SyntaxError: Element script embeds close tag&#39; Web Driver to the rescue</title>
      <link>http://mneedham.github.io/2013/03/24/when-nokogiri-fails-with-nokogirixmlsyntaxerror-element-script-embeds-close-tag-web-driver-to-the-rescue/</link>
      <pubDate>Sun, 24 Mar 2013 21:20:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/24/when-nokogiri-fails-with-nokogirixmlsyntaxerror-element-script-embeds-close-tag-web-driver-to-the-rescue/</guid>
      <description>As I mentioned in my previous post I wanted to add televised games to my football graph and the Premier League website seemed like the best case to find out which games those were.
I initially tried to use Nokogiri to grab the data that I wanted...
&amp;gt; require &#39;nokogiri&#39; &amp;gt; require &#39;open-air&#39; &amp;gt; tv_times = Nokogiri::HTML(open(&#39;http://www.premierleague.com/en-gb/matchday/broadcast-schedules.tv.html?rangeType=.dateSeason&amp;amp;country=GB&amp;amp;clubId=ALL&amp;amp;season=2012-2013&amp;amp;isLive=true&#39;))  ...but when I tried to query by CSS selector for all the matches nothing came back:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: CypherTypeException: Failed merging Number with Relationship</title>
      <link>http://mneedham.github.io/2013/03/24/neo4jcypher-cyphertypeexception-failed-merging-number-with-relationship/</link>
      <pubDate>Sun, 24 Mar 2013 13:00:29 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/24/neo4jcypher-cyphertypeexception-failed-merging-number-with-relationship/</guid>
      <description>The latest thing that I added to my football graph was the matches that are shown on TV as I have the belief that players who score on televised games get more attention than players who score in other games.
I thought it&#39;d be interesting to work out who the top scorers are on each of these game types.
I added the following relationship type to allow me to do this:</description>
    </item>
    
    <item>
      <title>beanstalkd: Getting the status of the queue</title>
      <link>http://mneedham.github.io/2013/03/21/beanstalkd-getting-the-status-of-the-queue/</link>
      <pubDate>Thu, 21 Mar 2013 23:25:09 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/21/beanstalkd-getting-the-status-of-the-queue/</guid>
      <description>For the last few days Jason and I have been porting a few of our applications across to a new puppet setup and one thing we needed to do was check that messages were passing through beanstalkd correctly.
We initially had the idea that it wasn&#39;t configured correctly so Paul showed us a way of checking whether that was the case by connecting to the port it runs on like so:</description>
    </item>
    
    <item>
      <title>Wiring up an Amazon S3 bucket to a CNAME entry - The specified bucket does not exist</title>
      <link>http://mneedham.github.io/2013/03/21/wiring-up-an-amazon-s3-bucket-to-a-cname-entry-the-specified-bucket-does-not-exist/</link>
      <pubDate>Thu, 21 Mar 2013 22:39:02 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/21/wiring-up-an-amazon-s3-bucket-to-a-cname-entry-the-specified-bucket-does-not-exist/</guid>
      <description>Jason and I were setting up an internal static website using an S3 bucket a couple of days ago and wanted to point a more friendly domain name at it.
We initially called our bucket &#39;static-site&#39; and then created a CNAME entry using zerigo to point our sub domain at the bucket.
The mapping was something like this:
our-subdomain.somedomain.com -&amp;gt; static-site.s3-website-eu-west-1.amazonaws.com  When we tried to access the site through our-subdomain.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: WITH, COLLECT &amp; EXTRACT</title>
      <link>http://mneedham.github.io/2013/03/20/neo4jcypher-with-collect-extract/</link>
      <pubDate>Wed, 20 Mar 2013 02:54:43 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/20/neo4jcypher-with-collect-extract/</guid>
      <description>As I mentioned in my last post I&#39;m trying to get the hang of the WITH statement in neo4j&#39;s cypher query language and I found another application when trying to work out which opponents teams played on certain days.
I started out with a query which grouped the data set by day and showed the opponents that were played on that day:
START team = node:teams(&#39;name:&amp;quot;Manchester United&amp;quot;&#39;) MATCH team-[h:home_team|away_team]-game-[:on_day]-day RETURN DISTINCT day.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Getting the hang of the WITH statement</title>
      <link>http://mneedham.github.io/2013/03/20/neo4jcypher-getting-the-hang-of-the-with-statement/</link>
      <pubDate>Wed, 20 Mar 2013 00:25:00 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/20/neo4jcypher-getting-the-hang-of-the-with-statement/</guid>
      <description>I wrote a post a few weeks ago showing an example of a cypher query which made use of the WITH statement but I still don&#39;t completely understand how it works so I thought I&#39;d write some more queries that use it.
I wanted to find out whether Luis Suárez has a better scoring record depending on which day a match is played on.
We start out by finding all the matches that he&#39;s played in and which days those matches were on:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: SQL style GROUP BY WITH LIMIT query</title>
      <link>http://mneedham.github.io/2013/03/18/neo4jcypher-sql-style-group-by-with-limit-query/</link>
      <pubDate>Mon, 18 Mar 2013 23:19:36 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/18/neo4jcypher-sql-style-group-by-with-limit-query/</guid>
      <description>A few weeks ago I wrote a blog post where I described how we could construct a SQL GROUP BY style query in cypher and last week I wanted to write a similar query but with what I think would be a LIMIT clause in SQL.
I wanted to find the maximum number of goals that players had scored in a match for a specific team and started off with the following query to find all the matches that players had scored in:</description>
    </item>
    
    <item>
      <title>clojure/Java Interop: The doto macro</title>
      <link>http://mneedham.github.io/2013/03/17/clojurejava-interop-the-doto-macro/</link>
      <pubDate>Sun, 17 Mar 2013 20:21:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/17/clojurejava-interop-the-doto-macro/</guid>
      <description>I recently wrote about some code I&#39;ve been playing with to import neo4j spatial data and while looking to simplify the code I came across the doto macro.
The doto macro allows us to chain method calls on an initial object and then returns the resulting object. e.g.
(doto (new java.util.HashMap) (.put &amp;quot;a&amp;quot; 1) (.put &amp;quot;b&amp;quot; 2)) -&amp;gt; {a=1, b=2}  In our case this comes in quite useful in the function used to create a stadium node which initially reads like this:~~~ ~~~lisp (defn create-stadium-node [db line] (let [stadium-node (.</description>
    </item>
    
    <item>
      <title>clojure/Java Interop - Importing neo4j spatial data</title>
      <link>http://mneedham.github.io/2013/03/17/clojurejava-interop-importing-neo4j-spatial-data/</link>
      <pubDate>Sun, 17 Mar 2013 18:56:36 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/17/clojurejava-interop-importing-neo4j-spatial-data/</guid>
      <description>I wrote a post about a week ago where I described how I&#39;d added football stadiums to my football graph using neo4j spatial and after I&#39;d done that I wanted to put it into my import script along with the rest of the data.
I thought leiningen would probably work quite well for this as you can point it at a Java class and have it be executed.
To start with I had to change the import code slightly to link stadiums to teams which have already been added to the graph:</description>
    </item>
    
    <item>
      <title>Understanding what lsof socket/port aliases refer to</title>
      <link>http://mneedham.github.io/2013/03/17/understanding-what-lsof-socketport-aliases-refer-to/</link>
      <pubDate>Sun, 17 Mar 2013 14:00:35 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/17/understanding-what-lsof-socketport-aliases-refer-to/</guid>
      <description>Earlier in the week we wanted to check which ports were being listened on and by what processes which we can do with the following command on Mac OS X:
$ lsof -ni | grep LISTEN idea 2398 markhneedham 58u IPv6 0xac8f13f77b903331 0t0 TCP *:49410 (LISTEN) idea 2398 markhneedham 65u IPv6 0xac8f13f7799a4af1 0t0 TCP *:58741 (LISTEN) idea 2398 markhneedham 122u IPv6 0xac8f13f7799a4711 0t0 TCP 127.0.0.1:6942 (LISTEN) idea 2398 markhneedham 249u IPv6 0xac8f13f777586711 0t0 TCP *:63342 (LISTEN) idea 2398 markhneedham 253u IPv6 0xac8f13f777586331 0t0 TCP 127.</description>
    </item>
    
    <item>
      <title>A quick and dirty way of testing the performance of a service</title>
      <link>http://mneedham.github.io/2013/03/16/a-quick-and-dirty-way-of-testing-the-performance-of-a-service/</link>
      <pubDate>Sat, 16 Mar 2013 11:58:42 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/16/a-quick-and-dirty-way-of-testing-the-performance-of-a-service/</guid>
      <description>We had a power outage in our data centre yesterday and once it had recovered Jason and I wanted to do a quick check that one of our backend services was still responding in an acceptable amount of time.
Since this particular service only serves HTTP GET requests it was reasonably easy to setup a cURL command to do this:
while true; do curl -k -s -w %{time_total} https://serviceurl/whatever/something; -o /dev/null; printf &amp;quot;\n&amp;quot;; done &amp;gt; service.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Finding football stadiums near a city using spatial</title>
      <link>http://mneedham.github.io/2013/03/10/neo4jcypher-finding-football-stadiums-near-a-city-using-spatial/</link>
      <pubDate>Sun, 10 Mar 2013 22:13:41 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/10/neo4jcypher-finding-football-stadiums-near-a-city-using-spatial/</guid>
      <description>One of the things that I wanted to add to my football graph was something location related so I could try out neo4j spatial and I thought the easiest way to do that was to model the location of football stadiums.
To start with I needed to add spatial as an unmanaged extension to my neo4j plugins folder which involved doing the following:
$ git clone git://github.com/neo4j/spatial.git spatial $ cd spatial $ mvn clean package -Dmaven.</description>
    </item>
    
    <item>
      <title>neo4j: Make properties relationships</title>
      <link>http://mneedham.github.io/2013/03/06/neo4j-make-properties-relationships/</link>
      <pubDate>Wed, 06 Mar 2013 00:59:36 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/06/neo4j-make-properties-relationships/</guid>
      <description>I spent some of the weekend working my way through Jim, Ian &amp; Emil&#39;s book &#39;Graph Databases&#39; and one of the things that they emphasise is that graphs allow us to make relationships first class citizens in our model.
Looking back on a couple of the graphs that I modelled last year I realise that I didn&#39;t quite get this and although the graphs I modelled had some relationships a lot of the time I was defining things as properties on nodes.</description>
    </item>
    
    <item>
      <title>Ruby/Haml: Conditionally/Optionally setting an attribute/class</title>
      <link>http://mneedham.github.io/2013/03/02/rubyhaml-conditionallyoptionally-setting-an-attributeclass/</link>
      <pubDate>Sat, 02 Mar 2013 23:22:50 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/02/rubyhaml-conditionallyoptionally-setting-an-attributeclass/</guid>
      <description>One of the things that we want to do reasonably frequently is set an attribute (most often a class) on a HTML element depending on the value of a variable.
I always forget how to do this in Haml so I thought I better write it down so I&#39;ll remember next time!
Let&#39;s say we want to add a success class to a paragraph if the variable correct is true and not have any value if it&#39;s false.</description>
    </item>
    
    <item>
      <title>Ruby/Haml: Maintaining white space/indentation in a &amp;lt;pre&amp;gt; tag</title>
      <link>http://mneedham.github.io/2013/03/02/rubyhaml-maintaining-white-spaceindentation-in-a-pre-tag/</link>
      <pubDate>Sat, 02 Mar 2013 22:19:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/03/02/rubyhaml-maintaining-white-spaceindentation-in-a-pre-tag/</guid>
      <description>I&#39;ve been writing a little web app in which I wanted to display cypher queries inside a  tag which was then prettified using SyntaxHighlighter but I was having problems with how code on new lines was being displayed.
I had the following Haml code to display a query looking up Gareth Bale in a graph:
%pre{ :class =&amp;gt; &amp;quot;brush: cypher; gutter: false; toolbar: false;&amp;quot;} START player = node:players(&#39;name:&amp;quot;Gareth Bale&amp;quot;&#39;) RETURN player.</description>
    </item>
    
    <item>
      <title>neo4j: Loading data - REST API vs Batch Import</title>
      <link>http://mneedham.github.io/2013/02/28/neo4j-loading-data-rest-api-vs-batch-import/</link>
      <pubDate>Thu, 28 Feb 2013 23:36:13 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/28/neo4j-loading-data-rest-api-vs-batch-import/</guid>
      <description>A couple of weeks ago when I first started playing around with my football data set I was loading all the data into neo4j using the REST API via neography which was taking around 4 minutes to load.
The data set consisted of just over 250 matches which translated into 8,000 nodes &amp; 30,000 relationships so it&#39;s very small by all means.
Ashok and I were discussing how that could be quicker and the first thing we tried was to store inserted nodes in an in memory hash map and look them up from there rather than doing an index lookup each time.</description>
    </item>
    
    <item>
      <title>Vertical/Horizontal Slicing</title>
      <link>http://mneedham.github.io/2013/02/28/verticalhorizontal-slicing/</link>
      <pubDate>Thu, 28 Feb 2013 22:23:27 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/28/verticalhorizontal-slicing/</guid>
      <description>A few years ago I wrote a bunch of posts exploring my experiences of outside in development eventually coming to the conclusion that it seemed to make sense to drive out functionality from the UI and work back from there.
i.e. we take a vertical slice of functionality and then drive it end to end.
On the team I&#39;m working on there&#39;s been success using an approach where the functionality is still split vertically but we work across a horizontal layer for all the cards before moving onto the next layer.</description>
    </item>
    
    <item>
      <title>Compatible Opinions &amp; Confirmation Bias</title>
      <link>http://mneedham.github.io/2013/02/28/compatible-opinions-confirmation-bias/</link>
      <pubDate>Thu, 28 Feb 2013 21:57:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/28/compatible-opinions-confirmation-bias/</guid>
      <description>In 2011 Jay Fields wrote a blog post in which he suggested that it&#39;s better to build teams in which people have a similar opinion on the way software should be built at a high level rather than having people whose opinions are in conflict.
He referred to this as having &#39;compatible opinions on software&#39; and since I read the post I&#39;ve become much more aware of this myself on the teams that I&#39;ve worked on.</description>
    </item>
    
    <item>
      <title>Micro Services: Where does the complexity go?</title>
      <link>http://mneedham.github.io/2013/02/28/micro-services-where-does-the-complexity-go/</link>
      <pubDate>Thu, 28 Feb 2013 00:00:03 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/28/micro-services-where-does-the-complexity-go/</guid>
      <description>For the past year every system that I&#39;ve worked on has been designed around a micro services architecture and while there are benefits with this approach there is an inherent complexity in software which has to go somewhere!
I thought it&#39;d be interesting to run through some of the new complexities that I&#39;ve noticed in what may well be an acknowledgement of the difficulty of designing distributed systems.
Interactions between components One of the advantages of having lots of small applications is that each one is conceptually easier to understand and we only need to keep the mental model of how that one application works when we&#39;re working on it.</description>
    </item>
    
    <item>
      <title>Reading Code: Assume it doesn&#39;t work</title>
      <link>http://mneedham.github.io/2013/02/27/reading-code-assume-it-doesnt-work/</link>
      <pubDate>Wed, 27 Feb 2013 23:12:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/27/reading-code-assume-it-doesnt-work/</guid>
      <description>Jae and I have spent a reasonable chunk of the past few weeks pairing on code that neither of us are familiar with and at times we&#39;ve found it quite difficult to work out exactly what it&#39;s supposed to be doing.
My default stance in this situation is to assume that the code is probably correct and then try and work out how that&#39;s the case.
After I&#39;d vocalised this a few times, Jae pointed out that we couldn&#39;t be sure that the code worked and it didn&#39;t make sense to start with that as an assumption.</description>
    </item>
    
    <item>
      <title>Micro Services: Readme files</title>
      <link>http://mneedham.github.io/2013/02/25/micro-services-readme-files/</link>
      <pubDate>Mon, 25 Feb 2013 23:58:51 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/25/micro-services-readme-files/</guid>
      <description>By my latest count I have around 15 different micro services/applications checked out on my machine which comprise the system that I&#39;m currently working on.
Most of these are Ruby related so it&#39;s easy to figure out how to start up a local copy because it&#39;s either bundle exec rails server if it&#39;s a rails application or bundle exec backup if it&#39;s a sinatra/rack application. The clojure applications follow a similar convention and we use rake to run any offline tasks.</description>
    </item>
    
    <item>
      <title>Pomodoros and the To-Do list</title>
      <link>http://mneedham.github.io/2013/02/25/pomodoros-and-the-to-do-list/</link>
      <pubDate>Mon, 25 Feb 2013 23:33:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/25/pomodoros-and-the-to-do-list/</guid>
      <description>Anna and I were recently discussing the way that we get things done outside of work and since December I&#39;ve been fairly religiously working through various &#39;to-do&#39; lists with a pomodoro timer.
So far I&#39;ve done 308 30 minute pomodoros in about 8 weeks which is just under 20 hours a week which is not bad but still leaves time for a ridiculous amount of procrastination.
These are some of the things that I&#39;ve noticed from only doing things when it&#39;s explicitly on a timer:</description>
    </item>
    
    <item>
      <title>Reading outside your area of interest</title>
      <link>http://mneedham.github.io/2013/02/25/reading-outside-your-area-of-interest/</link>
      <pubDate>Mon, 25 Feb 2013 22:56:11 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/25/reading-outside-your-area-of-interest/</guid>
      <description>A reasonable amount of the information that I consume comes either via scanning twitter or from my prismatic feed but I noticed that I&#39;m quite biased to reading things in similar subject areas.
I tend to end up reading about data mining/science, functional programming and startups and while the articles are mostly interesting it does eventually start to feel like you&#39;re in an echo chamber. I have a subscription to the ACM mainly because I enjoy reading the &#39;Communications of the ACM&#39; magazine which gets sent out every month and until recently I only read articles which I thought would be interesting.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Combining COUNT and COLLECT in one query</title>
      <link>http://mneedham.github.io/2013/02/24/neo4jcypher-combining-count-and-collect-in-one-query/</link>
      <pubDate>Sun, 24 Feb 2013 19:19:59 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/24/neo4jcypher-combining-count-and-collect-in-one-query/</guid>
      <description>In my continued playing around with football data I wanted to write a cypher query against neo4j which would show me which teams had missed the most penalties this season and who missed them.
I started off with a query that returned all the penalties that have been missed this season and the games those misses happened in:
START player = node:players(&#39;name:*&#39;) MATCH player-[:missed_penalty_in]-game, player-[:played|subbed_on]-stats-[:in]-game, stats-[:for]-team, game-[:home_team]-home, game-[:away_team]-away RETURN player.name, team.</description>
    </item>
    
    <item>
      <title>Ruby: Stripping out a non breaking space character (&amp;amp;nbsp;)</title>
      <link>http://mneedham.github.io/2013/02/23/ruby-stripping-out-a-non-breaking-space-character-nbsp/</link>
      <pubDate>Sat, 23 Feb 2013 15:04:58 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/23/ruby-stripping-out-a-non-breaking-space-character-nbsp/</guid>
      <description>A couple of days ago I was playing with some code to scrape data from a web page and I wanted to skip a row in a table if the row didn&#39;t contain any text.
I initially had the following code to do that:
rows.each do |row| next if row.strip.empty? # other scraping code end  Unfortunately that approach broke down fairly quickly because empty rows contained a non breaking space i.</description>
    </item>
    
    <item>
      <title>neo4j/cypher: Using a WHERE clause to filter paths</title>
      <link>http://mneedham.github.io/2013/02/19/neo4jcypher-using-a-where-clause-to-filter-paths/</link>
      <pubDate>Tue, 19 Feb 2013 00:03:18 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/19/neo4jcypher-using-a-where-clause-to-filter-paths/</guid>
      <description>One of the cypher queries that I wanted to write recently was one to find all the players that have started matches for Arsenal this season and the number of matches that they&#39;ve played in.
The data model that I&#39;m querying looks like this:
 I started off with the following query which traverses from Arsenal to all the games that they&#39;ve taken part in and finds all the players who&#39;ve played in those games:</description>
    </item>
    
    <item>
      <title>Micro Services Style Data Work Flow</title>
      <link>http://mneedham.github.io/2013/02/18/micro-services-style-data-work-flow/</link>
      <pubDate>Mon, 18 Feb 2013 22:16:39 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/18/micro-services-style-data-work-flow/</guid>
      <description>Having worked on a few data related applications over the last ten months or so Ashok and I were recently discussing some of the things that we&#39;ve learnt
One of the things he pointed out is that it&#39;s very helpful to separate the different stages of a data work flow into their own applications/scripts.
I decided to try out this idea with some football data that I&#39;m currently trying to model and I ended up with the following stages:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: SQL style GROUP BY functionality</title>
      <link>http://mneedham.github.io/2013/02/17/neo4jcypher-sql-style-group-by-functionality/</link>
      <pubDate>Sun, 17 Feb 2013 21:05:27 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/17/neo4jcypher-sql-style-group-by-functionality/</guid>
      <description>As I mentioned in a previous post I&#39;ve been playing around with some football related data over the last few days and one query I ran (using cypher) was to find all the players who&#39;ve been sent off this season in the Premiership.
The model in the graph around sending offs looks like this:
 My initial query looked like this:
START player = node:players(&#39;name:*&#39;) MATCH player-[:sent_off_in]-game-[:in_month]-month RETURN player.name, month.name  First we get the names of all the players which are stored in an index and then we follow relationships to the games they were sent off in and then find which months those games were played in.</description>
    </item>
    
    <item>
      <title>Data Science: Don&#39;t filter data prematurely</title>
      <link>http://mneedham.github.io/2013/02/17/data-science-dont-filter-data-prematurely/</link>
      <pubDate>Sun, 17 Feb 2013 20:02:31 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/17/data-science-dont-filter-data-prematurely/</guid>
      <description>Last year I wrote a post describing how I&#39;d gone about getting data for my ThoughtWorks graph and one mistake about my approach in retrospect is that I filtered the data too early.
My workflow looked like this:
 Scrape internal application using web driver and save useful data to JSON files Parse JSON files and load nodes/relationships into neo4j  The problem with the first step is that I was trying to determine up front what data was useful and as a result I ended up running the scrapping application multiple times when I realised I didn&#39;t have all the data I wanted.</description>
    </item>
    
    <item>
      <title>Regular Expressions: Non greedy matching</title>
      <link>http://mneedham.github.io/2013/02/16/regular-expressions-non-greedy-matching/</link>
      <pubDate>Sat, 16 Feb 2013 12:17:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/16/regular-expressions-non-greedy-matching/</guid>
      <description>I was playing around with some football data earlier in the week and I wanted to try and extract just the name &#39;Rooney&#39; from the following bit of text:
Rooney 8′, 27′  My initial regular expression was the following which annoyingly captures the time of the first goal:
&amp;gt; &amp;quot;Rooney 8′, 27′&amp;quot;.match(/(.*)\s\d(.*)/)[1] =&amp;gt; &amp;quot;Rooney 8,&amp;quot;  It works fine if the player has only scored one goal…
&amp;gt; &amp;quot;Rooney 8′&amp;quot;.</description>
    </item>
    
    <item>
      <title>Onboarding: Sketch the landscape</title>
      <link>http://mneedham.github.io/2013/02/15/onboarding-sketch-the-landscape/</link>
      <pubDate>Fri, 15 Feb 2013 07:36:06 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/15/onboarding-sketch-the-landscape/</guid>
      <description>For four months during 2012 I was working on the GDS infrastructure team and one of the first tasks that Gareth suggested I do was update a diagram showing how all the different applications and databases worked together.
I thought this was quite a strange thing to ask the &#39;new guy&#39; to do since I obviously knew nothing at all about how anything worked but he told me that was partly why he wanted me to do it.</description>
    </item>
    
    <item>
      <title>Feature Extraction/Selection - What I&#39;ve learnt so far</title>
      <link>http://mneedham.github.io/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</link>
      <pubDate>Sun, 10 Feb 2013 15:42:07 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</guid>
      <description>A couple of weeks ago I wrote about some feature extraction work that I&#39;d done on the Kaggle Digit Recognizer data set and having realised that I had no idea what I was doing I thought I should probably learn a bit more.
I came across Dunja Mladenic&#39;s &#39;Dimensionality Reduction by Feature Selection in Machine Learning&#39; presentation in which she sweeps across the landscape of feature selection and explains how everything fits together.</description>
    </item>
    
    <item>
      <title>R: Building up a data frame row by row</title>
      <link>http://mneedham.github.io/2013/02/10/r-building-up-a-data-frame-row-by-row/</link>
      <pubDate>Sun, 10 Feb 2013 13:29:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/10/r-building-up-a-data-frame-row-by-row/</guid>
      <description>Jen and I recently started working on the Kaggle Titanic problem and we thought it&#39;d probably be useful to start with some exploratory data analysis to get a feel for the data set.
For this problem you are given a selection of different features describing the passengers on board the Titanic and you have to predict whether or not they would have survived or died based on those features.
I thought an interesting first thing to look at would be the survival rate for passengers of different socio economic status as you would imagine that people of a higher status were more likely to have survived.</description>
    </item>
    
    <item>
      <title>R: Modelling a conversion rate with a binomial distribution</title>
      <link>http://mneedham.github.io/2013/02/07/r-modelling-a-conversion-rate-with-a-binomial-distribution/</link>
      <pubDate>Thu, 07 Feb 2013 01:26:12 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/07/r-modelling-a-conversion-rate-with-a-binomial-distribution/</guid>
      <description>As part of some work Sid and I were doing last week we wanted to simulate the conversion rate for an A/B testing we were planning.
We started with the following function which returns the simulated conversion rate for a given conversion rate of 12%:
generateConversionRates &amp;lt;- function(sampleSize) { sample_a &amp;lt;- rbinom(seq(0, sampleSize), 1, 0.12) conversion_a &amp;lt;- length(sample_a[sample_a == 1]) / sampleSize sample_b &amp;lt;- rbinom(seq(0, sampleSize), 1, 0.12) conversion_b &amp;lt;- length(sample_b[sample_b == 1]) / sampleSize c(conversion_a, conversion_b) }  If we call it:</description>
    </item>
    
    <item>
      <title>R: Mapping over a list of lists</title>
      <link>http://mneedham.github.io/2013/02/03/r-mapping-over-a-list-of-lists/</link>
      <pubDate>Sun, 03 Feb 2013 10:40:48 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/02/03/r-mapping-over-a-list-of-lists/</guid>
      <description>As part of the coursera Data Analysis course I had the following code to download and then read in a file:
&amp;gt; file &amp;lt;- &amp;quot;https://dl.dropbox.com/u/7710864/data/csv_hid/ss06hid.csv&amp;quot; &amp;gt; download.file(file, destfile=&amp;quot;americancommunity.csv&amp;quot;, method=&amp;quot;curl&amp;quot;) &amp;gt; acomm &amp;lt;- read.csv(&amp;quot;americancommunity.csv&amp;quot;)  We then had to filter the data based on the values in a couple of columns and work out how many rows were returned in each case:
&amp;gt; one &amp;lt;- acomm[acomm$RMS == 4 &amp;amp; !is.na(acomm$RMS) &amp;amp; acomm$BDS == 3 &amp;amp; !</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>http://mneedham.github.io/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I&amp;rsquo;ve written a few blog posts about our attempts at the Kaggle Digit Recogniser problem and one thing we haven&amp;rsquo;t yet tried is feature extraction.
Feature extraction in this context means that we&#39;d generate some other features to train a classifier with rather than relying on just the pixel values we were provided.
Every week Jen would try and persuade me that we should try it out but it wasn&#39;t until I was flicking through the notes from the Columbia Data Science class that it struck home:</description>
    </item>
    
    <item>
      <title>Levels of automation</title>
      <link>http://mneedham.github.io/2013/01/31/levels-of-automation/</link>
      <pubDate>Thu, 31 Jan 2013 22:36:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/31/levels-of-automation/</guid>
      <description>Over the last 18 months or so I&#39;ve worked on a variety of different projects in different organisations and seen some patterns around the way that automation was done which I thought would be interesting to document.
The approaches tend to fall into roughly three categories:
Predominantly Manual This tends to be less frequent these days as most developers have at some stage flicked through The Pragmatic Programmer and been persuaded that automating away boring and repetitive tasks is probably a good idea.</description>
    </item>
    
    <item>
      <title>Ruby: invalid multibyte char (US-ASCII)</title>
      <link>http://mneedham.github.io/2013/01/27/ruby-invalid-multibyte-char-us-ascii/</link>
      <pubDate>Sun, 27 Jan 2013 15:14:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/27/ruby-invalid-multibyte-char-us-ascii/</guid>
      <description>I&#39;ve used Ruby on and off for the last few years but somehow had never come across the following error which we got last week while attempting to print out a currency value:
blah.ruby
amount = &amp;quot;£10.00&amp;quot; puts amount  $ ruby blah.ruby blah.ruby:1: invalid multibyte char (US-ASCII) blah.ruby:1: invalid multibyte char (US-ASCII)  Luckily my pair Jae had come across this before and showed me a blog post which explains what&#39;s going on and how to sort it out.</description>
    </item>
    
    <item>
      <title>A/B Testing: Thoughts so far</title>
      <link>http://mneedham.github.io/2013/01/27/ab-testing-thoughts-so-far/</link>
      <pubDate>Sun, 27 Jan 2013 13:27:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/27/ab-testing-thoughts-so-far/</guid>
      <description>I&#39;ve been working at uSwitch for about two months now and for the majority of that time have been working on an A/B test we were running to try and make it easier for users to go through the energy comparison process.
I found the &#39;Practical Guide to Controlled Experiments on the Web&#39; paper useful for explaining how to go about doing an A/B test and there&#39;s also an interesting presentation by Dan McKinley about how etsy do A/B testing.</description>
    </item>
    
    <item>
      <title>Python: (Conceptually) removing an item from a tuple</title>
      <link>http://mneedham.github.io/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</link>
      <pubDate>Sun, 27 Jan 2013 02:30:05 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</guid>
      <description>As part of some code I&#39;ve been playing around I wanted to remove an item from a tuple which wasn&#39;t particularly easy because Python&#39;s tuple data structure is immutable.
I therefore needed to create a new tuple excluding the value which I wanted to remove.
I ended up writing the following function to do this but I imagine there might be an easier way because it&#39;s quite verbose:
def tuple_without(original_tuple, element_to_remove): new_tuple = [] for s in list(original_tuple): if not s == element_to_remove: new_tuple.</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting values by multiple indices</title>
      <link>http://mneedham.github.io/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</link>
      <pubDate>Sun, 27 Jan 2013 02:21:39 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</guid>
      <description>As I mentioned in my previous post I&#39;ve been playing around with numpy and I wanted to get the values of a collection of different indices in a 2D array.
If we had a 2D array that looked like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])  I knew that it was possible to retrieve the first 3 rows by using the following code:</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting specific column in 2D array</title>
      <link>http://mneedham.github.io/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</link>
      <pubDate>Sun, 27 Jan 2013 02:10:10 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</guid>
      <description>I&#39;ve been playing around with numpy this evening in an attempt to improve the performance of a Travelling Salesman Problem implementation and I wanted to get every value in a specific column of a 2D array.
The array looked something like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])  I wanted to get the values for the 2nd column of each row which would return an array containing 1, 6, 11 and 16.</description>
    </item>
    
    <item>
      <title>R: Ordering rows in a data frame by multiple columns</title>
      <link>http://mneedham.github.io/2013/01/23/r-ordering-rows-in-a-data-frame-by-multiple-columns/</link>
      <pubDate>Wed, 23 Jan 2013 23:09:28 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/23/r-ordering-rows-in-a-data-frame-by-multiple-columns/</guid>
      <description>In one of the assignments of Computing for Data Analysis we needed to sort a data frame based on the values in two of the columns and then return the top value.
The initial data frame looked a bit like this:
&amp;gt; names &amp;lt;- c(&amp;quot;paul&amp;quot;, &amp;quot;mark&amp;quot;, &amp;quot;dave&amp;quot;, &amp;quot;will&amp;quot;, &amp;quot;john&amp;quot;) &amp;gt; values &amp;lt;- c(1,4,1,2,1) &amp;gt; smallData &amp;lt;- data.frame(name = names, value = values) &amp;gt; smallData name value 1 paul 1 2 mark 4 3 dave 1 4 will 2 5 john 1  I want to be able to sort the data frame by value and name both in ascending order so the final result should look like this:</description>
    </item>
    
    <item>
      <title>R: Filter a data frame based on values in two columns</title>
      <link>http://mneedham.github.io/2013/01/23/r-filter-a-data-frame-based-on-values-in-two-columns/</link>
      <pubDate>Wed, 23 Jan 2013 22:34:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/23/r-filter-a-data-frame-based-on-values-in-two-columns/</guid>
      <description>In the most recent assignment of the Computing for Data Analysis course we had to filter a data frame which contained N/A values in two columns to only return rows which had no N/A&#39;s.
I started with a data frame that looked like this:
&amp;gt; data &amp;lt;- read.csv(&amp;quot;specdata/002.csv&amp;quot;) &amp;gt; # we&#39;ll just use a few rows to make it easier to see what&#39;s going on &amp;gt; data[2494:2500,] Date sulfate nitrate ID 2494 2007-10-30 3.</description>
    </item>
    
    <item>
      <title>Bellman-Ford algorithm in Python using vectorisation/numpy</title>
      <link>http://mneedham.github.io/2013/01/20/bellman-ford-algorithm-in-python-using-vectorisationnumpy/</link>
      <pubDate>Sun, 20 Jan 2013 19:14:08 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/20/bellman-ford-algorithm-in-python-using-vectorisationnumpy/</guid>
      <description>I recently wrote about an implementation of the Bellman Ford shortest path algorithm and concluded by saying that it took 27 seconds to calculate the shortest path in the graph for any node.
This seemed a bit slow and while browsing the Coursera forums I came across a suggestion that the algorithm would run much more quickly if we used vectorization with numpy rather than nested for loops.
Vectorisation refers to a problem solving approach where we make use of matrices operations which is what numpy allows us to do.</description>
    </item>
    
    <item>
      <title>telnet/netcat: Waiting for a port to be open</title>
      <link>http://mneedham.github.io/2013/01/20/waiting-for-a-port-to-be-open/</link>
      <pubDate>Sun, 20 Jan 2013 15:53:02 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/20/waiting-for-a-port-to-be-open/</guid>
      <description>On Friday Nathan and I were setting up a new virtual machine and we needed a firewall rule to be created to allow us to connect to another machine which had some JAR files we wanted to download.
We wanted to know when it had been done by one of our operations team and I initially thought we might be able to do that using telnet:
$ telnet 10.0.0.1 8081 Trying 10.</description>
    </item>
    
    <item>
      <title>Bellman-Ford algorithm in Python</title>
      <link>http://mneedham.github.io/2013/01/18/bellman-ford-algorithm-in-python/</link>
      <pubDate>Fri, 18 Jan 2013 00:40:32 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/18/bellman-ford-algorithm-in-python/</guid>
      <description>The latest problem of the Algorithms 2 class required us to write an algorithm to calculate the shortest path between two nodes on a graph and one algorithm which allows us to do this is Bellman-Ford.
Bellman-Ford computes the single source shortest path which means that if we have a 5 vertex graph we&#39;d need to run it 5 times to find the shortest path for each vertex and then find the shortest paths of those shortest paths.</description>
    </item>
    
    <item>
      <title>Fabric/Boto: boto.exception.NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. [&#39;QuerySignatureV2AuthHandler&#39;] Check your credentials</title>
      <link>http://mneedham.github.io/2013/01/15/fabricboto-boto-exception-noauthhandlerfound-no-handler-was-ready-to-authenticate-1-handlers-were-checked-querysignaturev2authhandler-check-your-credentials/</link>
      <pubDate>Tue, 15 Jan 2013 00:37:01 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/15/fabricboto-boto-exception-noauthhandlerfound-no-handler-was-ready-to-authenticate-1-handlers-were-checked-querysignaturev2authhandler-check-your-credentials/</guid>
      <description>In our Fabric code we make use of Boto to connect to the EC2 API and pull back various bits of information and the first time anyone tries to use it they end up with the following stack trace:
File &amp;quot;/Library/Python/2.7/site-packages/fabric/main.py&amp;quot;, line 717, in main *args, **kwargs File &amp;quot;/Library/Python/2.7/site-packages/fabric/tasks.py&amp;quot;, line 332, in execute results[&#39;&amp;lt;local-only&amp;gt;&#39;] = task.run(*args, **new_kwargs) File &amp;quot;/Library/Python/2.7/site-packages/fabric/tasks.py&amp;quot;, line 112, in run return self.wrapped(*args, **kwargs) File &amp;quot;/Users/mark/projects/forward-puppet/ec2.py&amp;quot;, line 131, in running instances = instances_by_zones(running_instances(region, role_name)) File &amp;quot;/Users/mark/projects/forward-puppet/ec2.</description>
    </item>
    
    <item>
      <title>Fabric: Tailing log files on multiple machines</title>
      <link>http://mneedham.github.io/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</link>
      <pubDate>Tue, 15 Jan 2013 00:20:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</guid>
      <description>We wanted to tail one of the log files simultaneously on 12 servers this afternoon to try and see if a particular event was being logged and rather than opening 12 SSH sessions decided to get Fabric to help us out.
My initial attempt to do this was the following:
fab -H host1,host2,host3 -- tail -f /var/www/awesome/current/log/production.log  It works but the problem is that by default Fabric runs the specified command one machine after the other so we&#39;ve actually managed to block Fabric with the tail command on &#39;host1&#39;.</description>
    </item>
    
    <item>
      <title>Clojure: Reading and writing a reasonably sized file</title>
      <link>http://mneedham.github.io/2013/01/11/clojure-reading-and-writing-a-reasonably-sized-file/</link>
      <pubDate>Fri, 11 Jan 2013 00:40:49 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/11/clojure-reading-and-writing-a-reasonably-sized-file/</guid>
      <description>In a post a couple of days ago I described some code I&#39;d written in R to find out all the features with zero variance in the Kaggle Digit Recognizer data set and yesterday I started working on some code to remove those features.
Jen and I had previously written some code to parse the training data in Clojure so I thought I&#39;d try and adapt that to write out a new file without the unwanted pixels.</description>
    </item>
    
    <item>
      <title>Knapsack Problem in Haskell</title>
      <link>http://mneedham.github.io/2013/01/09/knapsack-problem-in-haskell/</link>
      <pubDate>Wed, 09 Jan 2013 00:12:25 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/09/knapsack-problem-in-haskell/</guid>
      <description>I recently described two versions of the Knapsack problem written in Ruby and Python and one common thing is that I used a global cache to store the results of previous calculations.
From my experience of coding in Haskell it&#39;s not considered very idiomatic to write code like that and although I haven&#39;t actually tried it, potentially more tricky to achieve.
I thought it&#39;d be interesting to try and write the algorithm in Haskell with that constraint in mind and my first version looked like this:</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>http://mneedham.github.io/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>I&#39;ve written previously about our attempts at the Kaggle Digit Recogniser problem and our approach so far has been to use the data provided and plug it into different algorithms and see what we end up with.
From browsing through the forums we saw others mentioning feature extraction - an approach where we transform the data into another format , the thinking being that we can train a better classifier with better data.</description>
    </item>
    
    <item>
      <title>Knapsack Problem: Python vs Ruby</title>
      <link>http://mneedham.github.io/2013/01/07/knapsack-problem-python-vs-ruby/</link>
      <pubDate>Mon, 07 Jan 2013 00:47:34 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/07/knapsack-problem-python-vs-ruby/</guid>
      <description>The latest algorithm that we had to code in Algorithms 2 was the Knapsack problem which is as follows:
 The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.</description>
    </item>
    
    <item>
      <title>A new year&#39;s idea: Share what you learn</title>
      <link>http://mneedham.github.io/2013/01/05/a-new-years-idea-share-what-you-learn/</link>
      <pubDate>Sat, 05 Jan 2013 00:25:30 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/05/a-new-years-idea-share-what-you-learn/</guid>
      <description>Apologies in advance for how meta this post is.
About 4 1/2 years ago Jay Fields wrote a blog post where he encouraged people to write, present and contribute and outlined the advantages he&#39;d seen in his career from doing so. In hindsight the bit which stood out the most for me was the following paragraph:
 Don&#39;t know what to write about? The answers are all around you. Anything you do that&#39;s interesting, there&#39;s 100 people searching Google for how to do it.</description>
    </item>
    
    <item>
      <title>Haskell: Reading files</title>
      <link>http://mneedham.github.io/2013/01/02/haskell-reading-files/</link>
      <pubDate>Wed, 02 Jan 2013 00:16:50 +0000</pubDate>
      
      <guid>http://mneedham.github.io/2013/01/02/haskell-reading-files/</guid>
      <description>In writing the clustering algorithm which I&#39;ve mentioned way too many times already I needed to process a text file which contained all the points and my initial approach looked like this:
import System.IO main = do withFile &amp;quot;clustering2.txt&amp;quot; ReadMode (\handle -&amp;gt; do contents &amp;lt;- hGetContents handle putStrLn contents)  It felt a bit clunky but I didn&#39;t realise there was an easier way until I came across this thread. We can simplify reading a file to the following by using the readFile function:</description>
    </item>
    
  </channel>
</rss>