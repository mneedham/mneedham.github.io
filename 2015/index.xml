<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2015s on Mark Needham</title>
    <link>https://mneedham.github.io/2015/</link>
    <description>Recent content in 2015s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Dec 2015 13:58:39 +0000</lastBuildDate>
    
	<atom:link href="https://mneedham.github.io/2015/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>2015: A year in the life of the Neo4j London meetup group</title>
      <link>https://mneedham.github.io/2015/12/31/2015-a-year-in-the-life-of-the-neo4j-london-meetup-group/</link>
      <pubDate>Thu, 31 Dec 2015 13:58:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/31/2015-a-year-in-the-life-of-the-neo4j-london-meetup-group/</guid>
      <description>Given we&#39;ve only got a few more hours left of 2015 I thought it&#39;d be fun to do a quick overview of how things have been going in the London chapter of the Neo4j meetup using Neo4j with a bit of R mixed in. We&#39;re going to be using the RNeo4j library to interact with the database along with a few other libraries which will help us out with different tasks:</description>
    </item>
    
    <item>
      <title>Study until your mind wanders</title>
      <link>https://mneedham.github.io/2015/12/31/study-until-your-mind-wanders/</link>
      <pubDate>Thu, 31 Dec 2015 10:47:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/31/study-until-your-mind-wanders/</guid>
      <description>I&#39;ve previously found it very difficult to read math heavy content which has made it challenging to read Distributed Computing which I bought last May.  After several false starts where I gave up after getting frustrated that I couldn&#39;t understand things the first time around and forgot everything if I left it a couple of days I decided to try again with a different approach. I&#39;ve been trying a technique I learned from Mini Habits where every day I have a (very small) goal of reading one page of the book.</description>
    </item>
    
    <item>
      <title>R: Error in approxfun(x.values.1, y.values.1, method = &#34;constant&#34;, f = 1, :  zero non-NA points</title>
      <link>https://mneedham.github.io/2015/12/27/r-error-in-approxfunx-values-1-y-values-1-method-constant-f-1-zero-non-na-points/</link>
      <pubDate>Sun, 27 Dec 2015 12:24:05 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/27/r-error-in-approxfunx-values-1-y-values-1-method-constant-f-1-zero-non-na-points/</guid>
      <description>I&#39;ve been following Michy Alice&#39;s logistic regression tutorial to create an attendance model for London dev meetups and ran into an interesting problem while doing so.  Our dataset has a class imbalance i.e. most people RSVP &#39;no&#39; to events which can lead to misleading accuracy score where predicting &#39;no&#39; every time would lead to supposed high accuracy. Source: local data frame [2 x 2] attended n (dbl) (int) 1 0 1541 2 1 53  I sampled the data using caret&#39;s upSample function to avoid this: attended = as.</description>
    </item>
    
    <item>
      <title>Python: Squashing &#39;duplicate&#39; pairs together</title>
      <link>https://mneedham.github.io/2015/12/20/python-squashing-duplicate-pairs-together/</link>
      <pubDate>Sun, 20 Dec 2015 12:12:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/20/python-squashing-duplicate-pairs-together/</guid>
      <description>As part of a data cleaning pipeline I had pairs of ids of duplicate addresses that I wanted to group together.
I couldn&#39;t work out how to solve the problem immediately so I simplified the problem into pairs of letters i.e. A	B	(A is the same as B) B	C	(B is the same as C) C	D	... E	F	(E is the same as F) F	G	.</description>
    </item>
    
    <item>
      <title>Neo4j: Specific relationship vs Generic relationship &#43; property</title>
      <link>https://mneedham.github.io/2015/12/13/neo4j-specific-relationship-vs-generic-relationship-property/</link>
      <pubDate>Sun, 13 Dec 2015 21:22:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/13/neo4j-specific-relationship-vs-generic-relationship-property/</guid>
      <description>For optimal traversal speed in Neo4j queries we should make our relationship types as specific as possible.  Let&#39;s take a look at an example from the &#39;modelling a recommendations engine&#39; talk I presented at Skillsmatter a couple of weeks ago.  I needed to decided how to model the &#39;RSVP&#39; relationship between a Member and an Event. A person can RSVP &#39;yes&#39; or &#39;no&#39; to an event and I&#39;d like to capture both of these responses.</description>
    </item>
    
    <item>
      <title>Neo4j: Facts as nodes</title>
      <link>https://mneedham.github.io/2015/12/04/neo4j-facts-as-nodes/</link>
      <pubDate>Fri, 04 Dec 2015 07:52:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/12/04/neo4j-facts-as-nodes/</guid>
      <description>On Tuesday I spoke at the Neo4j London user group about incrementally building a recommendation engine and described the &#39;facts as nodes&#39; modeling pattern, defined as follows in the Graph Databases book:  When two or more domain entities interact for a period of time, a fact emerges. We represent a fact as a separate node with connections to each of the entities engaged in that fact. Modeling an action in terms of its product—that is, in terms of the thing that results from the action—produces a similar structure: an intermediate node that represents the outcome of an interaction between two or more entities.</description>
    </item>
    
    <item>
      <title>Python: Parsing a JSON HTTP chunking stream</title>
      <link>https://mneedham.github.io/2015/11/28/python-parsing-a-json-http-chunking-stream/</link>
      <pubDate>Sat, 28 Nov 2015 13:56:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/11/28/python-parsing-a-json-http-chunking-stream/</guid>
      <description>I&#39;ve been playing around with meetup.com&#39;s API again and this time wanted to consume the chunked HTTP RSVP stream and filter RSVPs for events I&#39;m interested in.  I use Python for most of my hacking these days and if HTTP requests are required the requests library is my first port of call. I started out with the following script import requests import json def stream_meetup_initial(): uri = &amp;quot;http://stream.meetup.com/2/rsvps&amp;quot; response = requests.</description>
    </item>
    
    <item>
      <title>jq: Cannot iterate over number / string and number cannot be added</title>
      <link>https://mneedham.github.io/2015/11/24/jq-cannot-iterate-over-number-string-and-number-cannot-be-added/</link>
      <pubDate>Tue, 24 Nov 2015 00:12:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/11/24/jq-cannot-iterate-over-number-string-and-number-cannot-be-added/</guid>
      <description>In my continued parsing of meetup.com&#39;s JSON API I wanted to extract some information from the following JSON file:
$ head -n40 data/members/18313232.json [ { &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;London&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;. .&amp;quot;, &amp;quot;other_services&amp;quot;: {}, &amp;quot;country&amp;quot;: &amp;quot;gb&amp;quot;, &amp;quot;topics&amp;quot;: [], &amp;quot;lon&amp;quot;: -0.13, &amp;quot;joined&amp;quot;: 1438866605000, &amp;quot;id&amp;quot;: 92951932, &amp;quot;state&amp;quot;: &amp;quot;17&amp;quot;, &amp;quot;link&amp;quot;: &amp;quot;http://www.meetup.com/members/92951932&amp;quot;, &amp;quot;photo&amp;quot;: { &amp;quot;thumb_link&amp;quot;: &amp;quot;http://photos1.meetupstatic.com/photos/member/8/d/6/b/thumb_250896203.jpeg&amp;quot;, &amp;quot;photo_id&amp;quot;: 250896203, &amp;quot;highres_link&amp;quot;: &amp;quot;http://photos1.meetupstatic.com/photos/member/8/d/6/b/highres_250896203.jpeg&amp;quot;, &amp;quot;photo_link&amp;quot;: &amp;quot;http://photos1.meetupstatic.com/photos/member/8/d/6/b/member_250896203.jpeg&amp;quot; }, &amp;quot;lat&amp;quot;: 51.49, &amp;quot;visited&amp;quot;: 1446745707000, &amp;quot;self&amp;quot;: { &amp;quot;common&amp;quot;: {} } }, { &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;London&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Abdelkader Idryssy&amp;quot;, &amp;quot;other_services&amp;quot;: {}, &amp;quot;country&amp;quot;: &amp;quot;gb&amp;quot;, &amp;quot;topics&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;Weekend Adventures&amp;quot;, &amp;quot;urlkey&amp;quot;: &amp;quot;weekend-adventures&amp;quot;, &amp;quot;id&amp;quot;: 16438 }, { &amp;quot;name&amp;quot;: &amp;quot;Community Building&amp;quot;, &amp;quot;urlkey&amp;quot;: &amp;quot;community-building&amp;quot;, ~~~	&amp;lt;p&amp;gt; In particular I want to extract the member&#39;s id, name, join date and the ids of topics they&#39;re interested in.</description>
    </item>
    
    <item>
      <title>jq: Filtering missing keys</title>
      <link>https://mneedham.github.io/2015/11/14/jq-filtering-missing-keys/</link>
      <pubDate>Sat, 14 Nov 2015 22:51:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/11/14/jq-filtering-missing-keys/</guid>
      <description>I&#39;ve been playing around with the meetup.com API again over the last few days and having saved a set of events to disk I wanted to extract the venues using jq.  This is what a single event record looks like: $ jq -r &amp;quot;.[0]&amp;quot; data/events/0.json { &amp;quot;status&amp;quot;: &amp;quot;past&amp;quot;, &amp;quot;rating&amp;quot;: { &amp;quot;count&amp;quot;: 1, &amp;quot;average&amp;quot;: 1 }, &amp;quot;utc_offset&amp;quot;: 3600000, &amp;quot;event_url&amp;quot;: &amp;quot;http://www.meetup.com/londonweb/events/3261890/&amp;quot;, &amp;quot;group&amp;quot;: { &amp;quot;who&amp;quot;: &amp;quot;Web Peeps&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;London Web&amp;quot;, &amp;quot;group_lat&amp;quot;: 51.</description>
    </item>
    
    <item>
      <title>Docker 1.9: Port forwarding on Mac OS X</title>
      <link>https://mneedham.github.io/2015/11/08/docker-1-9-port-forwarding-on-mac-os-x/</link>
      <pubDate>Sun, 08 Nov 2015 20:58:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/11/08/docker-1-9-port-forwarding-on-mac-os-x/</guid>
      <description>Since the Neo4j 2.3.0 release there&#39;s been an official docker image which I thought I&#39;d give a try this afternoon.  The last time I used docker about a year ago I had to install boot2docker which has now been deprecated in place of Docker Machine and the Docker Toolbox.  I created a container with the following command: docker run --detach --publish=7474:7474 neo4j/neo4j   And then tried to access the Neo4j server locally: $ curl http://localhost:7474 curl: (7) Failed to connect to localhost port 7474: Connection refused  I quickly checked that docker had started up Neo4j correctly:</description>
    </item>
    
    <item>
      <title>IntelliJ &#39;java: cannot find JDK 1.8&#39;</title>
      <link>https://mneedham.github.io/2015/11/08/intellij-java-cannot-find-jdk-1-8/</link>
      <pubDate>Sun, 08 Nov 2015 11:47:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/11/08/intellij-java-cannot-find-jdk-1-8/</guid>
      <description>I upgraded to IntelliJ 15.0 a few days ago and was initially seeing the following exception when trying to compile: module-name java: cannot find JDK 1.8   I&#39;ve been compiling against JDK 1.8 for a while now using IntelliJ 14 so I wasn&#39;t sure what was going on. I checked my project settings and they seemed fine:
   The error message suggested I look in the logs to find more information but I wasn&#39;t sure where those live!</description>
    </item>
    
    <item>
      <title>Hadoop: HDFS - ava.lang.NoSuchMethodError: org.apache.hadoop.fs.FSOutputSummer.&lt;init&gt;(Ljava/util/zip/Checksum;II)V</title>
      <link>https://mneedham.github.io/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</link>
      <pubDate>Sat, 31 Oct 2015 23:58:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</guid>
      <description>I wanted to write a little program to check that one machine could communicate a HDFS server running on the other and adapted some code from the Hadoop wiki as follows:
package org.playground; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; public class HadoopDFSFileReadWrite { static void printAndExit(String str) { System.err.println( str ); System.exit(1); } public static void main (String[] argv) throws IOException { Configuration conf = new Configuration(); conf.</description>
    </item>
    
    <item>
      <title>Spark: MatchError (of class org.apache.spark.sql.catalyst.expressions.GenericRow) spark</title>
      <link>https://mneedham.github.io/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</link>
      <pubDate>Tue, 27 Oct 2015 23:10:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</guid>
      <description>I&#39;ve been using Spark again lately to do some pre-processing on the Land Registry data set and ran into an initially confusing problem when trying to parse the CSV file.  I&#39;m using the Databricks CSV parsing library and wrote the following script to go over each row, collect up the address components and then derive a &#39;fullAddress&#39; field.  To refresh, this is what the CSV file looks like: $ head -n5 pp-complete.</description>
    </item>
    
    <item>
      <title>Exploring (potential) data entry errors in the Land Registry data set</title>
      <link>https://mneedham.github.io/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</link>
      <pubDate>Sun, 18 Oct 2015 10:03:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</guid>
      <description>I&#39;ve previously written a couple of blog posts describing the mechanics of analysing the Land Registry data set and I thought it was about time I described some of the queries I&#39;ve been running the discoveries I&#39;ve made. To recap, the land registry provides a 3GB, 20 million line CSV file containing all the property sales in the UK since 1995. We&#39;ll be loading and query the data in R using the data.</description>
    </item>
    
    <item>
      <title>jq: error - Cannot iterate over null (null)</title>
      <link>https://mneedham.github.io/2015/10/09/jq-error-cannot-iterate-over-null-null/</link>
      <pubDate>Fri, 09 Oct 2015 06:34:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/09/jq-error-cannot-iterate-over-null-null/</guid>
      <description>I&#39;ve been playing around with the jq library again over the past couple of days to convert the JSON from the Stack Overflow API into CSV and found myself needing to deal with an optional field.  I&#39;ve downloaded 100 or so questions and stored them as an array in a JSON array like so: $ head -n 100 so.json [ { &amp;quot;has_more&amp;quot;: true, &amp;quot;items&amp;quot;: [ { &amp;quot;is_answered&amp;quot;: false, &amp;quot;delete_vote_count&amp;quot;: 0, &amp;quot;body_markdown&amp;quot;: &amp;quot;.</description>
    </item>
    
    <item>
      <title>Mac OS X: Installing the PROJ.4 - Cartographic Projections Library</title>
      <link>https://mneedham.github.io/2015/10/05/mac-os-x-installing-the-proj-4-cartographic-projections-library/</link>
      <pubDate>Mon, 05 Oct 2015 22:41:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/05/mac-os-x-installing-the-proj-4-cartographic-projections-library/</guid>
      <description>I&#39;ve been following Scott Barnham&#39;s guide to transforming UK postcodes into (lat, long) coordinates and needed to install the PROJ.4 Cartographic Projections library which I initially struggled with.  The first step is to download a tar.gz version which is linked from the wiki page: $ wget http://download.osgeo.org/proj/proj-4.9.1.tar.gz   Next we&#39;ll unpack the file and then build the binaries: $ tar -xvf proj-4.9.1.tar.gz $ cd proj-4.9.1 $ ./configure --prefix ~/projects/land-registry/proj-4.</description>
    </item>
    
    <item>
      <title>R: data.table - Finding the maximum row</title>
      <link>https://mneedham.github.io/2015/10/02/r-data-table-finding-the-maximum-row/</link>
      <pubDate>Fri, 02 Oct 2015 18:42:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/10/02/r-data-table-finding-the-maximum-row/</guid>
      <description>In my continued playing around with the R data.table package I wanted to find the maximum row based on one of the columns, grouped by another column, and then return back the whole row. We&#39;ll use the following data table to illustrate:
&amp;gt; blogDT = data.table(name = c(&amp;quot;Property 1&amp;quot;,&amp;quot;Property 1&amp;quot;,&amp;quot;Property 1&amp;quot;,&amp;quot;Property 2&amp;quot;,&amp;quot;Property 2&amp;quot;,&amp;quot;Property 2&amp;quot;), price = c(10000, 12500, 18000, 245000, 512000, 1000000), date = c(&amp;quot;Day 1&amp;quot;, &amp;quot;Day 7&amp;quot;, &amp;quot;Day 10&amp;quot;, &amp;quot;Day 3&amp;quot;, &amp;quot;Day 5&amp;quot;, &amp;quot;Day 12&amp;quot;)) &amp;gt; blogDT[, lag.</description>
    </item>
    
    <item>
      <title>IntelliJ 14.1.5: Unable to import maven project</title>
      <link>https://mneedham.github.io/2015/09/30/intellij-14-1-5-unable-to-import-maven-project/</link>
      <pubDate>Wed, 30 Sep 2015 05:54:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/09/30/intellij-14-1-5-unable-to-import-maven-project/</guid>
      <description>After a recent IntelliJ upgrade I&#39;ve been running into the following error when trying to attach the sources of any library being pulled in via Maven:  Unable to import maven project  It seems like this is a recent issue in the 14.x series and luckily is reasonably easy to fix by adding the following flag to the VM options passed to the Maven importer:
-Didea.maven3.use.compat.resolver  And this is where you need to add it:</description>
    </item>
    
    <item>
      <title>R: data.table - Comparing adjacent rows</title>
      <link>https://mneedham.github.io/2015/09/27/r-data-table-comparing-adjacent-rows/</link>
      <pubDate>Sun, 27 Sep 2015 22:02:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/09/27/r-data-table-comparing-adjacent-rows/</guid>
      <description>As part of my exploration of the Land Registry price paid data set I wanted to compare the difference between consecutive sales of properties.  This means we need to group the sales by a property identifier and then get the previous sale price into a column on each row unless it&#39;s the first sale in which case we&#39;ll have &#39;NA&#39;. We can do this by creating a lag variable.</description>
    </item>
    
    <item>
      <title>R: Querying a 20 million line CSV file - data.table vs data frame</title>
      <link>https://mneedham.github.io/2015/09/25/r-querying-a-20-million-line-csv-file-data-table-vs-data-frame/</link>
      <pubDate>Fri, 25 Sep 2015 06:28:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/09/25/r-querying-a-20-million-line-csv-file-data-table-vs-data-frame/</guid>
      <description>As I mentioned in a couple of blog posts already, I&#39;ve been exploring the Land Registry price paid data set and although I&#39;ve initially been using SparkR I was curious how easy it would be to explore the data set using plain R.  I thought I&#39;d start out by loading the data into a data frame and run the same queries using deployer.  I&#39;ve come across Hadley Wickham&#39;s readr library before but hadn&#39;t used it and since I needed to load a 20 million line CSV file this seemed the perfect time to give it a try.</description>
    </item>
    
    <item>
      <title>SparkR: Add new column to data frame by concatenating other columns</title>
      <link>https://mneedham.github.io/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</link>
      <pubDate>Mon, 21 Sep 2015 22:30:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</guid>
      <description>Continuing with my exploration of the Land Registry open data set using SparkR I wanted to see which road in the UK has had the most property sales over the last 20 years. To recap, this is what the data frame looks like:
./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0 &amp;gt; sales &amp;lt;- read.df(sqlContext, &amp;quot;pp-complete.csv&amp;quot;, &amp;quot;com.databricks.spark.csv&amp;quot;, header=&amp;quot;false&amp;quot;) &amp;gt; head(sales) C0 C1 C2 C3 C4 C5 1 {0C7ADEF5-878D-4066-B785-0000003ED74A} 163000 2003-02-21 00:00 UB5 4PJ T N 2 {35F67271-ABD4-40DA-AB09-00000085B9D3} 247500 2005-07-15 00:00 TA19 9DD D N 3 {B20B1C74-E8E1-4137-AB3E-0000011DF342} 320000 2010-09-10 00:00 W4 1DZ F N 4 {7D6B0915-C56B-4275-AF9B-00000156BCE7} 104000 1997-08-27 00:00 NE61 2BH D N 5 {47B60101-B64C-413D-8F60-000002F1692D} 147995 2003-05-02 00:00 PE33 0RU D N 6 {51F797CA-7BEB-4958-821F-000003E464AE} 110000 2013-03-22 00:00 NR35 2SF T N C6 C7 C8 C9 C10 C11 1 F 106 READING ROAD NORTHOLT NORTHOLT 2 F 58 ADAMS MEADOW ILMINSTER ILMINSTER 3 L 58 WHELLOCK ROAD LONDON 4 F 17 WESTGATE MORPETH MORPETH 5 F 4 MASON GARDENS WEST WINCH KING&#39;S LYNN 6 F 5 WILD FLOWER WAY DITCHINGHAM BUNGAY C12 C13 C14 1 EALING GREATER LONDON A 2 SOUTH SOMERSET SOMERSET A 3 EALING GREATER LONDON A 4 CASTLE MORPETH NORTHUMBERLAND A 5 KING&#39;S LYNN AND WEST NORFOLK NORFOLK A 6 SOUTH NORFOLK NORFOLK A   This document explains the data stored in each field and for this particular query we&#39;re interested in fields C9-C12.</description>
    </item>
    
    <item>
      <title>SparkR: Error in invokeJava(isStatic = TRUE, className, methodName, ...) :  java.lang.ClassNotFoundException: Failed to load class for data source: csv.</title>
      <link>https://mneedham.github.io/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</link>
      <pubDate>Mon, 21 Sep 2015 22:06:44 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</guid>
      <description>I&#39;ve been wanting to play around with SparkR for a while and over the weekend deciding to explore a large Land Registry CSV file containing all the sales of properties in the UK over the last 20 years. First I started up the SparkR shell with the CSV package loaded in:
./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0   Next I tried to read the CSV file into a Spark data frame by modifying one of the examples from the tutorial: &amp;gt; sales &amp;lt;- read.</description>
    </item>
    
    <item>
      <title>Neo4j: Summarising neo4j-shell output</title>
      <link>https://mneedham.github.io/2015/08/21/neo4j-summarising-neo4j-shell-output/</link>
      <pubDate>Fri, 21 Aug 2015 20:59:37 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/21/neo4j-summarising-neo4j-shell-output/</guid>
      <description>I frequently find myself trying to optimise a set of cypher queries and I tend to group them together in a script that I fed to the Neo4j shell.  When tweaking the queries it&#39;s easy to make a mistake and end up not creating the same data so I decided to write a script which will show me the aggregates of all the commands executed.  I want to see the number of constraints created, indexes added, nodes, relationships and properties created.</description>
    </item>
    
    <item>
      <title>Python: Extracting Excel spreadsheet into CSV files</title>
      <link>https://mneedham.github.io/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</guid>
      <description>I&#39;ve been playing around with the Road Safety open data set and the download comes with several CSV files and an excel spreadsheet containing the legend.  There are 45 sheets in total and each of them looks like this:   I wanted to create a CSV file for each sheet so that I can import the data set into Neo4j using the LOAD CSV command. I came across the Python Excel website which pointed me at the xlrd library since I&#39;m working with a pre 2010 Excel file.</description>
    </item>
    
    <item>
      <title>Unix: Stripping first n bytes in a file / Byte Order Mark (BOM)</title>
      <link>https://mneedham.github.io/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</guid>
      <description>I&#39;ve previously written a couple of blog posts showing how to strip out the byte order mark (BOM) from CSV files to make loading them into Neo4j easier and today I came across another way to clean up the file using tail.  The BOM is 3 bytes long at the beginning of the file so if we know that a file contains it then we can strip out those first 3 bytes tail like this: $ time tail -c +4 Casualty7904.</description>
    </item>
    
    <item>
      <title>Unix: Redirecting stderr to stdout</title>
      <link>https://mneedham.github.io/2015/08/15/unix-redirecting-stderr-to-stdout/</link>
      <pubDate>Sat, 15 Aug 2015 15:55:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/15/unix-redirecting-stderr-to-stdout/</guid>
      <description>I&#39;ve been trying to optimise some Neo4j import queries over the last couple of days and as part of the script I&#39;ve been executed I wanted to redirect the output of a couple of commands into a file to parse afterwards.  I started with the following script which doesn&#39;t do any explicit redirection of the output: #!/bin/sh ./neo4j-community-2.2.3/bin/neo4j start  Now let&#39;s run that script and redirect the output to a file:</description>
    </item>
    
    <item>
      <title>Sed: Using environment variables</title>
      <link>https://mneedham.github.io/2015/08/13/sed-using-environment-variables/</link>
      <pubDate>Thu, 13 Aug 2015 19:30:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/13/sed-using-environment-variables/</guid>
      <description>I&#39;ve been playing around with the BBC football data set that I wrote about a couple of months ago and I wanted to write some code that would take the import script and replace all instances of remote URIs with a file system path.  For example the import file contains several lines similar to this: LOAD CSV WITH HEADERS FROM &amp;quot;https://raw.githubusercontent.com/mneedham/neo4j-bbc/master/data/matches.csv&amp;quot; AS row  And I want that to read:</description>
    </item>
    
    <item>
      <title>Java: Jersey - java.lang.NoSuchMethodError: com.sun.jersey.core.reflection.ReflectionHelper. getContextClassLoaderPA()Ljava/security/PrivilegedAction;</title>
      <link>https://mneedham.github.io/2015/08/11/java-jersey-java-lang-nosuchmethoderror-com-sun-jersey-core-reflection-reflectionhelper-getcontextclassloaderpaljavasecurityprivilegedaction/</link>
      <pubDate>Tue, 11 Aug 2015 06:59:50 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/11/java-jersey-java-lang-nosuchmethoderror-com-sun-jersey-core-reflection-reflectionhelper-getcontextclassloaderpaljavasecurityprivilegedaction/</guid>
      <description>I&#39;ve been trying to put some tests around an Neo4j unmanaged extension I&#39;ve been working on and ran into the following stack trace when launching the server using the Neo4j test harness: public class ExampleResourceTest { @Rule public Neo4jRule neo4j = new Neo4jRule() .withFixture(&amp;quot;CREATE (:Person {name: &#39;Mark&#39;})&amp;quot;) .withFixture(&amp;quot;CREATE (:Person {name: &#39;Nicole&#39;})&amp;quot;) .withExtension( &amp;quot;/unmanaged&amp;quot;, ExampleResource.class ); @Test public void shouldReturnAllTheNodes() { // Given URI serverURI = neo4j.httpURI(); // When HTTP.Response response = HTTP.</description>
    </item>
    
    <item>
      <title>Neo4j 2.2.3: Unmanaged extensions - Creating gzipped streamed responses with Jetty</title>
      <link>https://mneedham.github.io/2015/08/10/neo4j-2-2-3-unmanaged-extensions-creating-gzipped-streamed-responses-with-jetty/</link>
      <pubDate>Mon, 10 Aug 2015 23:57:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/10/neo4j-2-2-3-unmanaged-extensions-creating-gzipped-streamed-responses-with-jetty/</guid>
      <description>Back in 2013 I wrote a couple of blog posts showing examples of an unmanaged extension which had a streamed and gzipped response but two years on I realised they were a bit out of date and deserved a refresh.  When writing unmanaged extensions in Neo4j a good rule of thumb is to try and reduce the amount of objects you keep hanging around. In this context this means that we should stream our response to the client as quickly as possible rather than building it up in memory and sending it in one go.</description>
    </item>
    
    <item>
      <title>Record Linkage: Playing around with Duke</title>
      <link>https://mneedham.github.io/2015/08/08/record-linkage-playing-around-with-duke/</link>
      <pubDate>Sat, 08 Aug 2015 22:50:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/08/record-linkage-playing-around-with-duke/</guid>
      <description>I&#39;ve become quite interesting in record linkage recently and came across the Duke project which provides some tools to help solve this problem. I thought I&#39;d give it a try.
 The typical problem when doing record linkage is that we have two records from different data sets which represent the same entity but don&#39;t have a common key that we can use to merge them together. We therefore need to come up with a heuristic that will allow us to do so.</description>
    </item>
    
    <item>
      <title>Spark: Convert RDD to DataFrame</title>
      <link>https://mneedham.github.io/2015/08/06/spark-convert-rdd-to-dataframe/</link>
      <pubDate>Thu, 06 Aug 2015 21:11:44 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/06/spark-convert-rdd-to-dataframe/</guid>
      <description>As I mentioned in a previous blog post I&#39;ve been playing around with the Databricks Spark CSV library and wanted to take a CSV file, clean it up and then write out a new CSV file containing some of the columns.  I started by processing the CSV file and writing it into a temporary table: import org.apache.spark.sql.{SQLContext, Row, DataFrame} val sqlContext = new SQLContext(sc) val crimeFile = &amp;quot;Crimes_-_2001_to_present.csv&amp;quot; sqlContext.</description>
    </item>
    
    <item>
      <title>Spark: pyspark/Hadoop - py4j.protocol.Py4JJavaError: An error occurred while calling o23.load.: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>https://mneedham.github.io/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Tue, 04 Aug 2015 06:35:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>I&#39;ve been playing around with pyspark - Spark&#39;s Python library - and I wanted to execute the following job which takes a file from my local HDFS and then counts how many times each FBI code appears using Spark SQL: from pyspark import SparkContext from pyspark.sql import SQLContext sc = SparkContext(&amp;quot;local&amp;quot;, &amp;quot;Simple App&amp;quot;) sqlContext = SQLContext(sc) file = &amp;quot;hdfs://localhost:9000/user/markneedham/Crimes_-_2001_to_present.csv&amp;quot; sqlContext.load(source=&amp;quot;com.databricks.spark.csv&amp;quot;, header=&amp;quot;true&amp;quot;, path = file).registerTempTable(&amp;quot;crimes&amp;quot;) rows = sqlContext.sql(&amp;quot;select `FBI Code` AS fbiCode, COUNT(*) AS times FROM crimes GROUP BY `FBI Code` ORDER BY times DESC&amp;quot;).</description>
    </item>
    
    <item>
      <title>Spark: Processing CSV files using Databricks Spark CSV Library</title>
      <link>https://mneedham.github.io/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</link>
      <pubDate>Sun, 02 Aug 2015 18:08:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</guid>
      <description>Last year I wrote about exploring the Chicago crime data set using Spark and the OpenCSV parser and while this worked well, a few months ago I noticed that there&#39;s now a spark-csv library which I should probably use instead.  I thought it&#39;d be a fun exercise to translate my code to use it.  So to recap our goal: we want to count how many times each type of crime has been committed.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Removing consecutive duplicates</title>
      <link>https://mneedham.github.io/2015/07/30/neo4j-cypher-removing-consecutive-duplicates/</link>
      <pubDate>Thu, 30 Jul 2015 06:23:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/30/neo4j-cypher-removing-consecutive-duplicates/</guid>
      <description>When writing Cypher queries I sometimes find myself wanting to remove consecutive duplicates in collections that I&#39;ve joined together. e.g we might start with the following query where 1 and 7 appear consecutively:
RETURN [1,1,2,3,4,5,6,7,7,8] AS values ==&amp;gt; +-----------------------+ ==&amp;gt; | values | ==&amp;gt; +-----------------------+ ==&amp;gt; | [1,1,2,3,4,5,6,7,7,8] | ==&amp;gt; +-----------------------+ ==&amp;gt; 1 row   We want to end up with [1,2,3,4,5,6,7,8]. We can start by exploding our array and putting consecutive elements next to each other: WITH [1,1,2,3,4,5,6,7,7,8] AS values UNWIND RANGE(0, LENGTH(values) - 2) AS idx RETURN idx, idx+1, values[idx], values[idx+1] ==&amp;gt; +-------------------------------------------+ ==&amp;gt; | idx | idx+1 | values[idx] | values[idx+1] | ==&amp;gt; +-------------------------------------------+ ==&amp;gt; | 0 | 1 | 1 | 1 | ==&amp;gt; | 1 | 2 | 1 | 2 | ==&amp;gt; | 2 | 3 | 2 | 3 | ==&amp;gt; | 3 | 4 | 3 | 4 | ==&amp;gt; | 4 | 5 | 4 | 5 | ==&amp;gt; | 5 | 6 | 5 | 6 | ==&amp;gt; | 6 | 7 | 6 | 7 | ==&amp;gt; | 7 | 8 | 7 | 7 | ==&amp;gt; | 8 | 9 | 7 | 8 | ==&amp;gt; +-------------------------------------------+ ==&amp;gt; 9 rows  Next we can filter out rows which have the same values since that means they have consecutive duplicates:</description>
    </item>
    
    <item>
      <title>Neo4j: MERGE&#39;ing on super nodes</title>
      <link>https://mneedham.github.io/2015/07/28/neo4j-mergeing-on-super-nodes/</link>
      <pubDate>Tue, 28 Jul 2015 21:04:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/28/neo4j-mergeing-on-super-nodes/</guid>
      <description>In my continued playing with the Chicago crime data set I wanted to connect the crimes committed to their position in the FBI crime type hierarchy.  These are the sub graphs that I want to connect:   We have a &#39;fbiCode&#39; on each &#39;Crime&#39; node which indicates which &#39;Crime Sub Category&#39; the crime belongs to.
I started with the following query to connect the nodes together:
MATCH (crime:Crime) WITH crime SKIP {skip} LIMIT 10000 MATCH (subCat:SubCategory {code: crime.</description>
    </item>
    
    <item>
      <title>Python: Difference between two datetimes in milliseconds</title>
      <link>https://mneedham.github.io/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</link>
      <pubDate>Tue, 28 Jul 2015 20:05:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</guid>
      <description>I&#39;ve been doing a bit of adhoc measurement of some cypher queries executed via py2neo and wanted to work out how many milliseconds each query was taking end to end.   I thought there&#39;d be an obvious way of doing this but if there is it&#39;s evaded me so far and I ended up calculating the different between two datetime objects which gave me the following timedelta object: &amp;gt;&amp;gt;&amp;gt; import datetime &amp;gt;&amp;gt;&amp;gt; start = datetime.</description>
    </item>
    
    <item>
      <title>Neo4j: From JSON to CSV to LOAD CSV via jq</title>
      <link>https://mneedham.github.io/2015/07/25/neo4j-from-json-to-csv-to-load-csv-via-jq/</link>
      <pubDate>Sat, 25 Jul 2015 23:05:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/25/neo4j-from-json-to-csv-to-load-csv-via-jq/</guid>
      <description>In my last blog post I showed how to import a Chicago crime categories &amp;amp; sub categories JSON document using Neo4j&amp;rsquo;s cypher query language via the py2neo driver. While this is a good approach for people with a developer background, many of the users I encounter aren&amp;rsquo;t developers and favour using Cypher via the Neo4j browser. If we&#39;re going to do this we&#39;ll need to transform our JSON document into a CSV file so that we can use the LOAD CSV command on it.</description>
    </item>
    
    <item>
      <title>Neo4j: Loading JSON documents with Cypher</title>
      <link>https://mneedham.github.io/2015/07/23/neo4j-loading-json-documents-with-cypher/</link>
      <pubDate>Thu, 23 Jul 2015 06:15:11 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/23/neo4j-loading-json-documents-with-cypher/</guid>
      <description>One of the most commonly asked questions I get asked is how to load JSON documents into Neo4j and although Cypher doesn&#39;t have a &#39;LOAD JSON&#39; command we can still get JSON data into the graph.  Michael shows how to do this from various languages in this blog post and I recently wanted to load a JSON document that I generated from Chicago crime types.  This is a snippet of the JSON document: { &amp;quot;categories&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;Index Crime&amp;quot;, &amp;quot;sub_categories&amp;quot;: [ { &amp;quot;code&amp;quot;: &amp;quot;01A&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Homicide 1st &amp;amp; 2nd Degree&amp;quot; } ] }, { &amp;quot;name&amp;quot;: &amp;quot;Non-Index Crime&amp;quot;, &amp;quot;sub_categories&amp;quot;: [ { &amp;quot;code&amp;quot;: &amp;quot;01B&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Involuntary Manslaughter&amp;quot; } ] }, { &amp;quot;name&amp;quot;: &amp;quot;Violent Crime&amp;quot;, &amp;quot;sub_categories&amp;quot;: [ { &amp;quot;code&amp;quot;: &amp;quot;01A&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Homicide 1st &amp;amp; 2nd Degree&amp;quot; } ] } ] }   We want to create the following graph structure from this document:    We can then connect the crimes to the appropriate sub category and write aggregation queries that drill down from the category.</description>
    </item>
    
    <item>
      <title>Neo4j 2.2.3: neo4j-import - Encoder StringEncoder[2] returned an illegal encoded value 0</title>
      <link>https://mneedham.github.io/2015/07/21/neo4j-2-2-3-neo4j-import-encoder-stringencoder2-returned-an-illegal-encoded-value-0/</link>
      <pubDate>Tue, 21 Jul 2015 06:11:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/21/neo4j-2-2-3-neo4j-import-encoder-stringencoder2-returned-an-illegal-encoded-value-0/</guid>
      <description>I&#39;ve been playing around with the Chicago crime data set again while preparing for a Neo4j webinar next week and while running the import tool ran into the following exception: Importing the contents of these files into tmp/crimes.db: Nodes: /Users/markneedham/projects/neo4j-spark-chicago/tmp/crimes.csv /Users/markneedham/projects/neo4j-spark-chicago/tmp/beats.csv /Users/markneedham/projects/neo4j-spark-chicago/tmp/primaryTypes.csv /Users/markneedham/projects/neo4j-spark-chicago/tmp/locations.csv Relationships: /Users/markneedham/projects/neo4j-spark-chicago/tmp/crimesBeats.csv /Users/markneedham/projects/neo4j-spark-chicago/tmp/crimesPrimaryTypes.csv /Users/markneedham/projects/neo4j-spark-chicago/tmp/crimesLocationsCleaned.csv Available memory: Free machine memory: 263.17 MB Max heap memory : 3.56 GB Nodes [*&amp;gt;:17.41 MB/s-------------------------|PROPERTIES(3)=|NODE:3|LABEL SCAN----|v:36.30 MB/s(2)===] 3MImport error: Panic called, so exiting java.</description>
    </item>
    
    <item>
      <title>R: Bootstrap confidence intervals</title>
      <link>https://mneedham.github.io/2015/07/19/r-bootstrap-confidence-intervals/</link>
      <pubDate>Sun, 19 Jul 2015 19:44:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/19/r-bootstrap-confidence-intervals/</guid>
      <description>I recently came across an interesting post on Julia Evans&#39; blog showing how to generate a bigger set of data points by sampling the small set of data points that we actually have using bootstrapping. Julia&#39;s examples are all in Python so I thought it&#39;d be a fun exercise to translate them into R.  We&#39;re doing the bootstrapping to simulate the number of no-shows for a flight so we can work out how many seats we can overbook the plane by.</description>
    </item>
    
    <item>
      <title>R: Blog post frequency anomaly detection</title>
      <link>https://mneedham.github.io/2015/07/17/r-blog-post-frequency-anomaly-detection/</link>
      <pubDate>Fri, 17 Jul 2015 23:34:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/17/r-blog-post-frequency-anomaly-detection/</guid>
      <description>I came across Twitter&#39;s anomaly detection library last year but haven&#39;t yet had a reason to take it for a test run so having got my blog post frequency data into shape I thought it&#39;d be fun to run it through the algorithm. I wanted to see if it would detect any periods of time when the number of posts differed significantly - I don&#39;t really have an action I&#39;m going to take based on the results, it&#39;s curiosity more than anything else!</description>
    </item>
    
    <item>
      <title>Neo4j: The football transfers graph</title>
      <link>https://mneedham.github.io/2015/07/16/neo4j-the-football-transfers-graph/</link>
      <pubDate>Thu, 16 Jul 2015 06:40:26 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/16/neo4j-the-football-transfers-graph/</guid>
      <description>Given we&#39;re still in pre season transfer madness as far as European football is concerned I thought it&#39;d be interesting to put together a football transfers graph to see whether there are any interesting insights to be had.  It took me a while to find an appropriate source but I eventually came across transfermarkt.co.uk which contains transfers going back at least as far as the start of the Premier League in 1992.</description>
    </item>
    
    <item>
      <title>Python: UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 0: ordinal not in range(128)</title>
      <link>https://mneedham.github.io/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</link>
      <pubDate>Wed, 15 Jul 2015 06:20:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</guid>
      <description>I was recently doing some text scrubbing and had difficulty working out how to remove the &#39;†&#39; character from strings. e.g. I had a string like this:
&amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39; u&#39;foo \u2020&#39;  I wanted to get rid of the &#39;†&#39; character and then strip any trailing spaces so I&#39;d end up with the string &#39;foo&#39;. I tried to do this in one call to &#39;replace&#39;:
&amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39;.replace(&amp;quot; †&amp;quot;, &amp;quot;&amp;quot;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 1: ordinal not in range(128)   It took me a while to work out that &#34;</description>
    </item>
    
    <item>
      <title>R: I write more in the last week of the month, or do I?</title>
      <link>https://mneedham.github.io/2015/07/12/r-i-write-more-in-the-last-week-of-the-month-or-do-i/</link>
      <pubDate>Sun, 12 Jul 2015 09:53:04 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/12/r-i-write-more-in-the-last-week-of-the-month-or-do-i/</guid>
      <description>I&#39;ve been writing on this blog for almost 7 years and have always believed that I write more frequently towards the end of a month. Now that I&#39;ve got all the data I thought it&#39;d be interesting to test that belief.  I started with a data frame containing each post and its publication date and added an extra column which works out how many weeks from the end of the month that post was written: &amp;gt; df %&amp;gt;% sample_n(5) title date 946 Python: Equivalent to flatMap for flattening an array of arrays 2015-03-23 00:45:00 175 Ruby: Hash default value 2010-10-16 14:02:37 375 Java/Scala: Runtime.</description>
    </item>
    
    <item>
      <title>R: Filling in missing dates with 0s</title>
      <link>https://mneedham.github.io/2015/07/12/r-filling-in-missing-dates-with-0s/</link>
      <pubDate>Sun, 12 Jul 2015 08:30:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/12/r-filling-in-missing-dates-with-0s/</guid>
      <description>I wanted to plot a chart showing the number of blog posts published by month and started with the following code which makes use of zoo&#39;s &#39;as.yearmon&#39; function to add the appropriate column and grouping: &amp;gt; library(zoo) &amp;gt; library(dplyr) &amp;gt; df %&amp;gt;% sample_n(5) title date 888 R: Converting a named vector to a data frame 2014-10-31 23:47:26 144 Rails: Populating a dropdown list using &#39;form_for&#39; 2010-08-31 01:22:14 615 Onboarding: Sketch the landscape 2013-02-15 07:36:06 28 Javascript: The &#39;new&#39; keyword 2010-03-06 15:16:02 1290 Coding Dojo #16: Reading SUnit code 2009-05-28 23:23:19 &amp;gt; posts_by_date = df %&amp;gt;% mutate(year_mon = as.</description>
    </item>
    
    <item>
      <title>R: Date for given week/year</title>
      <link>https://mneedham.github.io/2015/07/10/r-date-for-given-weekyear/</link>
      <pubDate>Fri, 10 Jul 2015 22:01:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/10/r-date-for-given-weekyear/</guid>
      <description>As I mentioned in my last couple of blog posts I&#39;ve been looking at the data behind this blog and I wanted to plot a chart showing the number of posts per week since the blog started.  I started out with a data frame with posts and publication date: &amp;gt; library(dplyr) &amp;gt; df = read.csv(&amp;quot;posts.csv&amp;quot;) &amp;gt; df$date = ymd_hms(df$date) &amp;gt; df %&amp;gt;% sample_n(10) title date 538 Nygard Big Data Model: The Investigation Stage 2012-10-10 00:00:36 341 The read-only database 2011-08-29 23:32:26 1112 CSS in Internet Explorer - Some lessons learned 2008-10-31 15:24:51 143 Coding: Mutating parameters 2010-08-26 07:47:23 433 Scala: Counting number of inversions (via merge sort) for an unsorted collection 2012-03-20 06:53:18 618 neo4j/cypher: SQL style GROUP BY functionality 2013-02-17 21:05:27 1111 Testing Hibernate mappings: Setting up test data 2008-10-30 13:24:14 462 neo4j: What question do you want to answer?</description>
    </item>
    
    <item>
      <title>R: dplyr - Error: cannot modify grouping variable</title>
      <link>https://mneedham.github.io/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</link>
      <pubDate>Thu, 09 Jul 2015 05:55:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</guid>
      <description>I&#39;ve been doing some exploration of the posts made on this blog and I thought I&#39;d start with answering a simple question - on which dates did I write the most posts?  I started with a data frame containing each post and the date it was published: &amp;gt; library(dplyr) &amp;gt; df %&amp;gt;% sample_n(5) title date 1148 Taiichi Ohno&#39;s Workplace Management: Book Review 2008-12-08 14:14:48 158 Rails: Faking a delete method with &#39;form_for&#39; 2010-09-20 18:52:15 331 Retrospectives: The 4 L&#39;s Retrospective 2011-07-25 21:00:30 1035 msbuild - Use OutputPath instead of OutDir 2008-08-14 18:54:03 1181 The danger of commenting out code 2009-01-17 06:02:33   To find the most popular days for blog posts we can write the following aggregation function: &amp;gt; df %&amp;gt;% mutate(day = as.</description>
    </item>
    
    <item>
      <title>Python: Converting WordPress posts in CSV format</title>
      <link>https://mneedham.github.io/2015/07/07/python-converting-wordpress-posts-in-csv-format/</link>
      <pubDate>Tue, 07 Jul 2015 06:28:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/07/python-converting-wordpress-posts-in-csv-format/</guid>
      <description>Over the weekend I wanted to look into the Wordpress data behind this blog (very meta!) and wanted to get the data in CSV format so I could do some analysis in R.   I found a couple of WordPress CSV plugins but unfortunately I couldn&#39;t get any of them to work and ended up working with the raw XML data that WordPress produces when you &#39;export&#39; a blog.</description>
    </item>
    
    <item>
      <title>R: Wimbledon - How do the seeds get on?</title>
      <link>https://mneedham.github.io/2015/07/05/r-wimbledon-how-do-the-seeds-get-on/</link>
      <pubDate>Sun, 05 Jul 2015 08:38:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/05/r-wimbledon-how-do-the-seeds-get-on/</guid>
      <description>Continuing on with the Wimbledon data set I&#39;ve been playing with I wanted to do some exploration on how the seeded players have fared over the years.  Taking the last 10 years worth of data there have always had 32 seeds and with the following function we can feed in a seeding and get back the round they would be expected to reach: expected_round = function(seeding) { if(seeding == 1) { return(&amp;quot;Winner&amp;quot;) } else if(seeding == 2) { return(&amp;quot;Finals&amp;quot;) } else if(seeding &amp;lt;= 4) { return(&amp;quot;Semi-Finals&amp;quot;) } else if(seeding &amp;lt;= 8) { return(&amp;quot;Quarter-Finals&amp;quot;) } else if(seeding &amp;lt;= 16) { return(&amp;quot;Round of 16&amp;quot;) } else { return(&amp;quot;Round of 32&amp;quot;) } } &amp;gt; expected_round(1) [1] &amp;quot;Winner&amp;quot; &amp;gt; expected_round(4) [1] &amp;quot;Semi-Finals&amp;quot;   We can then have a look at each of the Wimbledon tournaments and work out how far they actually got.</description>
    </item>
    
    <item>
      <title>R: Calculating the difference between ordered factor variables</title>
      <link>https://mneedham.github.io/2015/07/02/r-calculating-the-difference-between-ordered-factor-variables/</link>
      <pubDate>Thu, 02 Jul 2015 22:55:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/07/02/r-calculating-the-difference-between-ordered-factor-variables/</guid>
      <description>In my continued exploration of Wimbledon data I wanted to work out whether a player had done as well as their seeding suggested they should. I therefore wanted to work out the difference between the round they reached and the round they were expected to reach. A &#39;round&#39; in the dataset is an ordered factor variable.  These are all the possible values: rounds = c(&amp;quot;Did not enter&amp;quot;, &amp;quot;Round of 128&amp;quot;, &amp;quot;Round of 64&amp;quot;, &amp;quot;Round of 32&amp;quot;, &amp;quot;Round of 16&amp;quot;, &amp;quot;Quarter-Finals&amp;quot;, &amp;quot;Semi-Finals&amp;quot;, &amp;quot;Finals&amp;quot;, &amp;quot;Winner&amp;quot;)  And if we want to factorise a couple of strings into this factor we would do it like this:</description>
    </item>
    
    <item>
      <title>R: write.csv - unimplemented type &#39;list&#39; in &#39;EncodeElement&#39;</title>
      <link>https://mneedham.github.io/2015/06/30/r-write-csv-unimplemented-type-list-in-encodeelement/</link>
      <pubDate>Tue, 30 Jun 2015 22:26:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/30/r-write-csv-unimplemented-type-list-in-encodeelement/</guid>
      <description>Everyone now and then I want to serialise an R data frame to a CSV file so I can easily load it up again if my R environment crashes without having to recalculate everything but recently ran into the following error: &amp;gt; write.csv(foo, &amp;quot;/tmp/foo.csv&amp;quot;, row.names = FALSE) Error in .External2(C_writetable, x, file, nrow(x), p, rnames, sep, eol, : unimplemented type &#39;list&#39; in &#39;EncodeElement&#39;  If we take a closer look at the data frame in question it looks ok:</description>
    </item>
    
    <item>
      <title>R: Speeding up the Wimbledon scraping job</title>
      <link>https://mneedham.github.io/2015/06/29/r-speeding-up-the-wimbledon-scraping-job/</link>
      <pubDate>Mon, 29 Jun 2015 05:36:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/29/r-speeding-up-the-wimbledon-scraping-job/</guid>
      <description>Over the past few days I&#39;ve written a few blog posts about a Wimbledon data set I&#39;ve been building and after running the scripts a few times I noticed that it was taking much longer to run that I expected.  To recap, I started out with the following function which takes in a URI and returns a data frame containing a row for each match: library(rvest) library(dplyr) scrape_matches1 = function(uri) { matches = data.</description>
    </item>
    
    <item>
      <title>R: dplyr - Update rows with earlier/previous rows values</title>
      <link>https://mneedham.github.io/2015/06/28/r-dplyr-update-rows-with-earlierprevious-rows-values/</link>
      <pubDate>Sun, 28 Jun 2015 22:30:08 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/28/r-dplyr-update-rows-with-earlierprevious-rows-values/</guid>
      <description>Recently I had a data frame which contained a column which had mostly empty values: &amp;gt; data.frame(col1 = c(1,2,3,4,5), col2 = c(&amp;quot;a&amp;quot;, NA, NA , &amp;quot;b&amp;quot;, NA)) col1 col2 1 1 a 2 2 &amp;lt;NA&amp;gt; 3 3 &amp;lt;NA&amp;gt; 4 4 b 5 5 &amp;lt;NA&amp;gt;   I wanted to fill in the NA values with the last non NA value from that column. So I want the data frame to look like this: 1 1 a 2 2 a 3 3 a 4 4 b 5 5 b   I spent ages searching around before I came across the na.</description>
    </item>
    
    <item>
      <title>R: Command line - Error in GenericTranslator$new : could not find function &#34;loadMethod&#34;</title>
      <link>https://mneedham.github.io/2015/06/27/r-command-line-error-in-generictranslatornew-could-not-find-function-loadmethod/</link>
      <pubDate>Sat, 27 Jun 2015 22:47:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/27/r-command-line-error-in-generictranslatornew-could-not-find-function-loadmethod/</guid>
      <description>I&#39;ve been reading Text Processing with Ruby over the last week or so and one of the ideas the author describes is setting up your scripts so you can run them directly from the command line.
 I wanted to do this with my Wimbledon R script and wrote the following script which uses the &#39;Rscript&#39; executable so that R doesn&#39;t launch in interactive mode: wimbledon
#!/usr/bin/env Rscript library(rvest) library(dplyr) library(stringr) library(readr) # stuff   Then I tried to run it: $ time .</description>
    </item>
    
    <item>
      <title>R: dplyr - squashing multiple rows per group into one</title>
      <link>https://mneedham.github.io/2015/06/27/r-dplyr-squashing-multiple-rows-per-group-into-one/</link>
      <pubDate>Sat, 27 Jun 2015 22:36:50 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/27/r-dplyr-squashing-multiple-rows-per-group-into-one/</guid>
      <description>I spent a bit of the day working on my Wimbledon data set and the next thing I explored is all the people that have beaten Andy Murray in the tournament. The following dplyr query gives us the names of those people and the year the match took place:
library(dplyr) &amp;gt; main_matches %&amp;gt;% filter(loser == &amp;quot;Andy Murray&amp;quot;) %&amp;gt;% select(winner, year) winner year 1 Grigor Dimitrov 2014 2 Roger Federer 2012 3 Rafael Nadal 2011 4 Rafael Nadal 2010 5 Andy Roddick 2009 6 Rafael Nadal 2008 7 Marcos Baghdatis 2006 8 David Nalbandian 2005  As you can see, Rafael Nadal shows up multiple times.</description>
    </item>
    
    <item>
      <title>R: ggplot - Show discrete scale even with no value</title>
      <link>https://mneedham.github.io/2015/06/26/r-ggplot-show-discrete-scale-even-with-no-value/</link>
      <pubDate>Fri, 26 Jun 2015 22:48:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/26/r-ggplot-show-discrete-scale-even-with-no-value/</guid>
      <description>As I mentioned in a previous blog post, I&#39;ve been scraping data for the Wimbledon tennis tournament, and having got the data for the last ten years I wrote a query using dplyr to find out how players did each year over that period.  I ended up with the following functions to filter my data frame of all the matches: round_reached = function(player, main_matches) { furthest_match = main_matches %&amp;gt;% filter(winner == player | loser == player) %&amp;gt;% arrange(desc(round)) %&amp;gt;% head(1) return(ifelse(furthest_match$winner == player, &amp;quot;Winner&amp;quot;, as.</description>
    </item>
    
    <item>
      <title>R: Scraping Wimbledon draw data</title>
      <link>https://mneedham.github.io/2015/06/25/r-scraping-wimbledon-draw-data/</link>
      <pubDate>Thu, 25 Jun 2015 23:14:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/25/r-scraping-wimbledon-draw-data/</guid>
      <description>Given Wimbledon starts next week I wanted to find a data set to explore before it gets underway. Having searched around and failed to find one I had to resort to scraping the ATP World Tour&#39;s event page which displays the matches in an easy to access format. We&#39;ll be using the Wimbledon 2013 draw since Andy Murray won that year! This is what the page looks like:
  Each match is in its own row of a table and each column has a class attribute which makes it really easy to scrape.</description>
    </item>
    
    <item>
      <title>R: Scraping the release dates of github projects</title>
      <link>https://mneedham.github.io/2015/06/23/r-scraping-the-release-dates-of-github-projects/</link>
      <pubDate>Tue, 23 Jun 2015 22:34:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/23/r-scraping-the-release-dates-of-github-projects/</guid>
      <description>Continuing on from my blog post about scraping Neo4j&#39;s release dates I thought it&#39;d be even more interesting to chart the release dates of some github projects.
In theory the release dates should be accessible through the github API but the few that I looked at weren&#39;t returning any data so I scraped the data together.  We&#39;ll be using rvest again and I first wrote the following function to extract the release versions and dates from a single page: library(dplyr) library(rvest) process_page = function(releases, session) { rows = session %&amp;gt;% html_nodes(&amp;quot;ul.</description>
    </item>
    
    <item>
      <title>R: Scraping Neo4j release dates with rvest</title>
      <link>https://mneedham.github.io/2015/06/21/r-scraping-neo4j-release-dates-with-rvest/</link>
      <pubDate>Sun, 21 Jun 2015 22:07:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/21/r-scraping-neo4j-release-dates-with-rvest/</guid>
      <description>As part of my log analysis I wanted to get the Neo4j release dates which are accessible from the release notes and decided to try out Hadley Wickham&#39;s rvest scraping library which he released at the end of 2014.  rvest is based on Python&#39;s beautifulsoup which has become my scraping library of choice so I didn&#39;t find it too difficult to pick up. To start with we need to download the release notes locally so we don&#39;t have to go over the network when we&#39;re doing our scraping: download.</description>
    </item>
    
    <item>
      <title>R: dplyr - segfault cause &#39;memory not mapped&#39;</title>
      <link>https://mneedham.github.io/2015/06/20/r-dplyr-segfault-cause-memory-not-mapped/</link>
      <pubDate>Sat, 20 Jun 2015 22:18:55 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/20/r-dplyr-segfault-cause-memory-not-mapped/</guid>
      <description>In my continued playing around with web logs in R I wanted to process the logs for a day and see what the most popular URIs were.  I first read in all the lines using the read_lines function in readr and put the vector it produced into a data frame so I could process it using dplyr. library(readr) dlines = data.frame(column = read_lines(&amp;quot;~/projects/logs/2015-06-18-22-docs&amp;quot;))   In the previous post I showed some code to extract the URI from a log line.</description>
    </item>
    
    <item>
      <title>R: Regex - capturing multiple matches of the same group</title>
      <link>https://mneedham.github.io/2015/06/19/r-regex-capturing-multiple-matches-of-the-same-group/</link>
      <pubDate>Fri, 19 Jun 2015 21:38:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/19/r-regex-capturing-multiple-matches-of-the-same-group/</guid>
      <description>I&#39;ve been playing around with some web logs using R and I wanted to extract everything that existed in double quotes within a logged entry.  This is an example of a log entry that I want to parse: log = &#39;2015-06-18-22:277:548311224723746831\t2015-06-18T22:00:11\t2015-06-18T22:00:05Z\t93317114\tip-127-0-0-1\t127.0.0.5\tUser\tNotice\tneo4j.com.access.log\t127.0.0.3 - - [18/Jun/2015:22:00:11 +0000] &amp;quot;GET /docs/stable/query-updating.html HTTP/1.1&amp;quot; 304 0 &amp;quot;http://neo4j.com/docs/stable/cypher-introduction.html&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36&amp;quot;&#39;  And I want to extract these 3 things:</description>
    </item>
    
    <item>
      <title>Coding: Explore and retreat</title>
      <link>https://mneedham.github.io/2015/06/17/coding-explore-and-retreat/</link>
      <pubDate>Wed, 17 Jun 2015 17:23:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/17/coding-explore-and-retreat/</guid>
      <description>When refactoring code or looking for the best way to integrate a new piece of functionality I generally favour a small steps/incremental approach but recent experiences have led me to believe that this isn&#39;t always the quickest approach.  Sometimes it seems to make more sense to go on little discovery missions in the code, make some bigger steps and then if necessary retreat and revert our changes and apply the lessons learnt on our next discovery mission.</description>
    </item>
    
    <item>
      <title>Northwind: Finding direct/transitive Reports in SQL and Neo4j&#39;s Cypher</title>
      <link>https://mneedham.github.io/2015/06/15/northwind-finding-directtransitive-reports-in-sql-and-neo4js-cypher/</link>
      <pubDate>Mon, 15 Jun 2015 22:53:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/15/northwind-finding-directtransitive-reports-in-sql-and-neo4js-cypher/</guid>
      <description>Every few months we run a relational to graph meetup at the Neo London office where we go through how to take your data from a relational database and into the graph.  We use the Northwind dataset which often comes as a demo dataset on relational databases and come up with some queries which seem graph in nature.  My favourite query is one which finds out how employees are organised and who reports to whom.</description>
    </item>
    
    <item>
      <title>The Willpower Instinct: Reducing time spent mindlessly scrolling for things to read</title>
      <link>https://mneedham.github.io/2015/06/12/the-willpower-instinct-reducing-time-spent-mindlessly-scrolling-for-things-to-read/</link>
      <pubDate>Fri, 12 Jun 2015 23:12:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/12/the-willpower-instinct-reducing-time-spent-mindlessly-scrolling-for-things-to-read/</guid>
      <description>I recently finished reading Kelly McGonigal&#39;s excellent book &#39;The Willpower Instinct&#39; having previously watched her Google talk of the same title  My main takeaway from the book is that there are things that we want to do (or not do) but doing them (or not as the case may be) isn&#39;t necessarily instinctive and so we need to develop some strategies to help ourselves out.  In one of the early chapters she suggests picking a habit that you want to do less off and write down on a piece of paper every time you want to do it and how you&#39;re feeling at that point.</description>
    </item>
    
    <item>
      <title>Neo4j: Using LOAD CSV to help explore CSV files</title>
      <link>https://mneedham.github.io/2015/06/11/neo4j-using-load-csv-to-help-explore-csv-files/</link>
      <pubDate>Thu, 11 Jun 2015 23:15:06 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/11/neo4j-using-load-csv-to-help-explore-csv-files/</guid>
      <description>During the Neo4j How I met your mother hackathon that we ran last week one of the attendees noticed that one of the CSV files we were importing wasn&#39;t creating as many records as they expected it to.  This is typically the case when there&#39;s some odd quoting in the CSV file but we decided to look into it. The file in question was one containing references made in HIMYM.</description>
    </item>
    
    <item>
      <title>Mac OS X: GNU sed -  Hex string replacement / replacing new line characters</title>
      <link>https://mneedham.github.io/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</link>
      <pubDate>Thu, 11 Jun 2015 21:38:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</guid>
      <description>Recently I was working with a CSV file which contained both Windows and Unix line endings which was making it difficult to work with.  The actual line endings were HEX &#39;0A0D&#39; i.e. Windows line breaks but there were also HEX &#39;OA&#39; i.e. Unix line breaks within one of the columns.  I wanted to get rid of the Unix line breaks and discovered that you can do HEX sequence replacement using the GNU version of sed - unfortunately the Mac ships with the BSD version which doesn&#39;t have this functionaltiy.</description>
    </item>
    
    <item>
      <title>Unix: Converting a file of values into a comma separated list</title>
      <link>https://mneedham.github.io/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</link>
      <pubDate>Mon, 08 Jun 2015 22:23:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</guid>
      <description>I recently had a bunch of values in a file that I wanted to paste into a Java program which required a comma separated list of strings.  This is what the file looked like: $ cat foo2.txt | head -n 5 1.0 1.0 1.0 1.0 1.0   And the idea is that we would end up with something like this: &amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;   The first thing we need to do is quote each of the values.</description>
    </item>
    
    <item>
      <title>Netty: Testing encoders/decoders</title>
      <link>https://mneedham.github.io/2015/06/05/netty-testing-encodersdecoders/</link>
      <pubDate>Fri, 05 Jun 2015 21:25:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/05/netty-testing-encodersdecoders/</guid>
      <description>I&#39;ve been working with Netty a bit recently and having built a pipeline of encoders/decoders as described in this excellent tutorial wanted to test that the encoders and decoders were working without having to send real messages around.
Luckily there is a EmbeddedChannel which makes our life very easy indeed.
Let&#39;s say we&#39;ve got a message &#39;Foo&#39; that we want to send across the wire. It only contains a single integer value so we&#39;ll just send that and reconstruct &#39;Foo&#39; on the other side.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Step by step to creating a linked list of adjacent nodes using UNWIND</title>
      <link>https://mneedham.github.io/2015/06/04/neo4j-cypher-step-by-step-to-creating-a-linked-list-of-adjacent-nodes-using-unwind/</link>
      <pubDate>Thu, 04 Jun 2015 22:17:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/04/neo4j-cypher-step-by-step-to-creating-a-linked-list-of-adjacent-nodes-using-unwind/</guid>
      <description>In late 2013 I wrote a post showing how to create a linked list connecting different football seasons together using Neo4j&#39;s Cypher query language, a post I&#39;ve frequently copy &amp; pasted from!  Now 18 months later, and using Neo4j 2.2 rather than 2.0, we can actually solve this problem in what I believe is a more intuitive way using the UNWIND function. Credit for the idea goes to Michael, I&#39;m just the messenger.</description>
    </item>
    
    <item>
      <title>R: ggplot geom_density - Error in exists(name, envir = env, mode = mode) : argument &#34;env&#34; is missing, with no default</title>
      <link>https://mneedham.github.io/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</link>
      <pubDate>Wed, 03 Jun 2015 05:52:08 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</guid>
      <description>Continuing on from yesterday&#39;s blog post where I worked out how to clean up the Think Bayes Price is Right data set, the next task was to plot a distribution of the prices of show case items.  To recap, this is what the data frame we&#39;re working with looks like: library(dplyr) df2011 = read.csv(&amp;quot;~/projects/rLearning/showcases.2011.csv&amp;quot;, na.strings = c(&amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;)) df2011 = df2011 %&amp;gt;% na.omit() &amp;gt; df2011 %&amp;gt;% head() X Sep.</description>
    </item>
    
    <item>
      <title>R: dplyr - removing empty rows</title>
      <link>https://mneedham.github.io/2015/06/02/r-dplyr-removing-empty-rows/</link>
      <pubDate>Tue, 02 Jun 2015 06:49:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/06/02/r-dplyr-removing-empty-rows/</guid>
      <description>I&#39;m still working my way through the exercises in Think Bayes and in Chapter 6 needed to do some cleaning of the data in a CSV file containing information about the Price is Right.  I downloaded the file using wget: wget ￼http://www.greenteapress.com/thinkbayes/showcases.2011.csv￼  And then loaded it into R and explored the first few rows using dplyr
library(dplyr) df2011 = read.csv(&amp;quot;~/projects/rLearning/showcases.2011.csv&amp;quot;) &amp;gt; df2011 %&amp;gt;% head(10) X Sep..19 Sep..20 Sep.</description>
    </item>
    
    <item>
      <title>R: Think Bayes Euro Problem</title>
      <link>https://mneedham.github.io/2015/05/31/r-think-bayes-euro-problem/</link>
      <pubDate>Sun, 31 May 2015 23:11:50 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/31/r-think-bayes-euro-problem/</guid>
      <description>I&#39;ve got back to working my way through Think Bayes after a month&#39;s break and started out with the one euro coin problem in Chapter 4:  A statistical statement appeared in “The Guardian&#34; on Friday January 4, 2002: When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. ‘It looks very suspicious to me,’ said Barry Blight, a statistics lecturer at the London School of Economics.</description>
    </item>
    
    <item>
      <title>Python: CSV writing - TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;__getitem__&#39;</title>
      <link>https://mneedham.github.io/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</link>
      <pubDate>Sun, 31 May 2015 22:33:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</guid>
      <description>When I&#39;m working in Python I often find myself writing to CSV files using the in built library and every now and then make a mistake when calling writerow: import csv writer = csv.writer(file, delimiter=&amp;quot;,&amp;quot;) writer.writerow[&amp;quot;player&amp;quot;, &amp;quot;team&amp;quot;]  This results in the following error message:
TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;__getitem__&#39;  The error message is a bit weird at first but it&#39;s basically saying that I&#39;ve tried to do an associative lookup on an object which doesn&#39;t support that operation.</description>
    </item>
    
    <item>
      <title>Neo4j: The BBC Champions League graph</title>
      <link>https://mneedham.github.io/2015/05/30/neo4j-the-bbc-champions-league-graph/</link>
      <pubDate>Sat, 30 May 2015 21:45:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/30/neo4j-the-bbc-champions-league-graph/</guid>
      <description>A couple of weekends ago I started scraping the BBC live text feed of the Bayern Munich/Barcelona match, initially starting out with just the fouls and building the foul graph.  I&#39;ve spent a bit more time on it since then and have managed to model several other events as well including attempts, goals, cards and free kicks.  I started doing this just for the Bayern Munich/Barcelona match but realised it wasn&#39;t particularly difficult to extend this out and graph the events for every match in the Champions League 2014/2015.</description>
    </item>
    
    <item>
      <title>Python: Look ahead multiple elements in an iterator/generator</title>
      <link>https://mneedham.github.io/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</link>
      <pubDate>Thu, 28 May 2015 20:56:08 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</guid>
      <description>As part of the BBC live text scraping code I&#39;ve been working on I needed to take an iterator of raw events created by a generator and transform this into an iterator of cards shown in a match.  The structure of the raw events I&#39;m interested in is as follows:  Line 1: Player booked Line 2: Player fouled Line 3: Information about the foul   e.g. events = [ {&#39;event&#39;: u&#39;Booking Pedro (Barcelona) is shown the yellow card for a bad foul.</description>
    </item>
    
    <item>
      <title>Neo4j: The foul revenge graph</title>
      <link>https://mneedham.github.io/2015/05/26/neo4j-the-foul-revenge-graph/</link>
      <pubDate>Tue, 26 May 2015 07:03:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/26/neo4j-the-foul-revenge-graph/</guid>
      <description>Last week I was showing the foul graph to my colleague Alistair who came up with the idea of running a &#39;foul revenge&#39; query to find out which players gained revenge for a foul with one of their own later in them match.  Queries like this are very path centric and therefore work well in a graph. To recap, this is what the foul graph looks like:    The first thing that we need to do is connect the fouls in a linked list based on time so that we can query their order more easily.</description>
    </item>
    
    <item>
      <title>Python: Joining multiple generators/iterators</title>
      <link>https://mneedham.github.io/2015/05/24/python-joining-multiple-generatorsiterators/</link>
      <pubDate>Sun, 24 May 2015 23:51:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/24/python-joining-multiple-generatorsiterators/</guid>
      <description>In my previous blog post I described how I&#39;d refactored some scraping code I&#39;ve been working on to use iterators and ended up with a function which returned a generator containing all the events for one BBC live text match: match_id = &amp;quot;32683310&amp;quot; events = extract_events(&amp;quot;data/raw/%s&amp;quot; % (match_id)) &amp;gt;&amp;gt;&amp;gt; print type(events) &amp;lt;type &#39;generator&#39;&amp;gt;   The next thing I wanted to do is get the events for multiple matches which meant I needed to glue together multiple generators into one big generator.</description>
    </item>
    
    <item>
      <title>Python: Refactoring to iterator</title>
      <link>https://mneedham.github.io/2015/05/23/python-refactoring-to-iterator/</link>
      <pubDate>Sat, 23 May 2015 10:14:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/23/python-refactoring-to-iterator/</guid>
      <description>Over the last week I&#39;ve been building a set of scripts to scrape the events from the Bayern Munich/Barcelona game and I&#39;ve ended up with a few hundred lines of nested for statements, if statements and mutated lists. I thought it was about time I did a bit of refactoring. The following is a function which takes in a match file and spits out a collection of maps containing times &amp; events.</description>
    </item>
    
    <item>
      <title>Python: UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\xfc&#39; in position 11: ordinal not in range(128)</title>
      <link>https://mneedham.github.io/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</link>
      <pubDate>Thu, 21 May 2015 06:14:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</guid>
      <description>I&#39;ve been trying to write some Python code to extract the players and the team they represented in the Bayern Munich/Barcelona match into a CSV file and had much more difficulty than I expected.  I have some scraping code (which is beyond the scope of this article) which gives me a list of (player, team) pairs that I want to write to disk. The contents of the list is as follows: $ python extract_players.</description>
    </item>
    
    <item>
      <title>Neo4j: Finding all shortest paths</title>
      <link>https://mneedham.github.io/2015/05/19/neo4j-finding-all-shortest-paths/</link>
      <pubDate>Tue, 19 May 2015 22:45:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/19/neo4j-finding-all-shortest-paths/</guid>
      <description>One of the Cypher language features we show in Neo4j training courses is the shortest path function which allows you to find the shortest path in terms of number of relationships between two nodes.  Using the movie graph, which you can import via the &#39;:play movies&#39; command in the browser, we&#39;ll first create a &#39;KNOWS&#39; relationship between any people that have appeared in the same movie: MATCH (p1:Person)-[:ACTED_IN]-&amp;gt;()&amp;lt;-[:ACTED_IN]-(p2:Person) MERGE (p1)-[:KNOWS]-(p2)  Now that we&#39;ve got that relationship we can easily find the shortest path between two people, say Tom Cruise and Tom Hanks:</description>
    </item>
    
    <item>
      <title>Neo4j: Refactoring the BBC football live text fouls graph</title>
      <link>https://mneedham.github.io/2015/05/17/neo4j-refactoring-the-bbc-football-live-text-fouls-graph/</link>
      <pubDate>Sun, 17 May 2015 11:04:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/17/neo4j-refactoring-the-bbc-football-live-text-fouls-graph/</guid>
      <description>Yesterday I wrote about a Neo4j graph I&#39;ve started building which contains all the fouls committed in the Champions League game between Barcelona &amp; Bayern Munich and surrounding meta data.  While adding other events into the graph I realised that I&#39;d added some duplication in the model and the model could do with some refactoring to make it easier to use.  To recap, this is the model that we designed in the previous blog post:    The duplication is on the left hand side of the model - we model a foul as being committed by one player against another and then hook the foul back into the match.</description>
    </item>
    
    <item>
      <title>Neo4j: BBC football live text fouls graph</title>
      <link>https://mneedham.github.io/2015/05/16/neo4j-bbc-football-live-text-fouls-graph/</link>
      <pubDate>Sat, 16 May 2015 21:13:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/16/neo4j-bbc-football-live-text-fouls-graph/</guid>
      <description>I recently came across the Partially Derivative podcast and in episode 17 they describe how Kirk Goldsberry scraped a bunch of data about shots in basketball matches then ran some analysis on that data.  It got me thinking that we might be able to do something similar for football matches and although event based data for football matches only comes from Opta, the BBC does expose some of them in live text feeds.</description>
    </item>
    
    <item>
      <title>R: ggplot - Displaying multiple charts with a for loop</title>
      <link>https://mneedham.github.io/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</link>
      <pubDate>Thu, 14 May 2015 00:17:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</guid>
      <description>Continuing with my analysis of the Neo4j London user group I wanted to drill into some individual meetups and see the makeup of the people attending those meetups with respect to the cohort they belong to.  I started by writing a function which would take in an event ID and output a bar chart showing the number of people who attended that event from each cohort.   We can work out the cohort that a member belongs to by querying for the first event they attended.</description>
    </item>
    
    <item>
      <title>R: Cohort heatmap of Neo4j London meetup</title>
      <link>https://mneedham.github.io/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</link>
      <pubDate>Mon, 11 May 2015 23:16:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</guid>
      <description>A few months ago I had a go at doing some cohort analysis of the Neo4j London meetup group which was an interesting experiment but unfortunately resulted in a chart that was completely illegible.    I wasn&#39;t sure how to progress from there but a few days ago I came across the cohort heatmap which seemed like a better way of visualising things over time. The underlying idea is still the same - we&#39;ve comparing different cohorts of users against each other to see whether a change or intervention we did at a certain time had any impact.</description>
    </item>
    
    <item>
      <title>R: Neo4j London meetup group - How many events do people come to?</title>
      <link>https://mneedham.github.io/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</link>
      <pubDate>Sat, 09 May 2015 22:33:05 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</guid>
      <description>Earlier this week the number of members in the Neo4j London meetup group creeped over the 2,000 mark and I thought it&#39;d be fun to re-explore the data that I previously imported into Neo4j.  How often do people come to meetups? library(RNeo4j) library(dplyr) graph = startGraph(&amp;quot;http://localhost:7474/db/data/&amp;quot;) query = &amp;quot;MATCH (g:Group {name: &#39;Neo4j - London User Group&#39;})-[:HOSTED_EVENT]-&amp;gt;(event)&amp;lt;-[:TO]-({response: &#39;yes&#39;})&amp;lt;-[:RSVPD]-(profile)-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g) WHERE (event.time + event.utc_offset) &amp;lt; timestamp() RETURN event.id, event.time + event.utc_offset AS eventTime, profile.</description>
    </item>
    
    <item>
      <title>Python: Selecting certain indexes in an array</title>
      <link>https://mneedham.github.io/2015/05/05/python-selecting-certain-indexes-in-an-array/</link>
      <pubDate>Tue, 05 May 2015 21:39:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/05/python-selecting-certain-indexes-in-an-array/</guid>
      <description>A couple of days ago I was scrapping the UK parliament constituencies from Wikipedia in preparation for the Graph Connect hackathon and had got to the point where I had an array with one entry per column in the table.   import requests from bs4 import BeautifulSoup from soupselect import select page = open(&amp;quot;constituencies.html&amp;quot;, &#39;r&#39;) soup = BeautifulSoup(page.read()) for row in select(soup, &amp;quot;table.wikitable tr&amp;quot;): if select(row, &amp;quot;th&amp;quot;): print [cell.text for cell in select(row, &amp;quot;th&amp;quot;)] if select(row, &amp;quot;td&amp;quot;): print [cell.</description>
    </item>
    
    <item>
      <title>Neo4j: LOAD CSV - java.io.InputStreamReader there&#39;s a field starting with a quote and whereas it ends that quote there seems  to be character in that field after that ending quote. That isn&#39;t supported.</title>
      <link>https://mneedham.github.io/2015/05/04/neo4j-load-csv-java-io-inputstreamreader-theres-a-field-starting-with-a-quote-and-whereas-it-ends-that-quote-there-seems-to-be-character-in-that-field-after-that-ending-quote-that-isnt-suppor/</link>
      <pubDate>Mon, 04 May 2015 09:56:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/04/neo4j-load-csv-java-io-inputstreamreader-theres-a-field-starting-with-a-quote-and-whereas-it-ends-that-quote-there-seems-to-be-character-in-that-field-after-that-ending-quote-that-isnt-suppor/</guid>
      <description>I recently came across the last.fm dataset via Ben Frederickson&#39;s blog and thought it&#39;d be an interesting one to load into Neo4j and explore.  I started with a simple query to parse the CSV file and count the number of rows: LOAD CSV FROM &amp;quot;file:///Users/markneedham/projects/neo4j-recommendations/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv&amp;quot; AS row FIELDTERMINATOR &amp;quot;\t&amp;quot; return COUNT(*) At java.io.InputStreamReader@4d307fda:6484 there&#39;s a field starting with a quote and whereas it ends that quote there seems to be character in that field after that ending quote.</description>
    </item>
    
    <item>
      <title>Coding: Visualising a bitmap</title>
      <link>https://mneedham.github.io/2015/05/03/coding-visualising-a-bitmap/</link>
      <pubDate>Sun, 03 May 2015 00:19:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/05/03/coding-visualising-a-bitmap/</guid>
      <description>Over the last month or so I&#39;ve spent some time each day reading a new part of the Neo4j code base to get more familiar with it, and one of my favourite classes is the Bits class which does all things low level on the wire and to disk.
In particular I like its toString method which returns a binary representation of the values that we&#39;re storing in bytes, ints and longs.</description>
    </item>
    
    <item>
      <title>Deliberate Practice: Building confidence vs practicing</title>
      <link>https://mneedham.github.io/2015/04/30/deliberate-practice-building-confidence-vs-practicing/</link>
      <pubDate>Thu, 30 Apr 2015 07:48:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/30/deliberate-practice-building-confidence-vs-practicing/</guid>
      <description>A few weeks ago I wrote about the learning to cycle dependency graph which described some of the skills required to become proficient at riding a bike.
  While we&amp;rsquo;ve been practicing various skills/sub skills I&amp;rsquo;ve often found myself saying the following: 
 if it&#39;s not hard you&#39;re not practicing me, April 2015   i.e. you should find the skill you&amp;rsquo;re currently practicing difficult otherwise you&amp;rsquo;re not stretching yourself and therefore aren&amp;rsquo;t getting better.</description>
    </item>
    
    <item>
      <title>R: dplyr - Error in (list: invalid subscript type &#39;double&#39;</title>
      <link>https://mneedham.github.io/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</link>
      <pubDate>Mon, 27 Apr 2015 22:34:43 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</guid>
      <description>In my continued playing around with R I wanted to find the minimum value for a specified percentile given a data frame representing a cumulative distribution function (CDF).
e.g. imagine we have the following CDF represented in a data frame:
library(dplyr) df = data.frame(score = c(5,7,8,10,12,20), percentile = c(0.05,0.1,0.15,0.20,0.25,0.5))   and we want to find the minimum value for the 0.05 percentile. We can use the filter function to do so: &amp;gt; (df %&amp;gt;% filter(percentile &amp;gt; 0.</description>
    </item>
    
    <item>
      <title>Deliberate Practice: Watching yourself fail</title>
      <link>https://mneedham.github.io/2015/04/25/deliberate-practice-watching-yourself-fail/</link>
      <pubDate>Sat, 25 Apr 2015 22:26:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/25/deliberate-practice-watching-yourself-fail/</guid>
      <description>I&#39;ve recently been reading the literature written by K. Anders Eriksson and co on Deliberate Practice and one of the suggestions for increasing our competence at a skill is to put ourselves in a situation where we can fail.  I&#39;ve been reading Think Bayes - an introductory text on Bayesian statistics, something I know nothing about - and each chapter concludes with a set of exercises to practice, a potentially perfect exercise in failure!</description>
    </item>
    
    <item>
      <title>R: Think Bayes Locomotive Problem - Posterior probabilities for different priors</title>
      <link>https://mneedham.github.io/2015/04/24/r-think-bayes-locomotive-problem-posterior-probabilities-for-different-priors/</link>
      <pubDate>Fri, 24 Apr 2015 23:53:12 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/24/r-think-bayes-locomotive-problem-posterior-probabilities-for-different-priors/</guid>
      <description>In my continued reading of Think Bayes the next problem to tackle is the Locomotive problem which is defined thus:  A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many loco- motives the railroad has.   The interesting thing about this question is that it initially seems that we don&#39;t have enough information to come up with any sort of answer.</description>
    </item>
    
    <item>
      <title>R: Replacing for loops with data frames</title>
      <link>https://mneedham.github.io/2015/04/22/r-replacing-for-loops-with-data-frames/</link>
      <pubDate>Wed, 22 Apr 2015 22:18:00 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/22/r-replacing-for-loops-with-data-frames/</guid>
      <description>In my last blog post I showed how to derive posterior probabilities for the Think Bayes dice problem:  Suppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons &amp; Dragons, you know what I am talking about. Suppose I select a die from the box at random, roll it, and get a 6.</description>
    </item>
    
    <item>
      <title>R: Numeric keys in the nested list/dictionary</title>
      <link>https://mneedham.github.io/2015/04/21/r-numeric-keys-in-the-nested-listdictionary/</link>
      <pubDate>Tue, 21 Apr 2015 05:59:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/21/r-numeric-keys-in-the-nested-listdictionary/</guid>
      <description>Last week I described how I&#39;ve been creating fake dictionaries in R using lists and I found myself using the same structure while solving the dice problem in Think Bayes.
The dice problem is described as follows:
 Suppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons &amp; Dragons, you know what I am talking about.</description>
    </item>
    
    <item>
      <title>R: non-numeric argument to binary operator</title>
      <link>https://mneedham.github.io/2015/04/19/r-non-numeric-argument-to-binary-operator/</link>
      <pubDate>Sun, 19 Apr 2015 23:08:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/19/r-non-numeric-argument-to-binary-operator/</guid>
      <description>When debugging R code, given my Java background, I often find myself trying to print out the state of variables along with an appropriate piece of text like this: names = c(1,2,3,4,5,6) &amp;gt; print(&amp;quot;names: &amp;quot; + names) Error in &amp;quot;names: &amp;quot; + names : non-numeric argument to binary operator   We might try this next: &amp;gt; print(&amp;quot;names: &amp;quot;, names) [1] &amp;quot;names: &amp;quot;   which doesn&#39;t actually print the names variable - only the first argument to the print function is printed.</description>
    </item>
    
    <item>
      <title>R: Removing for loops</title>
      <link>https://mneedham.github.io/2015/04/18/r-removing-for-loops/</link>
      <pubDate>Sat, 18 Apr 2015 23:53:20 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/18/r-removing-for-loops/</guid>
      <description>In my last blog post I showed the translation of a likelihood function from Think Bayes into R and in my first attempt at this function I used a couple of nested for loops. likelihoods = function(names, mixes, observations) { scores = rep(1, length(names)) names(scores) = names for(name in names) { for(observation in observations) { scores[name] = scores[name] * mixes[[name]][observation] } } return(scores) }  Names = c(&amp;quot;Bowl 1&amp;quot;, &amp;quot;Bowl 2&amp;quot;) bowl1Mix = c(0.</description>
    </item>
    
    <item>
      <title>R: Think Bayes - More posterior probability calculations</title>
      <link>https://mneedham.github.io/2015/04/16/r-think-bayes-more-posterior-probability-calculations/</link>
      <pubDate>Thu, 16 Apr 2015 20:57:20 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/16/r-think-bayes-more-posterior-probability-calculations/</guid>
      <description>As I mentioned in a post last week I&#39;ve been reading through Think Bayes and translating some of the examples form Python to R.  After my first post Antonios suggested a more idiomatic way of writing the function in R so I thought I&#39;d give it a try to calculate the probability that combinations of cookies had come from each bowl.  In the simplest case we have this function which takes in the names of the bowls and the likelihood scores: f = function(names,likelihoods) { # Assume each option has an equal prior priors = rep(1, length(names)) / length(names) # create a data frame with all info you have dt = data.</description>
    </item>
    
    <item>
      <title>Spark: Generating CSV files to import into Neo4j</title>
      <link>https://mneedham.github.io/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</link>
      <pubDate>Tue, 14 Apr 2015 22:56:35 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</guid>
      <description>About a year ago Ian pointed me at a Chicago Crime data set which seemed like a good fit for Neo4j and after much procrastination I&#39;ve finally got around to importing it.  The data set covers crimes committed from 2001 until now. It contains around 4 million crimes and meta data around those crimes such as the location, type of crime and year to name a few.  The contents of the file follow this structure: $ head -n 10 ~/Downloads/Crimes_-_2001_to_present.</description>
    </item>
    
    <item>
      <title>R: Creating an object with functions to calculate conditional probability</title>
      <link>https://mneedham.github.io/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</link>
      <pubDate>Sun, 12 Apr 2015 07:55:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</guid>
      <description>I&#39;ve been working through Alan Downey&#39;s Thinking Bayes and I thought it&#39;d be an interesting exercise to translate some of the code from Python to R.  The first example is a simple one about conditional probablity and the author creates a class &#39;PMF&#39; (Probability Mass Function) to solve the following problem:  Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of each.</description>
    </item>
    
    <item>
      <title>R: Snakes and ladders markov chain</title>
      <link>https://mneedham.github.io/2015/04/09/r-snakes-and-ladders-markov-chain/</link>
      <pubDate>Thu, 09 Apr 2015 22:02:18 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/09/r-snakes-and-ladders-markov-chain/</guid>
      <description>A few days ago I read a really cool blog post explaining how Markov chains can be used to model the possible state transitions in a game of snakes and ladders, a use of Markov chains I hadn&#39;t even thought of!  While the example is very helpful for understanding the concept, my understanding of the code is that it works off the assumption that any roll of the dice that puts you on a score  100 is a winning roll.</description>
    </item>
    
    <item>
      <title>Neo4j: The learning to cycle dependency graph</title>
      <link>https://mneedham.github.io/2015/04/07/neo4j-the-learning-to-cycle-dependency-graph/</link>
      <pubDate>Tue, 07 Apr 2015 20:59:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/07/neo4j-the-learning-to-cycle-dependency-graph/</guid>
      <description>Over the past couple of weeks I&#39;ve been reading about skill building and the break down of skills into more manageable chunks, and recently had a chance to break down the skills required to learn to cycle.  I initially sketched out the skill progression but quickly realised I had drawn a dependency graph and thought that putting it into Neo4j would simplify things.  I started out with the overall goal for cycling which was to &#39;Be able to cycle through a public park&#39;:</description>
    </item>
    
    <item>
      <title>R: Markov Chain Wikipedia Example</title>
      <link>https://mneedham.github.io/2015/04/05/r-markov-chain-wikipedia-example/</link>
      <pubDate>Sun, 05 Apr 2015 10:07:12 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/05/r-markov-chain-wikipedia-example/</guid>
      <description>Over the weekend I&#39;ve been reading about Markov Chains and I thought it&#39;d be an interesting exercise for me to translate Wikipedia&#39;s example into R code.  But first a definition:  A Markov chain is a random process that undergoes transitions from one state to another on a state space. It is required to possess a property that is usually characterized as &#34;memoryless&#34;: the probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it.</description>
    </item>
    
    <item>
      <title>How I met your mother: Story arcs</title>
      <link>https://mneedham.github.io/2015/04/03/how-i-met-your-mother-story-arcs/</link>
      <pubDate>Fri, 03 Apr 2015 23:31:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/03/how-i-met-your-mother-story-arcs/</guid>
      <description>After weeks of playing around with various algorithms to extract story arcs in How I met your mother I&#39;ve come to the conclusion that I don&#39;t yet have the skills to completely automate this process so I&#39;m going to change my approach.  The new plan is to treat the outputs of the algorithms as suggestions for possible themes but then have a manual step where I extract what I think are interesting themes in the series.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Building the query for a movie&#39;s profile page</title>
      <link>https://mneedham.github.io/2015/04/01/neo4j-cypher-building-the-query-for-a-movies-profile-page/</link>
      <pubDate>Wed, 01 Apr 2015 11:54:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/01/neo4j-cypher-building-the-query-for-a-movies-profile-page/</guid>
      <description>Yesterday I spent the day in Berlin delivering a workshop as part of the Data Science Retreat and one of the exercises we did was write a query that would pull back all the information you&#39;d need to create the IMDB page for a movie.  Scanning the page we can see that need to get some basic meta data including the title. Next we&#39;ll need to pull in the actors, directors, producers and finally a recommendation for some other movies the viewer might like to see.</description>
    </item>
    
    <item>
      <title>Python: Creating a skewed random discrete distribution</title>
      <link>https://mneedham.github.io/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</link>
      <pubDate>Mon, 30 Mar 2015 22:28:23 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</guid>
      <description>I&#39;m planning to write a variant of the TF/IDF algorithm over the HIMYM corpus which weights in favour of term that appear in a medium number of documents and as a prerequisite needed a function that when given a number of documents would return a weighting.  It should return a higher value when a term appears in a medium number of documents i.e. if I pass in 10 I should get back a higher value than 200 as a term that appears in 10 episodes is likely to be more interesting than one which appears in almost every episode.</description>
    </item>
    
    <item>
      <title>InetAddressImpl#lookupAllHostAddr slow/hangs</title>
      <link>https://mneedham.github.io/2015/03/29/inetaddressimpllookupallhostaddr-slowhangs/</link>
      <pubDate>Sun, 29 Mar 2015 00:31:37 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/29/inetaddressimpllookupallhostaddr-slowhangs/</guid>
      <description>Since I upgraded to Yosemite I&#39;ve noticed that attempts to resolve localhost on my home network have been taking ages (sometimes over a minute) so I thought I&#39;d try and work out why.  This is what my initial /etc/hosts file looked like based on the assumption that my machine&#39;s hostname was teetotal: $ cat /etc/hosts ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting.</description>
    </item>
    
    <item>
      <title>Neo4j: Generating real time recommendations with Cypher</title>
      <link>https://mneedham.github.io/2015/03/27/neo4j-generating-real-time-recommendations-with-cypher/</link>
      <pubDate>Fri, 27 Mar 2015 06:59:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/27/neo4j-generating-real-time-recommendations-with-cypher/</guid>
      <description>One of the most common uses of Neo4j is for building real time recommendation engines and a common theme is that they make use of lots of different bits of data to come up with an interesting recommendation.  For example in this video Amanda shows how dating websites build real time recommendation engines by starting with social connections and then introducing passions, location and a few other things. Graph Aware have a neat framework that helps you to build your own recommendation engine using Java and I was curious what a Cypher version would look like.</description>
    </item>
    
    <item>
      <title>Python: matplotlib hangs and shows nothing (Mac OS X)</title>
      <link>https://mneedham.github.io/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</link>
      <pubDate>Thu, 26 Mar 2015 00:02:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</guid>
      <description>I&#39;ve been playing around with some of the matplotlib demos recently and discovered that simply copying one of the examples didn&#39;t actually work for me. I was following the bar chart example and had the following code:
import numpy as np import matplotlib.pyplot as plt N = 5 ind = np.arange(N) fig, ax = plt.subplots() menMeans = (20, 35, 30, 35, 27) menStd = (2, 3, 4, 1, 2) width = 0.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://mneedham.github.io/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>In my continued exploration of topic modelling I came across The Programming Historian blog and a post showing how to derive topics from a corpus using the Java library mallet.  The instructions on the blog make it very easy to get up and running but as with other libraries I&#39;ve used, you have to specify how many topics the corpus consists of. I&#39;m never sure what value to select but the authors make the following suggestion:  How do you know the number of topics to search for?</description>
    </item>
    
    <item>
      <title>Python: Equivalent to flatMap for flattening an array of arrays</title>
      <link>https://mneedham.github.io/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</link>
      <pubDate>Mon, 23 Mar 2015 00:45:00 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</guid>
      <description>I found myself wanting to flatten an array of arrays while writing some Python code earlier this afternoon and being lazy my first attempt involved building the flattened array manually:
episodes = [ {&amp;quot;id&amp;quot;: 1, &amp;quot;topics&amp;quot;: [1,2,3]}, {&amp;quot;id&amp;quot;: 2, &amp;quot;topics&amp;quot;: [4,5,6]} ] flattened_episodes = [] for episode in episodes: for topic in episode[&amp;quot;topics&amp;quot;]: flattened_episodes.append({&amp;quot;id&amp;quot;: episode[&amp;quot;id&amp;quot;], &amp;quot;topic&amp;quot;: topic}) for episode in flattened_episodes: print episode  If we run that we&#39;ll see this output:</description>
    </item>
    
    <item>
      <title>Python: Simplifying the creation of a stop word list with defaultdict</title>
      <link>https://mneedham.github.io/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</link>
      <pubDate>Sun, 22 Mar 2015 01:51:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</guid>
      <description>I&#39;ve been playing around with topics models again and recently read a paper by David Mimno which suggested the following heuristic for working out which words should go onto the stop list:  A good heuristic for identifying such words is to remove those that occur in more than 5-10% of documents (most common) and those that occur fewer than 5-10 times in the entire corpus (least common).   I decided to try this out on the HIMYM dataset that I&#39;ve been working on over the last couple of months.</description>
    </item>
    
    <item>
      <title>Python: Forgetting to use enumerate</title>
      <link>https://mneedham.github.io/2015/03/22/python-forgetting-to-use-enumerate/</link>
      <pubDate>Sun, 22 Mar 2015 01:28:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/22/python-forgetting-to-use-enumerate/</guid>
      <description>Earlier this evening I found myself writing the equivalent of the following Python code while building a stop list for a topic model... words = [&amp;quot;mark&amp;quot;, &amp;quot;neo4j&amp;quot;, &amp;quot;michael&amp;quot;] word_position = 0 for word in words: print word_position, word word_position +=1   ...which is very foolish given that there&#39;s already a function that makes it really easy to grab the position of an item in a list:
for word_position, word in enumerate(words): print word_position, word  Python does make things extremely easy at times - you&#39;re welcome future Mark!</description>
    </item>
    
    <item>
      <title>Badass: Making users awesome - Kathy Sierra: Book Review</title>
      <link>https://mneedham.github.io/2015/03/20/badass-making-users-awesome-kathy-sierra-book-review/</link>
      <pubDate>Fri, 20 Mar 2015 07:30:55 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/20/badass-making-users-awesome-kathy-sierra-book-review/</guid>
      <description>I started reading Kathy Sierra&#39;s new book &#39;Badass: Making users awesome&#39; a couple of weeks ago and with the gift of flights to/from Stockholm this week I&#39;ve got through the rest of it.  I really enjoyed the book and have found myself returning to it almost every day to check up exactly what was said on a particular topic.  There were a few things that I&#39;ve taken away and have been going on about to anyone who will listen.</description>
    </item>
    
    <item>
      <title>Neo4j: Detecting potential typos using EXPLAIN</title>
      <link>https://mneedham.github.io/2015/03/17/neo4j-detecting-potential-typos-using-explain/</link>
      <pubDate>Tue, 17 Mar 2015 22:46:13 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/17/neo4j-detecting-potential-typos-using-explain/</guid>
      <description>I&#39;ve been running a few intro to Neo4j training sessions recently using Neo4j 2.2.0 RC1 and at some stage in every session somebody will make a typo when writing out of the example queries.  For example one of the queries that we do about half way finds the actors and directors who have worked together and aggregates the movies they were in.  This is the correct query: MATCH (actor:Person)-[:ACTED_IN]-&amp;gt;(movie)&amp;lt;-[:DIRECTED]-(director) RETURN actor.</description>
    </item>
    
    <item>
      <title>One month of mini habits</title>
      <link>https://mneedham.github.io/2015/03/17/one-month-of-mini-habits/</link>
      <pubDate>Tue, 17 Mar 2015 01:32:18 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/17/one-month-of-mini-habits/</guid>
      <description>I recently read a book in the &#39;getting things done&#39; genre written by Stephen Guise titled &#39;Mini Habits&#39; and although I generally don&#39;t like those types of books I quite enjoyed this one and decided to give his system a try.  The underlying idea is that there are two parts of actually doing stuff:  Planning what to do Doing it   We often get stuck in between the first and second steps because what we&#39;ve planned to do is too big and overwhelming.</description>
    </item>
    
    <item>
      <title>Python: Transforming Twitter datetime string to timestamp (z&#39; is a bad directive in format)</title>
      <link>https://mneedham.github.io/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</link>
      <pubDate>Sun, 15 Mar 2015 22:43:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</guid>
      <description>I&#39;ve been playing around with importing Twitter data into Neo4j and since Neo4j can&#39;t store dates natively just yet I needed to convert a date string to timestamp. I started with the following which unfortunately throws an exception:
from datetime import datetime date = &amp;quot;Sat Mar 14 18:43:19 +0000 2015&amp;quot; &amp;gt;&amp;gt;&amp;gt; datetime.strptime(date, &amp;quot;%a %b %d %H:%M:%S %z %Y&amp;quot;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/System/Library/Frameworks/Python.</description>
    </item>
    
    <item>
      <title>Python: Checking any value in a list exists in a line of text</title>
      <link>https://mneedham.github.io/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</link>
      <pubDate>Sat, 14 Mar 2015 02:52:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</guid>
      <description>I&#39;ve been doing some log file analysis to see what cypher queries were being run on a Neo4j instance and I wanted to narrow down the lines I looked at to only contain ones which had mutating operations i.e. those containing the words MERGE, DELETE, SET or CREATE Here&#39;s an example of the text file I was parsing:
$ cat blog.txt MATCH n RETURN n MERGE (n:Person {name: &amp;quot;Mark&amp;quot;}) RETURN n MATCH (n:Person {name: &amp;quot;Mark&amp;quot;}) ON MATCH SET n.</description>
    </item>
    
    <item>
      <title>Python/Neo4j: Finding interesting computer sciency people to follow on Twitter</title>
      <link>https://mneedham.github.io/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</link>
      <pubDate>Wed, 11 Mar 2015 21:13:26 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</guid>
      <description>At the beginning of this year I moved from Neo4j&#39;s field team to dev team and since the code we write there is much lower level than I&#39;m used to I thought I should find some people to follow on twitter whom I can learn from.  My technique for finding some of those people was to pick a person from the Neo4j kernel team who&#39;s very good at systems programming and uses twitter which led me to Mr Chris Vest.</description>
    </item>
    
    <item>
      <title>Python: Streaming/Appending to a file</title>
      <link>https://mneedham.github.io/2015/03/09/python-streamingappending-to-a-file/</link>
      <pubDate>Mon, 09 Mar 2015 23:00:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/09/python-streamingappending-to-a-file/</guid>
      <description>I&#39;ve been playing around with Twitter&#39;s API (via the tweepy library) and due to the rate limiting it imposes I wanted to stream results to a CSV file rather than waiting until my whole program had finished.  I wrote the following program to simulate what I was trying to do: import csv import time with open(&amp;quot;rows.csv&amp;quot;, &amp;quot;a&amp;quot;) as file: writer = csv.writer(file, delimiter = &amp;quot;,&amp;quot;) end = time.time() + 10 while True: if time.</description>
    </item>
    
    <item>
      <title>Neo4j: TF/IDF (and variants) with cypher</title>
      <link>https://mneedham.github.io/2015/03/08/neo4j-tfidf-and-variants-with-cypher/</link>
      <pubDate>Sun, 08 Mar 2015 13:24:19 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/08/neo4j-tfidf-and-variants-with-cypher/</guid>
      <description>A few weeks ago I wrote a blog post on running TF/IDF over HIMYM transcripts using scikit-learn to find the most important phrases by episode and afterwards I was curious how difficult it&#39;d be to do in Neo4j. I started by translating one of wikipedia&#39;s TF/IDF examples to cypher to see what the algorithm would look like:
WITH 3 as termFrequency, 2 AS numberOfDocuments, 1 as numberOfDocumentsWithTerm WITH termFrequency, log10(numberOfDocuments / numberOfDocumentsWithTerm) AS inverseDocumentFrequency return termFrequency * inverseDocumentFrequency 0.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://mneedham.github.io/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>Following on from Rik van Bruggen&#39;s blog post on a QCon graph he&#39;s created ahead of this week&#39;s conference, I was curious whether we could extract any interesting relationships between talks based on their abstracts.  Talks are already grouped by their hosting track but there&#39;s likely to be some overlap in topics even for talks on different tracks. I therefore wanted to extract topics and connect each talk to the topic that describes it best.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn - Training a classifier with non numeric features</title>
      <link>https://mneedham.github.io/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</link>
      <pubDate>Mon, 02 Mar 2015 07:48:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</guid>
      <description>Following on from my previous posts on training a classifier to pick out the speaker in sentences of HIMYM transcripts the next thing to do was train a random forest of decision trees to see how that fared.  I&#39;ve used scikit-learn for this before so I decided to use that. However, before building a random forest I wanted to check that I could build an equivalent decision tree.  I initially thought that scikit-learn&#39;s DecisionTree classifier would take in data in the same format as nltk&#39;s so I started out with the following code: import json import nltk import collections from himymutil.</description>
    </item>
    
    <item>
      <title>Python: Detecting the speaker in HIMYM using Parts of Speech (POS) tagging</title>
      <link>https://mneedham.github.io/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</link>
      <pubDate>Sun, 01 Mar 2015 02:36:06 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</guid>
      <description>Over the last couple of weeks I&#39;ve been experimenting with different classifiers to detect speakers in HIMYM transcripts and in all my attempts so far the only features I&#39;ve used have been words.  This led to classifiers that were overfitted to the training data so I wanted to generalise them by introducing parts of speech of the words in sentences which are more generic.  First I changed the function which generates the features for each word to also contain the parts of speech of the previous and next words as well as the word itself: def pos_features(sentence, sentence_pos, i): features = {} features[&amp;quot;word&amp;quot;] = sentence[i] features[&amp;quot;word-pos&amp;quot;] = sentence_pos[i][1] if i == 0: features[&amp;quot;prev-word&amp;quot;] = &amp;quot;&amp;lt;START&amp;gt;&amp;quot; features[&amp;quot;prev-word-pos&amp;quot;] = &amp;quot;&amp;lt;START&amp;gt;&amp;quot; else: features[&amp;quot;prev-word&amp;quot;] = sentence[i-1] features[&amp;quot;prev-word-pos&amp;quot;] = sentence_pos[i-1][1] if i == len(sentence) - 1: features[&amp;quot;next-word&amp;quot;] = &amp;quot;&amp;lt;END&amp;gt;&amp;quot; features[&amp;quot;next-word-pos&amp;quot;] = &amp;quot;&amp;lt;END&amp;gt;&amp;quot; else: features[&amp;quot;next-word&amp;quot;] = sentence[i+1] features[&amp;quot;next-word-pos&amp;quot;] = sentence_pos[i+1][1] return features   Next we need to tweak our calling code to calculate the parts of speech tags for each sentence and pass it in: featuresets = [] for tagged_sent in tagged_sents: untagged_sent = nltk.</description>
    </item>
    
    <item>
      <title>R/ggplot: Controlling X axis order</title>
      <link>https://mneedham.github.io/2015/02/27/rggplot-controlling-x-axis-order/</link>
      <pubDate>Fri, 27 Feb 2015 00:49:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/27/rggplot-controlling-x-axis-order/</guid>
      <description>As part of a talk I gave at the Neo4j London meetup earlier this week I wanted to show how you could build a simple chart showing the number of friends that different actors had using the ggplot library. I started out with the following code:
df = read.csv(&amp;quot;/tmp/friends.csv&amp;quot;) top = df %&amp;gt;% head(20) ggplot(aes(x = p.name, y = colleagues), data = top) + geom_bar(fill = &amp;quot;dark blue&amp;quot;, stat = &amp;quot;identity&amp;quot;)  The friends CSV file is available as a gist if you want to reproduce the chart.</description>
    </item>
    
    <item>
      <title>R: Conditionally updating rows of a data frame</title>
      <link>https://mneedham.github.io/2015/02/26/r-conditionally-updating-rows-of-a-data-frame/</link>
      <pubDate>Thu, 26 Feb 2015 00:45:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/26/r-conditionally-updating-rows-of-a-data-frame/</guid>
      <description>In a blog post I wrote a couple of days ago about cohort analysis I had to assign a monthNumber to each row in a data frame and started out with the following code: library(zoo) library(dplyr) monthNumber = function(cohort, date) { cohortAsDate = as.yearmon(cohort) dateAsDate = as.yearmon(date) if(cohortAsDate &amp;gt; dateAsDate) { &amp;quot;NA&amp;quot; } else { paste(round((dateAsDate - cohortAsDate) * 12), sep=&amp;quot;&amp;quot;) } } cohortAttendance %&amp;gt;% group_by(row_number()) %&amp;gt;% mutate(monthNumber = monthNumber(cohort, date)) %&amp;gt;% filter(monthNumber !</description>
    </item>
    
    <item>
      <title>Python/nltk: Naive vs Naive Bayes vs Decision Tree</title>
      <link>https://mneedham.github.io/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</link>
      <pubDate>Tue, 24 Feb 2015 22:39:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</guid>
      <description>Last week I wrote a blog post describing a decision tree I&#39;d trained to detect the speakers in a How I met your mother transcript and after writing the post I wondered whether a simple classifier would do the job.  The simple classifier will work on the assumption that any word followed by a &#34;:&#34; is a speaker and anything else isn&#39;t. Here&#39;s the definition of a NaiveClassifier: import nltk from nltk import ClassifierI class NaiveClassifier(ClassifierI): def classify(self, featureset): if featureset[&#39;next-word&#39;] == &amp;quot;:&amp;quot;: return True else: return False  As you can see it only implements the classify method and executes a static check.</description>
    </item>
    
    <item>
      <title>R: Cohort analysis of Neo4j meetup members</title>
      <link>https://mneedham.github.io/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</link>
      <pubDate>Tue, 24 Feb 2015 01:19:26 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</guid>
      <description>A few weeks ago I came across a blog post explaining how to apply cohort analysis to customer retention using R and I thought it&#39;d be a fun exercise to calculate something similar for meetup attendees.  In the customer retention example we track customer purchases on a month by month basis and each customer is put into a cohort or bucket based on the first month they made a purchase in.</description>
    </item>
    
    <item>
      <title>R/dplyr: Extracting data frame column value for filtering with %in%</title>
      <link>https://mneedham.github.io/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</link>
      <pubDate>Sun, 22 Feb 2015 08:58:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</guid>
      <description>I&#39;ve been playing around with dplyr over the weekend and wanted to extract the values from a data frame column to use in a later filtering step.  I had a data frame: library(dplyr) df = data.frame(userId = c(1,2,3,4,5), score = c(2,3,4,5,5))  And wanted to extract the userIds of those people who have a score greater than 3. I started with:
highScoringPeople = df %&amp;gt;% filter(score &amp;gt; 3) %&amp;gt;% select(userId) &amp;gt; highScoringPeople userId 1 3 2 4 3 5   And then filtered the data frame expecting to get back those 3 people: &amp;gt; df %&amp;gt;% filter(userId %in% highScoringPeople) [1] userId score &amp;lt;0 rows&amp;gt; (or 0-length row.</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Detecting which sentences in a transcript contain a speaker</title>
      <link>https://mneedham.github.io/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</link>
      <pubDate>Fri, 20 Feb 2015 22:42:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</guid>
      <description>Over the past couple of months I&#39;ve been playing around with How I met your mother transcripts and the most recent thing I&#39;ve been working on is how to extract the speaker for a particular sentence. This initially seemed like a really simple problem as most of the initial sentences I looked at weere structured like this: &amp;lt;speaker&amp;gt;: &amp;lt;sentence&amp;gt;   If there were all in that format then we could write a simple regular expression and then move on but unfortunately they aren&#39;t.</description>
    </item>
    
    <item>
      <title>Python&#39;s pandas vs Neo4j&#39;s cypher: Exploring popular phrases in How I met your mother transcripts</title>
      <link>https://mneedham.github.io/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 19 Feb 2015 00:52:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</guid>
      <description>I&#39;ve previously written about extracting TF/IDF scores for phrases in documents using scikit-learn and the final step in that post involved writing the words into a CSV file for analysis later on.  I wasn&#39;t sure what the most appropriate tool of choice for that analysis was so I decided to explore the data using Python&#39;s pandas library and load it into Neo4j and write some Cypher queries. To do anything with Neo4j we need to first load the CSV file into the database.</description>
    </item>
    
    <item>
      <title>Python/pandas: Column value in list (ValueError: The truth value of a Series is ambiguous.)</title>
      <link>https://mneedham.github.io/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Mon, 16 Feb 2015 21:39:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>I&#39;ve been using Python&#39;s pandas library while exploring some CSV files and although for the most part I&#39;ve found it intuitive to use, I had trouble filtering a data frame based on checking whether a column value was in a list. A subset of one of the CSV files I&#39;ve been working with looks like this:
$ cat foo.csv &amp;quot;Foo&amp;quot; 1 2 3 4 5 6 7 8 9 10  Loading it into a pandas data frame is reasonably simple:</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Calculating TF/IDF on How I met your mother transcripts</title>
      <link>https://mneedham.github.io/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</link>
      <pubDate>Sun, 15 Feb 2015 15:56:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</guid>
      <description>Over the past few weeks I&#39;ve been playing around with various NLP techniques to find interesting insights into How I met your mother from its transcripts and one technique that kept coming up is TF/IDF.
The Wikipedia definition reads like this:
 tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</description>
    </item>
    
    <item>
      <title>Neo4j: Building a topic graph with Prismatic Interest Graph API</title>
      <link>https://mneedham.github.io/2015/02/13/neo4j-building-a-topic-graph-with-prismatic-interest-graph-api/</link>
      <pubDate>Fri, 13 Feb 2015 23:38:43 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/13/neo4j-building-a-topic-graph-with-prismatic-interest-graph-api/</guid>
      <description>Over the last few weeks I&#39;ve been using various NLP libraries to derive topics for my corpus of How I met your mother episodes without success and was therefore enthused to see the release of Prismatic&#39;s Interest Graph API  The Interest Graph API exposes a web service to which you feed a block of text and get back a set of topics and associated score.
It has been trained over the last few years with millions of articles that people share on their social media accounts and in my experience using Prismatic the topics have been very useful for finding new material to read.</description>
    </item>
    
    <item>
      <title>Python/gensim: Creating bigrams over How I met your mother transcripts</title>
      <link>https://mneedham.github.io/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 12 Feb 2015 23:45:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</guid>
      <description>As part of my continued playing around with How I met your mother transcripts I wanted to identify plot arcs and as a first step I wrote some code using the gensim and nltk libraries to identify bigrams (two word phrases).  There&#39;s an easy to follow tutorial in the gensim docs showing how to go about this but I needed to do a couple of extra steps to get my text data from a CSV file into the structure gensim expects.</description>
    </item>
    
    <item>
      <title>R: Weather vs attendance at NoSQL meetups</title>
      <link>https://mneedham.github.io/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</link>
      <pubDate>Wed, 11 Feb 2015 07:09:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</guid>
      <description>A few weeks ago I came across a tweet by Sean Taylor asking for a weather data set with a few years worth of recording and I was surprised to learn that R already has such a thing - the weatherData package. Winner is: @UTVilla! &amp;#10;library(weatherData)&amp;#10;df &amp;lt;- getWeatherForYear(&amp;quot;SFO&amp;quot;, 2013)&amp;#10;ggplot(df, aes(x=Date, y = Mean_TemperatureF)) + geom_line()
&amp;mdash; Sean J. Taylor (@seanjtaylor) January 22, 2015 
weatherData provides a thin veneer around the wunderground API and was exactly what I&#39;d been looking for to compare meetup at London&#39;s NoSQL against weather conditions that day.</description>
    </item>
    
    <item>
      <title>Python/matpotlib: Plotting occurrences of the main characters in How I Met Your Mother</title>
      <link>https://mneedham.github.io/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</link>
      <pubDate>Fri, 30 Jan 2015 21:29:00 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</guid>
      <description>Normally when I&#39;m playing around with data sets in R I get out ggplot2 to plot some charts to get a feel for the data but having spent quite a bit of time with Python and How I met your mother transcripts I haven&#39;t created a single plot. I thought I&#39;d better change change that.  After a bit of searching around it seems that matplotlib is the go to library for this job and I thought an interesting thing to plot would be how often each of the main characters appear in each episode across the show.</description>
    </item>
    
    <item>
      <title>R: ggplot2 - Each group consist of only one observation. Do you need to adjust the group aesthetic?</title>
      <link>https://mneedham.github.io/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</link>
      <pubDate>Fri, 30 Jan 2015 00:27:53 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</guid>
      <description>I&#39;ve been playing around with some weather data over the last couple of days which I aggregated down to the average temperature per month over the last 4 years and stored in a CSV file.
This is what the file looks like:
$ cat /tmp/averageTemperatureByMonth.csv &amp;quot;month&amp;quot;,&amp;quot;aveTemperature&amp;quot; &amp;quot;January&amp;quot;,6.02684563758389 &amp;quot;February&amp;quot;,5.89380530973451 &amp;quot;March&amp;quot;,7.54838709677419 &amp;quot;April&amp;quot;,10.875 &amp;quot;May&amp;quot;,13.3064516129032 &amp;quot;June&amp;quot;,15.9666666666667 &amp;quot;July&amp;quot;,18.8387096774194 &amp;quot;August&amp;quot;,18.3709677419355 &amp;quot;September&amp;quot;,16.2583333333333 &amp;quot;October&amp;quot;,13.4596774193548 &amp;quot;November&amp;quot;,9.19166666666667 &amp;quot;December&amp;quot;,7.01612903225806  I wanted to create a simple line chart which would show the months of the year in ascending order with the appropriate temperature.</description>
    </item>
    
    <item>
      <title>Python: Find the highest value in a group</title>
      <link>https://mneedham.github.io/2015/01/25/python-find-the-highest-value-in-a-group/</link>
      <pubDate>Sun, 25 Jan 2015 12:47:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/25/python-find-the-highest-value-in-a-group/</guid>
      <description>In my continued playing around with a How I met your mother data set I needed to find out the last episode that happened in a season so that I could use it in a chart I wanted to plot.  I had this CSV file containing each of the episodes: $ head -n 10 data/import/episodes.csv NumberOverall,NumberInSeason,Episode,Season,DateAired,Timestamp 1,1,/wiki/Pilot,1,&amp;quot;September 19, 2005&amp;quot;,1127084400 2,2,/wiki/Purple_Giraffe,1,&amp;quot;September 26, 2005&amp;quot;,1127689200 3,3,/wiki/Sweet_Taste_of_Liberty,1,&amp;quot;October 3, 2005&amp;quot;,1128294000 4,4,/wiki/Return_of_the_Shirt,1,&amp;quot;October 10, 2005&amp;quot;,1128898800 5,5,/wiki/Okay_Awesome,1,&amp;quot;October 17, 2005&amp;quot;,1129503600 6,6,/wiki/Slutty_Pumpkin,1,&amp;quot;October 24, 2005&amp;quot;,1130108400 7,7,/wiki/Matchmaker,1,&amp;quot;November 7, 2005&amp;quot;,1131321600 8,8,/wiki/The_Duel,1,&amp;quot;November 14, 2005&amp;quot;,1131926400 9,9,/wiki/Belly_Full_of_Turkey,1,&amp;quot;November 21, 2005&amp;quot;,1132531200   I started out by parsing the CSV file into a dictionary of (seasons - episode ids): import csv from collections import defaultdict seasons = defaultdict(list) with open(&amp;quot;data/import/episodes.</description>
    </item>
    
    <item>
      <title>Python/pdfquery: Scraping the FIFA World Player of the Year votes PDF into shape</title>
      <link>https://mneedham.github.io/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</link>
      <pubDate>Thu, 22 Jan 2015 00:25:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</guid>
      <description>Last week the FIFA Ballon d&#39;Or 2014 was announced and along with the announcement of the winner the individual votes were also made available.  Unfortunately they weren&#39;t made open in a way that Ben Wellington (of IQuantNY fame) would approve of - the choice of format for the data is a PDF file!  I wanted to extract this data to play around with it but I wanted to automate the extraction as I&#39;d done when working with Google Trends data.</description>
    </item>
    
    <item>
      <title>Python/NLTK: Finding the most common phrases in How I Met Your Mother</title>
      <link>https://mneedham.github.io/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</link>
      <pubDate>Mon, 19 Jan 2015 00:24:23 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</guid>
      <description>Following on from last week&#39;s blog post where I found the most popular words in How I met your mother transcripts, in this post we&#39;ll have a look at how we can pull out sentences and then phrases from our corpus. The first thing I did was tweak the scraping script to pull out the sentences spoken by characters in the transcripts.
Each dialogue is separated by two line breaks so we use that as our separator.</description>
    </item>
    
    <item>
      <title>Python: Counter - ValueError: too many values to unpack</title>
      <link>https://mneedham.github.io/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</link>
      <pubDate>Mon, 12 Jan 2015 23:16:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</guid>
      <description>I recently came across Python&#39;s Counter tool which makes it really easy to count the number of occurrences of items in a list.  In my case I was trying to work out how many times words occurred in a corpus so I had something like the following: &amp;gt;&amp;gt; from collections import Counter &amp;gt;&amp;gt; counter = Counter([&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;, &amp;quot;word3&amp;quot;, &amp;quot;word1&amp;quot;]) &amp;gt;&amp;gt; print counter Counter({&#39;word1&#39;: 2, &#39;word3&#39;: 1, &#39;word2&#39;: 1})   I wanted to write a for loop to iterate over the counter and print the (key, value) pairs and started with the following: &amp;gt;&amp;gt;&amp;gt; for key, value in counter: .</description>
    </item>
    
    <item>
      <title>Python: scikit-learn: ImportError: cannot import name __check_build</title>
      <link>https://mneedham.github.io/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</link>
      <pubDate>Sat, 10 Jan 2015 08:48:04 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</guid>
      <description>In part 3 of Kaggle&#39;s series on text analytics I needed to install scikit-learn and having done so ran into the following error when trying to use one of its classes: &amp;gt;&amp;gt;&amp;gt; from sklearn.feature_extraction.text import CountVectorizer Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/sklearn/__init__.py&amp;quot;, line 37, in &amp;lt;module&amp;gt; from . import __check_build ImportError: cannot import name __check_build  This error doesn&#39;t reveal very much but I found that when I exited the REPL and tried the same command again I got a different error which was a bit more useful:</description>
    </item>
    
    <item>
      <title>Python: gensim - clang: error: unknown argument: &#39;-mno-fused-madd&#39; [-Wunused-command-line-argument-hard-error-in-future]</title>
      <link>https://mneedham.github.io/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</link>
      <pubDate>Sat, 10 Jan 2015 08:39:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</guid>
      <description>While working through part 2 of Kaggle&#39;s bag of words tutorial I needed to install the gensim library and initially ran into the following error: $ pip install gensim ... cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/Users/markneedham/projects/neo4j-himym/himym/build/gensim/gensim/models -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.</description>
    </item>
    
    <item>
      <title>Python NLTK/Neo4j: Analysing the transcripts of How I Met Your Mother</title>
      <link>https://mneedham.github.io/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</link>
      <pubDate>Sat, 10 Jan 2015 01:22:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</guid>
      <description>After reading Emil&#39;s blog post about dark data a few weeks ago I became intrigued about trying to find some structure in free text data and I thought How I met your mother&#39;s transcripts would be a good place to start. I found a website which has the transcripts for all the episodes and then having manually downloaded the two pages which listed all the episodes, wrote a script to grab each of the transcripts so I could use them on my machine.</description>
    </item>
    
  </channel>
</rss>