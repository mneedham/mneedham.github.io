<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2012s on Mark Needham</title>
    <link>https://mneedham.github.io/2012/</link>
    <description>Recent content in 2012s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 Dec 2012 23:59:42 +0000</lastBuildDate>
    
	<atom:link href="https://mneedham.github.io/2012/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>TextMate Bundles location on Mountain Lion</title>
      <link>https://mneedham.github.io/2012/12/31/textmate-bundles-location-on-mountain-lion/</link>
      <pubDate>Mon, 31 Dec 2012 23:59:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/textmate-bundles-location-on-mountain-lion/</guid>
      <description>Something that I&#39;ve noticed when trying to install various different bundles is that the installation instructions which worked flawlessly on Snow Leopard don&#39;t seem to do the job on Mountain Lion.
For example, the Clojure bundle assumes that the installation directory is &#39;~/Library/Application\ Support/TextMate/Bundles&#39; but for some reason the &#39;Bundles&#39; folder doesn&#39;t exist.
We therefore have two choices:
 mkdir -p ~/Library/Application\ Support/TextMate/Bundles and then continue as normal Install our bundle into &#39;/Applications/TextMate.</description>
    </item>
    
    <item>
      <title>Haskell: Downloading the core library source code</title>
      <link>https://mneedham.github.io/2012/12/31/haskell-downloading-the-core-library-source-code/</link>
      <pubDate>Mon, 31 Dec 2012 22:39:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/haskell-downloading-the-core-library-source-code/</guid>
      <description>I&#39;ve started playing around with Haskell again and since I&#39;m doing so on a new machine I don&#39;t have a copy of the language source code. I wanted to rectify that situation but my Google fu was weak and it took me way too long to figure out how to get it so I thought I&#39;d better document it for future me.
The easiest way is to clone the copy of the GHC repository on github:</description>
    </item>
    
    <item>
      <title>Haskell: Strictness and the monadic bind</title>
      <link>https://mneedham.github.io/2012/12/31/haskell-strictness-and-the-monadic-bind/</link>
      <pubDate>Mon, 31 Dec 2012 22:27:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/haskell-strictness-and-the-monadic-bind/</guid>
      <description>As I mentioned towards the end of my post about implementing the union find data structure in Haskell I wrote another version using a mutable array and having not seen much of a performance improvement started commenting out code to try and find the problem.
I eventually narrowed it down to the union function which was defined like so:
union :: IO (IOArray Int Int) -&amp;gt; Int -&amp;gt; Int -&amp;gt; IO (IOArray Int Int) union arrayContainer x y = do actualArray &amp;lt;- arrayContainer ls &amp;lt;- getAssocs actualArray leader1 &amp;lt;- readArray actualArray x leader2 &amp;lt;- readArray actualArray y let newValues = (map (\(index, value) -&amp;gt; (index, leader1)) .</description>
    </item>
    
    <item>
      <title>Haskell: An impressively non performant union find</title>
      <link>https://mneedham.github.io/2012/12/31/haskell-an-impressively-non-performant-union-find/</link>
      <pubDate>Mon, 31 Dec 2012 20:44:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/haskell-an-impressively-non-performant-union-find/</guid>
      <description>I&#39;ve spent the best part of the last day debugging a clustering algorithm I wrote as part of the Algorithms 2 course, eventually coming to the conclusion that the union find data structure I was using wasn&#39;t working as expected.
In our algorithm we&#39;re trying to group together points which are &#39;close&#39; to each other and the data structure is particular useful for doing that.
To paraphrase from my previous post about how we use the union find data structure:</description>
    </item>
    
    <item>
      <title>Bitwise operations in Ruby and Haskell</title>
      <link>https://mneedham.github.io/2012/12/31/bitwise-operations-in-ruby-and-haskell/</link>
      <pubDate>Mon, 31 Dec 2012 13:14:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/bitwise-operations-in-ruby-and-haskell/</guid>
      <description>Part of one of the most recent problems in the Algorithms 2 course required us to find the &#39;neighbours&#39; of binary values.
In this case a neighbour is described as being any other binary value which has an equivalent value or differs in 1 or 2 bits.
e.g. the neighbours of &#39;10000&#39; would be &#39;00000&#39;, &#39;00001&#39;, &#39;00010&#39;, &#39;00100&#39;, &#39;&#39;01000&#39;, &#39;10001&#39;, &#39;10010&#39;, &#39;10011&#39;, &#39;10100&#39;, &#39;10101&#39;, &#39;10110&#39;, &#39;11000&#39;, &#39;11001&#39;, &#39;11010&#39; and &#39;11100&#39;~~~ I initially treated &#39;10000&#39; as an array of 1s and 0s and wrote a function to recursively come up with the above combinations before it was pointed out to me that it&#39;d be much easier to use bit wise logic instead.</description>
    </item>
    
    <item>
      <title>Gamification and Software: Some thoughts</title>
      <link>https://mneedham.github.io/2012/12/31/gamification-and-software-some-thoughts/</link>
      <pubDate>Mon, 31 Dec 2012 10:57:19 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/31/gamification-and-software-some-thoughts/</guid>
      <description>On the recommendation of J.B. Rainsberger I&#39;ve been reading &#39;Reality is Broken&#39; - a book which talks about how we can apply some of the things games designers have learned about getting people engaged to real life.
The author, Jane McGonigal, also has a TED talk on the topic which will help you get a flavour for the topic.
I was particularly interested in trying to see how her ideas could be applied in a software context and indeed how they are already being applied.</description>
    </item>
    
    <item>
      <title>Haskell: Using qualified imports to avoid polluting the namespace</title>
      <link>https://mneedham.github.io/2012/12/30/haskell-using-qualified-imports-to-avoid-polluting-the-namespace/</link>
      <pubDate>Sun, 30 Dec 2012 23:16:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/30/haskell-using-qualified-imports-to-avoid-polluting-the-namespace/</guid>
      <description>In most of the Haskell code I&#39;ve read any functions from other modules have been imported directly into the namespace and I reached the stage where I had this list of imports in a file:
import System.IO import Data.List.Split import Data.Char import Data.Bits import Control.Monad import Data.Map import Data.Set import Data.List import Data.Maybe  This becomes a problem when you want to use a function which is defined in multiple modules such as filter:</description>
    </item>
    
    <item>
      <title>Haskell: Pattern matching a list</title>
      <link>https://mneedham.github.io/2012/12/30/haskell-pattern-matching-a-list/</link>
      <pubDate>Sun, 30 Dec 2012 22:39:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/30/haskell-pattern-matching-a-list/</guid>
      <description>As I mentioned in a post yesterday I&#39;ve been converting a clustering algorithm into Haskell and I wanted to get the value from doing a bit wise or on two values in a list.
I forgot it was possible to pattern match on lists until I came across a post I wrote about 8 months ago where I&#39;d done this so my initial code looked like this:
&amp;gt; import Data.Bits &amp;gt; map (\pair -&amp;gt; (pair !</description>
    </item>
    
    <item>
      <title>Haskell: A cleaner way of initialising a map</title>
      <link>https://mneedham.github.io/2012/12/29/haskell-a-cleaner-way-of-initialising-a-map/</link>
      <pubDate>Sat, 29 Dec 2012 20:14:12 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/29/haskell-a-cleaner-way-of-initialising-a-map/</guid>
      <description>I recently wrote a blog post showing a way of initialising a Haskell map and towards the end of the post I realised how convoluted my approach was and wondered if there was an easier way and indeed there is!
To recap, this is the code I ended up with to populate a map with binary based values as the keys and node ids as the values:
import Data.Map toMap :: [Int] -&amp;gt; Map Int [Int] toMap nodes = fromList $ map asMapEntry $ (groupIgnoringIndex .</description>
    </item>
    
    <item>
      <title>Haskell: Initialising a map</title>
      <link>https://mneedham.github.io/2012/12/29/haskell-initialising-a-map/</link>
      <pubDate>Sat, 29 Dec 2012 19:27:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/29/haskell-initialising-a-map/</guid>
      <description>I&#39;ve been converting a variation of Kruskal&#39;s algorithm from Ruby into Haskell and one thing I needed to do was create a map of binary based values to node ids.
In Ruby I wrote the following code to do this:
nodes = [1,2,5,7,2,4] @magical_hash = {} nodes.each_with_index do |node, index| @magical_hash[node] ||= [] @magical_hash[node] &amp;lt;&amp;lt; index end =&amp;gt; {1=&amp;gt;[0], 2=&amp;gt;[1, 4], 5=&amp;gt;[2], 7=&amp;gt;[3], 4=&amp;gt;[5]}  From looking at the documentation it seemed like the easiest way to do this in Haskell would be to convert the nodes into an appropriate list and then call the fromList function to build the map.</description>
    </item>
    
    <item>
      <title>Sed: Replacing characters with a new line</title>
      <link>https://mneedham.github.io/2012/12/29/sed-replacing-characters-with-a-new-line/</link>
      <pubDate>Sat, 29 Dec 2012 17:49:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/29/sed-replacing-characters-with-a-new-line/</guid>
      <description>I&#39;ve been playing around with writing some algorithms in both Ruby and Haskell and the latter wasn&#39;t giving the correct result so I wanted to output an intermediate state of the two programs and compare them.
I didn&#39;t do any fancy formatting of the output from either program so I had the raw data structures in text files which I needed to transform so that they were comparable.
The main thing I wanted to do was get each of the elements of the collection onto their own line.</description>
    </item>
    
    <item>
      <title>Restricting your own learning</title>
      <link>https://mneedham.github.io/2012/12/27/restricting-your-own-learning/</link>
      <pubDate>Thu, 27 Dec 2012 00:45:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/27/restricting-your-own-learning/</guid>
      <description>For the first few years that I worked professionally* every project that I worked on was different enough to the previous ones that I was always learning something new without having to put much effort in.
After a while this became less the case because I&#39;d seen more things and if I saw something even remotely similar I would abstract it away as something that I&#39;d done before.
A couple of months ago Martin Fowler wrote a blog post about priming and how research has showed that exposure to a stimulus influences a response to a later stimulus.</description>
    </item>
    
    <item>
      <title>Mahout: Parallelising the creation of DecisionTrees</title>
      <link>https://mneedham.github.io/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</link>
      <pubDate>Thu, 27 Dec 2012 00:08:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</guid>
      <description>A couple of months ago I wrote a blog post describing our use of Mahout random forests for the Kaggle Digit Recogniser Problem and after seeing how long it took to create forests with 500+ trees I wanted to see if this could be sped up by parallelising the process.
From looking at the DecisionTree it seemed like it should be possible to create lots of small forests and then combine them together.</description>
    </item>
    
    <item>
      <title>The Tracer Bullet Approach: An example</title>
      <link>https://mneedham.github.io/2012/12/24/the-tracer-bullet-approach-an-example/</link>
      <pubDate>Mon, 24 Dec 2012 09:09:44 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/24/the-tracer-bullet-approach-an-example/</guid>
      <description>A few weeks ago my former colleague Kief Morris wrote a blog post describing the tracer bullet approach he&#39;s used to setup a continuous delivery pipeline on his current project.
 The idea is to get the simplest implementation of a pipeline in place, prioritizing a fully working skeleton that stretches across the full path to production over a fully featured, final-design functionality for each stage of the pipeline.  Kief goes on to explain in detail how we can go about executing this and it reminded of a project I worked on almost 3 years ago where we took a similar approach.</description>
    </item>
    
    <item>
      <title>Kruskal&#39;s Algorithm using union find in Ruby</title>
      <link>https://mneedham.github.io/2012/12/23/kruskals-algorithm-using-union-find-in-ruby/</link>
      <pubDate>Sun, 23 Dec 2012 21:43:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/23/kruskals-algorithm-using-union-find-in-ruby/</guid>
      <description>I recently wrote a blog post describing my implementation of Kruskal&#39;s algorithm - a greedy algorithm using to find a minimum spanning tree (MST) of a graph - and while it does the job it&#39;s not particularly quick.
It takes 20 seconds to calculate the MST for a 500 node, ~2000 edge graph.
One way that we can improve the performance of the algorithm is by storing the MST in a union find/disjoint set data structure.</description>
    </item>
    
    <item>
      <title>Kruskal&#39;s Algorithm in Ruby</title>
      <link>https://mneedham.github.io/2012/12/23/kruskals-algorithm-in-ruby/</link>
      <pubDate>Sun, 23 Dec 2012 14:18:53 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/23/kruskals-algorithm-in-ruby/</guid>
      <description>Last week I wrote a couple of posts showing different implementations of Prim&#39;s algorithm - an algorithm using to find a minimum spanning tree in a graph - and a similar algorithm is Kruskal&#39;s algorithm.
Kruskal&#39;s algorithm also finds a minimum spanning tree but it goes about it in a slightly different way.
Prim&#39;s algorithm takes an approach whereby we select nodes and then find connecting edges until we&#39;ve covered all the nodes.</description>
    </item>
    
    <item>
      <title>Prim&#39;s algorithm using a heap/priority queue in Ruby</title>
      <link>https://mneedham.github.io/2012/12/15/prims-algorithm-using-a-heappriority-queue-in-ruby/</link>
      <pubDate>Sat, 15 Dec 2012 16:31:05 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/15/prims-algorithm-using-a-heappriority-queue-in-ruby/</guid>
      <description>I recently wrote a blog post describing my implementation of Prim&#39;s Algorithm for the Algorithms 2 class and while it comes up with the right answer for the supplied data set it takes almost 30 seconds to do so!
In one of the lectures Tim Roughgarden points out that we&#39;re doing the same calculations multiple times to work out the next smallest edge to include in our minimal spanning tree and could use a heap to speed things up.</description>
    </item>
    
    <item>
      <title>Prim&#39;s Algorithm in Ruby</title>
      <link>https://mneedham.github.io/2012/12/15/prims-algorithm-in-ruby/</link>
      <pubDate>Sat, 15 Dec 2012 02:51:14 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/15/prims-algorithm-in-ruby/</guid>
      <description>One of the first programming assignments of the Algorithms 2 course was to code Prim&#39;s algorithm - a greedy algorithm used to find the minimum spanning tree of a connected weighted undirected graph.
In simpler terms we need to find the path of least cost which connects all of the nodes together and there can&#39;t be any cycles in that path.
Wikipedia has a neat diagram which shows this more clearly:</description>
    </item>
    
    <item>
      <title>Weka: Saving and loading classifiers</title>
      <link>https://mneedham.github.io/2012/12/12/weka-saving-and-loading-classifiers/</link>
      <pubDate>Wed, 12 Dec 2012 00:04:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/12/weka-saving-and-loading-classifiers/</guid>
      <description>In our continued machine learning travels Jen and I have been building some classifiers using Weka and one thing we wanted to do was save the classifier and then reuse it later.
There is documentation for how to do this from the command line but we&#39;re doing everything programatically and wanted to be able to save our classifiers from Java code.
As it turns out it&#39;s not too tricky when you know which classes to call and saving a classifier to a file is as simple as this:</description>
    </item>
    
    <item>
      <title>rsyncing to an AWS instance</title>
      <link>https://mneedham.github.io/2012/12/11/rsyncing-to-an-aws-instance/</link>
      <pubDate>Tue, 11 Dec 2012 23:44:05 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/11/rsyncing-to-an-aws-instance/</guid>
      <description>I wanted to try running some of the machine learning algorithms that Jen and I have been playing around with on a beefier machine so I thought spinning up an AWS instance would be the best way to do that.
I built the JAR with the appropriate algorithms on my machine and then wanted to copy it up onto an AWS instance.
I could have used scp but I quite like the progress bar that you can get with rsync and since the JAR had somehow drifted to a size of 47MB the progress bar was useful.</description>
    </item>
    
    <item>
      <title>apt-get update: 416 Requested Range Not Satisfiable</title>
      <link>https://mneedham.github.io/2012/12/10/apt-get-update-416-requested-range-not-satisfiable/</link>
      <pubDate>Mon, 10 Dec 2012 00:39:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/10/apt-get-update-416-requested-range-not-satisfiable/</guid>
      <description>We were trying to run a puppet update on some machines last week and one of the first things it does is run &#39;apt-get update&#39; which was working on all but one node for which it was returning the following exception:
Err http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ i386 Packages 416 Requested Range Not Satisfiable Fetched 5,079B in 2s (2,296B/s) W: Failed to fetch http://us-west-1.ec2.archive.ubuntu.com/ubuntu/dists/maverick-updates/main/binary-i386/Packages.gz 416 Requested Range Not Satisfiable  It turns out one way that exception can manifest is if you&#39;ve got a partial copy of the index files from the repository and in this case the solution was as simple as deleting those and trying again:</description>
    </item>
    
    <item>
      <title>Data Science: Discovery work</title>
      <link>https://mneedham.github.io/2012/12/09/data-science-discovery-work/</link>
      <pubDate>Sun, 09 Dec 2012 10:36:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/09/data-science-discovery-work/</guid>
      <description>Aaron Erickson recently wrote a blog post where he talks through some of the problems he&#39;s seen with big data initiatives where organisations end up buying a product and expecting it to magically produce results.
 […] corporate IT departments are suddenly are looking at their long running “Business Intelligence” initiatives and wondering why they are not seeing the same kinds of return on investment. They are thinking… if only we tweaked that “BI” initiative and somehow mix in some “Big Data”, maybe *we* could become the next Amazon.</description>
    </item>
    
    <item>
      <title>Micro Services: Plugging in 3rd party components</title>
      <link>https://mneedham.github.io/2012/12/04/micro-services-plugging-in-3rd-party-components/</link>
      <pubDate>Tue, 04 Dec 2012 23:38:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/04/micro-services-plugging-in-3rd-party-components/</guid>
      <description>Over the past few weeks I&#39;ve been involved in conversations with different clients around micro services and one thing about this architecture that seems quite popular is the ability to easily plug in 3rd party components.
In one case we were talking through the design of a system which would calculate and then apply price optimisations on products. The parts of the system we were discussing looked roughly like this:</description>
    </item>
    
    <item>
      <title>There&#39;s No such thing as a &#39;DevOps Team&#39;: Some thoughts</title>
      <link>https://mneedham.github.io/2012/11/30/theres-no-such-thing-as-a-devops-team-some-thoughts/</link>
      <pubDate>Fri, 30 Nov 2012 16:56:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/30/theres-no-such-thing-as-a-devops-team-some-thoughts/</guid>
      <description>A few weeks ago Jez Humble wrote a blog post titled &#34;There&#39;s no such thing as a &#39;DevOps team&#39;&#34; where he explains what DevOps is actually supposed to be about and describes a model of how developers and operations folk can work together.
Jez&#39;s suggestion is for developers to take responsibility for the systems they create but he notes that:
[...] they need support from operations to understand how to build reliable software that can be continuous deployed to an unreliable platform that scales horizontally.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://mneedham.github.io/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle&amp;rsquo;s Digit Recognizer Jen and I decided to try out boosting on our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
We couldn&amp;rsquo;t find any documentation that it was possible to apply boosting to Mahout&amp;rsquo;s random forest algorithm but we knew it was possible with Weka so we decided to use that instead!
As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it&amp;rsquo;s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>Micro Services: The curse of code &#39;duplication&#39;</title>
      <link>https://mneedham.github.io/2012/11/28/micro-services-the-curse-of-code-duplication/</link>
      <pubDate>Wed, 28 Nov 2012 08:11:04 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/28/micro-services-the-curse-of-code-duplication/</guid>
      <description>A common approach we&amp;rsquo;ve been taking on some of the applications I&amp;rsquo;ve worked on recently is to decompose the system we&amp;rsquo;re building into smaller micro services which are independently deployable and communicate with each other over HTTP.
An advantage of decomposing systems like that is that we could have separate teams working on each service and then make use of a consumer driven contract as a way of ensuring the contract between them is correct.</description>
    </item>
    
    <item>
      <title>Jersey: com.sun.jersey.api.client.ClientHandlerException: A message body reader for Java class [...] and MIME media type application/json was not found</title>
      <link>https://mneedham.github.io/2012/11/28/jersey-com-sun-jersey-api-client-clienthandlerexception-a-message-body-reader-for-java-class-and-mime-media-type-applicationjson-was-not-found/</link>
      <pubDate>Wed, 28 Nov 2012 06:03:55 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/28/jersey-com-sun-jersey-api-client-clienthandlerexception-a-message-body-reader-for-java-class-and-mime-media-type-applicationjson-was-not-found/</guid>
      <description>We&amp;rsquo;ve used the Jersey library on the last couple of Java based applications that I&amp;rsquo;ve worked on and one thing we&amp;rsquo;ve done on both of them is write services that communicate with each other using JSON.
On both occasions we didn&amp;rsquo;t quite setup the Jersey client correctly and ended up with an error along these lines when making a call to an end point:
com.sun.jersey.api.client.ClientHandlerException: A message body reader for Java class java.</description>
    </item>
    
    <item>
      <title>IntelliJ Debug Mode: Viewing beyond 100 frames/items in an array</title>
      <link>https://mneedham.github.io/2012/11/26/intellij-debug-mode-viewing-beyond-100-framesitems-in-an-array/</link>
      <pubDate>Mon, 26 Nov 2012 04:28:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/26/intellij-debug-mode-viewing-beyond-100-framesitems-in-an-array/</guid>
      <description>In my continued attempts at the Kaggle Digit Recognizer problem I&amp;rsquo;ve been playing around with the encog library to try and build a neural networks solution to the problem.
Unfortunately it&amp;rsquo;s not quite working at the moment so I wanted to debug the code and see whether the input parameters were being correctly translated from the CSV file.
Each input is an array containing 784 values but by default IntelliJ restricts you to seeing 100 elements which wasn&amp;rsquo;t helpful in my case since the early values tend to all be 0 and it&amp;rsquo;s not until you get half way through that you see different values:</description>
    </item>
    
    <item>
      <title>A first failed attempt at Natural Language Processing</title>
      <link>https://mneedham.github.io/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</link>
      <pubDate>Sat, 24 Nov 2012 19:43:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</guid>
      <description>One of the things I find fascinating about dating websites is that the profiles of people are almost identical so I thought it would be an interesting exercise to grab some of the free text that people write about themselves and prove the similarity.
I&amp;rsquo;d been talking to Matt Biddulph about some Natural Language Processing (NLP) stuff he&amp;rsquo;d been working on and he wrote up a bunch of libraries, articles and books that he&amp;rsquo;d found useful.</description>
    </item>
    
    <item>
      <title>Core Competency</title>
      <link>https://mneedham.github.io/2012/11/24/core-competency/</link>
      <pubDate>Sat, 24 Nov 2012 12:44:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/24/core-competency/</guid>
      <description>For at least the last few years I&amp;rsquo;ve heard colleagues talk about working out the core competency of our clients businesses and I&amp;rsquo;d confused myself into thinking that the software we helped them build was the core competency.
I think Martin Fowler best explains how technology and business core competences work in his post about utility and strategic projects where he describes the difference between these like so:
 So what is the distinguishing factor between utility and strategic projects?</description>
    </item>
    
    <item>
      <title>Windows line endings: Exception in thread &#39;main&#39; java.io.FileNotFoundException /opt/app/config.yml^M (no such file or directory)</title>
      <link>https://mneedham.github.io/2012/11/24/windows-line-endings-exception-in-thread-main-java-io-filenotfoundexception-optappconfig-ymlm-no-such-file-or-directory/</link>
      <pubDate>Sat, 24 Nov 2012 09:04:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/24/windows-line-endings-exception-in-thread-main-java-io-filenotfoundexception-optappconfig-ymlm-no-such-file-or-directory/</guid>
      <description>As I mentioned in my previous post we&amp;rsquo;ve been making it possible to deploy our application to a new environment and as part of this we defined an upstart script which would run the JAR.
We tend to edit code on Windows and then test it out on the vagrant VM afterwards.
The end of our upstart script looked a bit like this:
script cd /opt/app java -jar /opt/app/app.jar /opt/app/config.yml end script  Unfortunately when we tried to launch the application using &amp;lsquo;start app&amp;rsquo; we got this error:</description>
    </item>
    
    <item>
      <title>Java: java.lang.UnsupportedClassVersionError - Unsupported major.minor version 51.0</title>
      <link>https://mneedham.github.io/2012/11/24/java-java-lang-unsupportedclassversionerror-unsupported-major-minor-version-51-0/</link>
      <pubDate>Sat, 24 Nov 2012 08:49:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/24/java-java-lang-unsupportedclassversionerror-unsupported-major-minor-version-51-0/</guid>
      <description>On my current project we&amp;rsquo;ve spent the last day or so setting up an environment where we can deploy a couple of micro services to.
Although the machines are Windows based we&amp;rsquo;re deploying the application onto a vagrant managed VM since the production environment will be a flavour of Linux.
Initially I was getting quite confused about whether or not we were in the VM or not and ended up with this error when trying to run the compiled JAR:</description>
    </item>
    
    <item>
      <title>Looking inside the black box</title>
      <link>https://mneedham.github.io/2012/11/21/looking-inside-the-black-box/</link>
      <pubDate>Wed, 21 Nov 2012 19:42:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/21/looking-inside-the-black-box/</guid>
      <description>I recently came across a really interesting post about black box abstraction by Angeleah where she talks about developers desire to know how things work and the need to understand when and when not to follow that instinct.
Angeleah defines black box abstraction like so:
 It is a technique for controlling complexity and abstracting detail. The point of doing this is to allow you to to build bigger things. Hopefully bigger boxes.</description>
    </item>
    
    <item>
      <title>Learning: Switching between theory and practice</title>
      <link>https://mneedham.github.io/2012/11/19/learning-switching-between-theory-and-practice/</link>
      <pubDate>Mon, 19 Nov 2012 13:31:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/19/learning-switching-between-theory-and-practice/</guid>
      <description>In one of my first ever blog posts I wrote about the differences I&amp;rsquo;d experienced in learning the theory about a topic and then seeing it in practice.
The way I remember learning at school and university was that you learn all the theory first and then put it into practice but I typically don&amp;rsquo;t find myself doing this whenever I learn something new.
I spent a bit of time over the weekend learning more about neural networks as my colleague Jen Smith suggested this might be a more effective technique for getting a higher accuracy score on the Kaggle Digit Recogniser problem.</description>
    </item>
    
    <item>
      <title>Incremental/iterative development: Breaking down work</title>
      <link>https://mneedham.github.io/2012/11/19/incrementaliterative-development-breaking-down-work/</link>
      <pubDate>Mon, 19 Nov 2012 08:50:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/19/incrementaliterative-development-breaking-down-work/</guid>
      <description>Over the past couple of years I&amp;rsquo;ve worked on several different applications and one thing they had in common was that they had a huge feature which would take a few months to complete and initially seemed difficult to break down.
Since we favoured an incremental/iterative approach to building these features and wanted to add value in short feedback cycles we needed to find a way to break them down.</description>
    </item>
    
    <item>
      <title>Buy vs Build: Driving from the problem</title>
      <link>https://mneedham.github.io/2012/11/17/buy-vs-build-driving-from-the-problem/</link>
      <pubDate>Sat, 17 Nov 2012 16:56:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/17/buy-vs-build-driving-from-the-problem/</guid>
      <description>My colleague Erik Doernenburg has written a couple of articles recently discussing the reasons why people buy and build IT solutions and one part in particular resonated with me:
 it is also possible, and not uncommon, that the software package does not do exactly what the business needs, leading to decreased productivity and lost opportunities.  I feel like there&amp;rsquo;s a mindset change once you start thinking which package you could buy to solve your problem whereby you stop solving the problem you actually have and focus instead on what features the package offers.</description>
    </item>
    
    <item>
      <title>Web Operations: Feature flags to turn off failing parts of infrastructure</title>
      <link>https://mneedham.github.io/2012/11/13/web-operations-feature-flags-to-turn-off-failing-parts-of-infrastructure/</link>
      <pubDate>Tue, 13 Nov 2012 12:19:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/13/web-operations-feature-flags-to-turn-off-failing-parts-of-infrastructure/</guid>
      <description>On most of the projects I&amp;rsquo;ve worked on over the last couple of years we&amp;rsquo;ve made use of feature toggles that we used to turn pending features on and off while they were still being built but while reading Web Operations I came across another usage.
In the chapter titled &amp;lsquo;Dev and Ops Collaboration and Cooperation&amp;rsquo; Paul Hammond suggests the following:
 Eventually some of your infrastructure will fail in an unexpected way.</description>
    </item>
    
    <item>
      <title>Unix: Counting the number of commas on a line</title>
      <link>https://mneedham.github.io/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</link>
      <pubDate>Sat, 10 Nov 2012 16:30:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</guid>
      <description>A few weeks ago I was playing around with some data stored in a CSV file and wanted to do a simple check on the quality of the data by making sure that each line had the same number of fields.
One way this can be done is with awk:
awk -F &amp;quot;,&amp;quot; &#39; { print NF-1 } &#39; file.csv  Here we&amp;rsquo;re specifying the file separator -F as &amp;lsquo;,&amp;rsquo; and then using the NF (number of fields) variable to print how many commas there are on the line.</description>
    </item>
    
    <item>
      <title>Clojure: Thread last (-&gt;&gt;) vs Thread first (-&gt;)</title>
      <link>https://mneedham.github.io/2012/11/06/clojure-thread-last-vs-thread-first/</link>
      <pubDate>Tue, 06 Nov 2012 12:42:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/06/clojure-thread-last-vs-thread-first/</guid>
      <description>In many of the Clojure examples that I&amp;rsquo;ve come across the thread last (-&amp;gt;&amp;gt;) macro is used to make it easier (for people from a non lispy background!) to see the transformations that the initial data structure is going through.
In one of my recent posts I showed how Jen &amp;amp; I had rewritten Mahout&amp;rsquo;s entropy function in Clojure:
(defn calculate-entropy [counts data-size] (-&amp;gt;&amp;gt; counts (remove #{0}) (map (partial individual-entropy data-size)) (reduce +)))  Here we are using the thread last operator to first pass counts as the last argument of the remove function on the next line, then to pass the result of that to the map function on the next line and so on.</description>
    </item>
    
    <item>
      <title>Emacs/Clojure: Starting out with paredit</title>
      <link>https://mneedham.github.io/2012/10/31/emacsclojure-starting-out-with-paredit/</link>
      <pubDate>Wed, 31 Oct 2012 08:41:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/31/emacsclojure-starting-out-with-paredit/</guid>
      <description>I&amp;rsquo;ve been complaining recently to Jen and Bruce about the lack of a beginner&amp;rsquo;s guide to emacs paredit mode which seems to be the defacto approach for people working with Clojure and both pointed me to the paredit cheat sheet.
While it&amp;rsquo;s very comprehensive, I found that it&amp;rsquo;s a little overwhelming for a complete newbie like myself.
I therefore thought it&amp;rsquo;d be useful to write a bit about a couple of things that I&amp;rsquo;ve picked up from pairing with Jen on little bits of Clojure over the last couple of months.</description>
    </item>
    
    <item>
      <title>Clojure: Mahout&#39;s &#39;entropy&#39; function</title>
      <link>https://mneedham.github.io/2012/10/30/clojure-mahouts-entropy-function/</link>
      <pubDate>Tue, 30 Oct 2012 22:46:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/30/clojure-mahouts-entropy-function/</guid>
      <description>As I mentioned in a couple of previous posts Jen and I have been playing around with Mahout random forests and for a few hours last week we spent some time looking through the code to see how it worked.
In particular we came across an entropy function which is used to determine how good a particular &amp;lsquo;split&amp;rsquo; point in a decision tree is going to be.
I quite like the following definition:  The level of certainty of a particular decision can be measured as a number from 1 (completely uncertain) to 0 (completely certain).</description>
    </item>
    
    <item>
      <title>Mahout: Using a saved Random Forest/DecisionTree</title>
      <link>https://mneedham.github.io/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</link>
      <pubDate>Sat, 27 Oct 2012 22:03:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</guid>
      <description>One of the things that I wanted to do while playing around with random forests using Mahout was to save the random forest and then use use it again which is something Mahout does cater for.
It was actually much easier to do this than I&amp;rsquo;d expected and assuming that we already have a DecisionForest built we&amp;rsquo;d just need the following code to save it to disc:
int numberOfTrees = 1; Data data = loadData(.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Mahout Random Forest attempt</title>
      <link>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 20:24:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</guid>
      <description>I&amp;rsquo;ve written previously about the K-means approach that Jen and I took when trying to solve Kaggle&amp;rsquo;s Digit Recognizer and having stalled at about 80% accuracy we decided to try one of the algorithms suggested in the tutorials section - the random forest!
We initially used a clojure random forests library but struggled to build the random forest from the training set data in a reasonable amount of time so we switched to Mahout&amp;rsquo;s version which is based on Leo Breiman&amp;rsquo;s random forests paper.</description>
    </item>
    
    <item>
      <title>Retrospectives: An alternative safety check</title>
      <link>https://mneedham.github.io/2012/10/27/retrospectives-an-alternative-safety-check/</link>
      <pubDate>Sat, 27 Oct 2012 18:21:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/retrospectives-an-alternative-safety-check/</guid>
      <description>At the start of most of the retrospectives I&amp;rsquo;ve been part of we&amp;rsquo;ve followed the safety check ritual whereby each person participating has to write a number from 1-5 on a sticky describing how they&amp;rsquo;ll be participating in the retrospective.
1 means you&amp;rsquo;ll probably keep quiet and not say much, 5 means you&amp;rsquo;re perfectly comfortable saying anything and the other numbers fall in between those two extremes.
In my experiences it&amp;rsquo;s a bit of a fruitless exercise because its viewed that a higher number is &amp;lsquo;better&amp;rsquo; and therefore the minimum people will tend to write down is &amp;lsquo;3&amp;rsquo; because they don&amp;rsquo;t want to stand out or cause a problem.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jen and I used the K-means algorithm to classify digits in Kaggle&amp;rsquo;s Digit Recognizer problem and one of the things we&amp;rsquo;d read was that with this algorithm you often end up with situations where it&amp;rsquo;s difficult to classify a new item because if falls between two labels.
We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
    <item>
      <title>Configuration in DNS</title>
      <link>https://mneedham.github.io/2012/10/24/configuration-in-dns/</link>
      <pubDate>Wed, 24 Oct 2012 17:40:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/24/configuration-in-dns/</guid>
      <description>In the latest version of the ThoughtWorks Technology Radar one of the areas covered is &amp;lsquo;configuration in DNS&amp;rsquo;, a term which I first came across earlier in the year from a mailing list post by my former colleague Daniel Worthington-Bodart.
The radar describes it like so:
 Application deployments often suffer from an excess of environment-specific configuration settings, including the hostnames of dependent services. Configuration in DNS is a valuable technique to reduce this complexity by using standard hostnames like ‘mail’ or ‘db’ and have DNS resolve to the correct host for that environment.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A K-means attempt</title>
      <link>https://mneedham.github.io/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</link>
      <pubDate>Tue, 23 Oct 2012 19:04:20 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</guid>
      <description>Over the past couple of months Jen and I have been playing around with the Kaggle Digit Recognizer problem - a &amp;lsquo;competition&amp;rsquo; created to introduce people to Machine Learning.
 The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.  You are given an input file which contains multiple rows each containing 784 pixel values representing a 28x28 pixel image as well as a label indicating which number that image actually represents.</description>
    </item>
    
    <item>
      <title>How we&#39;re using story points</title>
      <link>https://mneedham.github.io/2012/10/21/how-were-using-story-points/</link>
      <pubDate>Sun, 21 Oct 2012 23:08:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/21/how-were-using-story-points/</guid>
      <description>A couple of weeks ago Joshua Kerievsky wrote a post describing how he and his teams don&amp;rsquo;t use story points anymore because of the problems they&amp;rsquo;d had with them which included:
 Story Point Inflation - inflating estimates of stories so that the velocity for an iteration is higher Comparing teams by points - judging comparative performance of teams by how many points they&#39;re able to complete  On the team I&amp;rsquo;m currently working on we still estimate the relative size of stories using points but we don&amp;rsquo;t use velocity per iteration to keep score - most of the time it&amp;rsquo;s barely even mentioned.</description>
    </item>
    
    <item>
      <title>Do the simple thing</title>
      <link>https://mneedham.github.io/2012/10/21/do-the-simple-thing/</link>
      <pubDate>Sun, 21 Oct 2012 21:35:35 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/21/do-the-simple-thing/</guid>
      <description>One of the most unexpected things that I picked up while pairing with Ashok for a few days in August/September is his ability to pick the simplest solution when confronted with a problem.
On numerous occasions we&amp;rsquo;d be trying to do something and I&amp;rsquo;d end up on a yak shaving mission trying to get a complicated approach to work while he watched on with bemusement.
I thought I&amp;rsquo;d actually learnt this lesson from working with Ashok but on a couple of occasions over the last week I&amp;rsquo;ve caught myself doing the same thing again!</description>
    </item>
    
    <item>
      <title>Environment agnostic machines and applications</title>
      <link>https://mneedham.github.io/2012/10/14/environment-agnostic-machines-and-applications/</link>
      <pubDate>Sun, 14 Oct 2012 18:49:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/14/environment-agnostic-machines-and-applications/</guid>
      <description>On my current project we&amp;rsquo;ve been setting up production and staging environments and Shodhan came up with the idea of making staging and production identical to the point that a machine wouldn&amp;rsquo;t even know what environment it was in.
Identical in this sense means:
 Puppet doesn&#39;t know which environment the machine is in. Our factor variables suggest the environment is production. We set the RACK_ENV variable to production so applications don&#39;t know what environment they&#39;re in.</description>
    </item>
    
    <item>
      <title>Play Framework 2.0: Rendering JSON data in the view</title>
      <link>https://mneedham.github.io/2012/10/14/play-framework-2-0-rendering-json-data-in-the-view/</link>
      <pubDate>Sun, 14 Oct 2012 09:28:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/14/play-framework-2-0-rendering-json-data-in-the-view/</guid>
      <description>I&amp;rsquo;ve been playing around with the Play Framework which we&amp;rsquo;re using to front a bunch of visualisations and one thing I wanted to do is send a data structure to a view and then convert that into JSON.
I&amp;rsquo;ve got a simple controller which looks like this:
package controllers; import play.mvc.Controller; import play.mvc.Result; import views.html.*; public class SalesByCategory extends Controller { public static Result index() { ArrayList&amp;lt;Map&amp;lt;String, Object&amp;gt;&amp;gt; series = new ArrayList&amp;lt;Map&amp;lt;String, Object&amp;gt;&amp;gt;(); Map&amp;lt;String, Object&amp;gt; oneSeries = new HashMap&amp;lt;String, Object&amp;gt;(); oneSeries.</description>
    </item>
    
    <item>
      <title>Varnish: Purging the cache</title>
      <link>https://mneedham.github.io/2012/10/10/varnish-purging-the-cache/</link>
      <pubDate>Wed, 10 Oct 2012 23:28:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/10/varnish-purging-the-cache/</guid>
      <description>We&amp;rsquo;re using varnish to cache all the requests that come through our web servers and especially in our pre-production environments we deploy quite frequently and want to see the changes that we&amp;rsquo;ve made.
This means that we need to purge the pages we&amp;rsquo;re accessing from varnish so that it will actually pass the request through to the application server and serve up the latest version of the page.
For some reason my google-fu when trying to remember/work out how to do this has always been weak but my colleague Shodhan helped me understand how to do this today so I thought I better record it so I don&amp;rsquo;t forget!</description>
    </item>
    
    <item>
      <title>Nygard Big Data Model: The Investigation Stage</title>
      <link>https://mneedham.github.io/2012/10/10/nygard-big-data-model-the-investigation-stage/</link>
      <pubDate>Wed, 10 Oct 2012 00:00:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/10/nygard-big-data-model-the-investigation-stage/</guid>
      <description>Earlier this year Michael Nygard wrote an extremely detailed post about his experiences in the world of big data projects and included in the post was the following diagram which I&amp;rsquo;ve found very useful.
 Nygard&amp;rsquo;s Big Data Model (shamelessly borrowed by me because it&amp;rsquo;s awesome) Ashok and I have been doing some work in this area helping one of our clients make sense of and visualise some of their data and we realised retrospectively that we were very acting very much in the investigation stage of the model.</description>
    </item>
    
    <item>
      <title>Mac OS X: Removing Byte Order Mark with an editor</title>
      <link>https://mneedham.github.io/2012/10/07/mac-os-x-removing-byte-order-mark-with-an-editor/</link>
      <pubDate>Sun, 07 Oct 2012 10:43:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/07/mac-os-x-removing-byte-order-mark-with-an-editor/</guid>
      <description>About a month ago I wrote about some problems I was having working with Windows generated CSV files which had a Byte Order Mark (BOM) at the beginning of the file and I described a way to get rid of it using awk.
It&amp;rsquo;s a bit of a long winded process though and I always forget what the parameters I need to pass to awk are so I thought it would probably be quicker if I could just work out a way to get rid of the BOM using an editor.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 2 Wrap Up</title>
      <link>https://mneedham.github.io/2012/10/03/strata-conf-london-day-2-wrap-up/</link>
      <pubDate>Wed, 03 Oct 2012 06:46:13 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/03/strata-conf-london-day-2-wrap-up/</guid>
      <description>Yesterday I attended the second day of Strata Conf London and these are the some of the things I learned from the talks I attended:
 John Graham Cunningham opened the series of keynotes with a talk describing the problems British Rail had in 1955 when trying to calculate the distances between all train stations and comparing them to the problems we have today. British Rail were trying to solve a graph problem when people didn&#39;t know about graphs and Dijkstra&#39;s algorithm hadn&#39;t been invented and it was effectively invented on this project but never publicised.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 1 Wrap Up</title>
      <link>https://mneedham.github.io/2012/10/02/strata-conf-london-day-1-wrap-up/</link>
      <pubDate>Tue, 02 Oct 2012 23:42:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/02/strata-conf-london-day-1-wrap-up/</guid>
      <description>For the past couple of days I attended the first Strata Conf to be held in London - a conference which seems to bring together people from the data science and big data worlds to talk about the stuff they&amp;rsquo;re doing.
Since I&amp;rsquo;ve been playing around with a couple of different things in this area over the last 4&amp;frasl;5 months I thought it&amp;rsquo;d be interesting to come along and see what people much more experienced in this area had to say!</description>
    </item>
    
    <item>
      <title>neo4j: Handling SUM&#39;s scientific notation</title>
      <link>https://mneedham.github.io/2012/09/30/neo4j-handling-sums-scientific-notation/</link>
      <pubDate>Sun, 30 Sep 2012 19:47:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/30/neo4j-handling-sums-scientific-notation/</guid>
      <description>In some of the recent work I&amp;rsquo;ve been doing with neo4j the queries I&amp;rsquo;ve written have been summing up the values from multiple nodes and after a certain number is reached the value returned used scientific notation.
For example in a cypher query like this:
START category = node:categories(&#39;category_id:1&#39;) MATCH p = category-[:has_child*1..5]-&amp;gt;subCategory-[:has_product]-&amp;gt;product-[:sold]-&amp;gt;sales RETURN EXTRACT(n in NODES(p) : n.category_id?),subCategory.category_id, SUM(sales.sales)  I might get a result set like this:
+------------------------------------------------------------------------------------------------+ | EXTRACT(n in NODES(p) : n.</description>
    </item>
    
    <item>
      <title>Testing XML generation with vimdiff</title>
      <link>https://mneedham.github.io/2012/09/30/testing-xml-generation-with-vimdiff/</link>
      <pubDate>Sun, 30 Sep 2012 15:48:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/30/testing-xml-generation-with-vimdiff/</guid>
      <description>A couple of weeks ago I spent a bit of time writing a Ruby DSL to automate the setup of load balancers, firewall and NAT rules through the VCloud API.
The VCloud API deals primarily in XML so the DSL is just a thin layer which creates the appropriate mark up.
When we started out we configured everything manually through the web console and then exported the XML so the first thing that the DSL needed to do was create XML that matched what we already had.</description>
    </item>
    
    <item>
      <title>Data Science: Making sense of the data</title>
      <link>https://mneedham.github.io/2012/09/30/data-science-making-sense-of-the-data/</link>
      <pubDate>Sun, 30 Sep 2012 14:58:11 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/30/data-science-making-sense-of-the-data/</guid>
      <description>Over the past month or so Ashok and I have been helping one of our clients explore and visualise some of their data and one of the first things we needed to do was make sense of the data that was available.
Start small Ashok suggested that we work with a subset of our eventual data set so that we could get a feel for the data and quickly see whether what we were planning to do made sense.</description>
    </item>
    
    <item>
      <title>Data Science: Scrapping the data together</title>
      <link>https://mneedham.github.io/2012/09/30/data-science-scrapping-the-data-together/</link>
      <pubDate>Sun, 30 Sep 2012 13:44:18 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/30/data-science-scrapping-the-data-together/</guid>
      <description>On Friday Martin, Darren and I were discussing the ThoughtWorks graph that I was working on earlier in the year and Martin pointed out that an interesting aspect of this type of work is that the data you want to work with isn&amp;rsquo;t easily available.
You therefore need to find a way to scrap the data together to make some headway and then maybe at a later stage once some progress has been made it will become easier to replace that with a cleaner solution.</description>
    </item>
    
    <item>
      <title>Upstart: Job getting stuck in the start/killed state</title>
      <link>https://mneedham.github.io/2012/09/29/upstart-job-getting-stuck-in-the-startkilled-state/</link>
      <pubDate>Sat, 29 Sep 2012 09:56:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/29/upstart-job-getting-stuck-in-the-startkilled-state/</guid>
      <description>We&amp;rsquo;re using upstart to handle the processes running on our machines and since the haproxy package only came package with an init.d script we wanted to make it upstartified.
When defining an upstart script you need to specify an expect stanza in which you specify whether or not the process which you&amp;rsquo;re launching is going to fork.
 If you do not specify the expect stanza, Upstart will track the life cycle of the first PID that it executes in the exec or script stanzas.</description>
    </item>
    
    <item>
      <title>Java: Parsing CSV files</title>
      <link>https://mneedham.github.io/2012/09/23/java-parsing-csv-files/</link>
      <pubDate>Sun, 23 Sep 2012 22:46:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/23/java-parsing-csv-files/</guid>
      <description>As I mentioned in a previous post I recently moved a bunch of neo4j data loading code from Ruby to Java and as part of that process I needed to parse some CSV files.
In Ruby I was using FasterCSV which became the standard CSV library from Ruby 1.9 but it&amp;rsquo;s been a while since I had to parse CSV files in Java so I wasn&amp;rsquo;t sure which library to use.</description>
    </item>
    
    <item>
      <title>Network Address Translation</title>
      <link>https://mneedham.github.io/2012/09/23/network-address-translation/</link>
      <pubDate>Sun, 23 Sep 2012 19:23:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/23/network-address-translation/</guid>
      <description>I&amp;rsquo;ve often heard people talking about Network Address Translation (NAT) but I never really understood exactly how it worked until we started configuring some virtual data centres on my current project.
This is an attempt at documenting my own current understanding so I won&amp;rsquo;t forget in future.
In our case we&amp;rsquo;ve been provisioning a bunch of machines into different private networks, and each machine therefore has an IP in the range of IPv4 addresses reserved for private networks:</description>
    </item>
    
    <item>
      <title>neo4j: The Batch Inserter and the sunk cost fallacy</title>
      <link>https://mneedham.github.io/2012/09/23/neo4j-the-batch-inserter-and-the-sunk-cost-fallacy/</link>
      <pubDate>Sun, 23 Sep 2012 10:29:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/23/neo4j-the-batch-inserter-and-the-sunk-cost-fallacy/</guid>
      <description>About a year and a half ago I wrote about the sunk cost fallacy which is defined like so:
 The Misconception: You make rational decisions based on the future value of objects, investments and experiences. The Truth: Your decisions are tainted by the emotional investments you accumulate, and the more you invest in something the harder it becomes to abandon it.  Over the past few weeks Ashok and I have been doing some exploration of one of our client&amp;rsquo;s data by modelling it in a neo4j graph and seeing what interesting things the traversals reveal.</description>
    </item>
    
    <item>
      <title>Finding ways to use bash command line history shortcuts</title>
      <link>https://mneedham.github.io/2012/09/19/finding-ways-to-use-bash-command-line-history-shortcuts/</link>
      <pubDate>Wed, 19 Sep 2012 07:00:22 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/19/finding-ways-to-use-bash-command-line-history-shortcuts/</guid>
      <description>A couple of months ago I wrote about a bunch of command line history shortcuts that Phil had taught me and after recently coming across Peteris Krumins&amp;rsquo; bash history cheat sheet I thought it&amp;rsquo;d be interesting to find some real ways to use them.
A few weeks ago I wrote about a UTF-8 byte order mark (BOM) that I wanted to remove from a file I was working on and I realised this evening that there were some other files with the same problem.</description>
    </item>
    
    <item>
      <title>zsh: Don&#39;t verify substituted history expansion a.k.a.  disabling histverify</title>
      <link>https://mneedham.github.io/2012/09/16/zsh-dont-verify-substituted-history-expansion-a-k-a-disabling-histverify/</link>
      <pubDate>Sun, 16 Sep 2012 13:35:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/16/zsh-dont-verify-substituted-history-expansion-a-k-a-disabling-histverify/</guid>
      <description>I use zsh on my Mac terminal and in general I prefer it to bash but it has an annoying default setting whereby when you try to repeat a command via substituted history expansion it asks you to verify that.
For example let&amp;rsquo;s say by mistake I try to vi into a directory rather than cd&amp;rsquo;ing into it:
vi ~/.oh-my-zsh  If I try to cd into the directory by using &amp;lsquo;!</description>
    </item>
    
    <item>
      <title>cURL and the case of the carriage return</title>
      <link>https://mneedham.github.io/2012/09/15/curl-and-the-case-of-the-carriage-return/</link>
      <pubDate>Sat, 15 Sep 2012 09:06:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/15/curl-and-the-case-of-the-carriage-return/</guid>
      <description>We were doing some work this week where we needed to make a couple of calls to an API via a shell script and in the first call we wanted to capture one of the lines of the HTTP response headers and use that as in input to the second call.
The way we were doing this was something like the following:
#!/bin/bash # We were actually grabbing a different header but for the sake # of this post we&#39;ll say it was &#39;Set-Cookie&#39; AUTH_HEADER=`curl -I http://www.</description>
    </item>
    
    <item>
      <title>Bash: Piping data into a command using heredocs</title>
      <link>https://mneedham.github.io/2012/09/15/bash-piping-data-into-a-command-using-heredocs/</link>
      <pubDate>Sat, 15 Sep 2012 07:54:04 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/15/bash-piping-data-into-a-command-using-heredocs/</guid>
      <description>I&amp;rsquo;ve been playing around with some data modelled in neo4j recently and one thing I wanted to do is run an adhoc query in the neo4j-shell and grab the results and do some text manipulation on them.
For example I wrote a query which outputted the following to the screen and I wanted to sum together all the values in the 3rd column:
| [&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;] | &amp;quot;3&amp;quot; | 1234567 | | [&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;,&amp;quot;6&amp;quot;] | &amp;quot;6&amp;quot; | 8910112 |  Initially I was pasting the output into a text file and then running the following sequence of commands to work it out:</description>
    </item>
    
    <item>
      <title>Unix: Caught out by shell significant characters</title>
      <link>https://mneedham.github.io/2012/09/13/unix-caught-out-by-shell-significant-characters/</link>
      <pubDate>Thu, 13 Sep 2012 00:17:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/13/unix-caught-out-by-shell-significant-characters/</guid>
      <description>One of the applications that Phil and I were deploying today needed a MySQL server and part of our puppet code to provision that node type runs a command to setup the privileges for a database user.
The unevaluated puppet code reads like this:
/usr/bin/mysql -h ${host} -uroot ${rootpassarg} -e &amp;quot;grant all on ${name}.* to ${user}@&#39;${remote_host}&#39; identified by &#39;$password&#39;; flush privileges;&amp;quot;  In the application we were deploying that expanded into something like this:</description>
    </item>
    
    <item>
      <title>While waiting for VMs to provision...</title>
      <link>https://mneedham.github.io/2012/09/12/while-waiting-for-vms-to-provision/</link>
      <pubDate>Wed, 12 Sep 2012 22:53:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/12/while-waiting-for-vms-to-provision/</guid>
      <description>Phil and I spent part of the day provisioning new virtual machines for some applications that we need to deploy which involves running a provisioning script and then opening another terminal and repeatedly trying to ssh into the box until it succeeds.
Eventually we got bored of doing that so we figured out a nice little one liner to use instead:
while :; do ssh 10.0.0.2; done  The &amp;lsquo;:&amp;rsquo; is a bash noop and is defined like so:</description>
    </item>
    
    <item>
      <title>neo4j/cypher: CREATE UNIQUE - &#34;SyntaxException: string matching regex `$&#39; expected but `p&#39; found&#34;</title>
      <link>https://mneedham.github.io/2012/09/09/neo4jcypher-create-unique-syntaxexception-string-matching-regex-expected-but-p-found/</link>
      <pubDate>Sun, 09 Sep 2012 22:29:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/09/neo4jcypher-create-unique-syntaxexception-string-matching-regex-expected-but-p-found/</guid>
      <description>I&amp;rsquo;ve been playing around with the mutating cypher syntax of neo4j which allows you to make changes to the graph as well as query it, a feature introduced into cypher in May in release 1.8 M01.
I was trying to make use of the &amp;lsquo;CREATE UNIQUE&amp;rsquo; syntax which allows you to create nodes/relationships if they&amp;rsquo;re missing but won&amp;rsquo;t do anything if they already exists.
I had something like the following:</description>
    </item>
    
    <item>
      <title>logstash not picking up some files</title>
      <link>https://mneedham.github.io/2012/09/07/logstash-not-picking-up-some-files/</link>
      <pubDate>Fri, 07 Sep 2012 23:49:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/07/logstash-not-picking-up-some-files/</guid>
      <description>We&amp;rsquo;re using logstash to collect all the logs across the different machines that we use in various environments and had noticed that on some of the nodes log files which we&amp;rsquo;d told the logstash-client to track weren&amp;rsquo;t being collected.
We wanted to check what the open file descriptors of logstash-client were so we first had to grab its process id:
$ ps aux | grep logstash logstash 19896 134 9.1 711404 187768 ?</description>
    </item>
    
    <item>
      <title>Apt-Cacher-Server: Extra junk at end of file</title>
      <link>https://mneedham.github.io/2012/09/07/apt-cacher-server-extra-junk-at-end-of-file/</link>
      <pubDate>Fri, 07 Sep 2012 15:45:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/07/apt-cacher-server-extra-junk-at-end-of-file/</guid>
      <description>We&amp;rsquo;ve been installing Apt-Cache-Server so that we can cache some of the packages that we&amp;rsquo;re installing using apt-get on our own network.
(Almost) Following the instructions from the home page we added the following to /etc/apt/apt.conf.d/01proxy:
Acquire::http::Proxy &amp;quot;http://apt-cache-server:3142&amp;quot;  And when we ran &amp;lsquo;apt-get update&amp;rsquo; we were getting the following error:
E: Syntax error /etc/apt/apt.conf.d/01proxy:2: Extra junk at end of file  We initially thought it must be a problem with having an extra space or line ending but it turns out we had just left off the semi colon.</description>
    </item>
    
    <item>
      <title>A rogue &#34;\357\273\277&#34; (UTF-8 byte order mark)</title>
      <link>https://mneedham.github.io/2012/09/03/a-rogue-357273277-utf-8-byte-order-mark/</link>
      <pubDate>Mon, 03 Sep 2012 06:31:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/09/03/a-rogue-357273277-utf-8-byte-order-mark/</guid>
      <description>We&amp;rsquo;ve been loading some data into neo4j from a CSV file - creating one node per row and using the value in the first column as the index lookup for the node.
Unfortunately the index lookup wasn&amp;rsquo;t working for the first row but was for every other row.
By coincidence we started saving each row into a hash map and were then able to see what was going wrong:
require &#39;rubygems&#39; require &#39;fastercsv&#39; things = FasterCSV.</description>
    </item>
    
    <item>
      <title>Book Review: The Retrospective Handbook - Pat Kua</title>
      <link>https://mneedham.github.io/2012/08/31/book-review-the-retrospective-handbook-pat-kua/</link>
      <pubDate>Fri, 31 Aug 2012 21:18:19 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/31/book-review-the-retrospective-handbook-pat-kua/</guid>
      <description>My colleague Pat Kua recently published a book he&amp;rsquo;s been working on for the first half of the year titled &amp;lsquo;The Retrospective Handbook&amp;rsquo; - a book in which Pat shares his experiences with retrospectives and gives advice to budding facilitators.
I was intrigued what the book would be like because the skill gap between Pat and me with respect to facilitating retrospectives is huge and I&amp;rsquo;ve often found that experts in a subject can have a tendency to be a bit preachy when writing about their subject!</description>
    </item>
    
    <item>
      <title>The Curse Of Knowledge</title>
      <link>https://mneedham.github.io/2012/08/28/the-curse-of-knowledge/</link>
      <pubDate>Tue, 28 Aug 2012 21:22:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/28/the-curse-of-knowledge/</guid>
      <description>My colleague Anand Vishwanath recently recommended the book &amp;lsquo;Made To Stick&amp;rsquo; and one thing that has really stood out for me while reading it is the idea of the &amp;lsquo;The Curse Of Knowledge&amp;rsquo; which is described like so:
 Once we know something, we find it hard to imagine what it was like not to know it. Our knowledge has &#34;cursed&#34; us. And it becomes difficult for us to share out knowledge with others, because can&#39;t readily re-create our listeners&#39; state of mind.</description>
    </item>
    
    <item>
      <title>Ruby: Finding where gems are</title>
      <link>https://mneedham.github.io/2012/08/25/ruby-finding-where-gems-are/</link>
      <pubDate>Sat, 25 Aug 2012 10:00:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/25/ruby-finding-where-gems-are/</guid>
      <description>In my infrequent travels into Ruby land I always seem to forget where the gems that I&amp;rsquo;ve installed actually live on the file system but my colleague Nick recently showed me a neat way of figuring it out.
If I&amp;rsquo;m in the folder that contains all my ThoughtWorks graph code I&amp;rsquo;d just need to run the following command:
$ gem which rubygems /Users/mneedham/.rbenv/versions/jruby-1.6.7/lib/ruby/site_ruby/1.8/rubygems.rb  I then loaded up irb and wrote a simple cypher query executed using neography:</description>
    </item>
    
    <item>
      <title>puppetdb: Failed to submit &#39;replace catalog&#39; command for client to PuppetDB at puppetmaster:8081: [500 Server Error]</title>
      <link>https://mneedham.github.io/2012/08/16/puppetdb-failed-to-submit-replace-catalog-command-for-client-to-puppetdb-at-puppetmaster8081-500-server-error/</link>
      <pubDate>Thu, 16 Aug 2012 23:31:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/16/puppetdb-failed-to-submit-replace-catalog-command-for-client-to-puppetdb-at-puppetmaster8081-500-server-error/</guid>
      <description>I&amp;rsquo;m still getting used to the idea of following the logs when working out what&amp;rsquo;s going wrong with distributed systems but it worked well when trying to work out why our puppet client which was throwing this error when we ran &amp;lsquo;puppet agent -tdv&amp;rsquo;:
err: Could not retrieve catalog from remote server: Error 400 on SERVER: Failed to submit &#39;replace catalog&#39; command for client to PuppetDB at puppetmaster:8081: [500 Server Error]  We were seeing the same error in /var/log/syslog on the puppet master and a quick look at the process list didn&amp;rsquo;t show that the puppet master or puppetdb services were under a particularly heavy load.</description>
    </item>
    
    <item>
      <title>Presentations; Tell a story</title>
      <link>https://mneedham.github.io/2012/08/14/presentations-tell-a-story/</link>
      <pubDate>Tue, 14 Aug 2012 22:16:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/14/presentations-tell-a-story/</guid>
      <description>A few years ago before an F# talk that I gave at the .NET user group in Sydney my colleague Erik Doernenburg gave me some advice about how I should structure the talk.
(paraphrasing)  He suggested that in a lot of talks he&amp;rsquo;d seen the presenter rattle off a bunch of information about a topic but hadn&amp;rsquo;t provided any insight into their own experience with the topic.
If two people give a talk on the same topic they therefore end up being fairly similar talks even though each person may have a totally different perspective.</description>
    </item>
    
    <item>
      <title>SSHing onto machines via a jumpbox</title>
      <link>https://mneedham.github.io/2012/08/10/sshing-onto-machines-via-a-jumpbox/</link>
      <pubDate>Fri, 10 Aug 2012 00:58:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/10/sshing-onto-machines-via-a-jumpbox/</guid>
      <description>We wanted to be able to ssh into some machines which were behind a firewall so we set up a jumpbox which our firewall directed any traffic on port 22 towards.
Initially if we wanted to SSH onto a machine inside the network we&amp;rsquo;d have to do a two step process:
$ ssh jumpbox # now on the jumpbx $ ssh internal-network-machine  That got a bit annoying after a while so Sam showed us a neat way of proxying the second ssh command through the first one by making use of netcat.</description>
    </item>
    
    <item>
      <title>VCloud Guest Customization Script : [: postcustomization: unexpected operator</title>
      <link>https://mneedham.github.io/2012/08/06/vcloud-guest-customization-script-postcustomization-unexpected-operator/</link>
      <pubDate>Mon, 06 Aug 2012 21:50:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/06/vcloud-guest-customization-script-postcustomization-unexpected-operator/</guid>
      <description>We have been doing some work to automatically provision machines using the VCloud API via fog and one of the things we wanted to do was run a custom script the first time that a node powers on.
The following explains how customization scripts work:  In vCloud Director, when setting a customization script in a virtual machine, the script:  Is called only on initial customization and force recustomization. Is called with the precustomization command line parameter before out-of-box customization begins.</description>
    </item>
    
    <item>
      <title>neo4j: Creating a custom index with neo4j.rb</title>
      <link>https://mneedham.github.io/2012/08/05/neo4j-creating-a-custom-index-with-neo4j-rb/</link>
      <pubDate>Sun, 05 Aug 2012 09:45:08 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/08/05/neo4j-creating-a-custom-index-with-neo4j-rb/</guid>
      <description>As I mentioned in my last post I&amp;rsquo;ve been playing around with the TFL Bus stop location and routes API and one thing I wanted to do was load all the bus stops into a neo4j database using the neo4j.rb gem.
I initially populated the database via neography but it was taking around 20 minutes each run and I figured it&amp;rsquo;d probably be much quicker to populate it directly rather than using the REST API.</description>
    </item>
    
    <item>
      <title>London Bus Stops API: Mapping northing/easting values to lat/long</title>
      <link>https://mneedham.github.io/2012/07/30/london-bus-stops-api-mapping-northingeasting-values-to-latlong/</link>
      <pubDate>Mon, 30 Jul 2012 22:28:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/30/london-bus-stops-api-mapping-northingeasting-values-to-latlong/</guid>
      <description>I started playing around with the TFL Bus stop location and routes API and one of the annoying things about the data is that it uses easting/northing values to describe the location of bus stops rather than lat/longs.
The first few lines of the CSV file look like this:
1000,91532,490000266G,WESTMINSTER STN &amp;lt;&amp;gt; / PARLIAMENT SQUARE,530171,179738,177,0K08,0 10001,72689,490013793E,TREVOR CLOSE,515781,174783,78,NB16,0 10002,48461,490000108F,HIGHBURY CORNER,531614,184603,5,C902,0  For each of the stops I wanted to convert from the easting/northing value to the equivalent lat/long value but I couldn&amp;rsquo;t find a simple way of doing it in code although I did come across an API that would do it for me.</description>
    </item>
    
    <item>
      <title>Puppet: Keeping the discipline</title>
      <link>https://mneedham.github.io/2012/07/29/puppet-keeping-the-discipline/</link>
      <pubDate>Sun, 29 Jul 2012 21:53:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/29/puppet-keeping-the-discipline/</guid>
      <description>For the last 5 weeks or so I&amp;rsquo;ve been working with puppet every day to automate the configuration of various nodes in our stack and my most interesting observation so far is that you really need to keep your discipline when doing this type of work.
We can keep that discipline in three main ways when developing modules.
Running from scratch Configuring various bits of software seems to follow the 80&amp;frasl;20 rule and we get very close to having each thing working quite quickly but then end up spending a disproportionate amount of time tweaking the last little bits.</description>
    </item>
    
    <item>
      <title>Unix: tee</title>
      <link>https://mneedham.github.io/2012/07/29/unix-tee/</link>
      <pubDate>Sun, 29 Jul 2012 19:11:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/29/unix-tee/</guid>
      <description>I&amp;rsquo;ve read about the Unix &amp;lsquo;tee&amp;rsquo; command before but never found a reason to use it until the last few weeks.
One of the things I repeatedly do by mistake is open /etc/hosts without sudo and then try to make changes to it:
$ vi /etc/hosts # Editing it leads to the dreaded &#39;W10: Changing a readonly file&#39;  I always used to close the file and then re-open it with sudo but I recently came across an approach which allows us to use &amp;lsquo;tee&amp;rsquo; to get around the problem.</description>
    </item>
    
    <item>
      <title>neo4j: Multiple starting nodes by index lookup</title>
      <link>https://mneedham.github.io/2012/07/28/neo4j-multiple-starting-nodes-by-index-lookup/</link>
      <pubDate>Sat, 28 Jul 2012 23:32:28 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/28/neo4j-multiple-starting-nodes-by-index-lookup/</guid>
      <description>I spent a bit of time this evening extracting some data from the ThoughtWorks graph for our marketing team who were interested in anything related to our three European offices in London, Manchester and Hamburg.
The most interesting things we can explore relate to the relationship between people and the offices.
The model around people and offices looks like this:
  I added a &amp;lsquo;current_home_office&amp;rsquo; relationship to make it easier to quickly get to the nodes of people who are currently working in a specific office.</description>
    </item>
    
    <item>
      <title>R: Mapping a function over a collection of values</title>
      <link>https://mneedham.github.io/2012/07/23/r-mapping-a-function-over-a-collection-of-values/</link>
      <pubDate>Mon, 23 Jul 2012 23:25:00 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/23/r-mapping-a-function-over-a-collection-of-values/</guid>
      <description>I spent a bit of Sunday playing around with R and one thing I wanted to do was map a function over a collection of values and transform each value slightly.
I loaded my data set using the &amp;lsquo;Import Dataset&amp;rsquo; option in R Studio (suggested to me by Rob) which gets converted to the following function call:
&amp;gt; data &amp;lt;- read.csv(&amp;quot;~/data.csv&amp;quot;, header=T, encoding=&amp;quot;ISO-8859&amp;quot;) &amp;gt; data Column1 InterestingColumn 1 Mark 12.50 2 Dave 100.</description>
    </item>
    
    <item>
      <title>neo4j: Graph Global vs Graph Local queries</title>
      <link>https://mneedham.github.io/2012/07/23/neo4j-graph-global-vs-graph-local-queries/</link>
      <pubDate>Mon, 23 Jul 2012 22:23:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/23/neo4j-graph-global-vs-graph-local-queries/</guid>
      <description>A few weeks ago I did a presentation at the ThoughtWorks EU away day on the graph I&amp;rsquo;ve been developing using neo4j and I wanted to show who the most connected people in each of our European offices were.
I started with the following cypher query:
START n = node(*) MATCH n-[r:colleagues*1..2]-&amp;gt;c, n-[r2:member_of]-&amp;gt;office WHERE n.type? = &#39;person&#39; AND (NOT(HAS(r2.end_date))) AND office.name = &#39;London - UK South&#39; AND (NOT(HAS(c.thoughtquitter))) RETURN n.name, count(distinct(c)) AS connections, office.</description>
    </item>
    
    <item>
      <title>neo4j: Embracing the sub graph</title>
      <link>https://mneedham.github.io/2012/07/21/neo4j-embracing-the-sub-graph/</link>
      <pubDate>Sat, 21 Jul 2012 22:46:06 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/21/neo4j-embracing-the-sub-graph/</guid>
      <description>In May I wrote a blog post explaining how I&amp;rsquo;d been designing a neo4j graph by thinking about what questions I wanted to answer about the data.
In the comments Josh Adell gave me the following advice:
 The neat things about graphs is that multiple subgraphs can live in the same data-space. ...
Keep your data model rich! Don&#39;t be afraid to have as many relationships as you need. The power of graph databases comes from finding surprising results when you have strongly interconnected data.</description>
    </item>
    
    <item>
      <title>neo4j: Shortest Path with and without cypher</title>
      <link>https://mneedham.github.io/2012/07/19/neo4j-shortest-path-with-and-without-cypher/</link>
      <pubDate>Thu, 19 Jul 2012 19:57:31 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/19/neo4j-shortest-path-with-and-without-cypher/</guid>
      <description>I was looking back at some code I wrote a few months ago to query a neo4j database to find the shortest path between two people via the colleagues relationships that exist.

The initial code, written using neography, looked like this:
neo = Neography::Rest.new start_node = neo.get_node(start_node_id) destination_node = neo.get_node(destination_node_id) neo.get_paths(start_node, destination_node, { &amp;quot;type&amp;quot; =&amp;gt; &amp;quot;colleagues&amp;quot; }, depth = 3, algorithm = &amp;quot;shortestPath&amp;quot;)  The neography code eventually makes a POST request to /node/{start_id}/paths and provides a JSON payload containing the other information about the query.</description>
    </item>
    
    <item>
      <title>neo4j: java.security.NoSuchAlgorithmException: Algorithm [JKS] of type [KeyStore] from provider [org.bouncycastle.jce.provider.BouncyCastleProvider: name=BC version=1.4]</title>
      <link>https://mneedham.github.io/2012/07/17/neo4j-java-security-nosuchalgorithmexception-algorithm-jks-of-type-keystore-from-provider-org-bouncycastle-jce-provider-bouncycastleprovider-namebc-version1-4/</link>
      <pubDate>Tue, 17 Jul 2012 00:02:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/17/neo4j-java-security-nosuchalgorithmexception-algorithm-jks-of-type-keystore-from-provider-org-bouncycastle-jce-provider-bouncycastleprovider-namebc-version1-4/</guid>
      <description>I&amp;rsquo;ve spent the last couple of hours moving my neo4j graph from my own machine onto a vanilla CentOS VM and initially tried to run neo using a non Sun version of Java which I installed like so:
yum install java  This is the version of Java that was installed:
$ java -version java version &amp;quot;1.5.0&amp;quot; gij (GNU libgcj) version 4.4.6 20120305 (Red Hat 4.4.6-4)  When I tried to start neo4j:</description>
    </item>
    
    <item>
      <title>tcpdump: Learning how to read UDP packets</title>
      <link>https://mneedham.github.io/2012/07/15/tcpdump-learning-how-to-read-udp-packets/</link>
      <pubDate>Sun, 15 Jul 2012 13:29:05 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/15/tcpdump-learning-how-to-read-udp-packets/</guid>
      <description>Phil and I spent some of Friday afternoon configuring statsd:
 A network daemon that runs on the Node.js platform and listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services  We configured it to listen on its default port 8125 and then used netcat to send UDP packets to see if it was working like so:
echo -n &amp;quot;blah:36|c&amp;quot; | nc -w 1 -u -4 localhost 8125  We used tcpdump to capture any UDP packets on port 8125 like so:</description>
    </item>
    
    <item>
      <title>netcat: localhost resolution not working when sending UDP packets</title>
      <link>https://mneedham.github.io/2012/07/15/netcat-localhost-resolution-not-working-when-sending-udp-packets/</link>
      <pubDate>Sun, 15 Jul 2012 08:14:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/15/netcat-localhost-resolution-not-working-when-sending-udp-packets/</guid>
      <description>As part of some work we were doing last week Phil and I needed to send UDP packets to a local port and check that they were being picked up.
We initially tried sending a UDP packet to localhost port 8125 using netcat like so:
echo -n &amp;quot;hello&amp;quot; | nc -w 1 -u localhost 8125  That message wasn&amp;rsquo;t being received by the application listening on the port so Phil decided to try and send the same packet from Ruby which worked fine:</description>
    </item>
    
    <item>
      <title>Racket: Wiring it up to a REPL ala SLIME/Swank</title>
      <link>https://mneedham.github.io/2012/07/11/racket-wiring-it-up-to-a-repl-ala-slimeswank/</link>
      <pubDate>Wed, 11 Jul 2012 19:34:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/11/racket-wiring-it-up-to-a-repl-ala-slimeswank/</guid>
      <description>One of the awesome things about working with clojure is that it&amp;rsquo;s possible to wire up clojure files in emacs to a REPL by making use of Slime/Swank.
I&amp;rsquo;ve started using Racket to work through the examples in The Little Schemer and wanted to achieve a similar thing there.
 Racket is a modern programming language in the Lisp/Scheme family, suitable for a wide range of applications  I don&amp;rsquo;t know much about configuring emacs so I made use of Phil Halgelberg&amp;rsquo;s emacs-starter-kit which is available on github.</description>
    </item>
    
    <item>
      <title>Data visualisation: Is &#39;interesting&#39; enough?</title>
      <link>https://mneedham.github.io/2012/07/08/data-visualisation-is-interesting-enough/</link>
      <pubDate>Sun, 08 Jul 2012 22:45:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/08/data-visualisation-is-interesting-enough/</guid>
      <description>I recently read a blog post by Julian Boot titled &amp;lsquo;visualisation without analysis is fine&amp;rsquo; where he suggests that we can learn things from visualising data in the right way - detailed statistical analysis isn&amp;rsquo;t always necessary.
I thought this was quite an interesting observation because over the past couple of months I&amp;rsquo;ve been playing around with ThoughtWorks data and looking at different ways to visualise aspects of the data.</description>
    </item>
    
    <item>
      <title>ganglia: Importing gmond Python modules</title>
      <link>https://mneedham.github.io/2012/07/08/ganglia-importing-gmond-python-modules/</link>
      <pubDate>Sun, 08 Jul 2012 21:55:53 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/08/ganglia-importing-gmond-python-modules/</guid>
      <description>My colleague Shohdan and I spent a couple of days last week wiring up various monitoring metrics into ganglia and while most of them come built in, we also found some  python based modules that we wanted to use.
Unfortunately we couldn&amp;rsquo;t find any instructions on github explaining how to set them up but after a bit of trial and error we figured it out.
One of the modules that we wanted to use was diskstat which provides I/O wait time metrics which we couldn&amp;rsquo;t find in the built in modules.</description>
    </item>
    
    <item>
      <title>Bash Shell: Reusing parts of previous commands</title>
      <link>https://mneedham.github.io/2012/07/05/bash-shell-reusing-parts-of-previous-commands/</link>
      <pubDate>Thu, 05 Jul 2012 23:42:35 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/05/bash-shell-reusing-parts-of-previous-commands/</guid>
      <description>I&amp;rsquo;ve paired a few times with my colleague Phil Potter over the last couple of weeks and since he&amp;rsquo;s a bit of a ninja with bash shortcuts/commands I wanted to record some of the things he&amp;rsquo;s shown me so I won&amp;rsquo;t forget them!
Let&amp;rsquo;s say we&amp;rsquo;re in the &amp;lsquo;/tmp&amp;rsquo; directory and want to create a folder a few levels down but forget to pass the &amp;lsquo;-p&amp;rsquo; option to &amp;lsquo;mkdir&amp;rsquo;:
$ mkdir blah/de/blah mkdir: cannot create directory `blah/de/blah&#39;: No such file or directory  One way of fixing that would be to press the up arrow and navigate along the previous command and put in the &amp;lsquo;-p&amp;rsquo; flag but it&amp;rsquo;s a bit fiddly so instead we can do the following:</description>
    </item>
    
    <item>
      <title>sudo, sudo -i &amp; sudo su</title>
      <link>https://mneedham.github.io/2012/07/04/sudo-sudo-i-sudo-su/</link>
      <pubDate>Wed, 04 Jul 2012 19:34:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/04/sudo-sudo-i-sudo-su/</guid>
      <description>On the project I&amp;rsquo;m currently working on we&amp;rsquo;re doing quite a bit of puppet and although we&amp;rsquo;re using the puppet master approach in production &amp;amp; test environments it&amp;rsquo;s still useful to be able to run puppet headless to test changes locally.
Since several of the commands require having write access to &amp;lsquo;root&amp;rsquo; folders we need to run &amp;lsquo;puppet apply&amp;rsquo; as a super user using sudo. We also need to run it in the context of some environment variables which the root user has.</description>
    </item>
    
    <item>
      <title>Debugging: Google vs The Manual</title>
      <link>https://mneedham.github.io/2012/07/04/debugging-google-vs-the-manual/</link>
      <pubDate>Wed, 04 Jul 2012 00:00:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/07/04/debugging-google-vs-the-manual/</guid>
      <description>Over the last six months or so I&amp;rsquo;ve worked with a bunch of different people and one of the things that I&amp;rsquo;ve noticed is that when something isn&amp;rsquo;t working there tend to be two quite distinct ways that people go about trying to solve the problem.
The Manual The RTFM crowd will go straight for the official documentation or source code if needs be in an attempt to work through the problem from first principals.</description>
    </item>
    
    <item>
      <title>Powerpoint saving movies as images</title>
      <link>https://mneedham.github.io/2012/06/30/powerpoint-saving-movies-as-images/</link>
      <pubDate>Sat, 30 Jun 2012 10:05:04 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/30/powerpoint-saving-movies-as-images/</guid>
      <description>I&amp;rsquo;ve been working on a presentation for the ThoughtWorks Europe away day over the last few days and I created some screen casts using Camtasia which I wanted to include.
It&amp;rsquo;s reasonably easy to insert movies into Powerpoint but I was finding that when I saved the file and then reloaded it the movies had been converted into images which wasn&amp;rsquo;t what I wanted at all!
Eventually I came across a blog post which explained that I&amp;rsquo;d been saving the file as the wrong format.</description>
    </item>
    
    <item>
      <title>neo4j: Handling optional relationships</title>
      <link>https://mneedham.github.io/2012/06/24/neo4j-handling-optional-relationships/</link>
      <pubDate>Sun, 24 Jun 2012 23:32:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/24/neo4j-handling-optional-relationships/</guid>
      <description>On my ThoughtWorks neo4j there are now two different types of relationships between people nodes - they can either be colleagues or one can be the sponsor of the other.
The graph looks like this:
I wanted to get a list of all the sponsor pairs but also have some indicator of whether the two people have worked together.
I started off by getting all of the sponsor pairs:
START n = node(*) MATCH n-[r:sponsor_of]-&amp;gt;n2 RETURN n.</description>
    </item>
    
    <item>
      <title>Why you shouldn&#39;t use name as a key a.k.a. I am an idiot</title>
      <link>https://mneedham.github.io/2012/06/24/why-you-shouldnt-use-name-as-a-key-a-k-a-i-am-an-idiot/</link>
      <pubDate>Sun, 24 Jun 2012 22:55:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/24/why-you-shouldnt-use-name-as-a-key-a-k-a-i-am-an-idiot/</guid>
      <description>I think one of the first things that I learnt about dealing with users in a data store is that you should never use name as a primary key because their might be two people with the same name.
Despite knowing that I foolishly chose to ignore this knowledge when building my neo4j graph and used name as the key for the Lucene index.
I thought I&amp;rsquo;d got away with it but NO!</description>
    </item>
    
    <item>
      <title>Brightbox Repository: GPG error: The following signatures couldn&#39;t be verified because the public key is not available</title>
      <link>https://mneedham.github.io/2012/06/24/brightbox-repository-gpg-error-the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/</link>
      <pubDate>Sun, 24 Jun 2012 00:58:43 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/24/brightbox-repository-gpg-error-the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/</guid>
      <description>We&amp;rsquo;re using the Brightbox Ruby repository to get the versions of Ruby which we install on our machines and although we eventually put the configuration for this repository into Puppet we initially tested it out on a local VM.
To start with you need to add the repository to /etc/apt/sources.list:
deb http://ppa.launchpad.net/brightbox/ruby-ng/ubuntu lucid main  To get that picked up we run the following:
apt-get update  Which initially threw this error because it&amp;rsquo;s a gpg signed repository and we hadn&amp;rsquo;t added the key:</description>
    </item>
    
    <item>
      <title>Creating a Samba share between Ubuntu and Mac OS X</title>
      <link>https://mneedham.github.io/2012/06/24/creating-a-samba-share-between-ubuntu-and-mac-os-x/</link>
      <pubDate>Sun, 24 Jun 2012 00:40:35 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/24/creating-a-samba-share-between-ubuntu-and-mac-os-x/</guid>
      <description>On the project I&amp;rsquo;m currently working on we have our development environment setup on a bare bones Ubuntu instance which we run via VmWare.
We wanted to be able to edit files on the VM from the host O/S so my colleague Phil suggested that we set up a Samba server on the VM and then connect to it from the Mac.
We first needed to install a couple of packages on the VM:</description>
    </item>
    
    <item>
      <title>Visualising a neo4j graph using gephi</title>
      <link>https://mneedham.github.io/2012/06/21/visualising-a-neo4j-graph-using-gephi/</link>
      <pubDate>Thu, 21 Jun 2012 05:02:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/21/visualising-a-neo4j-graph-using-gephi/</guid>
      <description>At ThoughtWorks we don&amp;rsquo;t have line managers but people can choose to have a sponsor - typically someone who has worked in the company for longer/has more experience in the industry than them - who can help them navigate the organisation better.
From hearing people talk about sponsors over the last 6 years it seemed like quite a few people sponsored the majority and there were probably a few people who didn&amp;rsquo;t have a sponsor.</description>
    </item>
    
    <item>
      <title>Haskell: Mixed type lists</title>
      <link>https://mneedham.github.io/2012/06/19/haskell-mixed-type-lists/</link>
      <pubDate>Tue, 19 Jun 2012 23:09:39 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/19/haskell-mixed-type-lists/</guid>
      <description>I&amp;rsquo;ve been continuing to work through the exercises in The Little Schemer and came across a problem which needed me to write a function to take a mixed list of Integers and Strings and filter out the Integers.
As I mentioned in my previous post I&amp;rsquo;ve been doing the exercises in Haskell but I thought I might struggle with that approach here because Haskell collections are homogeneous i.e. all the elements need to be of the same type.</description>
    </item>
    
    <item>
      <title>The Little Schemer: Attempt #2</title>
      <link>https://mneedham.github.io/2012/06/19/the-little-schemer-attempt-2/</link>
      <pubDate>Tue, 19 Jun 2012 00:21:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/19/the-little-schemer-attempt-2/</guid>
      <description>A few weeks ago I asked the twittersphere for some advice on how I could get better at writing recursive functions and one of the pieces of advice was to work through The Little Schemer.
I first heard about The Little Schemer a couple of years ago and after going through the first few pages I got bored and gave up.
I still found the first few pages a bit trivial this time around as well but my colleague Jen Smith encouraged me to keep going and once I&amp;rsquo;d got about 20 pages in it became clearer to me why the first few pages had been written the way they had.</description>
    </item>
    
    <item>
      <title>neo4j/Cypher: Finding the most connected node on the graph</title>
      <link>https://mneedham.github.io/2012/06/16/neo4jcypher-finding-the-most-connected-node-on-the-graph/</link>
      <pubDate>Sat, 16 Jun 2012 10:41:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/16/neo4jcypher-finding-the-most-connected-node-on-the-graph/</guid>
      <description>As I mentioned in another post about a month ago I&amp;rsquo;ve been playing around with a neo4j graph in which I have the following relationship between nodes:
One thing I wanted to do was work out which node is the most connected on the graph, which would tell me who&amp;rsquo;s worked with the most people.
I started off with the following cypher query:
query = &amp;quot; START n = node(*)&amp;quot; query &amp;lt;&amp;lt; &amp;quot; MATCH n-[r:colleagues]-&amp;gt;c&amp;quot; query &amp;lt;&amp;lt; &amp;quot; WHERE n.</description>
    </item>
    
    <item>
      <title>Functional Thinking: Separating concerns</title>
      <link>https://mneedham.github.io/2012/06/12/functional-thinking-separating-concerns/</link>
      <pubDate>Tue, 12 Jun 2012 23:50:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/12/functional-thinking-separating-concerns/</guid>
      <description>Over the weekend I was trying to port some of the neo4j import code for the ThoughtWorks graph I&amp;rsquo;ve been working on to make use of the REST Batch API and I came across an interesting example of imperative vs functional thinking.
I&amp;rsquo;m using the neography gem to populate the graph and to start with I was just creating a person node and then creating an index entry for it:</description>
    </item>
    
    <item>
      <title>CSV parsing/UTF-8 encoding</title>
      <link>https://mneedham.github.io/2012/06/10/csv-parsingutf-8-encoding/</link>
      <pubDate>Sun, 10 Jun 2012 23:30:23 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/10/csv-parsingutf-8-encoding/</guid>
      <description>I was recently trying to parse a CSV file which I&amp;rsquo;d converted from an Excel spreadsheet but was having problems with characters beyond the standard character set.
This is an example of what was going wrong:
&amp;gt; require &#39;csv&#39; &amp;gt; people = CSV.open(&amp;quot;sponsors.csv&amp;quot;, &#39;r&#39;, ?,, ?\r).to_a [&amp;quot;Erik D\366rnenburg&amp;quot;, &amp;quot;N/A&amp;quot;] &amp;gt; people.each { |sponsee, sponsor| puts &amp;quot;#{sponsee} #{sponsor}&amp;quot; } Erik D?rnenburg N/A  I came across a Ruby gem called chardet which allowed me to work out the character set of Erik&amp;rsquo;s name like so:</description>
    </item>
    
    <item>
      <title>Haskell: Writing a function that can take Ints or Doubles</title>
      <link>https://mneedham.github.io/2012/06/05/haskell-writing-a-function-that-can-take-ints-or-doubles/</link>
      <pubDate>Tue, 05 Jun 2012 00:10:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/05/haskell-writing-a-function-that-can-take-ints-or-doubles/</guid>
      <description>In my continued reading of SICP I wanted to recreate a &amp;lsquo;sum&amp;rsquo; function used to demonstrate a function which could take another function as one of its parameters.
In Scheme the function is defined like this:
(define (sum term a next b) (if (&amp;gt; a b) 0 (+ (term a) (sum term (next a) next b))))  And can be used like this to sum the values between two numbers:</description>
    </item>
    
    <item>
      <title>Haskell: Building a range of numbers from command line arguments</title>
      <link>https://mneedham.github.io/2012/06/03/haskell-building-a-range-of-numbers-from-command-line-arguments/</link>
      <pubDate>Sun, 03 Jun 2012 20:13:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/06/03/haskell-building-a-range-of-numbers-from-command-line-arguments/</guid>
      <description>I&amp;rsquo;m working through some of the SICP problems in Haskell and for problem 1.22 you need to write a function which will indicate the first 3 prime numbers above a starting value.
It is also suggested to only consider odd numbers so to find the prime numbers above 1000 the function call would look like this:
&amp;gt; searchForPrimes [1001,1003..] [1009,1013,1019]  I wanted to be able to feed in the range of numbers from the command line so that I&amp;rsquo;d be able to call the function with different values and see how long it took to work it out.</description>
    </item>
    
    <item>
      <title>Google Maps without any labels/country names</title>
      <link>https://mneedham.github.io/2012/05/31/google-maps-without-any-labelscountry-names/</link>
      <pubDate>Thu, 31 May 2012 21:52:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/31/google-maps-without-any-labelscountry-names/</guid>
      <description>I wanted to get a blank version of Google Maps without any of the country names on for a visualisation I&amp;rsquo;m working on but I&amp;rsquo;d been led to believe that this wasn&amp;rsquo;t actually possible.
In actual fact we do have control over whether the labels are shown via the &amp;lsquo;styles&amp;rsquo; option which we can call on the map.
In my case the code looks like this:
var map = new google.</description>
    </item>
    
    <item>
      <title>Haskell: Using type classes to generify Project Euler #31</title>
      <link>https://mneedham.github.io/2012/05/30/haskell-using-type-classes-to-generify-project-euler-31/</link>
      <pubDate>Wed, 30 May 2012 12:08:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/30/haskell-using-type-classes-to-generify-project-euler-31/</guid>
      <description>As I mentioned in my previous post I&amp;rsquo;ve been working on Project Euler #31 and initially wasn&amp;rsquo;t sure how to write the algorithm.
I came across a post on StackOverflow which explained it in more detail but unfortunately the example used US coins rather than UK ones like in the Project Euler problem.
To start with I created two versions of the function - one for US coins and one for UK coins:</description>
    </item>
    
    <item>
      <title>Haskell: Java Style Enums</title>
      <link>https://mneedham.github.io/2012/05/30/haskell-java-style-enums/</link>
      <pubDate>Wed, 30 May 2012 11:10:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/30/haskell-java-style-enums/</guid>
      <description>I&amp;rsquo;ve been playing around with problem 31 of Project Euler which is defined as follows:
 In England the currency is made up of pound, £, and pence, p, and there are eight coins in general circulation: 1p, 2p, 5p, 10p, 20p, 50p, £1 (100p) and £2 (200p). It is possible to make £2 in the following way: 1 £1 + 150p + 220p + 15p + 12p + 31p How many different ways can £2 be made using any number of coins?</description>
    </item>
    
    <item>
      <title>Haskell: Finding the minimum &amp; maximum values of a Foldable in one pass</title>
      <link>https://mneedham.github.io/2012/05/28/haskell-finding-the-minimum-maximum-values-of-a-foldable-in-one-pass/</link>
      <pubDate>Mon, 28 May 2012 11:18:13 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/28/haskell-finding-the-minimum-maximum-values-of-a-foldable-in-one-pass/</guid>
      <description>I recently came across Dan Piponi&amp;rsquo;s blog post &amp;lsquo;Haskell Monoids &amp;amp; their Uses&amp;rsquo; and towards the end of the post he suggests creating monoids to work out the maximum and minimum values of a Foldable value in one pass.
 The Foldable type class provides a generic approach to walking through a datastructure, accumulating values as we go. The foldMap function applies a function to each element of our structure and then accumulates the return values of each of these applications.</description>
    </item>
    
    <item>
      <title>Haskell: Debugging code</title>
      <link>https://mneedham.github.io/2012/05/27/haskell-debugging-code/</link>
      <pubDate>Sun, 27 May 2012 22:16:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/27/haskell-debugging-code/</guid>
      <description>In my continued attempts to learn QuickCheck, one thing I&amp;rsquo;ve been doing is comparing the results of my brute force and divide &amp;amp; conquer versions of the closest pairs algorithm.
I started with this property:
let prop_dc_bf xs = (length xs &amp;gt; 2) ==&amp;gt; (fromJust $ bfClosest xs) == dcClosest xs  And then ran it from GHCI, which resulted in the following error:
&amp;gt; quickCheck (prop_dc_bf :: [(Double, Double)] -&amp;gt; Property) *** Failed!</description>
    </item>
    
    <item>
      <title>Haskell: Using monoids when sorting by multiple parameters</title>
      <link>https://mneedham.github.io/2012/05/23/haskell-using-monoids-when-sorting-by-multiple-parameters/</link>
      <pubDate>Wed, 23 May 2012 06:44:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/23/haskell-using-monoids-when-sorting-by-multiple-parameters/</guid>
      <description>On the project I&amp;rsquo;ve been working on we had a requirement to sort a collection of rows by 4 different criteria such that if two items matched for the first criteria we should consider the second criteria and so on.
If we wrote that code in Haskell it would read a bit like this:
data Row = Row { shortListed :: Bool, cost :: Float, distance1 :: Int, distance2 :: Int } deriving (Show, Eq)  import Data.</description>
    </item>
    
    <item>
      <title>Scala/Haskell: A simple example of type classes</title>
      <link>https://mneedham.github.io/2012/05/22/scalahaskell-a-simple-example-of-type-classes/</link>
      <pubDate>Tue, 22 May 2012 10:26:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/22/scalahaskell-a-simple-example-of-type-classes/</guid>
      <description>I never really understood type classes when I was working with Scala but I recently came across a video where Dan Rosen explains them pretty well.
Since the last time I worked in Scala I&amp;rsquo;ve been playing around with Haskell where type classes are much more common - for example if we want to compare two values we need to make sure that their type extends the &amp;lsquo;Eq&amp;rsquo; type class.</description>
    </item>
    
    <item>
      <title>Haskell: My first attempt with QuickCheck and HUnit</title>
      <link>https://mneedham.github.io/2012/05/20/haskell-my-first-attempt-with-quickcheck-and-hunit/</link>
      <pubDate>Sun, 20 May 2012 19:09:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/20/haskell-my-first-attempt-with-quickcheck-and-hunit/</guid>
      <description>As I mentioned in a blog post a few days I&amp;rsquo;ve started learning QuickCheck with the test-framework package as suggested by David Turner.
I first needed to install test-framework and some dependencies using cabal:
&amp;gt; cabal install test-framework &amp;gt; cabal install test-framework-quickcheck &amp;gt; cabal install test-framework-hunit  I thought it&amp;rsquo;d be interesting to try and write some tests around the windowed function that I wrote a few months ago:
Windowed.hs</description>
    </item>
    
    <item>
      <title>Building an API: Test Harness UI</title>
      <link>https://mneedham.github.io/2012/05/19/building-an-api-test-harness-ui/</link>
      <pubDate>Sat, 19 May 2012 20:03:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/19/building-an-api-test-harness-ui/</guid>
      <description>On the project I&amp;rsquo;ve been working on we&amp;rsquo;re building an API to be used by other applications in the organisation but when we started none of those applications were ready to integrate with us and therefore drive the API design.
Initially we tried driving the API through integration style tests but we realised that taking this approach made it quite difficult for us to imagine how an application would use it.</description>
    </item>
    
    <item>
      <title>Haskell: Writing a custom equality operator</title>
      <link>https://mneedham.github.io/2012/05/16/haskell-writing-a-custom-equality-operator/</link>
      <pubDate>Wed, 16 May 2012 13:16:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/16/haskell-writing-a-custom-equality-operator/</guid>
      <description>In the comments on my post about generating random numbers to test a function David Turner suggested that this was exactly the use case for which QuickCheck was intended for so I&amp;rsquo;ve been learning a bit more about that this week.
I started with a simple property to check that the brute force (bf) and divide and conquer (dc) versions of the algorithm returned the same result, assuming that there were enough values in the list to have a closest pair:</description>
    </item>
    
    <item>
      <title>Haskell: Removing if statements</title>
      <link>https://mneedham.github.io/2012/05/12/haskell-removing-if-statements/</link>
      <pubDate>Sat, 12 May 2012 15:46:31 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/12/haskell-removing-if-statements/</guid>
      <description>When I was looking over my solution to the closest pairs algorithm which I wrote last week I realised there there were quite a few if statements, something I haven&amp;rsquo;t seen in other Haskell code I&amp;rsquo;ve read.
This is the initial version that I wrote:
dcClosest :: (Ord a, Floating a) =&amp;gt; [Point a] -&amp;gt; (Point a, Point a) dcClosest pairs if length pairs &amp;lt;= 3 then = fromJust $ bfClosest pairs else foldl (\closest (p1:p2:_) -&amp;gt; if distance (p1, p2) &amp;lt; distance closest then (p1, p2) else closest) closestPair (windowed 2 pairsWithinMinimumDelta) where sortedByX = sortBy compare pairs	(leftByX:rightByX:_) = chunk (length sortedByX `div` 2) sortedByX closestPair = if distance closestLeftPair &amp;lt; distance closestRightPair then closestLeftPair else closestRightPair where closestLeftPair = dcClosest leftByX closestRightPair = dcClosest rightByX pairsWithinMinimumDelta = sortBy (compare `on` snd) $ filter withinMinimumDelta sortedByX where withinMinimumDelta (x, _) = abs (xMidPoint - x) &amp;lt;= distance closestPair where (xMidPoint, _) = last leftByX  We can remove the first if statement which checks the length of the list and replace it with pattern matching code like so:</description>
    </item>
    
    <item>
      <title>neo4j/Cypher: Finding the shortest path between two nodes while applying predicates</title>
      <link>https://mneedham.github.io/2012/05/12/neo4jcypher-finding-the-shortest-path-between-two-nodes-while-applying-predicates/</link>
      <pubDate>Sat, 12 May 2012 14:55:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/12/neo4jcypher-finding-the-shortest-path-between-two-nodes-while-applying-predicates/</guid>
      <description>As I mentioned in a blog post about a week ago I decided to restructure the ThoughtWorks graph I&amp;rsquo;ve modelled in neo4j so that I could explicitly model projects and clients.
As a result I had to update a traversal I&amp;rsquo;d written for finding the shortest path between two people in the graph.
The original traversal query I had was really simple because I had a direct connection between the people nodes:</description>
    </item>
    
    <item>
      <title>Haskell: Explicit type declarations in GHCI</title>
      <link>https://mneedham.github.io/2012/05/10/haskell-explicit-type-declarations-in-ghci/</link>
      <pubDate>Thu, 10 May 2012 07:11:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/10/haskell-explicit-type-declarations-in-ghci/</guid>
      <description>On a few occasions I&amp;rsquo;ve wanted to be able to explicitly define the type of something when trying things out in the Haskell REPL (GHCI) but I didn&amp;rsquo;t actually realise this was possible until a couple of days ago.
For example say we want to use the read function to parse an input string into an integer.
We could do this:
&amp;gt; read &amp;quot;1&amp;quot; :: Int 1  But if we just evaluate the function alone and try and assign the result without casting to a type we get an exception:</description>
    </item>
    
    <item>
      <title>Haskell: Closest Pairs Algorithm</title>
      <link>https://mneedham.github.io/2012/05/09/haskell-closest-pairs-algorithm/</link>
      <pubDate>Wed, 09 May 2012 00:05:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/09/haskell-closest-pairs-algorithm/</guid>
      <description>As I mentioned in a post a couple of days ago I&amp;rsquo;ve been writing the closest pairs algorithm in Haskell and while the brute force version works for small numbers of pairs it starts to fall apart as the number of pairs increases:
time ./closest_pairs 100 bf ./closest_pairs 100 bf 0.01s user 0.00s system 87% cpu 0.016 total time ./closest_pairs 1000 bf ./closest_pairs 1000 bf 3.59s user 0.01s system 99% cpu 3.</description>
    </item>
    
    <item>
      <title>Haskell: Generating random numbers</title>
      <link>https://mneedham.github.io/2012/05/08/haskell-generating-random-numbers/</link>
      <pubDate>Tue, 08 May 2012 22:09:17 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/08/haskell-generating-random-numbers/</guid>
      <description>As I mentioned in my last post I&amp;rsquo;ve been coding the closest pairs algorithm in Haskell and needed to create some pairs of coordinates to test it against.
I&amp;rsquo;ve tried to work out how to create lists of random numbers in Haskell before and always ended up giving up because it seemed way more difficult than it should be but this time I came across a really good explanation of how to do it by jrockway on Stack Overflow.</description>
    </item>
    
    <item>
      <title>Haskell: Maximum Int value</title>
      <link>https://mneedham.github.io/2012/05/07/haskell-maximum-int-value/</link>
      <pubDate>Mon, 07 May 2012 09:18:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/07/haskell-maximum-int-value/</guid>
      <description>One of the algorithms covered in Algo Class was the closest pairs algorithm - an algorithm used to determine which pair of points on a plane are closest to each other based on their Euclidean distance.
My real interest lies in writing the divide and conquer version of the algorithm but I started with the brute force version so that I&amp;rsquo;d be able to compare my answers.
This is the algorithm:</description>
    </item>
    
    <item>
      <title>neo4j: What question do you want to answer?</title>
      <link>https://mneedham.github.io/2012/05/05/neo4j-what-question-do-you-want-to-answer/</link>
      <pubDate>Sat, 05 May 2012 13:20:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/05/05/neo4j-what-question-do-you-want-to-answer/</guid>
      <description>Over the past few weeks I&amp;rsquo;ve been modelling ThoughtWorks project data in neo4j and I realised that the way that I&amp;rsquo;ve been doing this is by considering what question I want to answer and then building a graph to answer it.
When I first started doing this the main question I wanted to answer was &amp;lsquo;how connected are people to each other&amp;rsquo; which led to me modelling the data like this:</description>
    </item>
    
    <item>
      <title>gephi: Centring a graph around an individual node</title>
      <link>https://mneedham.github.io/2012/04/30/gephi-centring-a-graph-around-an-individual-node/</link>
      <pubDate>Mon, 30 Apr 2012 22:20:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/30/gephi-centring-a-graph-around-an-individual-node/</guid>
      <description>I spent some time recently playing around with gephi - an open source platform for creating visualisations of graphs - to get a bit more insight into the ThoughtWorks graph which I&amp;rsquo;ve created in neo4j.
I followed Max De Marxi&amp;rsquo;s blog post to create a GEFX (Graph Exchange XML Format) file to use in gephi although I later learned that you can import directly from neo4j into gephi which I haven&amp;rsquo;t tried yet.</description>
    </item>
    
    <item>
      <title>Performance: Caching per request</title>
      <link>https://mneedham.github.io/2012/04/30/performance-caching-per-request/</link>
      <pubDate>Mon, 30 Apr 2012 21:45:50 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/30/performance-caching-per-request/</guid>
      <description>A couple of years ago I wrote a post describing an approach my then colleague Christian Blunden used to help improve the performance of an application where you try to do expensive things less or find another way to do them.
On the application I&amp;rsquo;m currently working on we load reference data from an Oracle database into memory based on configurations provided by the user.
There are multiple configurations and then multiple ways that those configurations can be priced so we have two nested for loops in which we load data and then perform calculations on it.</description>
    </item>
    
    <item>
      <title>Haskell: Colour highlighting when writing to the shell</title>
      <link>https://mneedham.github.io/2012/04/29/haskell-colour-highlighting-when-writing-to-the-shell/</link>
      <pubDate>Sun, 29 Apr 2012 00:01:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/29/haskell-colour-highlighting-when-writing-to-the-shell/</guid>
      <description>I spent a few hours writing a simple front end on top of the Rabin Karp algorithm so that I could show the line of the first occurrence of a pattern in a piece of text on the shell.
I thought it would be quite cool if I could highlight the appropriate text on the line like how grep does when the &amp;lsquo;&amp;ndash;color=auto&amp;rsquo; flag is supplied.
We can make use of ANSI escape codes to do this.</description>
    </item>
    
    <item>
      <title>Haskell: Int and Integer</title>
      <link>https://mneedham.github.io/2012/04/28/haskell-int-and-integer/</link>
      <pubDate>Sat, 28 Apr 2012 17:39:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/28/haskell-int-and-integer/</guid>
      <description>In my last post about the Rabin Karp algorithm I mentioned that I was having some problems when trying to write a hash function which closely matched its English description.
 ((rm-1 * ascii char) + (rm-2 * ascii char) + … (r0 * ascii char)) % q where r = 256, q = 1920475943  This is my current version of the hash function:
hash = hash&#39; globalR globalQ hash&#39; r q string m = foldl (\acc x -&amp;gt; (r * acc + ord x) `mod` q) 0 $ take m string  And my initial attempt to write the alternate version was this:</description>
    </item>
    
    <item>
      <title>Algorithms: Rabin Karp in Haskell</title>
      <link>https://mneedham.github.io/2012/04/25/algorithms-rabin-karp-in-haskell/</link>
      <pubDate>Wed, 25 Apr 2012 21:28:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/25/algorithms-rabin-karp-in-haskell/</guid>
      <description>I recently came across a blog post describing the Rabin Karp algorithm - an algorithm that uses hashing to find a pattern string in some text - and thought it would be interesting to try and write a version of it in Haskell.
This algorithm is typically used when we want to search for multiple pattern strings in a text e.g. when detecting plagiarism or a primitive way of detecting code duplication but my initial version only lets your search for one pattern.</description>
    </item>
    
    <item>
      <title>Algo Class: Start simple and build up</title>
      <link>https://mneedham.github.io/2012/04/24/algo-class-start-simple-and-build-up/</link>
      <pubDate>Tue, 24 Apr 2012 07:17:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/24/algo-class-start-simple-and-build-up/</guid>
      <description>Over the last six weeks I&amp;rsquo;ve been working through Stanford&amp;rsquo;s Design and Analysis of Algorithms I class and each week there&amp;rsquo;s been a programming assignment on a specific algorithm for which a huge data set is provided.
For the first couple of assignments I tried writing the code for the algorithm and then running it directly against the provided data set.
As you might imagine it never worked first time and this approach led to me becoming very frustrated because there&amp;rsquo;s no way of telling what went wrong.</description>
    </item>
    
    <item>
      <title>Coding: Is there a name for everything?</title>
      <link>https://mneedham.github.io/2012/04/23/coding-is-there-a-name-for-everything/</link>
      <pubDate>Mon, 23 Apr 2012 00:20:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/23/coding-is-there-a-name-for-everything/</guid>
      <description>A month ago I wrote a post describing an approach my team has been taking to avoid premature abstractions whereby we leave code inline until we know enough about the domain to pull out meaningful classes or methods.
Since I wrote that post we&amp;rsquo;ve come across a couple of examples where there doesn&amp;rsquo;t seem to be a name to describe a data structure.
We are building a pricing engine where the input is a set of configurations and the output is a set of pricing rows associated with each configuration.</description>
    </item>
    
    <item>
      <title>neo4J: Searching for nodes by name</title>
      <link>https://mneedham.github.io/2012/04/20/neo4j-searching-for-nodes-by-name/</link>
      <pubDate>Fri, 20 Apr 2012 07:10:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/20/neo4j-searching-for-nodes-by-name/</guid>
      <description>As I mentioned in a post a few days ago I&amp;rsquo;ve been graphing connections between ThoughtWorks people using neo4j and wanted to build auto complete functionality so I can search for the names of people in the graph.
The solution I came up was to create a Lucene index with an entry for each node and a common property on each document in the index so that I&amp;rsquo;d be able to get all the index entries easily.</description>
    </item>
    
    <item>
      <title>Algorithms: Flood Fill in Haskell - Abstracting the common</title>
      <link>https://mneedham.github.io/2012/04/17/algorithms-flood-fill-in-haskell-abstracting-the-common/</link>
      <pubDate>Tue, 17 Apr 2012 07:22:12 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/17/algorithms-flood-fill-in-haskell-abstracting-the-common/</guid>
      <description>In the comments of my blog post describing the flood fill algorithm in Haskell David Turner pointed out that the way I was passing the grid around was quite error prone.
floodFill :: Array (Int, Int) Colour -&amp;gt; (Int, Int) -&amp;gt; Colour -&amp;gt; Colour -&amp;gt; Array (Int, Int) Colour floodFill grid point@(x, y) target replacement = if((not $ inBounds grid point) || grid ! (x,y) /= target) then grid else gridNorth where grid&#39; = replace grid point replacement gridEast = floodFill grid&#39; (x+1, y) target replacement gridWest = floodFill gridEast (x-1, y) target replacement gridSouth = floodFill gridWest (x, y+1) target replacement gridNorth = floodFill gridSouth (x, y-1) target replacement  I actually did pass the wrong grid variable around while I was writing it and ended up quite confused as to why it wasn&amp;rsquo;t working as I expected.</description>
    </item>
    
    <item>
      <title>neography/neo4j/Lucene: Getting a list of all the nodes indexed</title>
      <link>https://mneedham.github.io/2012/04/17/neographyneo4jlucene-getting-a-list-of-all-the-nodes-indexed/</link>
      <pubDate>Tue, 17 Apr 2012 06:54:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/17/neographyneo4jlucene-getting-a-list-of-all-the-nodes-indexed/</guid>
      <description>I&amp;rsquo;ve been playing around with neo4j using the neography gem to create a graph of all the people in ThoughtWorks and the connections between them based on working with each other.
I created a UI where you could type in the names of two people and see when they&amp;rsquo;ve worked together or the path between the shortest path between them if they haven&amp;rsquo;t.
I thought it would be cool to have auto complete functionality when typing in a name but I couldn&amp;rsquo;t figure out how to partially query the index of people&amp;rsquo;s names that I&amp;rsquo;d created.</description>
    </item>
    
    <item>
      <title>Haskell: A simple parsing example using pattern matching</title>
      <link>https://mneedham.github.io/2012/04/15/haskell-a-simple-parsing-example-using-pattern-matching/</link>
      <pubDate>Sun, 15 Apr 2012 14:22:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/15/haskell-a-simple-parsing-example-using-pattern-matching/</guid>
      <description>As part of the second question in the Google Code Jam I needed to be able to parse lines of data which looked like this:
3 1 5 15 13 11  where
The first integer will be N, the number of Googlers, and the second integer will be S, the number of surprising triplets of scores. The third integer will be p, as described above. Next will be N integers ti: the total points of the Googlers.</description>
    </item>
    
    <item>
      <title>Haskell: Reading in multiple lines of arguments</title>
      <link>https://mneedham.github.io/2012/04/15/haskell-reading-in-multiple-lines-of-arguments/</link>
      <pubDate>Sun, 15 Apr 2012 13:44:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/15/haskell-reading-in-multiple-lines-of-arguments/</guid>
      <description>I&amp;rsquo;ve mostly avoided doing any I/O in Haskell but as part of the Google Code Jam I needed to work out how to read a variable number of lines as specified by the user.
The input looks like this:
4 3 1 5 15 13 11 3 0 8 23 22 21 2 1 1 8 0 6 2 8 29 20 8 18 18 21  The first line indicates how many lines will follow.</description>
    </item>
    
    <item>
      <title>Ruby: neo4j gem - LoadError: no such file to load -- active_support/core_ext/class/inheritable_attributes</title>
      <link>https://mneedham.github.io/2012/04/14/ruby-neo4j-gem-loaderror-no-such-file-to-load-active_supportcore_extclassinheritable_attributes/</link>
      <pubDate>Sat, 14 Apr 2012 10:21:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/14/ruby-neo4j-gem-loaderror-no-such-file-to-load-active_supportcore_extclassinheritable_attributes/</guid>
      <description>I&amp;rsquo;ve been playing around with neo4j again over the past couple of days using the neo4j.rb gem to build up a graph.
I installed the gem but then ended up with the following error when I tried to &amp;lsquo;require neo4j&amp;rsquo; in &amp;lsquo;irb&amp;rsquo;:
LoadError: no such file to load -- active_support/core_ext/class/inheritable_attributes require at org/jruby/RubyKernel.java:1033 require at /Users/mneedham/.rbenv/versions/jruby-1.6.7/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:36 (root) at /Users/mneedham/.rbenv/versions/jruby-1.6.7/lib/ruby/gems/1.8/gems/neo4j-1.3.1-java/lib/neo4j.rb:9 require at org/jruby/RubyKernel.java:1033 require at /Users/mneedham/.rbenv/versions/jruby-1.6.7/lib/ruby/gems/1.8/gems/neo4j-1.3.1-java/lib/neo4j.rb:59 (root) at src/main/ruby/neo_test.rb:2  It seems a few others have come across this problem as well and the problem seems to be that ActiveSupport 3.</description>
    </item>
    
    <item>
      <title>Just Observe</title>
      <link>https://mneedham.github.io/2012/04/09/just-observe/</link>
      <pubDate>Mon, 09 Apr 2012 22:45:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/09/just-observe/</guid>
      <description>One of the most common instincts of a developer when starting on a new team is to look at the way the application has been designed and find ways that it can be done differently.
Most often &amp;lsquo;differently&amp;rsquo; means that a pattern used in a previous project will be favoured and while I think it&amp;rsquo;s good to make use of experience that we&amp;rsquo;ve gained, we do miss out on some learning if we write every application the same way.</description>
    </item>
    
    <item>
      <title>Haskell: Processing program arguments</title>
      <link>https://mneedham.github.io/2012/04/08/haskell-processing-program-arguments/</link>
      <pubDate>Sun, 08 Apr 2012 20:11:57 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/08/haskell-processing-program-arguments/</guid>
      <description>My Prismatic news feed recently threw up an interesting tutorial titled &amp;lsquo;Haskell the Hard Way&amp;rsquo; which has an excellent and easy to understand section showing how to do IO in Haskell.
About half way down the page there&amp;rsquo;s an exercise to write a program which sums all its arguments which I thought I&amp;rsquo;d have a go at.
We need to use the System.getArgs function to get the arguments passed to the program.</description>
    </item>
    
    <item>
      <title>Algorithms: Flood Fill in Haskell</title>
      <link>https://mneedham.github.io/2012/04/07/algorithms-flood-fill-in-haskell/</link>
      <pubDate>Sat, 07 Apr 2012 00:25:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/07/algorithms-flood-fill-in-haskell/</guid>
      <description>Flood fill is an algorithm used to work out which nodes are connected to a certain node in a multi dimensional array. In this case we&amp;rsquo;ll use a two dimensional array.
The idea is that we decide that we want to change the colour of one of the cells in the array and have its immediate neighbours who share its initial colour have their colour changed too i.e. the colour floods its way through the grid.</description>
    </item>
    
    <item>
      <title>Haskell: Print friendly representation of an Array</title>
      <link>https://mneedham.github.io/2012/04/03/haskell-print-friendly-representation-of-an-array/</link>
      <pubDate>Tue, 03 Apr 2012 21:52:56 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/04/03/haskell-print-friendly-representation-of-an-array/</guid>
      <description>Quite frequently I play around with 2D arrays in Haskell but I&amp;rsquo;ve never quite worked out how to print them in a way that makes it easy to see the contents.
I&amp;rsquo;m using the array from the &amp;lsquo;Data.Array&amp;rsquo; module because it seems to be easier to transform them into a new representation if I want to change a value in one of the cells.
The function to create one therefore looks like this:</description>
    </item>
    
    <item>
      <title>Haskell: Pattern matching data types with named fields</title>
      <link>https://mneedham.github.io/2012/03/31/haskell-pattern-matching-data-types-with-named-fields/</link>
      <pubDate>Sat, 31 Mar 2012 22:49:18 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/31/haskell-pattern-matching-data-types-with-named-fields/</guid>
      <description>One of my favourite things about coding in Haskell is that I often end up pattern matching against data types.
I&amp;rsquo;ve been playing around with modelling cars coming into and out from a car park and changing the state of the car park accordingly.
I started with these data type definitions:
data CarParkState = Available Bool Int Int | AlmostFull Bool Int Int | Full Bool Int deriving (Show) data Action = Entering | Leaving deriving (Show) data Sticker = Handicap | None deriving (Show)  which were used in the following function:</description>
    </item>
    
    <item>
      <title>Micro Services: A simple example</title>
      <link>https://mneedham.github.io/2012/03/31/micro-services-a-simple-example/</link>
      <pubDate>Sat, 31 Mar 2012 09:06:14 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/31/micro-services-a-simple-example/</guid>
      <description>In our code base we had the concept of a &amp;lsquo;ProductSpeed&amp;rsquo; with two different constructors which initialised the object in different ways:
public class ProductSpeed { public ProductSpeed(String name) { ... } public ProductSpeed(String name, int order)) { } }  In the cases where the first constructor was used the order of the product was irrelevant.
When the second constructor was used we did care about it because we wanted to be able sort the products before showing them in a drop down list to the user.</description>
    </item>
    
    <item>
      <title>IntelliJ: Find/Replace using regular expressions with capture groups</title>
      <link>https://mneedham.github.io/2012/03/30/intellij-findreplace-using-regular-expressions-with-capture-groups/</link>
      <pubDate>Fri, 30 Mar 2012 06:21:00 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/30/intellij-findreplace-using-regular-expressions-with-capture-groups/</guid>
      <description>Everyone now and then we end up having to write a bunch of mapping code and I quite like using IntelliJ&amp;rsquo;s &amp;lsquo;Replace&amp;rsquo; option to do it but always end up spending about 5 minutes trying to remember how to do capture groups so I thought I&amp;rsquo;d write it down this time.
Given the following text in our file:
val mark = 0 val dave = 0 val john = 0 val alex = 0  Let&amp;rsquo;s say we wanted to prefix each of those names with &amp;lsquo;cool&amp;rsquo; and had decided not to use Column mode for whatever reason.</description>
    </item>
    
    <item>
      <title>Readability/Performance</title>
      <link>https://mneedham.github.io/2012/03/29/readabilityperformance/</link>
      <pubDate>Thu, 29 Mar 2012 06:45:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/29/readabilityperformance/</guid>
      <description>I recently read the Graphite chapter of The Architecture of Open Source Applications book which mostly tells the story of how Chris Davis incrementally built out Graphite - a pretty cool tool that can be used to do real time graphing of metrics.
The whole chapter is a very good read but I found the design reflections especially interesting:
 One of Graphite&#39;s greatest strengths and greatest weaknesses is the fact that very little of it was actually &#34;</description>
    </item>
    
    <item>
      <title>Testing: Trying not to overdo it</title>
      <link>https://mneedham.github.io/2012/03/28/testing-trying-not-to-overdo-it/</link>
      <pubDate>Wed, 28 Mar 2012 00:10:46 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/28/testing-trying-not-to-overdo-it/</guid>
      <description>The design of the code which contains the main logic of the application that I&amp;rsquo;m currently working on looks a bit like the diagram on the right hand side:
 We load a bunch of stuff from an Oracle database, construct some objects from the data and then invoke a sequence of methods on those objects in order to execute our domain logic.
Typically we might expect to see unit level test against all the classes described in this diagram but we&amp;rsquo;ve actually been trying out an approach where we don&amp;rsquo;t test the orchestration code directly but rather only test it via the resource which makes use of it.</description>
    </item>
    
    <item>
      <title>Haskell: Memoization using the power of laziness</title>
      <link>https://mneedham.github.io/2012/03/24/haskell-memoization-using-the-power-of-laziness/</link>
      <pubDate>Sat, 24 Mar 2012 12:28:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/24/haskell-memoization-using-the-power-of-laziness/</guid>
      <description>I&amp;rsquo;ve been trying to solve problem 15 of Project Euler which requires you to find the number of routes that can be taken to navigate from the top corner of a grid down to the bottom right corner.
For example there are six routes across a 2x2 grid:
 My initial solution looked like this:
routes :: (Int, Int) -&amp;gt; Int -&amp;gt; Int routes origin size = inner origin size where inner origin@(x, y) size | x == size &amp;amp;&amp;amp; y == size = 0 | x == size || y == size = 1 | otherwise = inner (x+1, y) size + inner (x, y+1) size  Which can be called like this:</description>
    </item>
    
    <item>
      <title>Saving the values of dynamically populated dropdown on back button</title>
      <link>https://mneedham.github.io/2012/03/24/saving-the-values-of-dynamically-populated-dropdown-on-back-button/</link>
      <pubDate>Sat, 24 Mar 2012 00:40:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/24/saving-the-values-of-dynamically-populated-dropdown-on-back-button/</guid>
      <description>We wanted to be able to retain the value of a drop down menu that was being dynamically populated (via an AJAX call) when the user hit the back button but the AJAX request re-runs when we go hit back therefore losing our selection.
Our initial thinking was that we might be able to store the value of the dropdown in a hidden field and then restore it into the dropdown using jQuery on page load but that approach didn&amp;rsquo;t work since hidden fields don&amp;rsquo;t seem to retain their values when you hit back.</description>
    </item>
    
    <item>
      <title>Oracle Spatial: Querying by a point/latitude/longitude</title>
      <link>https://mneedham.github.io/2012/03/23/oracle-spatial-querying-by-a-pointlatitudelongitude/</link>
      <pubDate>Fri, 23 Mar 2012 23:54:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/23/oracle-spatial-querying-by-a-pointlatitudelongitude/</guid>
      <description>We&amp;rsquo;re using Oracle Spatial on the application I&amp;rsquo;m working on and while most of the time any spatial queries we make are done from Java code we wanted to be able to run them directly from SQL as well to verify the code was working correctly.
We normally end up forgetting how to construct a query so I thought I&amp;rsquo;d document it.
Assuming we have a table table_with_shape which has a column shape which is a polygon, if we want to check whether a lat/long value interacts with that shape we can do that with the following query:</description>
    </item>
    
    <item>
      <title>Functional Programming: Handling the Options</title>
      <link>https://mneedham.github.io/2012/03/21/functional-programming-handling-the-options/</link>
      <pubDate>Wed, 21 Mar 2012 00:50:37 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/21/functional-programming-handling-the-options/</guid>
      <description>A couple of weeks ago Channing Walton tweeted the following:
 Every time you call get on an Option a kitten dies.  As Channing points out in the comments he was referring to unguarded calls to &amp;lsquo;get&amp;rsquo; which would lead to an exception if the Option was empty, therefore pretty much defeating the point of using an Option in the first place!
We&amp;rsquo;re using Dan Bodart&amp;rsquo;s totallylazy library on the application I&amp;rsquo;m currently working on and in fact were calling &amp;lsquo;get&amp;rsquo; on an Option so I wanted to see if we could get rid of it.</description>
    </item>
    
    <item>
      <title>Haskell: Newbie currying mistake</title>
      <link>https://mneedham.github.io/2012/03/20/haskell-newbie-currying-mistake/</link>
      <pubDate>Tue, 20 Mar 2012 23:55:51 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/20/haskell-newbie-currying-mistake/</guid>
      <description>As I mentioned in my last post I&amp;rsquo;ve spent a bit of this evening writing a merge sort function and one of the mistakes I made a few times was incorrectly passing arguments to the recursive calls of &amp;lsquo;merge&amp;rsquo;.
For example, this is one of the earlier versions of the function:
middle :: [Int] -&amp;gt; Int middle = floor . (\y -&amp;gt; y / 2) . fromIntegral . length	msort :: [Int] -&amp;gt; [Int] msort unsorted = let n = middle unsorted in if n == 0 then unsorted else let (left, right) = splitAt n unsorted in merge (msort left) (msort right) where merge [] right = right merge left [] = left merge left@(x:xs) right@(y:ys) = if x &amp;lt; y then x : merge(xs, right) else y : merge (left, ys)  Which doesn&amp;rsquo;t actually compile:</description>
    </item>
    
    <item>
      <title>Haskell: Chaining functions to find the middle value in a collection</title>
      <link>https://mneedham.github.io/2012/03/20/haskell-chaining-functions-to-find-the-middle-value-in-a-collection/</link>
      <pubDate>Tue, 20 Mar 2012 23:36:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/20/haskell-chaining-functions-to-find-the-middle-value-in-a-collection/</guid>
      <description>I&amp;rsquo;ve been playing around with writing merge sort in Haskell and eventually ended up with the following function:
msort :: [Int] -&amp;gt; [Int] msort unsorted = let n = floor (fromIntegral(length unsorted) / 2) in if n == 0 then unsorted else let (left, right) = splitAt n unsorted in merge (msort left) (msort right) where merge [] right = right merge left [] = left merge left@(x:xs) right@(y:ys) = if x &amp;lt; y then x : merge xs right else y : merge left ys  The 3rd line was annoying me as it has way too many brackets on it and I was fairly sure that it should be possible to just combine the functions like I learnt to do in F# a few years ago.</description>
    </item>
    
    <item>
      <title>Scala: Counting number of inversions (via merge sort) for an unsorted collection</title>
      <link>https://mneedham.github.io/2012/03/20/scala-counting-number-of-inversions-via-merge-sort-for-an-unsorted-collection/</link>
      <pubDate>Tue, 20 Mar 2012 06:53:18 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/20/scala-counting-number-of-inversions-via-merge-sort-for-an-unsorted-collection/</guid>
      <description>The first programming questions of algo-class requires you to calculate the number of inversions it would take using merge sort to sort a collection in ascending order.
I found quite a nice explanation here too:
 Finding &#34;similarity&#34; between two rankings. Given a sequence of n numbers 1..n (assume all numbers are distinct). Define a measure that tells us how far this list is from being in ascending order. The value should be 0 if a_1 The simple/naive way of solving this problem is to iterate through the collection in two loops and compare each value and its current index with the others, looking for ones which are not in the right order.</description>
    </item>
    
    <item>
      <title>Functional Programming: One function at a time</title>
      <link>https://mneedham.github.io/2012/03/19/functional-programming-one-function-at-a-time/</link>
      <pubDate>Mon, 19 Mar 2012 23:25:47 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/19/functional-programming-one-function-at-a-time/</guid>
      <description>As I mentioned in an earlier post I got a bit stuck working out all the diagonals in the 20x20 grid of Project Euler problem 11 and my colleague Uday ended up showing me how to do it.
I realised while watching him solve the problem that we&amp;rsquo;d been using quite different approaches to solving the problem and that his way worked way better than mine, at least in this context.</description>
    </item>
    
    <item>
      <title>Coding: Wait for the abstractions to emerge</title>
      <link>https://mneedham.github.io/2012/03/17/coding-wait-for-the-abstractions-to-emerge/</link>
      <pubDate>Sat, 17 Mar 2012 11:19:11 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/17/coding-wait-for-the-abstractions-to-emerge/</guid>
      <description>One of the things that I&amp;rsquo;ve learnt while developing code in an incremental way is that the way the code should be designed isn&amp;rsquo;t going to be obvious straight away so we need to be patience and wait for it to emerge.
There&amp;rsquo;s often a tendency to pull out classes or methods but more recently I&amp;rsquo;ve been trying to follow an approach where I leave the code in one class/method and play around with/study it until I see a good abstraction to make.</description>
    </item>
    
    <item>
      <title>Mercurial: hg push to Google Code</title>
      <link>https://mneedham.github.io/2012/03/14/mercurial-hg-push-to-google-code/</link>
      <pubDate>Wed, 14 Mar 2012 21:25:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/14/mercurial-hg-push-to-google-code/</guid>
      <description>I wanted to make a change to add flatMap to Option in totallylazy so I had to clone the repository and make the change.
I thought I&amp;rsquo;d then be able to just push the change using my Google user name and password but instead ended up with the following error:
➜ mhneedham-totally-lazy hg push pushing to https://m.h.needham@code.google.com/r/mhneedham-totally-lazy/ searching for changes 1 changesets found http authorization required realm: Google Code hg Repository user: m.</description>
    </item>
    
    <item>
      <title>Functional Programming: Shaping the data to fit a function</title>
      <link>https://mneedham.github.io/2012/03/13/functional-programming-shaping-the-data-to-fit-a-function/</link>
      <pubDate>Tue, 13 Mar 2012 22:55:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/13/functional-programming-shaping-the-data-to-fit-a-function/</guid>
      <description>As I mentioned in my last post I&amp;rsquo;ve been working on Project Euler problem 11 and one thing I noticed was that I was shaping the data around a windowed function since it seemed to fit the problem quite well.
Problem 11 is defined like so:
 In the 20x20 grid below, four numbers along a diagonal line have been marked in red. The product of these numbers is 26 63 78 14 = 1788696.</description>
    </item>
    
    <item>
      <title>Haskell: Couldn&#39;t match expected type `Int&#39; with actual type `Integer&#39;</title>
      <link>https://mneedham.github.io/2012/03/13/haskell-couldnt-match-expected-type-int-with-actual-type-integer/</link>
      <pubDate>Tue, 13 Mar 2012 19:42:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/13/haskell-couldnt-match-expected-type-int-with-actual-type-integer/</guid>
      <description>One of the most frequent compilation error messages that I&amp;rsquo;ve been getting while working through the Project Euler problems in Haskell is the following:
Couldn&#39;t match expected type `Int&#39; with actual type `Integer&#39;  In problem 11, for example, I define the grid of numbers like so:
grid = [[08,02,22,97,38,15,00,40,00,75,04,05,07,78,52,12,50,77,91,08], [49,49,99,40,17,81,18,57,60,87,17,40,98,43,69,48,04,56,62,00], [81,49,31,73,55,79,14,29,93,71,40,67,53,88,30,03,49,13,36,65], [52,70,95,23,04,60,11,42,69,24,68,56,01,32,56,71,37,02,36,91], [22,31,16,71,51,67,63,89,41,92,36,54,22,40,40,28,66,33,13,80], [24,47,32,60,99,03,45,02,44,75,33,53,78,36,84,20,35,17,12,50], [32,98,81,28,64,23,67,10,26,38,40,67,59,54,70,66,18,38,64,70], [67,26,20,68,02,62,12,20,95,63,94,39,63,08,40,91,66,49,94,21], [24,55,58,05,66,73,99,26,97,17,78,78,96,83,14,88,34,89,63,72], [21,36,23,09,75,00,76,44,20,45,35,14,00,61,33,97,34,31,33,95], [78,17,53,28,22,75,31,67,15,94,03,80,04,62,16,14,09,53,56,92], [16,39,05,42,96,35,31,47,55,58,88,24,00,17,54,24,36,29,85,57], [86,56,00,48,35,71,89,07,05,44,44,37,44,60,21,58,51,54,17,58], [19,80,81,68,05,94,47,69,28,73,92,13,86,52,17,77,04,89,55,40], [04,52,08,83,97,35,99,16,07,97,57,32,16,26,26,79,33,27,98,66], [88,36,68,87,57,62,20,72,03,46,33,67,46,55,12,32,63,93,53,69], [04,42,16,73,38,25,39,11,24,94,72,18,08,46,29,32,40,62,76,36], [20,69,36,41,72,30,23,88,34,62,99,69,82,67,59,85,74,04,36,16], [20,73,35,29,78,31,90,01,74,31,49,71,48,86,81,16,23,57,05,54], [01,70,54,71,83,51,54,69,16,92,33,48,61,43,52,01,89,19,67,48]]  Which has the following type:</description>
    </item>
    
    <item>
      <title>Choosing where to put the complexity</title>
      <link>https://mneedham.github.io/2012/03/06/choosing-where-to-put-the-complexity/</link>
      <pubDate>Tue, 06 Mar 2012 01:17:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/03/06/choosing-where-to-put-the-complexity/</guid>
      <description>On the current application I&amp;rsquo;m working on we need to make use of some data which comes from another system so we&amp;rsquo;ve created an import script which creates a copy of that data so that we can use it in our application.
In general we&amp;rsquo;ve been trying not to do too much manipulation of the data and keeping it close to the initial structure so that if something goes wrong with the import we can more easily trace the problem back to the original data source.</description>
    </item>
    
    <item>
      <title>Haskell: Creating a sliding window over a collection</title>
      <link>https://mneedham.github.io/2012/02/28/haskell-creating-a-sliding-window-over-a-collection/</link>
      <pubDate>Tue, 28 Feb 2012 00:21:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/28/haskell-creating-a-sliding-window-over-a-collection/</guid>
      <description>A couple of years ago when I was playing around with F# I came across the Seq.windowed function which allows you to create a sliding window of a specific size over a collection.
Taking an example from the F# documentation page:
let seqNumbers = [ 1.0; 1.5; 2.0; 1.5; 1.0; 1.5 ] :&amp;gt; seq&amp;lt;float&amp;gt; let seqWindows = Seq.windowed 3 seqNumbers  We end up with this:
Initial sequence: 1.0 1.5 2.</description>
    </item>
    
    <item>
      <title>Haskell: Getting the nth element in a list</title>
      <link>https://mneedham.github.io/2012/02/28/haskell-getting-the-nth-element-in-a-list/</link>
      <pubDate>Tue, 28 Feb 2012 00:02:21 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/28/haskell-getting-the-nth-element-in-a-list/</guid>
      <description>I started trying to solve some of the Project Euler problems as a way to learn a bit of Haskell and problem 7 is defined like so:
 By listing the first six prime numbers: 2, 3, 5, 7, 11, and 13, we can see that the 6th prime is 13. What is the 10 001st prime number?  I read that the Sieve of Eratosthenes is a useful algorithm for working out all the prime numbers and there&amp;rsquo;s a page on the Literate Programs wiki explaining how to derive them using it.</description>
    </item>
    
    <item>
      <title>Java: Faking a closure with a factory to create a domain object</title>
      <link>https://mneedham.github.io/2012/02/26/java-faking-a-closure-with-a-factory-to-create-a-domain-object/</link>
      <pubDate>Sun, 26 Feb 2012 00:09:03 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/26/java-faking-a-closure-with-a-factory-to-create-a-domain-object/</guid>
      <description>Recently we wanted to create a domain object which needed to have an external dependency in order to do a calculation and we wanted to be able to stub out that dependency in our tests.
Originally we were just new&amp;rsquo;ing up the dependency inside the domain class but that makes it impossible to control it&amp;rsquo;s value in a test.
Equally it didn&amp;rsquo;t seem like we should be passing that dependency into the constructor of the domain object since it&amp;rsquo;s not a piece of state which defines the object, just something that it uses.</description>
    </item>
    
    <item>
      <title>Haskell: Viewing the steps of a reduce</title>
      <link>https://mneedham.github.io/2012/02/25/haskell-viewing-the-steps-of-a-reduce/</link>
      <pubDate>Sat, 25 Feb 2012 23:40:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/25/haskell-viewing-the-steps-of-a-reduce/</guid>
      <description>I&amp;rsquo;ve been playing around with Haskell a bit over the last week and in the bit of code I was working on I wanted to fold over a collection but see the state of the fold after each step.
I remembered Don Syme showing me how to do something similar during the F# Exchange last year while we were writing some code to score a tennis game by using Seq.scan.</description>
    </item>
    
    <item>
      <title>Thou shalt storm</title>
      <link>https://mneedham.github.io/2012/02/24/thou-shalt-storm/</link>
      <pubDate>Fri, 24 Feb 2012 02:03:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/24/thou-shalt-storm/</guid>
      <description>On the majority of the teams that I&amp;rsquo;ve worked on there&amp;rsquo;s been a time where everyone seems to be disagreeing with each other about almost everything and the whole situation becomes pretty tense for all involved.
The first time I came across this it seemed quite dysfunctional but I was introduced to Bruce Tuckman&amp;rsquo;s model of group development which helps to explain what&amp;rsquo;s going on.
Tuckman outlines four stages which teams tend to go through - forming, storming, norming and performing.</description>
    </item>
    
    <item>
      <title>Optimising for typing</title>
      <link>https://mneedham.github.io/2012/02/21/optimising-for-typing/</link>
      <pubDate>Tue, 21 Feb 2012 22:21:43 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/21/optimising-for-typing/</guid>
      <description>My colleague Ola Bini recently wrote a post describing his thoughts on the syntax of programming languages and while the post in general is interesting the bit that most resonates with me at the moment is the following:
 Typing fewer characters doesn’t actually optimize for writing either - the intuition behind that statement is quite easy: imagine you had to write a book. However, instead of writing it in English, you just wrote the gzipped version of the book directly.</description>
    </item>
    
    <item>
      <title>Coding: Packaging by vertical slice</title>
      <link>https://mneedham.github.io/2012/02/20/coding-packaging-by-vertical-slice/</link>
      <pubDate>Mon, 20 Feb 2012 21:54:55 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/20/coding-packaging-by-vertical-slice/</guid>
      <description>On most of the applications I&amp;rsquo;ve worked on we&amp;rsquo;ve tended to organise/package classes by the function that they have or the layer that they fit in.
A typical package structure might therefore end up looking like this:
 com.awesome.project  common  StringUtils     controllers  LocationController PricingController     domain  Address Cost CostFactory Location Price     repositories  LocationRepository PriceRepository     services  LocationService      This works reasonably well and allows you to find code which is similar in function but I find that more often than not a lot of the code that lives immediately around where you currently are isn&amp;rsquo;t actually relevant at the time.</description>
    </item>
    
    <item>
      <title>Tech Leads &amp; The Progress Principle</title>
      <link>https://mneedham.github.io/2012/02/18/tech-leads-the-progress-principle/</link>
      <pubDate>Sat, 18 Feb 2012 01:31:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/18/tech-leads-the-progress-principle/</guid>
      <description>I&amp;rsquo;ve been reading The Progress Principle on and off for the last couple of months and one of my favourite quotes from the book is the following:
 Truly effective video game designers know how to create a sense of progress for players within all stages of a game. Truly effective managers know how to do the same for their subordinates.  While a tech lead might not like to be referred to as a manager I think part of the role does involve helping developers to make progress and the best ones I&amp;rsquo;ve worked with seem to do that instinctively.</description>
    </item>
    
    <item>
      <title>Reading Code: boilerpipe</title>
      <link>https://mneedham.github.io/2012/02/13/reading-code-boilerpipe/</link>
      <pubDate>Mon, 13 Feb 2012 21:16:24 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/13/reading-code-boilerpipe/</guid>
      <description>I&amp;rsquo;m a big fan of the iPad application Flipboard, especially it&amp;rsquo;s ability to filter out the non important content on web pages and just show me the main content so I&amp;rsquo;ve been looking around at open source libraries which provide that facility.
I came across a quora page where someone had asked how this was done and the suggested libraries were readability, Goose and boilerpipe.
boilerpipe was written by Christian Kohlschütter and has a corresponding paper and video as well.</description>
    </item>
    
    <item>
      <title>Oracle Spatial: java.sql.SQLRecoverableException: No more data to read from socket</title>
      <link>https://mneedham.github.io/2012/02/11/oracle-spatial-java-sql-sqlrecoverableexception-no-more-data-to-read-from-socket/</link>
      <pubDate>Sat, 11 Feb 2012 10:55:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/11/oracle-spatial-java-sql-sqlrecoverableexception-no-more-data-to-read-from-socket/</guid>
      <description>We&amp;rsquo;re using Oracle Spatial on my current project so that we can locate points within geographical regions and decided earlier in the week to rename the table where we store the SDO_GEOMETRY objects for each region.
We did that by using a normal table alter statement but then started seeing the following error when we tried to insert test data in that column which takes an SDO_GEOMETRY object:
org.hibernate.exception.JDBCConnectionException: could not execute native bulk manipulation query at org.</description>
    </item>
    
    <item>
      <title>Java: Fooled by java.util.Arrays.asList</title>
      <link>https://mneedham.github.io/2012/02/11/java-fooled-by-java-util-arrays-aslist/</link>
      <pubDate>Sat, 11 Feb 2012 10:29:15 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/11/java-fooled-by-java-util-arrays-aslist/</guid>
      <description>I&amp;rsquo;ve been playing around with the boilerpipe code base by writing some tests around it to check my understanding but ran into an interesting problem using java.util.Arrays.asList to pass a list into one of the functions.
I was testing the BlockProximityFusion class which is used to merge together adjacent text blocks.
I started off calling that class like this:
import static java.util.Arrays.asList; @Test public void willCallBlockProximityFustion() throws Exception { TextDocument document = new TextDocument(asList(contentBlock(&amp;quot;some words&amp;quot;), contentBlock(&amp;quot;followed by more words&amp;quot;))); BlockProximityFusion.</description>
    </item>
    
    <item>
      <title>Downloading the JDK 6 source code</title>
      <link>https://mneedham.github.io/2012/02/11/downloading-the-jdk-6-source-code/</link>
      <pubDate>Sat, 11 Feb 2012 10:02:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/11/downloading-the-jdk-6-source-code/</guid>
      <description>Every now and then I want to get the JDK source code onto a new machine and it always seems to take me longer than I expect it to so this post is an attempt to help future me!
Googling for this takes me to this page and I always think I&amp;rsquo;ll just checkout the SVN repository and hook that up but it doesn&amp;rsquo;t seem to be available.
$ wget -S http://java.</description>
    </item>
    
    <item>
      <title>Delivery approach and constraints</title>
      <link>https://mneedham.github.io/2012/02/08/delivery-approach-and-constraints/</link>
      <pubDate>Wed, 08 Feb 2012 22:34:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/08/delivery-approach-and-constraints/</guid>
      <description>In my latest post I described an approach we&amp;rsquo;d been taking when analysing how to rewrite part of an existing system so that we could build the new version in an incremental way.
Towards the end I pointed out that we weren&amp;rsquo;t actually going to be using an incremental approach as we&amp;rsquo;d initially thought which was due to a couple of constraints that we have to work under.
Hardware provisioning One of the main reasons that we favoured an incremental approach is that we&amp;rsquo;d be able to deploy to production early which would allow us to show a quicker return on investment.</description>
    </item>
    
    <item>
      <title>Looking for the seam</title>
      <link>https://mneedham.github.io/2012/02/06/looking-for-the-seam/</link>
      <pubDate>Mon, 06 Feb 2012 22:22:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/06/looking-for-the-seam/</guid>
      <description>During December/early January we spent some time analysing an existing system which we were looking to rewrite and our approach was to look for how we could do this in an incremental way.
In order to do that we needed to look for what Michael Feathers refers to as a seam:
 A seam is a place where you can alter behaviour in your program without editing in that place  On previous times when I&amp;rsquo;ve been thinking about seams it&amp;rsquo;s been at a code level inside a single application but this time there were more than one pieces interacting.</description>
    </item>
    
    <item>
      <title>Scala: Converting a scala collection to java.util.List</title>
      <link>https://mneedham.github.io/2012/02/05/scala-converting-a-scala-collection-to-java-util-list/</link>
      <pubDate>Sun, 05 Feb 2012 21:40:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/02/05/scala-converting-a-scala-collection-to-java-util-list/</guid>
      <description>I&amp;rsquo;ve been playing around a little with Goose - a library for extracting the main body of text from web pages - and I thought I&amp;rsquo;d try converting some of the code to be more scala-esque in style.
The API of the various classes/methods is designed so it&amp;rsquo;s interoperable with Java code but in order to use functions like map/filter we need the collection to be a Scala one.
That&amp;rsquo;s achieved by importing &amp;lsquo;scala.</description>
    </item>
    
    <item>
      <title>Oracle: dbstart - ORACLE_HOME_LISTNER is not SET, unable to auto-start Oracle Net Listener</title>
      <link>https://mneedham.github.io/2012/01/26/oracle-dbstart-oracle_home_listner-is-not-set-unable-to-auto-start-oracle-net-listener/</link>
      <pubDate>Thu, 26 Jan 2012 21:58:27 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/26/oracle-dbstart-oracle_home_listner-is-not-set-unable-to-auto-start-oracle-net-listener/</guid>
      <description>We ran into an interesting problem when trying to start up an Oracle instance using dbstart whereby we were getting the following error:
-bash-3.2$ dbstart ORACLE_HOME_LISTNER is not SET, unable to auto-start Oracle Net Listener Usage: /u01/app/oracle/product/11.2.0/dbhome_1/bin/dbstart ORACLE_HOME Processing Database instance &amp;quot;orcl&amp;quot;: log file /u01/app/oracle/product/11.2.0/dbhome_1/startup.log  Ignoring the usage message we thought that setting the environment variable was what we needed to do, but&amp;hellip;
-bash-3.2$ export ORACLE_HOME_LISTNER=$ORACLE_HOME -bash-3.2$ dbstart ORACLE_HOME_LISTNER is not SET, unable to auto-start Oracle Net Listener Usage: /u01/app/oracle/product/11.</description>
    </item>
    
    <item>
      <title>Developer machine automation: Dependencies</title>
      <link>https://mneedham.github.io/2012/01/24/developer-machine-automation-dependencies/</link>
      <pubDate>Tue, 24 Jan 2012 23:16:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/24/developer-machine-automation-dependencies/</guid>
      <description>As I mentioned in a post last week we&amp;rsquo;ve been automating the setup of our developer machines with puppet over the last week and one thing that we&amp;rsquo;ve learnt is that you need to be careful about how you define dependencies.
The aim is to get your scripts to the point where the outcome is reasonably deterministic so that we can have confidence they&amp;rsquo;re going to work the next we run them.</description>
    </item>
    
    <item>
      <title>Playing around with pomodoros</title>
      <link>https://mneedham.github.io/2012/01/22/playing-around-with-pomodoros/</link>
      <pubDate>Sun, 22 Jan 2012 21:25:19 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/22/playing-around-with-pomodoros/</guid>
      <description>Over the last 3&amp;frasl;4 months I&amp;rsquo;ve been playing around with the idea of using pomodoros to track all coding/software related stuff that I do outside of work.
I originally started using this technique while I was doing the programming assignments for ml-class because I wanted to know how much time I was spending on it each week and make sure I didn&amp;rsquo;t run down rabbit holes too often.
One interesting observation that I noticed from keeping the data of these pomodoros was that while during the early programming assignments it would take me 7 or 8 pomodoros to finish, by the end it was down to around 4.</description>
    </item>
    
    <item>
      <title>Installing Puppet on Oracle Linux</title>
      <link>https://mneedham.github.io/2012/01/18/installing-puppet-on-oracle-linux/</link>
      <pubDate>Wed, 18 Jan 2012 00:30:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/18/installing-puppet-on-oracle-linux/</guid>
      <description>We&amp;rsquo;ve been spending some time trying to setup our developer environment on a Oracle Linux 5.7 build and one of the first steps was to install Puppet as we&amp;rsquo;ve already created scripts which automate the installation of most things.
Unfortunately Oracle Linux builds don&amp;rsquo;t come with any yum repos configured so when you run the following command&amp;hellip;
ls -alh /etc/yum.repos.d/  &amp;hellip;you don&amp;rsquo;t see anything :(
We eventually realised that there are a list of public yum repositories on the Oracle website, of which we needed to download the definition for Oracle Linux 5 like so:</description>
    </item>
    
    <item>
      <title>Application footprint</title>
      <link>https://mneedham.github.io/2012/01/16/application-footprint/</link>
      <pubDate>Mon, 16 Jan 2012 01:40:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/16/application-footprint/</guid>
      <description>I recently came across Carl Erickson&amp;rsquo;s &amp;lsquo;small teams are dramatically more efficient than large teams&amp;rsquo; blog post which reminded me of something which my colleague Ashok suggested as a useful way for determining team size - the application footprint.
As I understand it the application footprint is applicable for an application at a given point in time and determines how many parallel tasks/streams of work we have.
In the case of the project that I&amp;rsquo;m currently working on there are 3 separate components which need to interact with each other via an API but otherwise are independent.</description>
    </item>
    
    <item>
      <title>Focused Retrospectives: things to watch for</title>
      <link>https://mneedham.github.io/2012/01/16/focused-retrospectives-things-to-watch-for/</link>
      <pubDate>Mon, 16 Jan 2012 01:01:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/16/focused-retrospectives-things-to-watch-for/</guid>
      <description>A few weeks ago a slide deck from an Esther Derby presentation on retrospectives was doing the rounds on twitter and one thing that I found interesting in the deck was the suggestion that a retrospective needs to be focused in some way.
I&amp;rsquo;ve participated in a few focused retrospectives over the past 7&amp;frasl;8 months and I think there are some things to be careful about when we decide to focus on something specific rather than just looking back at a time period in general.</description>
    </item>
    
    <item>
      <title>Wireshark: Following HTTP requests/responses</title>
      <link>https://mneedham.github.io/2012/01/14/wireshark-following-http-requestsresponses/</link>
      <pubDate>Sat, 14 Jan 2012 23:20:44 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/14/wireshark-following-http-requestsresponses/</guid>
      <description>I like using Wireshark to have a look at the traffic going across different interfaces but because it shows what&amp;rsquo;s happening across the wire by the packet it&amp;rsquo;s quite difficult to tell what a request/response looked like.
I&amp;rsquo;ve been playing around with restfulie/Vraptor today so I wanted to be able to see the request/response pair when something wasn&amp;rsquo;t working.
I didn&amp;rsquo;t know it was actually possible but this post on StackOverflow describes how.</description>
    </item>
    
    <item>
      <title>Oracle: exp -  EXP-00008: ORACLE error 904 encountered/ORA-00904: &#34;POLTYP&#34;: invalid identifier</title>
      <link>https://mneedham.github.io/2012/01/13/oracle-exp-exp-00008-oracle-error-904-encounteredora-00904-poltyp-invalid-identifier/</link>
      <pubDate>Fri, 13 Jan 2012 21:46:58 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/13/oracle-exp-exp-00008-oracle-error-904-encounteredora-00904-poltyp-invalid-identifier/</guid>
      <description>I spent a bit of time this afternoon trying to export an Oracle test database so that we could use it locally using the exp tool.
I had to connect to exp like this:
exp user/password@remote_address  And then filled in the other parameters interactively.
Unfortunately when I tried to actually export the specified tables I got the following error message:
EXP-00008: ORACLE error 904 encountered ORA-00904: &amp;quot;POLTYP&amp;quot;: invalid identifier EXP-00000: Export terminated unsuccessfully  I eventually came across Oyvind Isene&amp;rsquo;s blog post which pointed out that you&amp;rsquo;d get this problem if you tried to export a 10g database using an 11g client which is exactly what I was trying to do!</description>
    </item>
    
    <item>
      <title>Learning Android: Roboguice - Injecting context into PreferenceManager</title>
      <link>https://mneedham.github.io/2012/01/12/learning-android-roboguice-injecting-context-into-preferencemanager/</link>
      <pubDate>Thu, 12 Jan 2012 17:24:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/12/learning-android-roboguice-injecting-context-into-preferencemanager/</guid>
      <description>In my last post I showed how I&amp;rsquo;d been able to write a test around saved preferences in my app by making use of a ShadowPreferenceManager but it seemed a bit hacky.
I didn&amp;rsquo;t want to have to do that for every test where I dealt with preferences - I thought it&amp;rsquo;d be better if I could wrap the preferences in an object of my own and then inject it where necessary.</description>
    </item>
    
    <item>
      <title>Learning Android: Robolectric - Testing details got saved to SharedPreferences</title>
      <link>https://mneedham.github.io/2012/01/10/learning-android-testing-details-got-saved-to-sharedpreferences/</link>
      <pubDate>Tue, 10 Jan 2012 09:53:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/10/learning-android-testing-details-got-saved-to-sharedpreferences/</guid>
      <description>I&amp;rsquo;ve been writing some tests around an app I&amp;rsquo;ve been working on using the Robolectric testing framework and one thing I wanted to do was check that an OAuth token/secret were being saved to the user&amp;rsquo;s preferences.
The code that saved the preferences looked like this:
public class AuthoriseWithTwitterActivity extends RoboActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(intent); ... save(&amp;quot;fakeToken&amp;quot;, &amp;quot;fakeSecret&amp;quot;); ... } private void save(String userKey, String userSecret) { SharedPreferences settings = PreferenceManager.</description>
    </item>
    
    <item>
      <title>Learning Android: Getting android-support jar/compatability package as a Maven dependency</title>
      <link>https://mneedham.github.io/2012/01/08/learning-android-getting-android-support-jarcompatability-package-as-a-maven-dependency/</link>
      <pubDate>Sun, 08 Jan 2012 20:56:45 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/08/learning-android-getting-android-support-jarcompatability-package-as-a-maven-dependency/</guid>
      <description>In the app I&amp;rsquo;m working on I make use of the ViewPager class which is only available in the compatibility package from revisions 3 upwards.
Initially I followed the instructions on the developer guide to get hold of the jar but now that I&amp;rsquo;m trying to adapt my code to fit the RobolectricSample, as I mentioned in my previous post, I needed to hook it up as a Maven dependency.</description>
    </item>
    
    <item>
      <title>Learning Android: java.lang.OutOfMemoryError: Java heap space with android-maven-plugin</title>
      <link>https://mneedham.github.io/2012/01/07/learning-android-java-lang-outofmemoryerror-java-heap-space-with-android-maven-plugin/</link>
      <pubDate>Sat, 07 Jan 2012 17:14:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/07/learning-android-java-lang-outofmemoryerror-java-heap-space-with-android-maven-plugin/</guid>
      <description>I&amp;rsquo;ve been trying to adapt my Android application to fit into the structure of the RobolectricSample so that I can add some tests around my code but I was running into a problem when trying to deploy the application.
To deploy the application you need to run the following command:
mvn package android:deploy  Which was resulting in the following error:
[INFO] UNEXPECTED TOP-LEVEL ERROR: [INFO] java.lang.OutOfMemoryError: Java heap space [INFO] at com.</description>
    </item>
    
    <item>
      <title>Learning Android: Freezing the UI with a BroadcastReceiver</title>
      <link>https://mneedham.github.io/2012/01/06/learning-android-freezing-the-ui-with-a-broadcastreceiver/</link>
      <pubDate>Fri, 06 Jan 2012 23:40:53 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/06/learning-android-freezing-the-ui-with-a-broadcastreceiver/</guid>
      <description>As I mentioned in a previous post I recently wrote some code in my Android app to inform a BroadcastReceiver whenever a service processed a tweet with a link in it but in implementing this I managed to freeze the UI every time that happened.
I made the stupid (in hindsight) mistake of not realising that I shouldn&amp;rsquo;t be doing a lot of logic in BroadcastReceiver.onReceive since that bit of code gets executed on the UI thread.</description>
    </item>
    
    <item>
      <title>Learning Android: Getting a service to communicate with an activity</title>
      <link>https://mneedham.github.io/2012/01/05/learning-android-getting-a-service-to-communicate-with-an-activity/</link>
      <pubDate>Thu, 05 Jan 2012 01:41:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/05/learning-android-getting-a-service-to-communicate-with-an-activity/</guid>
      <description>In the app I&amp;rsquo;m working on I created a service which runs in the background away from the main UI thread consuming the Twitter streaming API using twitter4j.
It looks like this:
public class TweetService extends IntentService { String consumerKey = &amp;quot;TwitterConsumerKey&amp;quot;; String consumerSecret = &amp;quot;TwitterConsumerSecret&amp;quot;; public TweetService() { super(&amp;quot;Tweet Service&amp;quot;); } @Override protected void onHandleIntent(Intent intent) { AccessToken accessToken = createAccessToken(); StatusListener listener = new UserStreamListener() { // override a whole load of methods - removed for brevity public void onStatus(Status status) { String theTweet = status.</description>
    </item>
    
    <item>
      <title>My Software Development journey: 2011</title>
      <link>https://mneedham.github.io/2012/01/03/my-software-development-journey-2011/</link>
      <pubDate>Tue, 03 Jan 2012 01:48:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/03/my-software-development-journey-2011/</guid>
      <description>A couple of years ago I used to write a blog post reflecting on what I&amp;rsquo;d worked on in the preceding year and what I&amp;rsquo;d learned and having read 2011 reviews by a couple of other people I thought I&amp;rsquo;d have a go.
Am I actually learning anything? A thought I had many times in 2011 was &#39;am I actually learning anything?&#39; as, although I was working with languages that I hadn&#39;t used professionally before, the applications that we I worked on were very similar to ones that I&#39;ve worked on previously.</description>
    </item>
    
    <item>
      <title>Learning Android: Authenticating with Twitter using OAuth</title>
      <link>https://mneedham.github.io/2012/01/02/learning-android-authenticating-with-twitter-using-oauth/</link>
      <pubDate>Mon, 02 Jan 2012 02:39:52 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/02/learning-android-authenticating-with-twitter-using-oauth/</guid>
      <description>I want to be able to get the tweets from my timeline into my app which means I need to authorise the app with Twitter using OAuth.
The last time I tried to authenticate using OAuth a couple of years ago was a bit of a failure but luckily this time Honza Pokorny has written a blog post explaining what to do.
I had to adjust the code a little bit from what&amp;rsquo;s written on his post so I thought I&amp;rsquo;d document what I&amp;rsquo;ve done.</description>
    </item>
    
    <item>
      <title>Learning Android: &#39;Unable to start service Intent not found&#39;</title>
      <link>https://mneedham.github.io/2012/01/01/learning-android-unable-to-start-service-intent-not-found/</link>
      <pubDate>Sun, 01 Jan 2012 03:22:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/01/01/learning-android-unable-to-start-service-intent-not-found/</guid>
      <description>In the Android application that I&amp;rsquo;ve been playing around with I wrote a service which consumes the Twitter streaming API which I trigger from the app&amp;rsquo;s main activity like so:
public class MyActivity extends Activity { ... @Override public void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); Intent intent = new Intent(this, TweetService.class); startService(intent); ... } }  Where TweetService is defined roughly like this:
public class TweetService extends IntentService { @Override protected void onHandleIntent(Intent intent) { // Twitter streaming API stuff goes here } }  Unfortunately when I tried to deploy the app the service wasn&amp;rsquo;t starting and I got this message in the log:</description>
    </item>
    
  </channel>
</rss>