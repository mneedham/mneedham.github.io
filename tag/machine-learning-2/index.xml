<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning 2 on Mark Needham</title>
    <link>https://mneedham.github.io/tag/machine-learning-2/</link>
    <description>Recent content in Machine Learning 2 on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 Aug 2016 20:32:09 +0000</lastBuildDate>
    
	<atom:link href="https://mneedham.github.io/tag/machine-learning-2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://mneedham.github.io/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:  Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://mneedham.github.io/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>A couple of months ago Praveena and I created a Game of Thrones dataset to use in a workshop and I thought it&#39;d be fun to run it through some machine learning algorithms and hopefully find some interesting insights.  The dataset is available as CSV files but for this analysis I&#39;m assuming that it&#39;s already been imported into neo4j. If you want to import the data you can run the tutorial by typing the following into the query bar of the neo4j browser: :play http://guides.</description>
    </item>
    
    <item>
      <title>Mahout: Exception in thread &#34;main&#34; java.lang.IllegalArgumentException: Wrong FS: file:/... expected: hdfs://</title>
      <link>https://mneedham.github.io/2016/07/21/mahout-exception-in-thread-main-java-lang-illegalargumentexception-wrong-fs-file-expected-hdfs/</link>
      <pubDate>Thu, 21 Jul 2016 17:57:41 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/07/21/mahout-exception-in-thread-main-java-lang-illegalargumentexception-wrong-fs-file-expected-hdfs/</guid>
      <description>I&#39;ve been playing around with Mahout over the last couple of days to see how well it works for content based filtering.
I started following a mini tutorial from Stack Overflow but ran into trouble on the first step: bin/mahout seqdirectory \ --input file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo \ --output file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo-out \ -c UTF-8 \ -chunk 64 \ -prefix mah  16/07/21 21:19:20 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo], --keyPrefix=[mah], --method=[mapreduce], --output=[file:///Users/markneedham/Downloads/apache-mahout-distribution-0.</description>
    </item>
    
    <item>
      <title>How I met your mother: Story arcs</title>
      <link>https://mneedham.github.io/2015/04/03/how-i-met-your-mother-story-arcs/</link>
      <pubDate>Fri, 03 Apr 2015 23:31:33 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/04/03/how-i-met-your-mother-story-arcs/</guid>
      <description>After weeks of playing around with various algorithms to extract story arcs in How I met your mother I&#39;ve come to the conclusion that I don&#39;t yet have the skills to completely automate this process so I&#39;m going to change my approach.  The new plan is to treat the outputs of the algorithms as suggestions for possible themes but then have a manual step where I extract what I think are interesting themes in the series.</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>https://mneedham.github.io/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>Nathan and I have been looking at Kaggle&#39;s Titanic problem and while working through the Python tutorial Nathan pointed out that we could greatly simplify the code if we used pandas instead.
The problem we had with numpy is that you use integers to reference columns. We spent a lot of time being thoroughly confused as to why something wasn&#39;t working only to realise we were using the wrong column.</description>
    </item>
    
    <item>
      <title>Feature Extraction/Selection - What I&#39;ve learnt so far</title>
      <link>https://mneedham.github.io/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</link>
      <pubDate>Sun, 10 Feb 2013 15:42:07 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</guid>
      <description>A couple of weeks ago I wrote about some feature extraction work that I&#39;d done on the Kaggle Digit Recognizer data set and having realised that I had no idea what I was doing I thought I should probably learn a bit more.
I came across Dunja Mladenic&#39;s &#39;Dimensionality Reduction by Feature Selection in Machine Learning&#39; presentation in which she sweeps across the landscape of feature selection and explains how everything fits together.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>https://mneedham.github.io/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I&amp;rsquo;ve written a few blog posts about our attempts at the Kaggle Digit Recogniser problem and one thing we haven&amp;rsquo;t yet tried is feature extraction.
Feature extraction in this context means that we&#39;d generate some other features to train a classifier with rather than relying on just the pixel values we were provided.
Every week Jen would try and persuade me that we should try it out but it wasn&#39;t until I was flicking through the notes from the Columbia Data Science class that it struck home:</description>
    </item>
    
    <item>
      <title>Mahout: Parallelising the creation of DecisionTrees</title>
      <link>https://mneedham.github.io/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</link>
      <pubDate>Thu, 27 Dec 2012 00:08:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</guid>
      <description>A couple of months ago I wrote a blog post describing our use of Mahout random forests for the Kaggle Digit Recogniser Problem and after seeing how long it took to create forests with 500+ trees I wanted to see if this could be sped up by parallelising the process.
From looking at the DecisionTree it seemed like it should be possible to create lots of small forests and then combine them together.</description>
    </item>
    
    <item>
      <title>Weka: Saving and loading classifiers</title>
      <link>https://mneedham.github.io/2012/12/12/weka-saving-and-loading-classifiers/</link>
      <pubDate>Wed, 12 Dec 2012 00:04:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/12/12/weka-saving-and-loading-classifiers/</guid>
      <description>In our continued machine learning travels Jen and I have been building some classifiers using Weka and one thing we wanted to do was save the classifier and then reuse it later.
There is documentation for how to do this from the command line but we&#39;re doing everything programatically and wanted to be able to save our classifiers from Java code.
As it turns out it&#39;s not too tricky when you know which classes to call and saving a classifier to a file is as simple as this:</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://mneedham.github.io/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle&amp;rsquo;s Digit Recognizer Jen and I decided to try out boosting on our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
We couldn&amp;rsquo;t find any documentation that it was possible to apply boosting to Mahout&amp;rsquo;s random forest algorithm but we knew it was possible with Weka so we decided to use that instead!
As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it&amp;rsquo;s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>A first failed attempt at Natural Language Processing</title>
      <link>https://mneedham.github.io/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</link>
      <pubDate>Sat, 24 Nov 2012 19:43:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</guid>
      <description>One of the things I find fascinating about dating websites is that the profiles of people are almost identical so I thought it would be an interesting exercise to grab some of the free text that people write about themselves and prove the similarity.
I&amp;rsquo;d been talking to Matt Biddulph about some Natural Language Processing (NLP) stuff he&amp;rsquo;d been working on and he wrote up a bunch of libraries, articles and books that he&amp;rsquo;d found useful.</description>
    </item>
    
    <item>
      <title>Mahout: Using a saved Random Forest/DecisionTree</title>
      <link>https://mneedham.github.io/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</link>
      <pubDate>Sat, 27 Oct 2012 22:03:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</guid>
      <description>One of the things that I wanted to do while playing around with random forests using Mahout was to save the random forest and then use use it again which is something Mahout does cater for.
It was actually much easier to do this than I&amp;rsquo;d expected and assuming that we already have a DecisionForest built we&amp;rsquo;d just need the following code to save it to disc:
int numberOfTrees = 1; Data data = loadData(.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Mahout Random Forest attempt</title>
      <link>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 20:24:48 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</guid>
      <description>I&amp;rsquo;ve written previously about the K-means approach that Jen and I took when trying to solve Kaggle&amp;rsquo;s Digit Recognizer and having stalled at about 80% accuracy we decided to try one of the algorithms suggested in the tutorials section - the random forest!
We initially used a clojure random forests library but struggled to build the random forest from the training set data in a reasonable amount of time so we switched to Mahout&amp;rsquo;s version which is based on Leo Breiman&amp;rsquo;s random forests paper.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jen and I used the K-means algorithm to classify digits in Kaggle&amp;rsquo;s Digit Recognizer problem and one of the things we&amp;rsquo;d read was that with this algorithm you often end up with situations where it&amp;rsquo;s difficult to classify a new item because if falls between two labels.
We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A K-means attempt</title>
      <link>https://mneedham.github.io/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</link>
      <pubDate>Tue, 23 Oct 2012 19:04:20 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</guid>
      <description>Over the past couple of months Jen and I have been playing around with the Kaggle Digit Recognizer problem - a &amp;lsquo;competition&amp;rsquo; created to introduce people to Machine Learning.
 The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.  You are given an input file which contains multiple rows each containing 784 pixel values representing a 28x28 pixel image as well as a label indicating which number that image actually represents.</description>
    </item>
    
  </channel>
</rss>