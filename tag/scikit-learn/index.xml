<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scikit Learn on Mark Needham</title>
    <link>https://mneedham.github.io/tag/scikit-learn/</link>
    <description>Recent content in Scikit Learn on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Dec 2017 07:55:43 +0000</lastBuildDate>
    
	<atom:link href="https://mneedham.github.io/tag/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>scikit-learn: Using GridSearch to tune the hyper-parameters of VotingClassifier</title>
      <link>https://mneedham.github.io/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</link>
      <pubDate>Sun, 10 Dec 2017 07:55:43 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</guid>
      <description>In my last blog post I showed how to create a multi class classification ensemble using scikit-learn&#39;s VotingClassifier and finished mentioning that I didn&#39;t know which classifiers should be part of the ensemble.  We need to get a better score with each of the classifiers in the ensemble otherwise they can be excluded.  We have a TF/IDF based classifier as well as well as the classifiers I wrote about in the last post.</description>
    </item>
    
    <item>
      <title>scikit-learn: Building a multi class classification ensemble</title>
      <link>https://mneedham.github.io/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</link>
      <pubDate>Tue, 05 Dec 2017 22:19:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</guid>
      <description>For the Kaggle Spooky Author Identification I wanted to combine multiple classifiers together into an ensemble and found the VotingClassifier that does exactly that.  We need to predict the probability that a sentence is written by one of three authors so the VotingClassifier needs to make a &#39;soft&#39; prediction. If we only needed to know the most likely author we could have it make a &#39;hard&#39; prediction instead.  We start with three classifiers which generate different n-gram based features.</description>
    </item>
    
    <item>
      <title>Python: Learning about defaultdict&#39;s handling of missing keys</title>
      <link>https://mneedham.github.io/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</link>
      <pubDate>Fri, 01 Dec 2017 15:26:36 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</guid>
      <description>While reading the scikit-learn code I came across a bit of code that I didn&#39;t understand for a while but in retrospect is quite neat.  This is the code snippet that intrigued me: vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__   Let&#39;s quickly see how it works by adapting an example from scikit-learn: &amp;gt;&amp;gt;&amp;gt; from collections import defaultdict &amp;gt;&amp;gt;&amp;gt; vocabulary = defaultdict() &amp;gt;&amp;gt;&amp;gt; vocabulary.default_factory = vocabulary.__len__ &amp;gt;&amp;gt;&amp;gt; vocabulary[&amp;quot;foo&amp;quot;] 0 &amp;gt;&amp;gt;&amp;gt; vocabulary.</description>
    </item>
    
    <item>
      <title>scikit-learn: Creating a matrix of named entity counts</title>
      <link>https://mneedham.github.io/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</link>
      <pubDate>Wed, 29 Nov 2017 23:01:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</guid>
      <description>I&#39;ve been trying to improve my score on Kaggle&#39;s Spooky Author Identification competition, and my latest idea was building a model which used named entities extracted using the polyglot NLP library.  We&#39;ll start by learning how to extract entities form a sentence using polyglot which isn&#39;t too tricky: &amp;gt;&amp;gt;&amp;gt; from polyglot.text import Text &amp;gt;&amp;gt;&amp;gt; doc = &amp;quot;My name is David Beckham. Hello from London, England&amp;quot; &amp;gt;&amp;gt;&amp;gt; Text(doc, hint_language_code=&amp;quot;en&amp;quot;).entities [I-PER([&#39;David&#39;, &#39;Beckham&#39;]), I-LOC([&#39;London&#39;]), I-LOC([&#39;England&#39;])]   This sentence contains three entities.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://mneedham.github.io/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, I&#39;ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.  Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant I&#39;d overfitted the model.  I wasn&#39;t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albon&#39;s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>scikit-learn: First steps with log_loss</title>
      <link>https://mneedham.github.io/2016/09/14/scikit-learn-first-steps-with-log_loss/</link>
      <pubDate>Wed, 14 Sep 2016 05:33:38 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/09/14/scikit-learn-first-steps-with-log_loss/</guid>
      <description>Over the last week I&#39;ve spent a little bit of time playing around with the data in the Kaggle TalkingData Mobile User Demographics competition, and came across a notebook written by dune_dweller showing how to run a logistic regression algorithm on the dataset.  The metric used to evaluate the output in this competition is multi class logarithmic loss, which is implemented by the log_loss function in the scikit-learn library.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://mneedham.github.io/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:  Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).</description>
    </item>
    
    <item>
      <title>scikit-learn: Trying to find clusters of Game of Thrones episodes</title>
      <link>https://mneedham.github.io/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</link>
      <pubDate>Thu, 25 Aug 2016 22:07:25 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</guid>
      <description>In my last post I showed how to find similar Game of Thrones episodes based on the characters that appear in different episodes. This allowed us to find similar episodes on an episode by episode basis, but I was curious whether there were groups of similar episodes that we could identify. scikit-learn provides several clustering algorithms that can run over our episode vectors and hopefully find clusters of similar episodes.</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Detecting which sentences in a transcript contain a speaker</title>
      <link>https://mneedham.github.io/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</link>
      <pubDate>Fri, 20 Feb 2015 22:42:59 +0000</pubDate>
      
      <guid>https://mneedham.github.io/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</guid>
      <description>Over the past couple of months I&#39;ve been playing around with How I met your mother transcripts and the most recent thing I&#39;ve been working on is how to extract the speaker for a particular sentence. This initially seemed like a really simple problem as most of the initial sentences I looked at weere structured like this: &amp;lt;speaker&amp;gt;: &amp;lt;sentence&amp;gt;   If there were all in that format then we could write a simple regular expression and then move on but unfortunately they aren&#39;t.</description>
    </item>
    
  </channel>
</rss>