<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2018s on Mark Needham</title>
    <link>https://markhneedham.com/blog/2018/</link>
    <description>Recent content in 2018s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Dec 2018 21:09:00 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/2018/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python: Pandas - DataFrame plotting ignoring figure</title>
      <link>https://markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</link>
      <pubDate>Tue, 25 Dec 2018 21:09:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</guid>
      <description>In my continued use of matplotlib I wanted to change the size of the chart I was plotting and struggled a bit to start with. We&amp;#8217;ll use the same DataFrame as before:
 df = pd.DataFrame({ &#34;name&#34;: [&#34;Mark&#34;, &#34;Arya&#34;, &#34;Praveena&#34;], &#34;age&#34;: [34, 1, 31] }) df   In my last blog post I showed how we can create a bar chart by executing the following code:
 df.plot.bar(x=&#34;name&#34;) plt.tight_layout() plt.</description>
    </item>
    
    <item>
      <title>Neo4j: Pruning transaction logs more aggressively</title>
      <link>https://markhneedham.com/blog/2018/12/24/neo4j-prune-transaction-logs-more-aggressively/</link>
      <pubDate>Mon, 24 Dec 2018 21:09:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/12/24/neo4j-prune-transaction-logs-more-aggressively/</guid>
      <description>One thing that new users of Neo4j when playing around with it locally is how much space the transaction logs can take up, especially when we&amp;#8217;re creating and deleting lots of data while we get started. We can see this by running the following query a few times:
 UNWIND range(0, 1000) AS id CREATE (:Foo {id: id}); MATCH (f:Foo) DELETE f   This query creates a bunch of data before immediately deleting it.</description>
    </item>
    
    <item>
      <title>Pandas: Create matplotlib plot with x-axis label not index</title>
      <link>https://markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</link>
      <pubDate>Fri, 21 Dec 2018 16:57:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</guid>
      <description>I&amp;#8217;ve been using matplotlib a bit recently, and wanted to share a lesson I learnt about choosing the label of the x-axis. Let&amp;#8217;s first import the libraries we&amp;#8217;ll use in this post:
 import pandas as pd import matplotlib.pyplot as plt   And now we&amp;#8217;ll create a DataFrame of values that we want to chart:
 df = pd.DataFrame({ &#34;name&#34;: [&#34;Mark&#34;, &#34;Arya&#34;, &#34;Praveena&#34;], &#34;age&#34;: [34, 1, 31] }) df   This is what our DataFrame looks like:</description>
    </item>
    
    <item>
      <title>PySpark: Creating DataFrame with one column - TypeError: Can not infer schema for type: &lt;type &#39;int&#39;&gt;</title>
      <link>https://markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</link>
      <pubDate>Sun, 09 Dec 2018 10:25:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</guid>
      <description>I&amp;#8217;ve been playing with PySpark recently, and wanted to create a DataFrame containing only one column. I tried to do this by writing the following code:
 spark.createDataFrame([(1)], [&#34;count&#34;])   If we run that code we&amp;#8217;ll get the following error message:
 Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; File &#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&#34;, line 748, in createDataFrame rdd, schema = self._createFromLocal(map(prepare, data), schema) File &#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&#34;, line 416, in _createFromLocal struct = self.</description>
    </item>
    
    <item>
      <title>Neo4j: Storing inferred relationships with APOC triggers</title>
      <link>https://markhneedham.com/blog/2018/11/05/neo4j-inferred-relationships-apoc-triggers/</link>
      <pubDate>Mon, 05 Nov 2018 06:15:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/11/05/neo4j-inferred-relationships-apoc-triggers/</guid>
      <description>One of my favourite things about modelling data in graphs is how easy it makes it to infer relationships between pieces of data based on other relationships. In this post we&amp;#8217;re going to learn how to compute and store those inferred relationships using the triggers feature from the APOC library.
 Meetup Graph Before we get to that, let&amp;#8217;s first understand what we mean when we say inferred relationship. We&amp;#8217;ll create a small graph containing Person, Meetup, and Topic nodes with the following query:</description>
    </item>
    
    <item>
      <title>Neo4j Graph Algorithms: Visualising Projected Graphs</title>
      <link>https://markhneedham.com/blog/2018/10/31/neo4j-graph-algorithms-visualise-projected-graph/</link>
      <pubDate>Wed, 31 Oct 2018 18:12:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/10/31/neo4j-graph-algorithms-visualise-projected-graph/</guid>
      <description>A few weeks ago I wrote a blog post showing how to work out the best tennis player of all time using the Weighted PageRank algorithm, and in the process created a projected credibility graph which I want to explore in more detail in this post.
 As I pointed out in that post, sometimes the graph model doesn&amp;#8217;t fit well with what the algorithm expects, so we need to project the graph on which we run graph algorithms.</description>
    </item>
    
    <item>
      <title>Neo4j Graph Algorithms: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://markhneedham.com/blog/2018/09/28/neo4j-graph-algorithms-cosine-game-of-thrones/</link>
      <pubDate>Fri, 28 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/09/28/neo4j-graph-algorithms-cosine-game-of-thrones/</guid>
      <description>A couple of years ago I wrote a blog post showing how to calculate cosine similarity on Game of Thrones episodes using scikit-learn, and with the release of Similarity Algorithms in the Neo4j Graph Algorithms library I thought it was a good time to revisit that post.
 The dataset contains characters and episodes, and we want to calculate episode similarity based on the characters that appear in each episode. Before we run any algorithms we need to get the data into Neo4j.</description>
    </item>
    
    <item>
      <title>matplotlib - Create a histogram/bar chart for ratings/full numbers</title>
      <link>https://markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</link>
      <pubDate>Mon, 24 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</guid>
      <description>In my continued work with matplotlib I wanted to plot a histogram (or bar chart) for a bunch of star ratings to see how they were distributed.
 Before we do anything let&amp;#8217;s import matplotlib as well as pandas:
 import random import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.pyplot as plt plt.style.use(&#39;fivethirtyeight&#39;)   Next we&amp;#8217;ll create an array of randomly chosen star ratings between 1 and 5:</description>
    </item>
    
    <item>
      <title>matplotlib - MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.</title>
      <link>https://markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</link>
      <pubDate>Tue, 18 Sep 2018 07:56:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</guid>
      <description>In my last post I showed how to remove axes legends from a matplotlib chart, and while writing the post I actually had the change the code I used as my initial approach is now deprecated.
 As in the previous post, we&amp;#8217;ll first import pandas and matplotlib:
 import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.pyplot as plt plt.style.use(&#39;fivethirtyeight&#39;)   And we&amp;#8217;ll still use this DataFrame:
 df = pd.</description>
    </item>
    
    <item>
      <title>matplotlib - Remove axis legend</title>
      <link>https://markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</link>
      <pubDate>Tue, 18 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</guid>
      <description>I&amp;#8217;ve been working with matplotlib a bit recently, and I wanted to remove all axis legends from my chart. It took me a bit longer than I expected to figure it out so I thought I&amp;#8217;d write it up.
 Before we do anything let&amp;#8217;s import matplotlib as well as pandas, since we&amp;#8217;re going to plot data from a pandas DataFrame.
 import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.</description>
    </item>
    
    <item>
      <title>Neo4j: Using LOAD CSV to process csv.gz files from S3</title>
      <link>https://markhneedham.com/blog/2018/09/05/neo4j-load-csv-gz-s3/</link>
      <pubDate>Wed, 05 Sep 2018 07:26:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/09/05/neo4j-load-csv-gz-s3/</guid>
      <description>I&amp;#8217;ve been building some training material for the GraphConnect conference that happens in a couple of weeks time and I wanted to load gzipped CSV files. I got this working using Cypher&amp;#8217;s LOAD CSV command with the file stored locally, but when I uploaded it to S3 it didn&amp;#8217;t work as I expected.
 I uploaded the file to an S3 bucket and then tried to read it back like this:</description>
    </item>
    
    <item>
      <title>QuickGraph: Analysing Python Dependency Graph with PageRank, Closeness Centrality, and Betweenness Centrality</title>
      <link>https://markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</link>
      <pubDate>Mon, 16 Jul 2018 05:25:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</guid>
      <description>I&amp;#8217;ve always wanted to build a dependency graph of libraries in the Python ecosytem but I never quite got around to it&amp;#8230;&amp;#8203;until now! I thought I might be able to get a dump of all the libraries and their dependencies, but while searching I came across this article which does a good job of explaining why that&amp;#8217;s not possible.
 Finding Python Dependencies The best we can do is generate a dependency graph of our locally installed packages using the excellent pipdeptree tool.</description>
    </item>
    
    <item>
      <title>Python: Parallel download files using requests</title>
      <link>https://markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</link>
      <pubDate>Sun, 15 Jul 2018 15:10:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</guid>
      <description>I often find myself downloading web pages with Python&amp;#8217;s requests library to do some local scrapping when building datasets but I&amp;#8217;ve never come up with a good way for downloading those pages in parallel.
 Below is the code that I use. First we&amp;#8217;ll import the required libraries:
 import os import requests from time import time as timer   And now a function that streams a response into a local file:</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Grouping Datetimes</title>
      <link>https://markhneedham.com/blog/2018/07/10/neo4j-grouping-datetimes/</link>
      <pubDate>Tue, 10 Jul 2018 04:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/07/10/neo4j-grouping-datetimes/</guid>
      <description>In my continued analysis of Strava runs I wanted to try and find my best runs grouped by different time components, which was actually much easier than I was expecting.
 Importing the dataset If you want to try out the examples below you can execute the following LOAD CSV commands to load the data:
 LOAD CSV WITH HEADERS FROM &#34;https://github.com/mneedham/strava/raw/master/runs.csv&#34; AS row MERGE (run:Run {id: toInteger(row.id)}) SET run.distance = toFloat(row.</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Syntax Error - Text cannot be parsed to a Duration (aka dealing with empty durations)</title>
      <link>https://markhneedham.com/blog/2018/07/09/neo4j-text-cannot-be-parsed-to-duration/</link>
      <pubDate>Mon, 09 Jul 2018 18:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/07/09/neo4j-text-cannot-be-parsed-to-duration/</guid>
      <description>As I continued with my travels with Neo4j 3.4&amp;#8217;s temporal data type I came across some fun edge cases when dealing with empty durations while importing data.
 Imagine we&amp;#8217;re trying to create 3 nodes from the following array of input data. Two of the rows have invalid durations!
 UNWIND [ {id: 12345, duration: &#34;PT2M20S&#34;}, {id: 12346, duration: &#34;&#34;}, {id: 12347, duration: null} ] AS row MERGE (run:Run {id: row.</description>
    </item>
    
    <item>
      <title>Neo4j: Querying the Strava Graph using Py2neo</title>
      <link>https://markhneedham.com/blog/2018/06/15/neo4j-querying-strava-graph-py2neo/</link>
      <pubDate>Fri, 15 Jun 2018 13:45:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/15/neo4j-querying-strava-graph-py2neo/</guid>
      <description>Last week Nigel released v4 of Py2neo and given I was just getting ready to write some queries against my Strava activity graph I thought I&amp;#8217;d give it a try.
 If you want to learn how to create your own Strava graph you should read my previous post, but just to recap, this is the graph model that we created:
   Let&amp;#8217;s get to it!
 tl;dr the code in this post is available as a Jupyter notebook so if you want the code and nothing but the code head over there!</description>
    </item>
    
    <item>
      <title>Neo4j: Building a graph of Strava activities</title>
      <link>https://markhneedham.com/blog/2018/06/12/neo4j-building-strava-graph/</link>
      <pubDate>Tue, 12 Jun 2018 05:30:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/12/neo4j-building-strava-graph/</guid>
      <description>In my last post I showed how to import activities from Strava&amp;#8217;s API into Neo4j using only the APOC library, but that was only part of the graph so I thought I&amp;#8217;d share the rest of what I&amp;#8217;ve done.
 The Graph Model In the previous post I showed how to import nodes with Run label, but there are some other pieces of data that I wanted to import as well.</description>
    </item>
    
    <item>
      <title>Neo4j APOC: Importing data from Strava&#39;s paginated JSON API</title>
      <link>https://markhneedham.com/blog/2018/06/05/neo4j-apoc-loading-data-strava-paginated-json-api/</link>
      <pubDate>Tue, 05 Jun 2018 05:30:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/05/neo4j-apoc-loading-data-strava-paginated-json-api/</guid>
      <description>Over the weekend I&amp;#8217;ve been playing around with loading data from the Strava API into Neo4j and I started with the following Python script which creates a node with a Run label for each of my activities.
 If you want to follow along on your own data you&amp;#8217;ll need to get an API key via the &#39;My API Application&#39; section of the website. Once you&amp;#8217;ve got that put it in the TOKEN environment variable and you should be good to go.</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Gotchas when working with Durations</title>
      <link>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-gotchas-working-with-durations/</link>
      <pubDate>Sun, 03 Jun 2018 20:11:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-gotchas-working-with-durations/</guid>
      <description>Continuing with my explorations of Strava data in Neo4j I wanted to share some things I learnt while trying to work out my pace for certain distances.
 Before we get into the pace calculations let&amp;#8217;s first understand how the duration function works. If we run the following query we might expect to get back the same value that we put in&amp;#8230;&amp;#8203;
 RETURN duration({seconds: 413.77}).seconds AS seconds   ╒═════════╕ │&#34;</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Formatting instances of the Duration and Datetime date types</title>
      <link>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-formatting-instances-durations-dates/</link>
      <pubDate>Sun, 03 Jun 2018 04:08:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-formatting-instances-durations-dates/</guid>
      <description>In my last blog post I showed how to compare instances of Neo4j&amp;#8217;s Duration data type, and in the middle of the post I realised that I needed to use the APOC library to return the value in the format I wanted. This was the solution I ended up with:
 WITH duration({seconds: 100}) AS duration RETURN apoc.text.lpad(toString(duration.minutes), 2, &#34;0&#34;) + &#34;:&#34; + apoc.text.lpad(toString(duration.secondsOfMinute), 2, &#34;0&#34;)   If we run that query this is the output:</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Comparing durations</title>
      <link>https://markhneedham.com/blog/2018/06/02/neo4j-3.4-comparing-durations/</link>
      <pubDate>Sat, 02 Jun 2018 03:24:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/02/neo4j-3.4-comparing-durations/</guid>
      <description>Neo4j 3.4 saw the introduction of the temporal date type, which my colleague Adam Cowley covered in his excellent blog post, and in this post I want to share my experience using durations from my Strava runs.
 I&amp;#8217;ll show how to load the whole Strava dataset in another blog post but for now we&amp;#8217;ll just manually create some durations based on the elapsed time in seconds that Strava provides. We can run the following query to convert duration in seconds into the duration type:</description>
    </item>
    
    <item>
      <title>Interpreting Word2vec or GloVe embeddings using scikit-learn and Neo4j graph algorithms</title>
      <link>https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</link>
      <pubDate>Sat, 19 May 2018 09:47:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</guid>
      <description>A couple of weeks I came across a paper titled Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings via Abigail See&#39;s blog post about ACL 2017.
  The paper explains an algorithm that helps to make sense of word embeddings generated by algorithms such as Word2vec and GloVe.
 I&amp;#8217;m fascinated by how graphs can be used to interpret seemingly black box data, so I was immediately intrigued and wanted to try and reproduce their findings using Neo4j.</description>
    </item>
    
    <item>
      <title>Predicting movie genres with node2Vec and Tensorflow</title>
      <link>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</link>
      <pubDate>Fri, 11 May 2018 08:12:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</guid>
      <description>In my previous post we looked at how to get up and running with the node2Vec algorithm, and in this post we&amp;#8217;ll learn how we can feed graph embeddings into a simple Tensorflow model.
 Recall that node2Vec takes in a list of edges (or relationships) and gives us back an embedding (array of numbers) for each node.
 This time we&amp;#8217;re going to run the algorithm over a movies recommendation dataset from the Neo4j Sandbox.</description>
    </item>
    
    <item>
      <title>Exploring node2vec - a graph embedding algorithm</title>
      <link>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</link>
      <pubDate>Fri, 11 May 2018 08:08:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</guid>
      <description>In my explorations of graph based machine learning, one algorithm I came across is called node2Vec. The paper describes it as &#34;an algorithmic framework for learning continuous feature representations for nodes in networks&#34;.
 So what does the algorithm do? From the website:
  The node2vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective. The objective is flexible, and the algorithm accommodates for various definitions of network neighborhoods by simulating biased random walks.</description>
    </item>
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week I&amp;#8217;ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiri&amp;#8217;s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
 I&amp;#8217;ll use the same data as Sidath, so if you want to follow along you&amp;#8217;ll need to download these files:
   iris_training.csv
  iris_test.csv
   Loading data The way we load data will remain exactly the same - we&amp;#8217;ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Python via virtualenv on Mac OS X: RuntimeError: Python is not installed as a framework.</title>
      <link>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</link>
      <pubDate>Fri, 04 May 2018 22:03:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</guid>
      <description>I&amp;#8217;ve previously written a couple of blog posts about my troubles getting matplotlib to play nicely and I run into a slightly different variant today while following Sidath Asiri&amp;#8217;s Hello World in TensorFlow tutorial.
 When I ran the script using a version of Python installed via virtualenv I got the following exception:
 Traceback (most recent call last): File &#34;iris.py&#34;, line 4, in &amp;lt;module&amp;gt; from matplotlib import pyplot as plt File &#34;</description>
    </item>
    
    <item>
      <title>PyData London 2018 Conference Experience Report</title>
      <link>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</link>
      <pubDate>Sun, 29 Apr 2018 11:54:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</guid>
      <description>Over the last few days I attended PyData London 2018 and wanted to share my experience. The PyData series of conferences aim to bring together users and developers of data analysis tools to share ideas and learn from each other. I presented a talk on building a recommendation with Python and Neo4j at the 2016 version but didn&amp;#8217;t attend last year.
 The organisers said there were ~ 550 attendees spread over 1 day of tutorials and 2 days of talks.</description>
    </item>
    
    <item>
      <title>Python: Serialize and Deserialize Numpy 2D arrays</title>
      <link>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</link>
      <pubDate>Sat, 07 Apr 2018 19:38:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</guid>
      <description>I&amp;#8217;ve been playing around with saving and loading scikit-learn models and needed to serialize and deserialize Numpy arrays as part of the process.
 I could use pickle but that seems a bit overkill so I decided instead to save the byte representation of the array. We can get that representation by calling the tobytes method on a Numpy array:
 import numpy as np &amp;gt;&amp;gt;&amp;gt; np.array([ [1,2,3], [4,5,6], [7,8,9] ]) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Python 3: Converting a list to a dictionary with dictionary comprehensions</title>
      <link>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</link>
      <pubDate>Mon, 02 Apr 2018 04:20:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</guid>
      <description>When coding in Python I often find myself with lists containing key/value pairs that I want to convert to a dictionary.
 In a recent example I had the following code:
 values = [{&#39;key&#39;: &#39;name&#39;, &#39;value&#39;: &#39;Mark&#39;}, {&#39;key&#39;: &#39;age&#39;, &#39;value&#39;: 34}]   And I wanted to create a dictionary that had the keys name and age and their respective values. The easiest way to convert this list to a dictionary is to iterate over the list and construct the dictionary key by key:</description>
    </item>
    
    <item>
      <title>GitHub: Getting the download count for a release</title>
      <link>https://markhneedham.com/blog/2018/03/23/github-release-download-count/</link>
      <pubDate>Fri, 23 Mar 2018 15:49:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/23/github-release-download-count/</guid>
      <description>At Neo4j we distribute several of our Developer Relations projects via GitHub Releases so I was curious whether there was a way to see how many people had downloaded them.
 I found an article explaining how to do it on v3 of the GitHub API, but I&amp;#8217;ve got used to the v4 GraphQL API and I&amp;#8217;m not going back! Thankfully it&amp;#8217;s not too difficult to figure out.
 GitHub let you explore the API via the GitHub GraphQL Explorer and the following query gets us the information we require:</description>
    </item>
    
    <item>
      <title>Neo4j Desktop: undefined: Unable to extract host from undefined</title>
      <link>https://markhneedham.com/blog/2018/03/20/neo4j-undefined-unable-to-extract-host-from-undefined/</link>
      <pubDate>Tue, 20 Mar 2018 17:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/20/neo4j-undefined-unable-to-extract-host-from-undefined/</guid>
      <description>During a training session I facilitated today one of the attendees got the following error message while trying to execute a query inside the Neo4j Desktop.
   This error message happens if we try to run a query when the database hasn&amp;#8217;t been started, and would usually be accompanied by this screen:
   On this occasion that wasn&amp;#8217;t happening, but we can easily fix it by going back to the project screen and starting the database:</description>
    </item>
    
    <item>
      <title>Neo4j: Using the Neo4j Import Tool with the Neo4j Desktop</title>
      <link>https://markhneedham.com/blog/2018/03/19/neo4j-using-neo4j-import-tool-with-neo4j-desktop/</link>
      <pubDate>Mon, 19 Mar 2018 21:38:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/19/neo4j-using-neo4j-import-tool-with-neo4j-desktop/</guid>
      <description>Last week as part of a modelling and import webinar I showed how to use the Neo4j Import Tool to create a graph of the Yelp Open Dataset:
  Afterwards I realised that I didn&amp;#8217;t show how to use the tool if you already have an existing database in place so this post will show how to do that.
 Imagine we have a Neo4j Desktop project that looks like this:</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Neo.ClientError.Statement.TypeError: Don&#39;t know how to add Double and String</title>
      <link>https://markhneedham.com/blog/2018/03/14/neo4j-cypher-neo-clienterror-statement-typeerror-dont-know-add-double-string/</link>
      <pubDate>Wed, 14 Mar 2018 16:53:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/14/neo4j-cypher-neo-clienterror-statement-typeerror-dont-know-add-double-string/</guid>
      <description>I recently upgraded a Neo4j backed application from Neo4j 3.2 to Neo4j 3.3 and came across an interesting change in behaviour around type coercion which led to my application throwing a bunch of errors.  In Neo4j 3.2 and earlier if you added a String to a Double it would coerce the Double to a String and concatenate the values. The following would therefore be valid Cypher: RETURN toFloat(&amp;quot;1.0&amp;quot;) + &amp;quot; Mark&amp;quot; ╒══════════╕ │&amp;quot;result&amp;quot; │ ╞══════════╡ │&amp;quot;1.</description>
    </item>
    
    <item>
      <title>Yelp: Reverse geocoding businesses to extract detailed location information</title>
      <link>https://markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</link>
      <pubDate>Wed, 14 Mar 2018 08:53:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</guid>
      <description>I&#39;ve been playing around with the Yelp Open Dataset and wanted to extract more detailed location information for each business.  This is an example of the JSON representation of one business: $ cat dataset/business.json | head -n1 | jq { &amp;quot;business_id&amp;quot;: &amp;quot;FYWN1wneV18bWNgQjJ2GNg&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Dental by Design&amp;quot;, &amp;quot;neighborhood&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;address&amp;quot;: &amp;quot;4855 E Warner Rd, Ste B9&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Ahwatukee&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;AZ&amp;quot;, &amp;quot;postal_code&amp;quot;: &amp;quot;85044&amp;quot;, &amp;quot;latitude&amp;quot;: 33.3306902, &amp;quot;longitude&amp;quot;: -111.9785992, &amp;quot;stars&amp;quot;: 4, &amp;quot;review_count&amp;quot;: 22, &amp;quot;is_open&amp;quot;: 1, &amp;quot;attributes&amp;quot;: { &amp;quot;AcceptsInsurance&amp;quot;: true, &amp;quot;ByAppointmentOnly&amp;quot;: true, &amp;quot;BusinessAcceptsCreditCards&amp;quot;: true }, &amp;quot;categories&amp;quot;: [ &amp;quot;Dentists&amp;quot;, &amp;quot;General Dentistry&amp;quot;, &amp;quot;Health &amp;amp; Medical&amp;quot;, &amp;quot;Oral Surgeons&amp;quot;, &amp;quot;Cosmetic Dentists&amp;quot;, &amp;quot;Orthodontists&amp;quot; ], &amp;quot;hours&amp;quot;: { &amp;quot;Friday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Tuesday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Thursday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Wednesday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Monday&amp;quot;: &amp;quot;7:30-17:00&amp;quot; } }   The businesses reside in different countries so I wanted to extract the area/county/state and the country for each of them.</description>
    </item>
    
    <item>
      <title>Running asciidoctor-pdf on TeamCity</title>
      <link>https://markhneedham.com/blog/2018/03/13/running-asciidoctor-pdf-teamcity/</link>
      <pubDate>Tue, 13 Mar 2018 21:57:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/13/running-asciidoctor-pdf-teamcity/</guid>
      <description>I&#39;ve been using asciidoctor-pdf to generate PDF and while I was initially running the tool locally I eventually decided to setup a build on TeamCity.  It was a bit trickier than I expected, mostly because I&#39;m not that familiar with deploying Ruby applications, but I thought I&#39;d capture what I&#39;ve done for future me.  I have the following Gemfile that installs asciidoctor-pdf and its dependencies: Gemfile
source &#39;https://rubygems.</description>
    </item>
    
    <item>
      <title>Neo4j Import: java.lang.IllegalStateException: Mixing specified and unspecified group belongings in a single import isn&#39;t supported</title>
      <link>https://markhneedham.com/blog/2018/03/07/neo4j-import-java-lang-illegalstateexception-mixing-specified-unspecified-group-belongings-single-import-isnt-supported/</link>
      <pubDate>Wed, 07 Mar 2018 03:11:12 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/07/neo4j-import-java-lang-illegalstateexception-mixing-specified-unspecified-group-belongings-single-import-isnt-supported/</guid>
      <description>I&#39;ve been working with the Neo4j Import Tool recently after a bit of a break and ran into an interesting error message that I initially didn&#39;t understand.  I had some CSV files containing nodes that I wanted to import into Neo4j. Their contents look like this: $ cat people_header.csv name:ID(Person) $ cat people.csv &amp;quot;Mark&amp;quot; &amp;quot;Michael&amp;quot; &amp;quot;Ryan&amp;quot; &amp;quot;Will&amp;quot; &amp;quot;Jennifer&amp;quot; &amp;quot;Karin&amp;quot; $ cat companies_header.csv name:ID(Company) $ cat companies.csv &amp;quot;Neo4j&amp;quot;   I find it easier to use separate header files because I often make typos with my column names and it&#39;s easier to update a single line file than to open a multi-million line file and change the first line.</description>
    </item>
    
    <item>
      <title>Asciidoctor: Creating a macro</title>
      <link>https://markhneedham.com/blog/2018/02/19/asciidoctor-creating-macro/</link>
      <pubDate>Mon, 19 Feb 2018 20:51:31 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/02/19/asciidoctor-creating-macro/</guid>
      <description>I&#39;ve been writing the TWIN4j blog for almost a year now and during that time I&#39;ve written a few different asciidoc macros to avoid repetition.  The most recent one I wrote does the formatting around the Featured Community Member of the Week. I call it like this from the asciidoc, passing in the name of the person and a link to an image: featured::https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/20180202004247/this-week-in-neo4j-3-february-2018.jpg[name=&amp;quot;Suellen Stringer-Hye&amp;quot;]   The code for the macro has two parts.</description>
    </item>
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>I&#39;ve been playing around with some Tensorflow tutorials recently and wanted to see if I could create a submission for Kaggle&#39;s Spooky Author Identification competition that I&#39;ve written about recently.  My model is based on one from the text classification tutorial. The tutorial shows how to create custom Estimators which we can learn more about in a post on the Google Developers blog. Imports  Let&#39;s get started. First, our imports: from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import pandas as pd import tensorflow as tf from sklearn import preprocessing from sklearn.</description>
    </item>
    
    <item>
      <title>Asciidoc to Asciidoc: Exploding includes</title>
      <link>https://markhneedham.com/blog/2018/01/23/asciidoc-asciidoc-exploding-includes/</link>
      <pubDate>Tue, 23 Jan 2018 21:11:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/23/asciidoc-asciidoc-exploding-includes/</guid>
      <description>One of my favourite features in AsciiDoc is the ability to include other files, but when using lots of includes is that it becomes difficult to read the whole document unless you convert it to one of the supported backends. $ asciidoctor --help Usage: asciidoctor [OPTION]... FILE... Translate the AsciiDoc source FILE or FILE(s) into the backend output format (e.g., HTML 5, DocBook 4.5, etc.) By default, the output is written to a file with the basename of the source file and the appropriate extension.</description>
    </item>
    
    <item>
      <title>Strava: Calculating the similarity of two runs</title>
      <link>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</link>
      <pubDate>Thu, 18 Jan 2018 23:35:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</guid>
      <description>I go running several times a week and wanted to compare my runs against each other to see how similar they are.  I record my runs with the Strava app and it has an API that returns lat/long coordinates for each run in the Google encoded polyline algorithm format.  We can use the polyline library to decode these values into a list of lat/long tuples. For example: import polyline polyline.</description>
    </item>
    
  </channel>
</rss>