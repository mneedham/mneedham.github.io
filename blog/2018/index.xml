<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2018s on Mark Needham</title>
    <link>https://markhneedham.com/blog/2018/</link>
    <description>Recent content in 2018s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Jun 2018 05:30:21 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/2018/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Neo4j APOC: Importing data from Strava&#39;s paginated JSON API</title>
      <link>https://markhneedham.com/blog/2018/06/05/neo4j-apoc-loading-data-strava-paginated-json-api/</link>
      <pubDate>Tue, 05 Jun 2018 05:30:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/05/neo4j-apoc-loading-data-strava-paginated-json-api/</guid>
      <description>Over the weekend I&amp;#8217;ve been playing around with loading data from the Strava API into Neo4j and I started with the following Python script which creates a node with a Run label for each of my activities.
 If you want to follow along on your own data you&amp;#8217;ll need to get an API key via the &#39;My API Application&#39; section of the website. Once you&amp;#8217;ve got that put it in the TOKEN environment variable and you should be good to go.</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Gotchas when working with Durations</title>
      <link>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-gotchas-working-with-durations/</link>
      <pubDate>Sun, 03 Jun 2018 20:11:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-gotchas-working-with-durations/</guid>
      <description>Continuing with my explorations of Strava data in Neo4j I wanted to share some things I learnt while trying to work out my pace for certain distances.
 Before we get into the pace calculations let&amp;#8217;s first understand how the duration function works. If we run the following query we might expect to get back the same value that we put in&amp;#8230;&amp;#8203;
 RETURN duration({seconds: 413.77}).seconds AS seconds   ╒═════════╕ │&#34;</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Formatting instances of the Duration and Datetime date types</title>
      <link>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-formatting-instances-durations-dates/</link>
      <pubDate>Sun, 03 Jun 2018 04:08:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/03/neo4j-3.4-formatting-instances-durations-dates/</guid>
      <description>In my last blog post I showed how to compare instances of Neo4j&amp;#8217;s Duration data type, and in the middle of the post I realised that I needed to use the APOC library to return the value in the format I wanted. This was the solution I ended up with:
 WITH duration({seconds: 100}) AS duration RETURN apoc.text.lpad(toString(duration.minutes), 2, &#34;0&#34;) + &#34;:&#34; + apoc.text.lpad(toString(duration.secondsOfMinute), 2, &#34;0&#34;)   If we run that query this is the output:</description>
    </item>
    
    <item>
      <title>Neo4j 3.4: Comparing durations</title>
      <link>https://markhneedham.com/blog/2018/06/02/neo4j-3.4-comparing-durations/</link>
      <pubDate>Sat, 02 Jun 2018 03:24:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/06/02/neo4j-3.4-comparing-durations/</guid>
      <description>Neo4j 3.4 saw the introduction of the temporal date type, which my colleague Adam Cowley covered in his excellent blog post, and in this post I want to share my experience using durations from my Strava runs.
 I&amp;#8217;ll show how to load the whole Strava dataset in another blog post but for now we&amp;#8217;ll just manually create some durations based on the elapsed time in seconds that Strava provides. We can run the following query to convert duration in seconds into the duration type:</description>
    </item>
    
    <item>
      <title>Interpreting Word2vec or GloVe embeddings using scikit-learn and Neo4j graph algorithms</title>
      <link>https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</link>
      <pubDate>Sat, 19 May 2018 09:47:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</guid>
      <description>A couple of weeks I came across a paper titled Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings via Abigail See&#39;s blog post about ACL 2017.
  The paper explains an algorithm that helps to make sense of word embeddings generated by algorithms such as Word2vec and GloVe.
 I&amp;#8217;m fascinated by how graphs can be used to interpret seemingly black box data, so I was immediately intrigued and wanted to try and reproduce their findings using Neo4j.</description>
    </item>
    
    <item>
      <title>Predicting movie genres with node2Vec and Tensorflow</title>
      <link>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</link>
      <pubDate>Fri, 11 May 2018 08:12:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</guid>
      <description>In my previous post we looked at how to get up and running with the node2Vec algorithm, and in this post we&amp;#8217;ll learn how we can feed graph embeddings into a simple Tensorflow model.
 Recall that node2Vec takes in a list of edges (or relationships) and gives us back an embedding (array of numbers) for each node.
 This time we&amp;#8217;re going to run the algorithm over a movies recommendation dataset from the Neo4j Sandbox.</description>
    </item>
    
    <item>
      <title>Exploring node2vec - a graph embedding algorithm</title>
      <link>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</link>
      <pubDate>Fri, 11 May 2018 08:08:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</guid>
      <description>In my explorations of graph based machine learning, one algorithm I came across is called node2Vec. The paper describes it as &#34;an algorithmic framework for learning continuous feature representations for nodes in networks&#34;.
 So what does the algorithm do? From the website:
  The node2vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective. The objective is flexible, and the algorithm accommodates for various definitions of network neighborhoods by simulating biased random walks.</description>
    </item>
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week I&amp;#8217;ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiri&amp;#8217;s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
 I&amp;#8217;ll use the same data as Sidath, so if you want to follow along you&amp;#8217;ll need to download these files:
   iris_training.csv
  iris_test.csv
   Loading data The way we load data will remain exactly the same - we&amp;#8217;ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Python via virtualenv on Mac OS X: RuntimeError: Python is not installed as a framework.</title>
      <link>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</link>
      <pubDate>Fri, 04 May 2018 22:03:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</guid>
      <description>I&amp;#8217;ve previously written a couple of blog posts about my troubles getting matplotlib to play nicely and I run into a slightly different variant today while following Sidath Asiri&amp;#8217;s Hello World in TensorFlow tutorial.
 When I ran the script using a version of Python installed via virtualenv I got the following exception:
 Traceback (most recent call last): File &#34;iris.py&#34;, line 4, in &amp;lt;module&amp;gt; from matplotlib import pyplot as plt File &#34;</description>
    </item>
    
    <item>
      <title>PyData London 2018 Conference Experience Report</title>
      <link>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</link>
      <pubDate>Sun, 29 Apr 2018 11:54:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</guid>
      <description>Over the last few days I attended PyData London 2018 and wanted to share my experience. The PyData series of conferences aim to bring together users and developers of data analysis tools to share ideas and learn from each other. I presented a talk on building a recommendation with Python and Neo4j at the 2016 version but didn&amp;#8217;t attend last year.
 The organisers said there were ~ 550 attendees spread over 1 day of tutorials and 2 days of talks.</description>
    </item>
    
    <item>
      <title>Python: Serialize and Deserialize Numpy 2D arrays</title>
      <link>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</link>
      <pubDate>Sat, 07 Apr 2018 19:38:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</guid>
      <description>I&amp;#8217;ve been playing around with saving and loading scikit-learn models and needed to serialize and deserialize Numpy arrays as part of the process.
 I could use pickle but that seems a bit overkill so I decided instead to save the byte representation of the array. We can get that representation by calling the tobytes method on a Numpy array:
 import numpy as np &amp;gt;&amp;gt;&amp;gt; np.array([ [1,2,3], [4,5,6], [7,8,9] ]) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Python 3: Converting a list to a dictionary with dictionary comprehensions</title>
      <link>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</link>
      <pubDate>Mon, 02 Apr 2018 04:20:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</guid>
      <description>When coding in Python I often find myself with lists containing key/value pairs that I want to convert to a dictionary.
 In a recent example I had the following code:
 values = [{&#39;key&#39;: &#39;name&#39;, &#39;value&#39;: &#39;Mark&#39;}, {&#39;key&#39;: &#39;age&#39;, &#39;value&#39;: 34}]   And I wanted to create a dictionary that had the keys name and age and their respective values. The easiest way to convert this list to a dictionary is to iterate over the list and construct the dictionary key by key:</description>
    </item>
    
    <item>
      <title>GitHub: Getting the download count for a release</title>
      <link>https://markhneedham.com/blog/2018/03/23/github-release-download-count/</link>
      <pubDate>Fri, 23 Mar 2018 15:49:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/23/github-release-download-count/</guid>
      <description>At Neo4j we distribute several of our Developer Relations projects via GitHub Releases so I was curious whether there was a way to see how many people had downloaded them.
 I found an article explaining how to do it on v3 of the GitHub API, but I&amp;#8217;ve got used to the v4 GraphQL API and I&amp;#8217;m not going back! Thankfully it&amp;#8217;s not too difficult to figure out.
 GitHub let you explore the API via the GitHub GraphQL Explorer and the following query gets us the information we require:</description>
    </item>
    
    <item>
      <title>Neo4j Desktop: undefined: Unable to extract host from undefined</title>
      <link>https://markhneedham.com/blog/2018/03/20/neo4j-undefined-unable-to-extract-host-from-undefined/</link>
      <pubDate>Tue, 20 Mar 2018 17:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/20/neo4j-undefined-unable-to-extract-host-from-undefined/</guid>
      <description>During a training session I facilitated today one of the attendees got the following error message while trying to execute a query inside the Neo4j Desktop.
   This error message happens if we try to run a query when the database hasn&amp;#8217;t been started, and would usually be accompanied by this screen:
   On this occasion that wasn&amp;#8217;t happening, but we can easily fix it by going back to the project screen and starting the database:</description>
    </item>
    
    <item>
      <title>Neo4j: Using the Neo4j Import Tool with the Neo4j Desktop</title>
      <link>https://markhneedham.com/blog/2018/03/19/neo4j-using-neo4j-import-tool-with-neo4j-desktop/</link>
      <pubDate>Mon, 19 Mar 2018 21:38:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/19/neo4j-using-neo4j-import-tool-with-neo4j-desktop/</guid>
      <description>Last week as part of a modelling and import webinar I showed how to use the Neo4j Import Tool to create a graph of the Yelp Open Dataset:
  Afterwards I realised that I didn&amp;#8217;t show how to use the tool if you already have an existing database in place so this post will show how to do that.
 Imagine we have a Neo4j Desktop project that looks like this:</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Neo.ClientError.Statement.TypeError: Don&#39;t know how to add Double and String</title>
      <link>https://markhneedham.com/blog/2018/03/14/neo4j-cypher-neo-clienterror-statement-typeerror-dont-know-add-double-string/</link>
      <pubDate>Wed, 14 Mar 2018 16:53:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/14/neo4j-cypher-neo-clienterror-statement-typeerror-dont-know-add-double-string/</guid>
      <description>I recently upgraded a Neo4j backed application from Neo4j 3.2 to Neo4j 3.3 and came across an interesting change in behaviour around type coercion which led to my application throwing a bunch of errors.  In Neo4j 3.2 and earlier if you added a String to a Double it would coerce the Double to a String and concatenate the values. The following would therefore be valid Cypher: RETURN toFloat(&amp;quot;1.0&amp;quot;) + &amp;quot; Mark&amp;quot; ╒══════════╕ │&amp;quot;result&amp;quot; │ ╞══════════╡ │&amp;quot;1.</description>
    </item>
    
    <item>
      <title>Yelp: Reverse geocoding businesses to extract detailed location information</title>
      <link>https://markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</link>
      <pubDate>Wed, 14 Mar 2018 08:53:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</guid>
      <description>I&#39;ve been playing around with the Yelp Open Dataset and wanted to extract more detailed location information for each business.  This is an example of the JSON representation of one business: $ cat dataset/business.json | head -n1 | jq { &amp;quot;business_id&amp;quot;: &amp;quot;FYWN1wneV18bWNgQjJ2GNg&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Dental by Design&amp;quot;, &amp;quot;neighborhood&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;address&amp;quot;: &amp;quot;4855 E Warner Rd, Ste B9&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Ahwatukee&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;AZ&amp;quot;, &amp;quot;postal_code&amp;quot;: &amp;quot;85044&amp;quot;, &amp;quot;latitude&amp;quot;: 33.3306902, &amp;quot;longitude&amp;quot;: -111.9785992, &amp;quot;stars&amp;quot;: 4, &amp;quot;review_count&amp;quot;: 22, &amp;quot;is_open&amp;quot;: 1, &amp;quot;attributes&amp;quot;: { &amp;quot;AcceptsInsurance&amp;quot;: true, &amp;quot;ByAppointmentOnly&amp;quot;: true, &amp;quot;BusinessAcceptsCreditCards&amp;quot;: true }, &amp;quot;categories&amp;quot;: [ &amp;quot;Dentists&amp;quot;, &amp;quot;General Dentistry&amp;quot;, &amp;quot;Health &amp;amp; Medical&amp;quot;, &amp;quot;Oral Surgeons&amp;quot;, &amp;quot;Cosmetic Dentists&amp;quot;, &amp;quot;Orthodontists&amp;quot; ], &amp;quot;hours&amp;quot;: { &amp;quot;Friday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Tuesday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Thursday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Wednesday&amp;quot;: &amp;quot;7:30-17:00&amp;quot;, &amp;quot;Monday&amp;quot;: &amp;quot;7:30-17:00&amp;quot; } }   The businesses reside in different countries so I wanted to extract the area/county/state and the country for each of them.</description>
    </item>
    
    <item>
      <title>Running asciidoctor-pdf on TeamCity</title>
      <link>https://markhneedham.com/blog/2018/03/13/running-asciidoctor-pdf-teamcity/</link>
      <pubDate>Tue, 13 Mar 2018 21:57:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/13/running-asciidoctor-pdf-teamcity/</guid>
      <description>I&#39;ve been using asciidoctor-pdf to generate PDF and while I was initially running the tool locally I eventually decided to setup a build on TeamCity.  It was a bit trickier than I expected, mostly because I&#39;m not that familiar with deploying Ruby applications, but I thought I&#39;d capture what I&#39;ve done for future me.  I have the following Gemfile that installs asciidoctor-pdf and its dependencies: Gemfile
source &#39;https://rubygems.</description>
    </item>
    
    <item>
      <title>Neo4j Import: java.lang.IllegalStateException: Mixing specified and unspecified group belongings in a single import isn&#39;t supported</title>
      <link>https://markhneedham.com/blog/2018/03/07/neo4j-import-java-lang-illegalstateexception-mixing-specified-unspecified-group-belongings-single-import-isnt-supported/</link>
      <pubDate>Wed, 07 Mar 2018 03:11:12 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/03/07/neo4j-import-java-lang-illegalstateexception-mixing-specified-unspecified-group-belongings-single-import-isnt-supported/</guid>
      <description>I&#39;ve been working with the Neo4j Import Tool recently after a bit of a break and ran into an interesting error message that I initially didn&#39;t understand.  I had some CSV files containing nodes that I wanted to import into Neo4j. Their contents look like this: $ cat people_header.csv name:ID(Person) $ cat people.csv &amp;quot;Mark&amp;quot; &amp;quot;Michael&amp;quot; &amp;quot;Ryan&amp;quot; &amp;quot;Will&amp;quot; &amp;quot;Jennifer&amp;quot; &amp;quot;Karin&amp;quot; $ cat companies_header.csv name:ID(Company) $ cat companies.csv &amp;quot;Neo4j&amp;quot;   I find it easier to use separate header files because I often make typos with my column names and it&#39;s easier to update a single line file than to open a multi-million line file and change the first line.</description>
    </item>
    
    <item>
      <title>Asciidoctor: Creating a macro</title>
      <link>https://markhneedham.com/blog/2018/02/19/asciidoctor-creating-macro/</link>
      <pubDate>Mon, 19 Feb 2018 20:51:31 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/02/19/asciidoctor-creating-macro/</guid>
      <description>I&#39;ve been writing the TWIN4j blog for almost a year now and during that time I&#39;ve written a few different asciidoc macros to avoid repetition.  The most recent one I wrote does the formatting around the Featured Community Member of the Week. I call it like this from the asciidoc, passing in the name of the person and a link to an image: featured::https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/20180202004247/this-week-in-neo4j-3-february-2018.jpg[name=&amp;quot;Suellen Stringer-Hye&amp;quot;]   The code for the macro has two parts.</description>
    </item>
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>I&#39;ve been playing around with some Tensorflow tutorials recently and wanted to see if I could create a submission for Kaggle&#39;s Spooky Author Identification competition that I&#39;ve written about recently.  My model is based on one from the text classification tutorial. The tutorial shows how to create custom Estimators which we can learn more about in a post on the Google Developers blog. Imports  Let&#39;s get started. First, our imports: from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import pandas as pd import tensorflow as tf from sklearn import preprocessing from sklearn.</description>
    </item>
    
    <item>
      <title>Asciidoc to Asciidoc: Exploding includes</title>
      <link>https://markhneedham.com/blog/2018/01/23/asciidoc-asciidoc-exploding-includes/</link>
      <pubDate>Tue, 23 Jan 2018 21:11:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/23/asciidoc-asciidoc-exploding-includes/</guid>
      <description>One of my favourite features in AsciiDoc is the ability to include other files, but when using lots of includes is that it becomes difficult to read the whole document unless you convert it to one of the supported backends. $ asciidoctor --help Usage: asciidoctor [OPTION]... FILE... Translate the AsciiDoc source FILE or FILE(s) into the backend output format (e.g., HTML 5, DocBook 4.5, etc.) By default, the output is written to a file with the basename of the source file and the appropriate extension.</description>
    </item>
    
    <item>
      <title>Strava: Calculating the similarity of two runs</title>
      <link>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</link>
      <pubDate>Thu, 18 Jan 2018 23:35:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</guid>
      <description>I go running several times a week and wanted to compare my runs against each other to see how similar they are.  I record my runs with the Strava app and it has an API that returns lat/long coordinates for each run in the Google encoded polyline algorithm format.  We can use the polyline library to decode these values into a list of lat/long tuples. For example: import polyline polyline.</description>
    </item>
    
  </channel>
</rss>