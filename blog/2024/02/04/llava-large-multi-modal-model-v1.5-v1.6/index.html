
<!DOCTYPE html>
<html lang="en-us">

    <head>

        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />

        
        <meta property="og:title" content="LLaVA 1.5 vs. 1.6 | Mark Needham" />
        <meta property="og:site_name" content="Mark Needham" />
        <meta property="og:url" content="https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" />

    
        <meta property="og:type" content="article" />
        <meta property="og:article:published_time" content="2024-02-04T00:44:37Z" />
        <meta property="og:article:tag" content="ollama" />
        <meta property="og:article:tag" content="generative-ai" />
        <meta property="og:article:tag" content="llava" />
        <meta property="og:article:tag" content="til" />
        

        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:site" content="@markhneedham" />
        <meta name="twitter:creator" content="@markhneedham" />
        <meta name="twitter:title" content="LLaVA 1.5 vs. 1.6" />
        
          <meta name="twitter:description" content="In this post, we&#39;ll learn about LLaVA, an open source large multi-modal model." />
        
        <meta name="twitter:url" content="https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" />

      
        
        <meta name="og:image" content="https://www.markhneedham.com/blog//uploads/2024/02/llava-banner.png" />
        <meta name="image" property="og:image" content="https://www.markhneedham.com/blog//uploads/2024/02/llava-banner.png">
        <meta name="twitter:image:src" content="https://www.markhneedham.com/blog//uploads/2024/02/llava-banner.png" />
        <meta name="twitter:image" content="https://www.markhneedham.com/blog//uploads/2024/02/llava-banner.png" />
      

    

        
        <title>LLaVA 1.5 vs. 1.6 | Mark Needham</title>

    
        <meta name="description" content="In this post, we&#39;ll learn about LLaVA, an open source large multi-modal model." />
    

        <meta name="p:domain_verify" content="fc173d84e3a4de948ed4bda2908afd3e"/>
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
        <link href="https://www.markhneedham.com/blog/index.xml" rel="alternate" type="application/rss+xml" title="Mark Needham" />
          
        

        <link rel="canonical" href="https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" />

    
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "LLaVA 1.5 vs. 1.6",
        "author": {
            "@type": "Person",
            "name": "http://profiles.google.com/100354797468725665406?rel=author"
        },
        "datePublished": "2024-02-04",
        "description": "LLaVA (or Large Language and Vision Assistant), an open-source large multi-modal model, just released version 1.6. It claims to have improvements over version 1.5, which was released a few months ago:\nIncreasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.\nBetter visual reasoning and OCR capability with an improved visual instruction tuning data mixture.",
        "wordCount":  1993 
    }
    </script>
    

    

<style>
*{padding:0;margin:0}body,html{font-size:1em;line-height:1.65em;font-family:-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;font-weight:300;color:#444}html{height:100%}body{padding:2em 2.5em 1em 20em}header{border-right:1px #eee solid;padding:2em;position:fixed;top:0;left:0;height:100%;width:13.5em}#content{display:block;width:100%}footer{padding:1em 0 2.5em;font-size:.8em;line-height:1.5em;color:#888}article{border-bottom:.1em #eee solid;padding-bottom:1.7em;max-width:56em}h4,h5,h6,hr,p{margin-top:.9em;margin-bottom:.9em}h1,h2,h3,h4,h5,h6{font-family:"Bree Serif",serif;font-weight:400!important}h1{font-size:2.5em;line-height:1.1em;margin-top:.6em;margin-bottom:.6em}h2{font-size:1.9em;line-height:1.2em;margin-top:.7em;margin-bottom:.7em}h3{font-size:1.4em;line-height:1.3em;margin-top:.8em;margin-bottom:.8em}h4{font-size:1.3em}h5{font-size:1.2em}h6{font-size:1.1em}iframe,img{max-width:100%}a{font-weight:700;text-decoration:none;color:#5cc265}a:hover{text-decoration:underline}h1 a,h2 a,h3 a,h4 a,h5 a,h6 a{font-weight:400!important}strong{font-weight:700}blockquote{border-left:.4em solid #008cc1;padding-left:1.2em;font-size:1.3em}hr{border:0;height:1px;background:#eee}ol,ul{margin-left:3em}code{font-size:1.1em;max-height: 500px;background:#0d1117}
pre{border-radius: .6em;font-size:.8em;line-height:1.7em;color:#c9d1d9;background:#0d1117;padding:2.0em 0.75em 2.0em 0.75em;word-break:break-all;word-wrap:break-word;white-space:pre;white-space:-moz-pre-wrap;white-space:pre-wrap}input{font-size:1em;padding:.3em}header h1{font-size:1.9em;margin-top:.8em;margin-bottom:.6em}header h1 a{color:#444}header h1 a:hover{text-decoration:none}header #logo img{width:9em;height:9em;border-radius:4.5em;-moz-border-radius:4.5em;-webkit-border-radius:4.5em;border:none}#follow-icons{font-size:.7em;margin-top:-.7em;margin-bottom:1.5em}#follow-icons a{color:#ccc}#follow-icons span{vertical-align:top;margin-left:-.15em;margin-right:-.15em}#follow-icons span .fa-stack-1x{font-size:1.05em;line-height:1.9em}header h6{margin-top:.5em}article span.post-stamp{color:#888}h1.post-title{margin-top:.35em;margin-bottom:.6em}h3.post-title{margin-top:.4em;padding-bottom:.9em;border-bottom:1px solid #eee;font-size:1.2em;color:#444}.post-title .feature-star{font-size:.9em}.feature-star,.separator,.taglist{color:#ccc}.taglist a{background-color:#008cc1;color:#fff;display:inline-block;line-height:1.5em;padding:.3em .6em;vertical-align:20%;font-size:.5em;font-family:"Open Sans",sans-serif;font-weight:700!important;text-transform:uppercase;letter-spacing:.05em;border-radius:.25em;-moz-border-radius:.25em;-webkit-border-radius:.25em}#social-bar{margin-top:1.5em;background-color:#eee;padding:.5em}#comments{margin-top:.15em;padding-bottom:.2em;border-bottom:1px solid #eee}.pagination{margin-bottom:1em}footer a{font-weight:300;color:#888;text-decoration:underline}footer a:hover{color:#444;text-decoration:none}@media only screen and (min-width:1281px){body,html{font-size:1.1em}}@media only screen and (max-width:800px){body{padding:0}header{border-right:none;border-bottom:1px #eee solid;position:relative;height:auto;width:auto;text-align:center;padding-bottom:1em}#content{margin-left:0;padding:2em 2em 1em;width:auto}footer{padding:0 2.5em 2em}}@media only screen and (max-width:320px){#content,header{padding:1.2em 1.2em .6em}footer{padding:0 1.5em 1.2em}ol,ul{margin-left:2em}}
</style>

<style>
@media only screen and (max-width: 1000px) {
    table, caption {
        width: 100%;
        overflow-x: auto;
        display: block;
    }
}

@media only screen and (max-width: 800px) {
    #header {
        padding: 5px;
        display: flex;
        align-items: center;   
        flex-wrap: wrap;       
        justify-content: center;  
        flex-wrap: nowrap;    
        overflow-x: auto;     
        padding: 5px;
    }

    #header a {
        margin-bottom: -0.5em;
        margin-right: 1em;
    }

    #header #logo img {
        width: 30px;    
        height: auto;  
    }
    #header h1 {
        font-size: 1em;    
        margin-right: 1em;
    }
    #header p {
        display: none;    
    }
    #follow-icons a i {
        font-size: 1em;    
    }

    #follow-icons {
        font-size: 1.5em;
        margin-bottom: 0;
        margin-top: 0;
    }

    #follow-icons a {    
        margin-right: 0;
    } 
}

p code, caption code, div.colist code, .hdlist1 code {
    font-size:1.2em;
    background:#fff
}

.hdlist1 {
    font-weight: 700;
}

.dlist dd p {
    margin-top: 0;
}

pre.highlight {
    position: relative;
}


.label-right {
    color: #FFF;
    padding: 2px 5px;   
    border-radius: 0 0 0 0.25em;
    font-size: 1.1em;
    font-weight: 500;
    position: absolute;
    top: 0;
    right: 0;
}

.green-icon {
    color: lime;
}

.copy-button {
    cursor: pointer;  
    margin-right: 0.25em;
    padding: 0.25em 0 0 0;
}

.copy-notification {
    position: absolute;
    right: 0;
    top: 0;
    padding: 2px 5px;
    background-color: #0d1117;
    color: #fff;
}


.stretch{width:100%}
table{border-collapse:collapse;border-spacing:0}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;}
table thead,table tfoot{background:#008cc1;}
table thead tr th{background:#008cc1;}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table thead tr th {color: #ffffff;}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8);}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6;}
table.tableblock{max-width:100%;border-collapse:separate}
p.tableblock:last-child{margin-bottom:0; margin-top: 0;}
td.tableblock>.content{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{margin-top: 0.5em;}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd){background:#f8f8f7}
table.stripes-none tr,table.stripes-odd tr:nth-of-type(even){background:none}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
td>div.verse{white-space:pre}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock,th.tableblock{font-size:0.8em}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#008cc1;font-weight:350;margin-top:.75em;margin-bottom:.25em}

.listingblock>.title {
    background: linear-gradient(90deg, rgba(151,231,255,1) 0%, rgba(0,140,193,1) 35%, rgba(151,231,255,1) 100%);
    padding: 0.4em;
    color: black;
    margin-bottom: 0;
    
    font-size: 0.7em;
    font-weight: 600;
    border-radius: 0.6em 0.6em 0 0;
}

.listingblock div.content {
    max-height: 500px;
    overflow-y: auto;
}

.listingblock>.title + div.content pre {
    border-radius: 0 0 .6em .6em;
}

.listingblock:has(code[data-lang="text"]) > .title {
    text-transform: uppercase;
}

.admonitionblock {
    margin: 0 0 1.5rem;
    border-left: 4px solid #000000;
    border-radius: .25rem;
}
.admonitionblock.note {
    background-color: #edf2f7;
    color: #19407c;
    border-left-color: #718096;
}
.admonitionblock.warning {
    background-color: #fff6e6;
    color: #19407c;
    border-left-color: #FFA500;
}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;display:none;}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}

tbody tr pre {margin-bottom: 27px}

.colist, .dlist, .exampleblock, .imageblock, .listingblock, .literalblock, .olist, .paragraph, .partintro, .quoteblock, .sidebarblock, .ulist, .verseblock {
    margin: 0 0 1.5rem;
}

.colist {
    margin: 0;
}

.colist table {
    margin: 0;
    border: none;
}

.conum { display: inline-block; color: white !important; background-color: #008cc1; -webkit-border-radius: 100px; border-radius: 100px; text-align: center; width: 20px; height: 20px; font-size: 12px; font-weight: bold; line-height: 20px; font-family: Arial, sans-serif; font-style: normal; position: relative; top: -2px; letter-spacing: -1px; }
.conum * { color: white !important; }
.conum + b { display: none; }
.conum:after { content: attr(data-value); }
.conum:not([data-value]):empty { display: none; }

.colist>table tr>td:first-of-type{ border:0; min-width:0; padding:0 .75em;}
.colist>table tr>td:last-of-type{ border:0; min-width:0; padding:.25em 0}
.colist>table tr:nth-child(even) { background:0 0 }

.listingblock + .colist {
    margin-top: -0.7rem;
}

h2 code {
    background: #ffffff;
}
</style>

    </head>
    <body>
        <header id="header">
            <a id="logo" href="https://www.markhneedham.com/blog/"><img src="https://www.markhneedham.com/blog//me.jpg" alt="Mark Needham" /></a>
            <h1><a href="https://www.markhneedham.com/blog/">Mark Needham</a></h1>
            <p>In this post, we&#39;ll learn about LLaVA, an open source large multi-modal model.</p>

            <div id="follow-icons">
	<a href="http://twitter.com/markhneedham" rel="me"><i class="fa fa-twitter-square fa-2x"></i></a>
	<a href="https://www.youtube.com/channel/UCKEk670ECmteGBehmDjVSSg" rel="me"><i class="fa fa-youtube-square fa-2x"></i></a>
	<a href="http://linkedin.com/in/markhneedham" rel="me"><i class="fa fa-linkedin-square fa-2x"></i></a>
	<a href="http://github.com/mneedham" rel="me"><i class="fa fa-github-square fa-2x"></i></a>
	<a href="mailto:m.h.needham@gmail.com"><i class="fa fa-envelope-square fa-2x"></i></a>
	<a href="https://www.markhneedham.com/blog/index.xml" rel="me"><i class="fa fa-rss-square fa-2x"></i></a>
</div>
            
        </header>

<main id="content">

<article id="" class="2024">
    <div class="post-stamp">
        <time datetime="2024-02-04T00:44:37Z">
            4 Feb 2024
        </time>
        <span class="taglist">
        
        &middot;
        
            <a href="https://www.markhneedham.com/blog/tag/ollama/">ollama</a>
        
            <a href="https://www.markhneedham.com/blog/tag/generative-ai/">generative-ai</a>
        
            <a href="https://www.markhneedham.com/blog/tag/llava/">llava</a>
        
            <a href="https://www.markhneedham.com/blog/tag/til/">til</a>
        
        
        </span>
    </div>
    <h1 class="post-title">LLaVA 1.5 vs. 1.6</h1>
    <div class="paragraph">
<p><a href="https://llava-vl.github.io/" target="_blank" rel="noopener">LLaVA</a> (or Large Language and Vision Assistant), an open-source large multi-modal model, just released <a href="https://llava-vl.github.io/blog/2024-01-30-llava-1-6/" target="_blank" rel="noopener">version 1.6</a>.
It claims to have improvements over version 1.5, which was released a few months ago:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.</p>
</li>
<li>
<p>Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.</p>
</li>
<li>
<p>Better visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning.</p>
</li>
<li>
<p>Efficient deployment and inference with SGLang.</p>
</li>
</ol>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Both the initial and new versions are available on <a href="https://ollama.ai/" target="_blank" rel="noopener">Ollama</a>, so I thought I’d give them a try on some of my images to see how they get on.</p>
</div>
<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>I’ve created a video showing how to do this on <a href="https://www.youtube.com/@learndatawithmark" target="_blank" rel="noopener">my YouTube channel, Learn Data with Mark</a>, so if you prefer to consume content through that medium, I’ve embedded it below:</p>
</div>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jXUv8mlQHq0?si=n7NDkvoqV2weOz1e" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>If you want to follow along, you’ll need to install Ollama, launch Ollama (<code>ollama serve</code>), and then pull down both models:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">ollama pull llava:7b-v1.5-q4_0
ollama pull llava:v1.6</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now let’s try out the models.</p>
</div>
<div class="sect1">
<h2 id="_a_photo_of_me">A photo of me</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We’ll start with the following image of me looking through a magnifying glass.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.markhneedham.com/blog/
/uploads/2024/02/images/image1.png" alt="image1" width="500"/>
</div>
<div class="title">Figure 1. Me with a magnifying glass</div>
</div>
<div class="paragraph">
<p>Let’s create a chat session with the 1.5 version of LLaVA:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">ollama run llava:7b-v1.5-q4_0</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can then ask it to describe the image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">&#34;Can you describe this image? ./images/image.png&#34;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Added image &#39;./images/image1.png&#39;
The image depicts a bald man wearing glasses, holding up a magnifying glass to his eye. He is standing in front of a black background, possibly to focus on the
subject matter being observed through the lens of the magnifying glass. His attentive gaze suggests that he might be examining an object or studying something in detail.</code></pre>
</div>
</div>
<div class="paragraph">
<p>One neat recent feature of Ollama is that we can switch to another model from within the session by calling the <code>load</code> command.
So to use the v1.6 model, we would call the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">/load llava:v1.6</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Loading model &#39;llava:v1.6&#39;</code></pre>
</div>
</div>
<div class="paragraph">
<p>And if we ask this model to describe the image, we’ll see the following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text"> In the image, there is a man who appears to be in his middle years. He has a bald head and is wearing glasses with black frames and a thin black strap. The man is holding up what looks like a magnifying glass over his eye, as if examining something closely. He has a slight smile on his face and is looking directly at the camera, creating an engaging perspective.

The man is dressed in a casual manner, with a white t-shirt adorned with a small pattern and a pair of dark pants. His stance is relaxed, suggesting he might be in a lighthearted or informal setting.

Behind him, there&#39;s a plain black background that contrasts with the man and draws attention to him. The image does not contain any text or additional objects. The focus is solely on the man and his interaction with the magnifying glass.</code></pre>
</div>
</div>
<div class="paragraph">
<p>1.6 gives a more detailed answer, but I think it’s more or less the same description as 1.5 gave.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ollama_python">Ollama Python</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For the rest of the examples, we’re going to use the Ollama Python library, which we can install by running the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">pip install ollama</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, we’ll run the following in a Jupyter notebook or from a Python REPL:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import ollama

def generate(model, image, prompt):
    stream = ollama.generate(
        model=model,
        prompt=prompt,
        images=[image],
        stream=True
    )
    response = &#34;&#34;
    for chunk in stream:
        response += chunk[&#39;response&#39;]
        print(chunk[&#39;response&#39;], end=&#39;&#39;, flush=True)
    print(&#34;&#34;)
    return response</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_captioning_an_image">Captioning an image</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Next, we’re going to ask models to come up with a caption for the following image:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.markhneedham.com/blog/
/uploads/2024/02/images/image2.png" alt="image2" width="500"/>
</div>
<div class="title">Figure 2. An arrow on bricks</div>
</div>
<div class="paragraph">
<p>We can do this by running the following code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">image = &#34;images/image2.png&#34;
prompt = &#34;Create a caption for this image&#34;
for model in [&#39;llava:7b-v1.5-q4_0&#39;, &#39;llava:v1.6&#39;]:
  print(f&#34;Model: {model}&#34;)
  generate(model, image, prompt)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0
 A blue brick wall with a white arrow pointing to the left.
Model: llava:v1.6
 &#34;A playful twist on direction, with a blue wall providing the backdrop.&#34;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we update the prompt to include the word &#39;creative&#39;, like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">image = &#34;images/image2.png&#34;
prompt = &#34;Create a creative caption for this image&#34;
for model in [&#39;llava:7b-v1.5-q4_0&#39;, &#39;llava:v1.6&#39;]:
  print(f&#34;Model: {model}&#34;)
  generate(model, image, prompt)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We get a slightly better output:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0
&#34;A blue arrow pointing the way, guiding us on our journey.&#34;
Model: llava:v1.6
 &#34;Guiding the way to endless possibilities: a sign pointing towards an unknown adventure.&#34;</code></pre>
</div>
</div>
<div class="paragraph">
<p>There’s not much in it, but it seems to me that version 1.6 has &#39;understood&#39; the prompt a bit better.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_reading_a_banner_image">Reading a banner image</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Next up, let’s see if the models can read the text from a banner image of one of my blog posts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.markhneedham.com/blog/
/uploads/2024/02/images/image3.png" alt="image3" width="500"/>
</div>
<div class="title">Figure 3. A blog image</div>
</div>
<div class="paragraph">
<p>We’ll use the following prompt:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">image = &#34;images/image3.png&#34;
prompt = &#34;What text is written on this image?&#34;
for model in [&#39;llava:7b-v1.5-q4_0&#39;, &#39;llava:v1.6&#39;]:
    print(f&#34;Model: {model}&#34;)
    generate(model, image, prompt)</code></pre>
</div>
</div>
<div class="paragraph">
<p>I ran this a few times because it was giving different answers each time.
Here’s the output for version 1.5:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0
5 Ways to Use AI in Your Business
Running a large language model locally on my laptop.
&#34;Hugging Face&#34;</code></pre>
</div>
</div>
<div class="paragraph">
<p>And 1.6:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0
The text on the image reads: &#34;HUGGING FACE - RUNNING A LARGE LANGUAGE MODEL LOCALLY ON MY LAPTOP&#34;
The text on the image reads: &#34;Hugging Face&#34; followed by a description of how to use Hugging Face for running a large language model locally on one&#39;s laptop.
The text on the image reads: &#34;Hugging Face Running a Large Language Model Locally On My Laptop&#34;</code></pre>
</div>
</div>
<div class="paragraph">
<p>It’s by no means perfect, but I think version 1.6 wins this one.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_extracting_code_from_an_image">Extracting code from an image</h2>
<div class="sectionbody">
<div class="paragraph">
<p>But can it extract code from an image?
These days lots of people create LinkedIn posts with code on images and I’d quite like to get that code and run it locally.
Perhaps LLaVA can help out here.
We’re going to use the following image:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.markhneedham.com/blog/
/uploads/2024/02/images/image4.png" alt="image4" width="500"/>
</div>
<div class="title">Figure 4. Python code for a window function</div>
</div>
<div class="paragraph">
<p>And we’ll use the following prompt:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">image = &#34;images/image4.png&#34;
prompt = &#34;Extract the code from this image&#34;
for model in [&#39;llava:7b-v1.5-q4_0&#39;, &#39;llava:v1.6&#39;]:
    print(f&#34;Model: {model}&#34;)
    generate(model, image, prompt)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And here’s how the models fared, starting with 1.5:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0

```sql
        a = A + 1
        b = B + 1
        c = C + 1
        d = D + 1

        if (a &gt; b and a &gt; c) then begin
            RP_RPT(&#34;A is greater than B but less than C&#34;)
        end if

        if (b &gt; a and b &gt; c) then begin
            RP_RPT(&#34;B is greater than A but less than C&#34;)
        end if

        if (c &gt; a and c &gt; b) then begin
            RP_RPT(&#34;C is greater than A and B&#34;)
        end if

        if (a = b and b = c) then begin
            RP_RPT(&#34;A,B,C are equal&#34;)
        end if

        if (a &lt; b and a &lt; c) then begin
            RP_RPT(&#34;A is less than B but greater than C&#34;)
        end if

        if (b &lt; a and b &lt; c) then begin
            RP_RPT(&#34;B is less than A but greater than C&#34;)
        end if

        if (c &lt; a and c &lt; b) then begin
            RP_RPT(&#34;C is less than A but greater than B&#34;)
        end if

        if (a &gt; b or not(b = c)) then begin
            RP_RPT(&#34;A,B are not equal&#34;)
        end if

        if (a &lt; b and a &lt;= C) then begin
            RP_RPT(&#34;A is less than B but greater than or equal to C&#34;)
        end if

        if (b &gt;= A and b &lt;= C) then begin
            RP_RPT(&#34;B is within the range of A &amp; C&#34;)
        end if

        if (c &gt; A and c &lt;= B) then begin
            RP_RPT(&#34;C is within the range of A &amp; B&#34;)
        end if
    end if

    if (A = -1024 or  B = -1024 or C = -1024) then begin
        RP_RPT(&#34;A,B,C are all equal to the value of -1024&#34;)
    end if

    if (D &gt;= A + B + C and D &lt;= 2**31 - 1) then begin
        RP_RPT(&#34;D is within the range of A+B+C&#34;)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And now 1.6</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:v1.6
 The image contains a screenshot of a computer screen with a code snippet displayed. Here is the extracted code:

```python
import statistics as stats

data = [10, 20, 30, 40, 50]
mean = stats.mode(data)
median = stats.median(data)
stdev = stats.stdev(data)

print(&#39;Mean:&#39;, mean)
print(&#39;Median:&#39;, median)
print(&#39;Standard Deviation (STDEV):&#39;, stdev)
```

This code is written in Python and uses the `statistics` module to calculate the mean, median, and standard deviation of a list of numbers. The list `data` contains five elements, which are then processed using the `mode`, `median`, and `stdev` functions from the `statistics` library.</code></pre>
</div>
</div>
<div class="paragraph">
<p>Neither model does a good job when it comes to extracting code.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_a_diagram">Understanding a diagram</h2>
<div class="sectionbody">
<div class="paragraph">
<p>And we’ll finish off by seeing whether the models can explain this diagram, which shows a relational vs graph approach to modelling a problem.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.markhneedham.com/blog/
/uploads/2024/02/images/image5.png" alt="image5" width="500"/>
</div>
<div class="title">Figure 5. Relational vs graph modelling</div>
</div>
<div class="paragraph">
<p>We’ll use this prompt:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">image = &#34;images/image5.png&#34;
prompt = &#34;Can you describe this diagram?&#34;
for model in [&#39;llava:7b-v1.5-q4_0&#39;, &#39;llava:v1.6&#39;]:
    print(f&#34;Model: {model}&#34;)
    generate(model, image, prompt)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, let’s see how they get on.
First up, 1.5:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:7b-v1.5-q4_0

This diagram displays a network of interconnected files, with multiple layers and different shapes. There is a mix of both square and rectangular shapes, which can be seen in various parts of the image. The relationships between these elements are represented by arrows connecting them to one another, indicating that some files are dependent or related to others.

This network diagram appears to represent an intricate system where different files have specific connections with other files within the structure. It provides a visual representation of how these files interact and engage with each other in this complex environment.</code></pre>
</div>
</div>
<div class="paragraph">
<p>This doesn’t make a whole lot of sense.
Let’s see if 1.6 does any better:</p>
</div>
<div class="listingblock">
<div class="title">Output</div>
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Model: llava:v1.6
 The image you&#39;ve shared is a flowchart that describes a process or workflow, likely from an algorithmic or computational context. Here&#39;s a description of the elements in the flowchart:

1. **Start Point**: There are two arrows pointing to a starting point labeled &#34;from this...&#34; suggesting the beginning of a process or algorithm.

2. **Sequence of Operations**: The sequence of operations is depicted by a series of boxes, each representing an action or function within the context of the flowchart. These actions seem to be related to data processing or transformation:
   - &#34;Node1&#34; appears to represent some sort of input or initial stage.
   - &#34;Node2&#34; and &#34;Node3&#34; seem to be intermediate steps or stages in the process.
   - &#34;Node4&#34; represents another significant step or decision point.
   - &#34;Node5&#34; is the final node, indicating the completion of the workflow or the output of the process.

3. **Arrows**: The arrows show the flow or direction of the operations. They indicate that the starting point leads to Node1 and then moves through the subsequent nodes until it reaches Node5.

4. **Labels on Nodes**: Each node has labels that are likely to represent actions, functions, or data elements involved in the process. However, without more context, these labels are not clearly defined.

5. **Final Arrow**: There&#39;s an arrow from Node5 pointing back to &#34;from this...&#34; which may indicate that the output of Node5 is being fed back into another part of the system.

6. **Connections Between Nodes**: The lines connecting the nodes represent the flow or relationship between them, showing that one node leads to the next in a logical sequence.

7. **Annotations**: There are annotations within the flowchart, but they&#39;re not fully legible due to the quality of the image. One can make out phrases like &#34;Node1&#34; and &#34;Node5&#34;, indicating the names or identities of these nodes in the workflow.

This type of diagram is commonly used in computer science and software engineering to visualize the steps involved in a process, making it easier to understand and communicate the logic behind a particular algorithm or system.</code></pre>
</div>
</div>
<div class="paragraph">
<p>It’s slightly better, but still doesn’t do a good job of explaining the difference between the approaches.</p>
</div>
</div>
</div>

    <div id="social-bar">
	<ul class="rrssb-buttons clearfix">
      <li class="email">
          <a href="mailto:?subject=LLaVA%201.5%20vs.%201.6&amp;body=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/">
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
                      <path transform="scale(0.014,-0.014) translate(0,-1670)" d="M1792 826v-794q0 -66 -47 -113t-113 -47h-1472q-66 0 -113 47t-47 113v794q44 -49 101 -87q362 -246 497 -345q57 -42 92.5 -65.5t94.5 -48t110 -24.5h1h1q51 0 110 24.5t94.5 48t92.5 65.5q170 123 498 345q57 39 100 87zM1792 1120q0 -79 -49 -151t-122 -123 q-376 -261 -468 -325q-10 -7 -42.5 -30.5t-54 -38t-52 -32.5t-57.5 -27t-50 -9h-1h-1q-23 0 -50 9t-57.5 27t-52 32.5t-54 38t-42.5 30.5q-91 64 -262 182.5t-205 142.5q-62 42 -117 115.5t-55 136.5q0 78 41.5 130t118.5 52h1472q65 0 112.5 -47t47.5 -113z"/>
                  </svg>
              </span>
              <span class="text">Email</span>
          </a>
      </li>
      <li class="facebook">
          <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" class="popup">
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
                      <path d="M27.825,4.783c0-2.427-2.182-4.608-4.608-4.608H4.783c-2.422,0-4.608,2.182-4.608,4.608v18.434 c0,2.427,2.181,4.608,4.608,4.608H14V17.379h-3.379v-4.608H14v-1.795c0-3.089,2.335-5.885,5.192-5.885h3.718v4.608h-3.726 c-0.408,0-0.884,0.492-0.884,1.236v1.836h4.609v4.608h-4.609v10.446h4.916c2.422,0,4.608-2.188,4.608-4.608V4.783z"/>
                  </svg>
              </span>
              <span class="text">Facebook</span>
          </a>
      </li>
			<li class="twitter">
          <a href="http://twitter.com/home?status=LLaVA%201.5%20vs.%201.6%20https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" class="popup">
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
		                  <path d="M24.253,8.756C24.689,17.08,18.297,24.182,9.97,24.62c-3.122,0.162-6.219-0.646-8.861-2.32 c2.703,0.179,5.376-0.648,7.508-2.321c-2.072-0.247-3.818-1.661-4.489-3.638c0.801,0.128,1.62,0.076,2.399-0.155 C4.045,15.72,2.215,13.6,2.115,11.077c0.688,0.275,1.426,0.407,2.168,0.386c-2.135-1.65-2.729-4.621-1.394-6.965 C5.575,7.816,9.54,9.84,13.803,10.071c-0.842-2.739,0.694-5.64,3.434-6.482c2.018-0.623,4.212,0.044,5.546,1.683 c1.186-0.213,2.318-0.662,3.329-1.317c-0.385,1.256-1.247,2.312-2.399,2.942c1.048-0.106,2.069-0.394,3.019-0.851 C26.275,7.229,25.39,8.196,24.253,8.756z"/>
                  </svg>
              </span>
              <span class="text">Twitter</span>
          </a>
      </li>
			<li class="linkedin">
          <a href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/&amp;title=LLaVA%201.5%20vs.%201.6&amp;summary=LLaVA%20%28or%20Large%20Language%20and%20Vision%20Assistant%29%2c%20an%20open-source%20large%20multi-modal%20model%2c%20just%20released%20version%201.6.%20It%20claims%20to%20have%20improvements%20over%20version%201.5%2c%20which%20was%20released%20a%20few%20months%20ago%3a%0aIncreasing%20the%20input%20image%20resolution%20to%204x%20more%20pixels.%20This%20allows%20it%20to%20grasp%20more%20visual%20details.%20It%20supports%20three%20aspect%20ratios%2c%20up%20to%20672x672%2c%20336x1344%2c%201344x336%20resolution.%0aBetter%20visual%20reasoning%20and%20OCR%20capability%20with%20an%20improved%20visual%20instruction%20tuning%20data%20mixture...." class="popup">
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
                      <path d="M25.424,15.887v8.447h-4.896v-7.882c0-1.979-0.709-3.331-2.48-3.331c-1.354,0-2.158,0.911-2.514,1.803 c-0.129,0.315-0.162,0.753-0.162,1.194v8.216h-4.899c0,0,0.066-13.349,0-14.731h4.899v2.088c-0.01,0.016-0.023,0.032-0.033,0.048 h0.033V11.69c0.65-1.002,1.812-2.435,4.414-2.435C23.008,9.254,25.424,11.361,25.424,15.887z M5.348,2.501 c-1.676,0-2.772,1.092-2.772,2.539c0,1.421,1.066,2.538,2.717,2.546h0.032c1.709,0,2.771-1.132,2.771-2.546 C8.054,3.593,7.019,2.501,5.343,2.501H5.348z M2.867,24.334h4.897V9.603H2.867V24.334z"/>
                  </svg>
              </span>
              <span class="text">LinkedIn</span>
          </a>
      </li>
      <li class="tumblr">
					<script>
						document.write('<a href="http://www.tumblr.com/share?v=3&amp;u=' + encodeURIComponent('https:\/\/www.markhneedham.com\/blog\/2024\/02\/04\/llava-large-multi-modal-model-v1.5-v1.6\/') + '&amp;t=LLaVA 1.5 vs. 1.6" class="popup">');
					</script>
              <span class="icon">
                  <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
											<path d="M18.02 21.842c-2.029 0.052-2.422-1.396-2.439-2.446v-7.294h4.729V7.874h-4.71V1.592c0 0-3.653 0-3.714 0 s-0.167 0.053-0.182 0.186c-0.218 1.935-1.144 5.33-4.988 6.688v3.637h2.927v7.677c0 2.8 1.7 6.7 7.3 6.6 c1.863-0.03 3.934-0.795 4.392-1.453l-1.22-3.539C19.595 21.6 18.7 21.8 18 21.842z"/>
									</svg>
              </span>
              <span class="text">Tumblr</span>
          <script>document.write('</a>');</script>
      </li>
      <li class="reddit">
          <a href="http://www.reddit.com/submit?url=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/">
              <span class="icon">
                  <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
											<g>
													<path d="M11.794 15.316c0-1.029-0.835-1.895-1.866-1.895c-1.03 0-1.893 0.865-1.893 1.895s0.863 1.9 1.9 1.9 C10.958 17.2 11.8 16.3 11.8 15.316z"/>
													<path d="M18.1 13.422c-1.029 0-1.895 0.864-1.895 1.895c0 1 0.9 1.9 1.9 1.865c1.031 0 1.869-0.836 1.869-1.865 C19.969 14.3 19.1 13.4 18.1 13.422z"/>
													<path d="M17.527 19.791c-0.678 0.678-1.826 1.006-3.514 1.006c-0.004 0-0.009 0-0.014 0c-0.004 0-0.01 0-0.015 0 c-1.686 0-2.834-0.328-3.51-1.005c-0.264-0.265-0.693-0.265-0.958 0c-0.264 0.265-0.264 0.7 0 1 c0.943 0.9 2.4 1.4 4.5 1.402c0.005 0 0 0 0 0c0.005 0 0 0 0 0c2.066 0 3.527-0.459 4.47-1.402 c0.265-0.264 0.265-0.693 0.002-0.958C18.221 19.5 17.8 19.5 17.5 19.791z"/>
													<path d="M27.707 13.267c0-1.785-1.453-3.237-3.236-3.237c-0.793 0-1.518 0.287-2.082 0.761c-2.039-1.295-4.646-2.069-7.438-2.219 l1.483-4.691l4.062 0.956c0.071 1.4 1.3 2.6 2.7 2.555c1.488 0 2.695-1.208 2.695-2.695C25.881 3.2 24.7 2 23.2 2 c-1.059 0-1.979 0.616-2.42 1.508l-4.633-1.091c-0.344-0.081-0.693 0.118-0.803 0.455l-1.793 5.7 C10.548 8.6 7.7 9.4 5.6 10.75C5.006 10.3 4.3 10 3.5 10.029c-1.785 0-3.237 1.452-3.237 3.2 c0 1.1 0.6 2.1 1.4 2.69c-0.04 0.272-0.061 0.551-0.061 0.831c0 2.3 1.3 4.4 3.7 5.9 c2.299 1.5 5.3 2.3 8.6 2.325c3.228 0 6.271-0.825 8.571-2.325c2.387-1.56 3.7-3.66 3.7-5.917 c0-0.26-0.016-0.514-0.051-0.768C27.088 15.5 27.7 14.4 27.7 13.267z M23.186 3.355c0.74 0 1.3 0.6 1.3 1.3 c0 0.738-0.6 1.34-1.34 1.34s-1.342-0.602-1.342-1.34C21.844 4 22.4 3.4 23.2 3.355z M1.648 13.3 c0-1.038 0.844-1.882 1.882-1.882c0.31 0 0.6 0.1 0.9 0.209c-1.049 0.868-1.813 1.861-2.26 2.9 C1.832 14.2 1.6 13.8 1.6 13.267z M21.773 21.57c-2.082 1.357-4.863 2.105-7.831 2.105c-2.967 0-5.747-0.748-7.828-2.105 c-1.991-1.301-3.088-3-3.088-4.782c0-1.784 1.097-3.484 3.088-4.784c2.081-1.358 4.861-2.106 7.828-2.106 c2.967 0 5.7 0.7 7.8 2.106c1.99 1.3 3.1 3 3.1 4.784C24.859 18.6 23.8 20.3 21.8 21.57z M25.787 14.6 c-0.432-1.084-1.191-2.095-2.244-2.977c0.273-0.156 0.59-0.245 0.928-0.245c1.035 0 1.9 0.8 1.9 1.9 C26.354 13.8 26.1 14.3 25.8 14.605z"/>
											</g>
									</svg>
              </span>
              <span class="text">Reddit</span>
          </a>
      </li>
      <li class="googleplus">
          <a href="https://plus.google.com/share?url=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/" class="popup">
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
                      <g>
                          <path d="M14.703,15.854l-1.219-0.948c-0.372-0.308-0.88-0.715-0.88-1.459c0-0.748,0.508-1.223,0.95-1.663 c1.42-1.119,2.839-2.309,2.839-4.817c0-2.58-1.621-3.937-2.399-4.581h2.097l2.202-1.383h-6.67c-1.83,0-4.467,0.433-6.398,2.027 C3.768,4.287,3.059,6.018,3.059,7.576c0,2.634,2.022,5.328,5.604,5.328c0.339,0,0.71-0.033,1.083-0.068 c-0.167,0.408-0.336,0.748-0.336,1.324c0,1.04,0.551,1.685,1.011,2.297c-1.524,0.104-4.37,0.273-6.467,1.562 c-1.998,1.188-2.605,2.916-2.605,4.137c0,2.512,2.358,4.84,7.289,4.84c5.822,0,8.904-3.223,8.904-6.41 c0.008-2.327-1.359-3.489-2.829-4.731H14.703z M10.269,11.951c-2.912,0-4.231-3.765-4.231-6.037c0-0.884,0.168-1.797,0.744-2.511 c0.543-0.679,1.489-1.12,2.372-1.12c2.807,0,4.256,3.798,4.256,6.242c0,0.612-0.067,1.694-0.845,2.478 c-0.537,0.55-1.438,0.948-2.295,0.951V11.951z M10.302,25.609c-3.621,0-5.957-1.732-5.957-4.142c0-2.408,2.165-3.223,2.911-3.492 c1.421-0.479,3.25-0.545,3.555-0.545c0.338,0,0.52,0,0.766,0.034c2.574,1.838,3.706,2.757,3.706,4.479 c-0.002,2.073-1.736,3.665-4.982,3.649L10.302,25.609z"/>
                          <polygon points="23.254,11.89 23.254,8.521 21.569,8.521 21.569,11.89 18.202,11.89 18.202,13.604 21.569,13.604 21.569,17.004 23.254,17.004 23.254,13.604 26.653,13.604 26.653,11.89"/>
                      </g>
                  </svg>
              </span>
              <span class="text">Google+</span>
          </a>
      </li>
      <li class="pinterest">
          <script>
						var imgurl = "https:\/\/www.markhneedham.com\/blog\/logo.png";
						var firstimg = document.getElementsByClassName("2024")[0].getElementsByTagName("img")[0];
						if (firstimg !== undefined) {
							imgurl = firstimg.src;
						}
						document.write('<a href="http://pinterest.com/pin/create/button/?url=https:\/\/www.markhneedham.com\/blog\/2024\/02\/04\/llava-large-multi-modal-model-v1.5-v1.6\/&amp;media=' + imgurl + '&amp;description=LLaVA 1.5 vs. 1.6" class="popup">');
					</script>
              <span class="icon">
                  <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="28px" height="28px" viewBox="0 0 28 28" enable-background="new 0 0 28 28" xml:space="preserve">
		                  <path d="M14.021,1.57C6.96,1.57,1.236,7.293,1.236,14.355c0,7.062,5.724,12.785,12.785,12.785c7.061,0,12.785-5.725,12.785-12.785 C26.807,7.294,21.082,1.57,14.021,1.57z M15.261,18.655c-1.161-0.09-1.649-0.666-2.559-1.219c-0.501,2.626-1.113,5.145-2.925,6.458 c-0.559-3.971,0.822-6.951,1.462-10.116c-1.093-1.84,0.132-5.545,2.438-4.632c2.837,1.123-2.458,6.842,1.099,7.557 c3.711,0.744,5.227-6.439,2.925-8.775c-3.325-3.374-9.678-0.077-8.897,4.754c0.19,1.178,1.408,1.538,0.489,3.168 C7.165,15.378,6.53,13.7,6.611,11.462c0.131-3.662,3.291-6.227,6.46-6.582c4.007-0.448,7.771,1.474,8.29,5.239 c0.579,4.255-1.816,8.865-6.102,8.533L15.261,18.655z"/>
                  </svg>
              </span>
              <span class="text">Pinterest</span>
          <script>document.write('</a>');</script>
      </li>
      <li class="pocket">
          <a href="https://getpocket.com/save?url=https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/"  class="popup">
              <span class="icon">
                  <svg width="32px" height="28px" viewBox="0 0 32 28" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
                      <path d="M28.7817528,0.00172488695 C30.8117487,0.00431221738 31.9749312,1.12074529 31.9644402,3.10781507 C31.942147,6.67703739 32.1336065,10.2669583 31.8057648,13.8090137 C30.7147076,25.5813672 17.2181194,31.8996281 7.20714461,25.3808491 C2.71833574,22.4571656 0.196577202,18.3122624 0.0549495772,12.9357897 C-0.0342233715,9.5774348 0.00642900214,6.21519891 0.0300336062,2.85555035 C0.0405245414,1.1129833 1.21157517,0.0146615391 3.01995012,0.00819321302 C7.34746087,-0.00603710433 11.6775944,0.00431221738 16.0064164,0.00172488695 C20.2644248,0.00172488695 24.5237444,-0.00215610869 28.7817528,0.00172488695 L28.7817528,0.00172488695 Z M8.64885184,7.85611511 C7.38773662,7.99113854 6.66148108,8.42606978 6.29310958,9.33228474 C5.90114134,10.2969233 6.17774769,11.1421181 6.89875951,11.8276216 C9.35282156,14.161969 11.8108164,16.4924215 14.2976518,18.7943114 C15.3844131,19.7966007 16.5354102,19.7836177 17.6116843,18.7813283 C20.0185529,16.5495467 22.4070683,14.2982907 24.7824746,12.0327533 C25.9845979,10.8850542 26.1012707,9.56468083 25.1469132,8.60653379 C24.1361858,7.59255976 22.8449191,7.6743528 21.5890476,8.85191291 C19.9936451,10.3488554 18.3680912,11.8172352 16.8395462,13.3777945 C16.1342655,14.093159 15.7200114,14.0048744 15.0566806,13.3440386 C13.4599671,11.7484252 11.8081945,10.2060421 10.1262706,8.70001155 C9.65564653,8.27936164 9.00411403,8.05345704 8.64885184,7.85611511 L8.64885184,7.85611511 L8.64885184,7.85611511 Z"></path>
                  </svg>
              </span>
              <span class="text">Pocket</span>
          </a>
      </li>
  </ul>
</div>

    <div itemprop="author" itemscope itemtype="http://schema.org/Person">
  <h5>About the author</h5>
  <section>
    <p><a href="https://twitter.com/markhneedham" itemprop="name" rel="author">I'm</a> currently working on short form content at <a href="https://clickhouse.com/">ClickHouse</a>. I publish short 5 minute videos showing how to solve data problems
      on YouTube <a href="https://www.youtube.com/@LearnDataWithMark">@LearnDataWithMark</a>. I previously worked on
      graph analytics at <a href="https://neo4j.com/">Neo4j</a>, where I also co-authored the <a
        href="https://www.oreilly.com/library/view/graph-algorithms/9781492047674/">O'Reilly Graph Algorithms Book</a> with Amy Hodler.</p>
  </section>
</div>
    <div id="comments">
    
    
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'markhneedham';
      var disqus_identifier = 'https:\/\/www.markhneedham.com\/blog\/2024\/02\/04\/llava-large-multi-modal-model-v1.5-v1.6\/';
      var disqus_title = 'LLaVA 1.5 vs. 1.6';
      var disqus_url = 'https:\/\/www.markhneedham.com\/blog\/2024\/02\/04\/llava-large-multi-modal-model-v1.5-v1.6\/';
      (function() {
        if (window.location.hostname == "localhost")
                return;

        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
    
</div>

</article>

</main>

  <footer id="footer">
			<section id="footer-message">&copy; Mark Needham. All rights reserved. Powered by <a href="http://gohugo.io" target="_blank">Hugo</a>. <a href="https://github.com/kathyqian/crisp-ghost-theme" target="_blank">Crisp</a> theme by <a href="http://kathyqian.com" target="_blank">Kathy Qian</a>.</section>
		</footer>

    <script>
      (function(c,f){asyncLoader=function(i,h){i.foreach(function(k,j){e(j,d(j),h)});if(typeof h.callback=="function"){var g=setInterval(function(){if(f.readyState==="complete"){clearInterval(g);h.callback()}},10)}};var d=function(g){var h=g.split(".");return h[h.length-1]},e=function(h,i,g){switch(i){case"js":a(h,g);break;case"css":b(h);break;default:break}},a=function(i,h){var g=document.createElement("script");g.type="text/javascript";g.async=true;g.src=i;document.getElementsByTagName("head")[0].appendChild(g)},b=function(g){var h=document.createElement("link");h.type="text/css";h.rel="stylesheet";h.href=g;document.getElementsByTagName("head")[0].appendChild(h)};Array.prototype.foreach=function(h){for(var g=0;g<this.length;g++){h(g,this[g])}}})(this,document);

      var WebFontConfig={google:{families:["Open Sans:300italic,700italic,300,700","Bree+Serif"]}};
      asyncLoader([
        "//netdna.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.css",
        "//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js",
        "//cdnjs.cloudflare.com/ajax/libs/webfont/1.5.16/webfontloader.js",
        "//unpkg.com/@highlightjs/cdn-assets@10.7.2/highlight.min.js"        
      ],{
        callback:function(){
          asyncLoader([
            "https:\/\/www.markhneedham.com\/blog\/css/rrssb.css",
            "https:\/\/www.markhneedham.com\/blog\/css/github-dark.css",
            "https:\/\/www.markhneedham.com\/blog\/js/image-zoom.js",
            "https:\/\/www.markhneedham.com\/blog\/js/gist.min.js",
            "https:\/\/www.markhneedham.com\/blog\/js/rrssb.min.js",
            "https://unpkg.com/highlightjs-cypher/dist/cypher.min.js",            
          ], { callback:function() {
              
              document.querySelectorAll('pre code').forEach((block) => {
                  hljs.highlightElement(block);                  
                  
                  var language = block.result.language;
                  if(language && language !== "text") {             
                    const originalCode = block.textContent.trim(); 
                    const labelAndButtonHTML = `
                      <span class="label-and-button">
                        <label class="label-right">${language.toUpperCase()}</label>
                        <i class="copy-button fa fa-copy"></i>
                      </span>
                    `;
                    block.insertAdjacentHTML("afterbegin", `<label class="label-right">${language.toUpperCase()} <i class="copy-button fa fa-copy"></i></label>`);

                    const button = block.querySelector('.copy-button');
                    button.addEventListener('click', (event) => {
                        var code = button.parentElement.textContent;
                        navigator.clipboard.writeText(originalCode).then(function() {
                          button.classList.remove('fa-copy');  // Remove the copy icon class
                          button.classList.add('fa-check', 'green-icon');  

                          setTimeout(function() {
                              button.classList.remove('fa-check', 'green-icon');  
                              button.classList.add('fa-copy');  
                          }, 2000);
                        }, function(err) {
                             
                            console.error('Could not copy code to clipboard: ', err);
                        });
                    });
                  }                  
              });
          }});
        }
      });
    </script>
	
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-KVLB7W6R20"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-KVLB7W6R20');
    </script>

    

    


  
	</body>
</html>

