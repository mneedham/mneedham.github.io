<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2020s on Mark Needham</title>
    <link>https://markhneedham.com/blog/2020/</link>
    <description>Recent content in 2020s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Dec 2020 00:44:37 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/2020/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>git: Ignore local changes on committed (env) file</title>
      <link>https://markhneedham.com/blog/2020/12/18/git-ignore-local-changes-committed-env-file/</link>
      <pubDate>Fri, 18 Dec 2020 00:44:37 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/12/18/git-ignore-local-changes-committed-env-file/</guid>
      <description>Whenever I&amp;#8217;ve writing scripts that rely on credentials defined as environment variables, I like to create a .env (or equivalent) file containing those variables. I then seed that file with placeholder values for each variable and make local changes that aren&amp;#8217;t checked in.
 I recently created the mneedham/materialize-sandbox/strava repository where I&amp;#8217;m using this approach with a .envsettings file that has the following contents:
 envsettings export CLIENT_ID=&#34;client_id&#34; export CLIENT_SECRET=&#34;client_secret&#34;   I have that file checked in so that anybody else can clone the repository and update this file with their own credentials.</description>
    </item>
    
    <item>
      <title>Materialize: Querying JSON files</title>
      <link>https://markhneedham.com/blog/2020/12/17/materialize-querying-json-file/</link>
      <pubDate>Thu, 17 Dec 2020 00:44:37 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/12/17/materialize-querying-json-file/</guid>
      <description>I recently learnt about Materialize, a SQL streaming database, via their Series B fundraising announcement, and thought I&amp;#8217;d take it for a spin.
 My go-to dataset for new databases is Strava, an app that I use to record my runs. It has an API that returns a JSON representation of each run, containing information like the distance covered, elapsed time, heart rate metrics, and more.
 I&amp;#8217;ve extracted my latest 30 activities to a file in the JSON lines format and in this post we&amp;#8217;re going to analyse that data using Materialize.</description>
    </item>
    
    <item>
      <title>Strava: Authorization Error - Missing activity:read_permission</title>
      <link>https://markhneedham.com/blog/2020/12/15/strava-authorization-error-missing-read-permission/</link>
      <pubDate>Tue, 15 Dec 2020 00:44:37 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/12/15/strava-authorization-error-missing-read-permission/</guid>
      <description>I&amp;#8217;m revisiting the Strava API after a two year absence and the approach to authenticating requests has changed in that time. You now need to generate an access token via OAuth 2.0, as described in the &#39;How to authenticate with OAuth 2.0&#39; section of the Getting Started with the Strava API guide.
 I want to generate a token that lets me retrieve all of my activities via the /athlete/activities end point.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - FOREACH vs CALL {} (subquery)</title>
      <link>https://markhneedham.com/blog/2020/10/29/neo4j-foreach-call-subquery/</link>
      <pubDate>Thu, 29 Oct 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/10/29/neo4j-foreach-call-subquery/</guid>
      <description>I recently wanted to create a graph based on an adjacency list, and in this post we&amp;#8217;ll learn how to do that using the FOREACH clause and then with the new CALL {} subquery clause.
 We&amp;#8217;ll start with the following map of ids &amp;#8594; arrays of ids:
 :param list =&amp;gt; ({`0`: [7, 9], `1`: [2, 4, 5, 6, 8, 9], `2`: [0, 6, 8, 9], `3`: [1, 2, 6, 9], `4`: [1, 2, 3, 7], `5`: [8, 9], `6`: [2, 4, 5, 7, 8, 9], `7`: [0, 3, 4, 6, 8, 9], `8`: [1, 6, 9], `9`: [0, 1, 3, 5]})   We want to create one node per id and create a relationship from each node to the nodes in its array.</description>
    </item>
    
    <item>
      <title>Unix: Get file name without extension from file path</title>
      <link>https://markhneedham.com/blog/2020/08/24/unix-get-file-name-without-extension-from-file-path/</link>
      <pubDate>Mon, 24 Aug 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/08/24/unix-get-file-name-without-extension-from-file-path/</guid>
      <description>I recently found myself needing to extract the file name but not file extension from a bunch of file paths and wanted to share a neat technique that I learnt to do it.
 I started with a bunch of Jupyter notebook files, which I listed usign the following command;
 $ find notebooks/ -maxdepth 1 -iname *ipynb notebooks/09_Predictions_sagemaker.ipynb notebooks/00_Environment.ipynb notebooks/05_Train_Evaluate_Model.ipynb notebooks/01_DataLoading.ipynb notebooks/05_SageMaker.ipynb notebooks/09_Predictions_sagemaker-Copy2.ipynb notebooks/09_Predictions_sagemaker-Copy1.ipynb notebooks/02_Co-Author_Graph.ipynb notebooks/04_Model_Feature_Engineering.ipynb notebooks/09_Predictions_scikit.ipynb notebooks/03_Train_Test_Split.ipynb   If we pick one of those files:</description>
    </item>
    
    <item>
      <title>pipenv: ImportError: No module named &#39;virtualenv.seed.via_app_data&#39;</title>
      <link>https://markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</link>
      <pubDate>Fri, 07 Aug 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</guid>
      <description>I&amp;#8217;ve been trying to install pipenv on a new computer and ran into a frustrating issue. After installing pipenv using pip, I tried to run the command below:
 $ /home/markhneedham/.local/bin/pipenv shell Creating a virtualenv for this project… Pipfile: /tmp/Pipfile Using /usr/bin/python3.8 (3.8.2) to create virtualenv… ⠙ Creating virtual environment...ModuleNotFoundError: No module named &#39;virtualenv.seed.via_app_data&#39; ✘ Failed creating virtual environment [pipenv.exceptions.VirtualenvCreationException]: Failed to create virtual environment.   Hmmm, for some reason it&amp;#8217;s unable to find one of the virtualenv modules.</description>
    </item>
    
    <item>
      <title>Google Docs: Find and replace script</title>
      <link>https://markhneedham.com/blog/2020/05/12/google-docs-find-and-replace-script/</link>
      <pubDate>Tue, 12 May 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/05/12/google-docs-find-and-replace-script/</guid>
      <description>I keep track of the podcasts that I&amp;#8217;ve listened to in a Google Doc, having pasted the episode title and podcast name from Player.FM. The format isn&amp;#8217;t exactly what I want so I&amp;#8217;ve been running the Find and Replace command to update each entry. This is obviously a very boring task, so I wanted to see if I could automate it.
 An example entry in the Google Doc reads like this:</description>
    </item>
    
    <item>
      <title>QuickGraph #7: An entity graph of TWIN4j using APOC NLP</title>
      <link>https://markhneedham.com/blog/2020/05/05/quick-graph-building-entity-graph-twin4j-apoc-nlp/</link>
      <pubDate>Tue, 05 May 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/05/05/quick-graph-building-entity-graph-twin4j-apoc-nlp/</guid>
      <description>One of the most popular use cases for Neo4j is knowledge graphs, and part of that process involves using NLP to create a graph structure from raw text. If we were doing a serious NLP project we&amp;#8217;d want to use something like GraphAware Hume, but in this blog post we&amp;#8217;re going to learn how to add basic NLP functionality to our graph applications.
  Figure 1. Building an entity graph of TWIN4j using APOC NLP  APOC NLP The big cloud providers (AWS, GCP, and Azure) all have Natural Language Processing APIs and, although their APIs aren&amp;#8217;t identical, they all let us extract entities, key phrases, and sentiment from text documents.</description>
    </item>
    
    <item>
      <title>Python: Select keys from map/dictionary</title>
      <link>https://markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</link>
      <pubDate>Mon, 27 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</guid>
      <description>In this post we&amp;#8217;re going to learn how to filter a Python map/dictionary to return a subset of keys or values. I needed to do this recently while logging some maps that had a lot of keys that I wasn&amp;#8217;t interested in.
 We&amp;#8217;ll start with the following map:
 x = {&#34;a&#34;: 1, &#34;b&#34;: 2, &#34;c&#34;: 3, &#34;d&#34;: 4} {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4}   We want to filter this map so that we only have the keys a and c.</description>
    </item>
    
    <item>
      <title>QuickGraph #6: COVID-19 Taxonomy Graph</title>
      <link>https://markhneedham.com/blog/2020/04/21/quick-graph-covid-19-taxonomy/</link>
      <pubDate>Tue, 21 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/04/21/quick-graph-covid-19-taxonomy/</guid>
      <description>It&amp;#8217;s been several months since our last QuickGraph and the world feels very different than it was back then. I&amp;#8217;ve been reading a couple of books about viruses - Spillover and Pale Rider - and am now very curious to learn more about the medical terms reference in the books.
 With the Pre Release of neosemantics (n10s) for Neo4j 4.0, I thought it would be interesting to create a graph of the taxonomy of the virus that caused COVID-19, using data extracted from Wikidata&amp;#8217;s SPARQL API.</description>
    </item>
    
    <item>
      <title>Python: Find the starting Sunday for all the weeks in a month</title>
      <link>https://markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</link>
      <pubDate>Sat, 18 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</guid>
      <description>In this post we&amp;#8217;re going to learn how to find the dates of all the Sundays in a given month, as well as the Sunday immediately preceding the 1st day in the month, assuming that day isn&amp;#8217;t a Sunday.
 Let&amp;#8217;s start by importing some libraries that we&amp;#8217;re going to use in this blog post:
 from dateutil import parser import datetime import calendar   Next we need to find the first day of the current month, which we can do with the following code:</description>
    </item>
    
    <item>
      <title>React Semantic-UI: Adding a custom icon to open link in a new window</title>
      <link>https://markhneedham.com/blog/2020/04/13/react-semantic-ui-custom-add-icon-open-new-window/</link>
      <pubDate>Mon, 13 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/04/13/react-semantic-ui-custom-add-icon-open-new-window/</guid>
      <description>I&amp;#8217;ve been building a little React app that uses the Semantic UI library and found myself wanting to render a custom icon.
 Semantic UI describes an icon as &#34;a glyph used to represent something else&#34;, and there are a big list of in built icons. For example, the following code renders a thumbs up icon:
 import {Icon} from &#34;semantic-ui-react&#34;; &amp;lt;Icon name=&#34;thumbs up outline icon green large&#34; style={{margin: 0}}/&amp;gt;    Figure 1.</description>
    </item>
    
    <item>
      <title>Streamlit: multiselect - AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;index&#39;</title>
      <link>https://markhneedham.com/blog/2020/03/31/streamlit-multiselect-numpy-no-attribute-index/</link>
      <pubDate>Tue, 31 Mar 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/03/31/streamlit-multiselect-numpy-no-attribute-index/</guid>
      <description>In this post we&amp;#8217;ll learn how to overcome a problem I encountered while building a small Streamlit application to analyse John Hopkin&amp;#8217;s data on the COVID-19 disease. The examples in this post use a CSV file that contains time series data of deaths in each country.
 I started with the following code to create a multiselect widget that lists all countries and selected the United Kingdom by default:
 import streamlit as st import pandas as pd default_countries = [&#34;</description>
    </item>
    
    <item>
      <title>SPARQL: OR conditions in a WHERE clause using the UNION clause</title>
      <link>https://markhneedham.com/blog/2020/02/07/sparql-or-conditions-where-union-query/</link>
      <pubDate>Fri, 07 Feb 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/02/07/sparql-or-conditions-where-union-query/</guid>
      <description>This is part 4 of my series of posts about querying the Wikidata API, in which I learn how to use SPARQL&amp;#8217;s UNION clause to handle an OR condition in a WHERE clause.
  Figure 1. Using SPARQL&amp;#8217;s UNION clause  But first, some context!
 After running queries against the Wikidata SPARQL API to pull the date of birth and nationality of tennis players into the Australian Open Graph, I noticed that several players hadn&amp;#8217;t actually been updated.</description>
    </item>
    
    <item>
      <title>Neo4j: Enriching an existing graph by querying the Wikidata SPARQL API</title>
      <link>https://markhneedham.com/blog/2020/02/04/neo4j-enriching-existing-graph-wikidata-sparql-api/</link>
      <pubDate>Tue, 04 Feb 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/02/04/neo4j-enriching-existing-graph-wikidata-sparql-api/</guid>
      <description>This is the third post in a series about querying Wikidata&amp;#8217;s SPARQL API. In the first post we wrote some basic queries, in the second we learnt about the SELECT and CONSTRUCT clauses, and in this post we&amp;#8217;re going to import query results into an existing Neo4j graph.
  Figure 1. Enriching a Neo4j Graph with Wikidata  Setting up Neo4j We&amp;#8217;re going to use the following Docker Compose configuration in this blog post:</description>
    </item>
    
    <item>
      <title>Neo4j: Cross database querying with Neo4j Fabric</title>
      <link>https://markhneedham.com/blog/2020/02/03/neo4j-cross-database-querying-fabric/</link>
      <pubDate>Mon, 03 Feb 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/02/03/neo4j-cross-database-querying-fabric/</guid>
      <description>A couple of weeks ago I wrote a QuickGraph blog post about the Australian Open, in which I showed how to use Neo4j 4.0&amp;#8217;s multi database feature.
 In that post we focused on queries that could be run on one database, but the 4.0 release also contains another feature for doing cross database querying - Neo4j Fabric - and we&amp;#8217;re going to learn how to use that in this post.</description>
    </item>
    
    <item>
      <title>Querying Wikidata: SELECT vs CONSTRUCT</title>
      <link>https://markhneedham.com/blog/2020/02/02/querying-wikidata-construct-select/</link>
      <pubDate>Sun, 02 Feb 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/02/02/querying-wikidata-construct-select/</guid>
      <description>In this blog post we&amp;#8217;re going to build upon the newbie&amp;#8217;s guide to querying Wikidata, and learn all about the CONSTRUCT clause.
  Figure 1. SPARQL&amp;#8217;s CONSTRUCT and SELECT clauses  In the newbie&amp;#8217;s guide, we wrote the following query to find a tennis player with the name &#34;Nick Kyrgios&#34; and return their date of birth:
 SELECT * WHERE { ?person wdt:P106 wd:Q10833314 ; rdfs:label &#39;Nick Kyrgios&#39;@en ; wdt:P569 ?</description>
    </item>
    
    <item>
      <title>Neo4j: Finding the longest path</title>
      <link>https://markhneedham.com/blog/2020/01/29/neo4j-finding-longest-path/</link>
      <pubDate>Wed, 29 Jan 2020 15:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/29/neo4j-finding-longest-path/</guid>
      <description>One on my favourite things about storing data in a graph database is executing path based queries against that data. I&amp;#8217;ve been trying to find a way to write such queries against the Australian Open QuickGraph, and in this blog post we&amp;#8217;re going to write what I think of as longest path queries against this graph.
  Figure 1. Finding longest paths in Neo4j  Setting up Neo4j We&amp;#8217;re going to use the following Docker Compose configuration in this blog post:</description>
    </item>
    
    <item>
      <title>A newbie&#39;s guide to querying Wikidata</title>
      <link>https://markhneedham.com/blog/2020/01/29/newbie-guide-querying-wikidata/</link>
      <pubDate>Wed, 29 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/29/newbie-guide-querying-wikidata/</guid>
      <description>After reading one of Jesús Barrasa&amp;#8217;s recent QuickGraph posts about enriching a knowledge graph with data from Wikidata, I wanted to learn how to query the Wikidata API so that I could pull in the data for my own QuickGraphs.
 I want to look up information about tennis players, and one of my favourite players is Nick Kyrgios, so this blog post is going to be all about him.</description>
    </item>
    
    <item>
      <title>Neo4j: Performing a database dump within a Docker container</title>
      <link>https://markhneedham.com/blog/2020/01/28/neo4j-database-dump-docker-container/</link>
      <pubDate>Tue, 28 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/28/neo4j-database-dump-docker-container/</guid>
      <description>Before the release of Neo4j 4.0, taking a dump of a database running within a Docker container was a tricky affair.
 We&amp;#8217;d need to stop the container and remove it, run the container again in bash mode, and finally take a dump of the database. With 4.0 things are simpler.
  Figure 1. Neo4j on Docker  We&amp;#8217;ll be using the following Docker Compose configuration in this blog post:</description>
    </item>
    
    <item>
      <title>Neo4j: Exporting a subset of data from one database to another</title>
      <link>https://markhneedham.com/blog/2020/01/27/neo4j-exporting-subset-database/</link>
      <pubDate>Mon, 27 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/27/neo4j-exporting-subset-database/</guid>
      <description>As part of the preparation for another blog post, I wanted to export a subset of data from one Neo4j database to another one, which seemed like a blog post in its own right.
  Figure 1. Exporting data using APOC&amp;#8217;s Export JSON  Setting up Neo4j We&amp;#8217;re going to use the following Docker Compose configuration in this blog post:
 Dockerfile version: &#39;3.7&#39; services: neo4j: image: neo4j:4.0.0-enterprise container_name: &#34;</description>
    </item>
    
    <item>
      <title>QuickGraph #5: Australian Open</title>
      <link>https://markhneedham.com/blog/2020/01/23/quick-graph-australian-open/</link>
      <pubDate>Thu, 23 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/23/quick-graph-australian-open/</guid>
      <description>It&amp;#8217;s time for another QuickGraph, this one based on data from the Australian Open tennis tournament. We&amp;#8217;re going to use data curated by Jeff Sackmann in the tennis_wta and tennis_atp repositories.
  Figure 1. Australian Open Graph (Background from https://www.freepik.com/free-photo/3d-network-background-with-connecting-lines-dots_3961382.htm)  Setting up Neo4j We&amp;#8217;re going to use the following Docker Compose configuration in this blog post:
 docker-compose.yml version: &#39;3.7&#39; services: neo4j: image: neo4j:4.0.0-enterprise container_name: &#34;quickgraph-aus-open&#34; volumes: - .</description>
    </item>
    
    <item>
      <title>Creating an Interactive UK Official Charts Data App with Streamlit and Neo4j</title>
      <link>https://markhneedham.com/blog/2020/01/16/interactive-uk-charts-quickgraph-neo4j-streamlit/</link>
      <pubDate>Thu, 16 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/16/interactive-uk-charts-quickgraph-neo4j-streamlit/</guid>
      <description>I recently came across Streamlit, a tool that makes it easy to build data based single page web applications. I wanted to give it a try, and the UK Charts QuickGraph that I recently wrote about seemed like a good opportunity for that.
 This blog post starts from where we left off. The data is loaded into Neo4j and we&amp;#8217;ve written some queries to explore different aspects of the dataset.</description>
    </item>
    
    <item>
      <title>Python: Altair - Setting the range of Date values for an axis</title>
      <link>https://markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</link>
      <pubDate>Tue, 14 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</guid>
      <description>In my continued experiments with the Altair visualisation library, I wanted to set a custom range of data values on the x axis of a chart. In this blog post we&amp;#8217;ll learn how to do that.
 We&amp;#8217;ll start where we left off in the last blog post, with the following code that renders a scatterplot containing the chart position of a song on a certain date:
 import altair as alt import pandas as pd import datetime df = pd.</description>
    </item>
    
    <item>
      <title>Python: Altair - TypeError: Object of type date is not JSON serializable</title>
      <link>https://markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</link>
      <pubDate>Fri, 10 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</guid>
      <description>I&amp;#8217;ve been playing with the Altair statistical visualisation library and recently ran into an error while trying to render a DataFrame that contained dates.
 I was trying to render a scatterplot containing the chart position of a song on a certain date, as seen in the code below:
 # pip install altair pandas import altair as alt import pandas as pd import datetime df = pd.DataFrame( [ {&#34;position&#34;: 2, &#34;</description>
    </item>
    
    <item>
      <title>QuickGraph #4: UK Official Singles Chart 2019</title>
      <link>https://markhneedham.com/blog/2020/01/04/quick-graph-uk-official-charts/</link>
      <pubDate>Sat, 04 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/04/quick-graph-uk-official-charts/</guid>
      <description>For our first QuickGraph of the new decade we&amp;#8217;re going to explore data from the Official UK Top 40 Chart. This chart ranks the top 100 songs of the week based on official sales of sales of downloads, CD, vinyl, audio streams and video streams. Every week BBC Radio 1 broadcast the top 40 songs, which explains the name of the chart.
  Figure 1. The Official UK Charts  Scraping the Official Charts I couldn&amp;#8217;t find a dump of the dataset, so we&amp;#8217;re going to use our web scraping skills again.</description>
    </item>
    
    <item>
      <title>Spotify API: Making my first call</title>
      <link>https://markhneedham.com/blog/2020/01/02/spotify-api-making-my-first-call/</link>
      <pubDate>Thu, 02 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/01/02/spotify-api-making-my-first-call/</guid>
      <description>I wanted to enrich the data for a little music application I&amp;#8217;m working on and realised it would be a perfect opportunity to try out the Spotify API. I want to extract data about individual tracks (via the Tracks API), but before we do that we&amp;#8217;ll need to create an app and have it approved for access to the Spotify API.
   Registering an application After logging into the Spotify Dashboard using my usual Spotify credentials, I was prompted to create an application:</description>
    </item>
    
  </channel>
</rss>