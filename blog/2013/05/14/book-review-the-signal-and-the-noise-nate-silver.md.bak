+++
draft = false
date="2013-05-14 00:16:56"
title="Book Review: The Signal and the Noise - Nate Silver"
tag=['books']
category=['Books']
+++

<p><a href="http://en.wikipedia.org/wiki/Nate_Silver">Nate Silver</a> is famous for having correctly predicted the winner of all 50 states in the 2012 United States elections and <a href="https://twitter.com/siddharthdawara">Sid</a> recommended <a href="http://www.amazon.co.uk/The-Signal-Noise-Science-Prediction/dp/0141975652/ref=sr_1_1?ie=UTF8&qid=1368486945&sr=8-1&keywords=nate+silver">his book</a> so I could learn more about statistics for the <a href="http://www.markhneedham.com/blog/2013/04/28/ab-testing-reporting/">A/B tests</a> that we were running.</p>


<p>I thought the book was a really good introduction to applied statistics and by using real life examples which most people would be able to relate to it makes a potentially dull subject interesting.</p>


<p>Reasonably early on the author points out that there's a difference between making a prediction and making a forecast:

<ul>
<li><strong>Prediction</strong> - a definitive and specific statement about when and where something will happen e.g. a major earthquake will hit Kyoto, Japan, on June 28.
</li>
<li><strong>Forecast</strong> - a probabilistic statement over a longer time scale e.g. there is a 60% chance of an earthquake in Southern California over the next 30 years.
</ul>

The book mainly focuses on the latter.
</p>


<p>We then move onto quite an interesting section about <strong>over fitting which is where we mistake noise for signal in our data</strong>.</p>


<p>I first came across this term when <a href="https://twitter.com/jennifersmithco">Jen</a> and I were working through one of the <a href="http://www.markhneedham.com/blog/tag/kaggle/">Kaggle</a> problems and were using a <a href="http://www.markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/">random forest</a> of deliberately over fitted Decision Trees to do digit recognition.</p>


<p>It's not a problem when we combine lots of decision trees together and use a majority wins algorithm to make our prediction but if we use just one of them its predictions on any new data will be completely wrong.</p>


<p>Later on in the book he points out that a lot of conspiracy theories come <strong>when we look at data retrospectively</strong> and can easily detect signal from noise in data when at the time it was much more difficult.</p>


<p>He also points out that sometimes there isn't actually any signal, it's all noise, and we can fall into the trap of looking for something that isn't there. I think this 'noise' is what we'd refer to as random variation in the context of an <a href="http://www.markhneedham.com/blog/2013/01/27/ab-testing-thoughts-so-far/">A/B test</a>.</p>


<p>Silver also encourages us to make sure that we understand the theory behind any inference we make:</p>


<blockquote>Statistical inferences are much stronger when backed up by theory or at least some deeper thinking about their root causes.</blockquote>

<p>When we were running A/B tests Sid encouraged people to <strong>think whether a theory about why conversion had changed made logical sense</strong> before assuming it was true which I think covers similar ground.</p>


<p>A big chunk of the book covers <a href="http://en.wikipedia.org/wiki/Bayes'_theorem">Bayes' theorem</a> and how often when we're making forecasts we have prior beliefs which it forces us to make explicit.</p>


<p>For example there is a section which talks about the probability a lady is being cheated on given that she's found some underwear that she doesn't recognise in her house.</p>


<p>In order to work out the probability she's being cheated on we need to know the probability that she was being cheated on before she found the underwear. Silver suggests that since 4% of married partners cheat on their spouses that would be a good number to use.</p>


<p>He then goes on to show multiple other problems throughout the book that we can apply Bayes' theorem to.</p>


<p>Some other interesting things I picked up are that <strong>if we're good at forecasting then being given more information should make our forecast better</strong> and that <strong>when we don't have any special information we're better off following the opinion of the crowd</strong>.</p>


<div style="float:right">
<img src="http://www.markhneedham.com/blog/wp-content/uploads/2013/05/IMG_20130514_011256.jpg" alt="IMG 20130514 011256" title="IMG_20130514_011256.jpg" border="0" width="400" />
</div>

<p>Silver also showed a clever trick for inferring data points on a data set which follows a power law i.e. the long tail distribution where there are very few massive events but lots of really small ones.</p>


<p>We have a power law distribution when modelling the number of terrorists attacks vs number of fatalities but if we <strong>change both scales to be logarithmic</strong> we can come up with a probability of how likely more deadly attacks are.</p>


<p>There is then some discussion of how we can make changes in the way that we treat terrorism to try and impact the shape of the chart e.g. in Israel Silver suggests that they really want to avoid a very deadly attack but at the expense of there being more smaller attacks.</p>


<p>A lot of the book is spent discussing weather/earthquake forecasting which is very interesting to read about but I couldn't quite see a link back to the software context.</p>


<p>Overall though I found it an interesting read although there are probably a few places that you can skim over the detail and still get the gist of what he's saying.</p>

