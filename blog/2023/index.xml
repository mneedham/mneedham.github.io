<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2023s on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/2023/</link>
    <description>Recent content in 2023s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 May 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/2023/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range[https://neo4j.com/docs/cypher-manual/current/functions/list/#functions-range] function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series[https://www.postgresql.org/docs/current/functions-srf.html] function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10) Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>Arc Browser: Building a plugin (Boost) with help from ChatGPT</title>
      <link>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</link>
      <pubDate>Tue, 23 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</guid>
      <description>I’ve been using the Arc Browser for a couple of months now and one of my favourite things is the simplicity of the plugin (or as they call it, &amp;#39;Boost&amp;#39;) functionality.
I wanted to port over a Chrome bookmark that I use to capture the podcasts that I’ve listened to on Player.FM. In this blog post I’ll show how ChatGPT helped me convert the bookmark code to an Arc Boost.</description>
    </item>
    
    <item>
      <title>Venkat - An inline code snippet execution extension for VS Code (Made in GPT-4)</title>
      <link>https://www.markhneedham.com/blog/2023/05/17/venkat-inline-code-snippet-execution-vs-code-execution/</link>
      <pubDate>Wed, 17 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/17/venkat-inline-code-snippet-execution-vs-code-execution/</guid>
      <description>(Co-authored with Michael Hunger)
Venkat Subramaniam is a legendary speaker on the tech conference circuit whose presentations are famous for executing arbitrary code snippets and showing the results as a tooltip directly in the editor. This makes it really great for videos or talks as you don’t need a second output terminal to run your code and you can just continue explaining what you’re doing. The results go away afterwards, so you don’t need to worry about that.</description>
    </item>
    
    <item>
      <title>Cropping a video using FFMPEG</title>
      <link>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</link>
      <pubDate>Mon, 15 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</guid>
      <description>I needed to crop a video that I used as part of a video on my YouTube channel, Learn Data With Mark, and Camtasia kept rendering a black screen. So I had to call for FFMPEG!
My initial video was 2160 x 3840 but I didn’t need the bottom 1920 pixels because I’m using that part of the screen for a video of me. If I was just rendering that video exactly as it is I wouldn’t have bothered cropping it, but because I wanted to zoom into different sections I needed to crop out the bottom bit.</description>
    </item>
    
    <item>
      <title>Python: Naming slices</title>
      <link>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</link>
      <pubDate>Sat, 13 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</guid>
      <description>Another gem from Fluent Python is that you can name slices. How did I not know that?!
Let’s have a look how it works using an example of a Vehicle Identification Number, which has 17 characters that act as a unique identifier for a vehicle. Different parts of that string mean different things.
So given the following VIN:
vin = &amp;#34;2B3HD46R02H210893&amp;#34; We can extract components like this:
print(f&amp;#34;&amp;#34;&amp;#34; World manufacturer identifier: {vin[0:3]} Vehicle Descriptor: {vin[3:9]} Vehicle Identifier: {vin[9:17]} &amp;#34;&amp;#34;&amp;#34;.</description>
    </item>
    
    <item>
      <title>Python 3.10: Pattern matching with match/case</title>
      <link>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</link>
      <pubDate>Tue, 09 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</guid>
      <description>I’ve been reading Fluent Python and learnt about pattern matching with the match/case statement, introduced in Python 3.10. You can use it instead of places where you’d otherwise use if, elif, else statements.
I created a small example to understand how it works. The following function takes in a list where the first argument should be foo, followed by a variable number of arguments, which we print to the console:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Get decade from date</title>
      <link>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</link>
      <pubDate>Thu, 20 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</guid>
      <description>Working with dates in SQL can sometimes be a bit tricky, especially when you need to extract specific information, like the decade a date belongs to. In this blog post, we’ll explore how to easily obtain the decade from a date using DuckDB, a lightweight and efficient SQL database engine.
First, install DuckDB and launch it:
./duckdb Next, we’re going to create a movies table that has columns for title and releaseDate:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert epoch to timestamp with timezone</title>
      <link>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</link>
      <pubDate>Wed, 05 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</guid>
      <description>I’ve been playing around with the Citi Bike Stations dataset on Kaggle with DuckDB and ran into trouble when trying to convert a column containing epoch timestamps to a timestamp with timezone support. In this blog we’ll learn how to do that, which will at least be helpful to future me, if noone else!
The dataset contains 4GB worth of CSV files, but I’ve just downloaded a few of them manually for now.</description>
    </item>
    
    <item>
      <title>Tennis Head to Head with DuckDB and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</link>
      <pubDate>Fri, 31 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</guid>
      <description>In this blog post we’re going to learn how to build an application to compare the matches between two ATP tennis players. DuckDB and Streamlit will be our partners in crime for this mission.
Set up To get started, let’s create a virtual environment:
python -m venv .venv source .venv/bin/activate And now install some libraries:
pip install duckdb streamlit streamlit-searchbox And now let’s open a file, app.py and import the packages:</description>
    </item>
    
    <item>
      <title>DuckDB/Python: Cannot combine LEFT and RIGHT relations of different connections!</title>
      <link>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</link>
      <pubDate>Mon, 20 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</guid>
      <description>I’ve been playing around with DuckDB over the weekend and ran into an interesting problem while using the Relational API in the Python package. We’re going to explore that in this blog post.
Set up To get started, let’s install DuckDB:
pip install duckdb And now let’s open a Python shell and import the package:
import duckdb Next, let’s create a DuckDB connection and import the httpfs module, which we’ll use in just a minute:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Geospatial - java.nio.BufferUnderflowException: null</title>
      <link>https://www.markhneedham.com/blog/2023/03/10/apache-pinot-geospatial-buffer-underflow/</link>
      <pubDate>Fri, 10 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/10/apache-pinot-geospatial-buffer-underflow/</guid>
      <description>I’ve been working on a blog post showing how to use Geospatial indexes in Apache Pinot and ran into an interesting exception that I’ll explain in this blog post.
Set up But first, let’s take a look at the structure of the data that I’m ingesting from Apache Kafka. Below is an example of one of those events:
{ &amp;#34;trainCompany&amp;#34;: &amp;#34;London Overground&amp;#34;, &amp;#34;atocCode&amp;#34;: &amp;#34;LO&amp;#34;, &amp;#34;lat&amp;#34;: 51.541615, &amp;#34;lon&amp;#34;: -0.122528896, &amp;#34;ts&amp;#34;: &amp;#34;2023-03-10 11:35:20&amp;#34;, &amp;#34;trainId&amp;#34;: &amp;#34;202303107145241&amp;#34; } As you’ve probably guessed, I’m importing the locations of trains in the UK.</description>
    </item>
    
    <item>
      <title>DuckDB: Join based on maximum value in other table</title>
      <link>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</link>
      <pubDate>Wed, 01 Feb 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</guid>
      <description>In this blog post we’re going to learn how to write a SQL query to join two tables where one of the tables has multiple rows for each key. We want to select only the rows that contain the most recent (or maximum) value from that table.
Our story begins with a YouTube video that I created showing how to query the European Soccer SQLite database with DuckDB. This database contains lots of different tables, but we are only interested in Player and Player_Attributes.</description>
    </item>
    
    <item>
      <title>Flink SQL: Could not execute SQL statement. Reason: java.io.IOException: Corrupt Debezium JSON message</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</guid>
      <description>As part of a JFokus workshop that I’m working on I wanted to create a Flink table around a Kafka stream that I’d populated from MySQL with help from Debezium. In this blog post I want to show how to do this and explain an error that I encountered along the way.
To start, we have a products table in MySQL that’s publishing events to Apache Kafka. We can see the fields in this event by running the following command:</description>
    </item>
    
    <item>
      <title>Flink SQL: Exporting nested JSON to a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</guid>
      <description>I’ve been playing around with Flink as part of a workshop that I’m doing at JFokus in a couple of weeks and I wanted to export some data from Flink to Apache Kafka in a nested format. In this blog post we’ll learn how to do that.
Setup We’re going to be using the following Docker Compose config:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:latest container_name: zookeeper hostname: zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOO_MY_ID: 1 ZOO_PORT: 2181 ZOO_SERVERS: server.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
