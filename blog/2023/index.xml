<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2023s on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/2023/</link>
    <description>Recent content in 2023s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Mar 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/2023/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB/Python: Cannot combine LEFT and RIGHT relations of different connections!</title>
      <link>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</link>
      <pubDate>Mon, 20 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</guid>
      <description>I’ve been playing around with DuckDB over the weekend and ran into an interesting problem while using the Relational API in the Python package. We’re going to explore that in this blog post.
Set up To get started, let’s install DuckDB:
pip install duckdb And now let’s open a Python shell and import the package:
import duckdb Next, let’s create a DuckDB connection and import the httpfs module, which we’ll use in just a minute:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Geospatial - java.nio.BufferUnderflowException: null</title>
      <link>https://www.markhneedham.com/blog/2023/03/10/apache-pinot-geospatial-buffer-underflow/</link>
      <pubDate>Fri, 10 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/10/apache-pinot-geospatial-buffer-underflow/</guid>
      <description>I’ve been working on a blog post showing how to use Geospatial indexes in Apache Pinot and ran into an interesting exception that I’ll explain in this blog post.
Set up But first, let’s take a look at the structure of the data that I’m ingesting from Apache Kafka. Below is an example of one of those events:
{ &amp;#34;trainCompany&amp;#34;: &amp;#34;London Overground&amp;#34;, &amp;#34;atocCode&amp;#34;: &amp;#34;LO&amp;#34;, &amp;#34;lat&amp;#34;: 51.541615, &amp;#34;lon&amp;#34;: -0.122528896, &amp;#34;ts&amp;#34;: &amp;#34;2023-03-10 11:35:20&amp;#34;, &amp;#34;trainId&amp;#34;: &amp;#34;202303107145241&amp;#34; } As you’ve probably guessed, I’m importing the locations of trains in the UK.</description>
    </item>
    
    <item>
      <title>DuckDB: Join based on maximum value in other table</title>
      <link>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</link>
      <pubDate>Wed, 01 Feb 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</guid>
      <description>In this blog post we’re going to learn how to write a SQL query to join two tables where one of the tables has multiple rows for each key. We want to select only the rows that contain the most recent (or maximum) value from that table.
Our story begins with a YouTube video that I created showing how to query the European Soccer SQLite database with DuckDB. This database contains lots of different tables, but we are only interested in Player and Player_Attributes.</description>
    </item>
    
    <item>
      <title>Flink SQL: Could not execute SQL statement. Reason: java.io.IOException: Corrupt Debezium JSON message</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</guid>
      <description>As part of a JFokus workshop that I’m working on I wanted to create a Flink table around a Kafka stream that I’d populated from MySQL with help from Debezium. In this blog post I want to show how to do this and explain an error that I encountered along the way.
To start, we have a products table in MySQL that’s publishing events to Apache Kafka. We can see the fields in this event by running the following command:</description>
    </item>
    
    <item>
      <title>Flink SQL: Exporting nested JSON to a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</guid>
      <description>I’ve been playing around with Flink as part of a workshop that I’m doing at JFokus in a couple of weeks and I wanted to export some data from Flink to Apache Kafka in a nested format. In this blog post we’ll learn how to do that.
Setup We’re going to be using the following Docker Compose config:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:latest container_name: zookeeper hostname: zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOO_MY_ID: 1 ZOO_PORT: 2181 ZOO_SERVERS: server.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
