<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2023s on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/2023/</link>
    <description>Recent content in 2023s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jan 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/2023/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink SQL: Exporting nested JSON to a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</guid>
      <description>I’ve been playing around with Flink as part of a workshop that I’m doing at JFokus in a couple of weeks and I wanted to export some data from Flink to Apache Kafka in a nested format. In this blog post we’ll learn how to do that.
Setup We’re going to be using the following Docker Compose config:
docker-compose.yml
version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:latest container_name: zookeeper hostname: zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOO_MY_ID: 1 ZOO_PORT: 2181 ZOO_SERVERS: server.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink/</guid>
      <description>CREATE TABLE EnrichedOrderItems ( `orderId` STRING, `createdAt` STRING, `orderItem` STRING, `product` STRING ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;upsert-kafka&amp;#39;, &amp;#39;topic&amp;#39; = &amp;#39;enriched-order-items&amp;#39;, &amp;#39;properties.bootstrap.servers&amp;#39; = &amp;#39;kafka:9092&amp;#39;, &amp;#39;properties.group.id&amp;#39; = &amp;#39;testGroup&amp;#39;, &amp;#39;value.format&amp;#39; = &amp;#39;json&amp;#39;, &amp;#39;key.format&amp;#39; = &amp;#39;json&amp;#39;, &amp;#39;key.fields&amp;#39; = &amp;#39;orderId&amp;#39; ); [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.api.TableException: Table sink &amp;#39;default_catalog.default_database.EnrichedOrderItems&amp;#39; doesn’t support consuming update and delete changes which is produced by node Join(joinType=[InnerJoin], where=[(id0 = productId)], select=[id, createdAt, productId, quantity, price0, id0, name, description, category, price, image], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])</description>
    </item>
    
  </channel>
</rss>
