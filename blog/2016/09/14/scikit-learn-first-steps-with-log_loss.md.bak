+++
draft = false
date="2016-09-14 05:33:38"
title="scikit-learn: First steps with log_loss"
tag=['scikit-learn']
category=['Machine Learning', 'Python']
+++

<p>
Over the last week I've spent a little bit of time playing around with the data in the <a href="https://www.kaggle.com/c/talkingdata-mobile-user-demographics/details/evaluation">Kaggle TalkingData Mobile User Demographics</a> competition, and came across <a href="https://www.kaggle.com/dvasyukova/talkingdata-mobile-user-demographics/a-linear-model-on-apps-and-labels/comments">a notebook</a> written by <a href="https://www.kaggle.com/dvasyukova">dune_dweller</a> showing how to run a logistic regression algorithm on the dataset. 
</p>


<p>
The metric used to evaluate the output in this competition is <a href="https://www.kaggle.com/c/talkingdata-mobile-user-demographics/details/evaluation">multi class logarithmic loss</a>, which is implemented by the <cite><a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">log_loss</a></cite> function in the <a href="http://scikit-learn.org/stable/index.html">scikit-learn</a> library.
</p>


<p>
I've not used it before so I created a small example to get to grips with it.
</p>


<p>
Let's say we have 3 rows to predict and we happen to know that they should be labelled 'bam', 'spam', and 'ham' respectively:
</p>



~~~python

>>> actual_labels = ["bam", "ham", "spam"]
~~~
<P>
To work out the log loss score we need to make a prediction for what we think each label actually is. We do this by passing an array containing a probability between 0-1 for each label</p>


<p>e.g. if we think the first label is definitely 'bam' then we'd pass <cite>[1, 0, 0]</cite>, whereas if we thought it had a 50-50 chance of being 'bam' or 'spam' then we might pass <cite>[0.5, 0, 0.5]</cite>. As far as I can tell the values get sorted into (alphabetical) order so we need to provide our predictions in the same order.
</p>


<p>Let's give it a try. First we'll import the function:</p>



~~~python

>>> from sklearn.metrics import log_loss
~~~

<p>
Now let's see what score we get if we make a perfect prediction:
</p>



~~~python

>>> log_loss(actual_labels,  [[1, 0, 0], [0, 1, 0], [0, 0, 1]])
2.1094237467877998e-15
~~~

<p>
What about if we make a completely wrong prediction?
</p>



~~~python

>>> log_loss(actual_labels,  [[0, 0, 1], [1, 0, 0], [0, 1, 0]])
34.538776394910684
~~~

<p>We can reverse engineer this score to work out the <a href="http://stackoverflow.com/questions/35013822/log-loss-output-is-greater-than-1">probability that we've predicted the correct class</a>.</p>


<blockquote>
If we look at the case where the average log loss exceeds 1, it is when log(pij) < -1 when i is the true class. 

This means that the predicted probability for that given class would be less than exp(-1) or around 0.368. 

So, seeing a log loss greater than one can be expected in the cass that that your model only gives less than a 36% probability estimate for the correct class.
</blockquote>
<div>

<p>This is the formula of logloss:</p>


<div>
<img src="http://www.markhneedham.com/blog/wp-content/uploads/2016/09/NEmt7.png" alt="NEmt7" title="NEmt7.png" border="0" width="168" height="39" />
</div>

<p>In which y<sub>ij</sub> is 1 for the correct class and 0 for other classes and p<sub>ij</sub> is the probability assigned for that class.
</p>


<p>The interesting thing about this formula is that we only care about the correct class. The y<sub>ij</sub> value of 0 cancels out the wrong classes.</p>


<p>
In our two examples so far we actually already know the probability estimate for the correct class - 100% in the first case and 0% in the second case, but we can plug in the numbers to check we end up with the same result.
</p>


<p>First we need to work out what value would have been passed to the log function which is easy in this case. The value of y<sub>ij</sub> is </p>



~~~python

# every prediction exactly right
>>> math.log(1)
0.0

>>> math.exp(0)
1.0
~~~


~~~python

# every prediction completely wrong
>>> math.log(0.000000001)
-20.72326583694641

>>> math.exp(-20.72326583694641)
1.0000000000000007e-09
~~~

<p>
I used a really small value instead of 0 in the second example because <cite>math.log(0)</cite> <a href="http://www.rapidtables.com/math/algebra/logarithm/Logarithm_of_0.htm">trends towards negative infinity</a>.
</p>


<p>Let's try another example where we have less certainty:</p>



~~~python

>>> print log_loss(actual_labels, [[0.8, 0.1, 0.1], [0.3, 0.6, 0.1], [0.15, 0.15, 0.7]])
0.363548039673
~~~

<p>
We'll have to do a bit more work to figure out what value was being passed to the log function this time, but not too much. This is roughly the calculation being performed:  
</p>



~~~python

# 0.363548039673 = -1/3 * (log(0.8) + log(0.6) + log(0.7)

>>> print log_loss(actual_labels,  [[0.8, 0.1, 0.1], [0.3, 0.6, 0.1], [0.15, 0.15, 0.7]])
0.363548039673
~~~

<p>
In this case, on average our probability estimate would be:
</p>



~~~python

# we put in the negative value since we multiplied by -1/N
>>> math.exp(-0.363548039673)
0.6952053289772744
~~~

<p>
We had 60%, 70%, and 80% accuracy for our 3 labels so an overall probability of 69.5% seems about right. 
</p>


<p>One more example. This time we'll make one more very certain (90%) prediction for 'spam':</p>



~~~python

>>> print log_loss(["bam", "ham", "spam", "spam"], [[0.8, 0.1, 0.1], [0.3, 0.6, 0.1], [0.15, 0.15, 0.7], [0.05, 0.05, 0.9]])
0.299001158669

>>> math.exp(-0.299001158669)
0.741558550213609
~~~

<p>
74% accuracy overall, sounds about right!
</p>

