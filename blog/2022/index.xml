<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2022s on Mark Needham</title>
    <link>http://localhost:8000/blog/2022/</link>
    <description>Recent content in 2022s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Dec 2022 02:44:37 +0000</lastBuildDate><atom:link href="http://localhost:8000/blog/2022/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kcat/jq: Reached end of topic at offset: exiting</title>
      <link>http://localhost:8000/blog/2022/12/06/kcat-jq-reached-end-of-topic-exiting/</link>
      <pubDate>Tue, 06 Dec 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/12/06/kcat-jq-reached-end-of-topic-exiting/</guid>
      <description>I’ve recently been working with Debezium to get the Pizza Shop product catalogue from MySQL into Apache Kafka and ran into an issue when querying the resulting stream using kcat and jq. In this blog I’ll show how I worked around that problem.
I configured Debezium to write any changes to the products table into the mysql.pizzashop.products topic. I then queriesthis topic to find the changes for just one of the products:</description>
    </item>
    
    <item>
      <title>Python: Sorting lists of dictionaries with sortedcontainers</title>
      <link>http://localhost:8000/blog/2022/12/02/python-sorting-lists-dictionaries-sortedcontainers/</link>
      <pubDate>Fri, 02 Dec 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/12/02/python-sorting-lists-dictionaries-sortedcontainers/</guid>
      <description>I was recently working on a Kafka streams data generator, where I only wanted to publish events once the time on those events had been reached. To solve this problem I needed a sorted list and in this blog post we’re going to explore how I went about doing this.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Blogging for Google: Why I write about error messages</title>
      <link>http://localhost:8000/blog/2022/11/22/blogging-for-google-error-messages/</link>
      <pubDate>Tue, 22 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/22/blogging-for-google-error-messages/</guid>
      <description>Writing blog posts that aim to go viral on social media is a well-known content strategy, but in this post, I want to persuade you that you should blog for Google as well.
Blog for Google? What does blogging for Google even mean?
The easiest demonstration is to look at a screenshot from the Google Console Insights report I was sent last week. This section of the report shows the most used search terms that result in someone ending up on my blog.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Inserts from SQL - Unable to get tasks states map - ClassNotFoundException: &#39;org.apache.pinot.plugin.filesystem.S3PinotFS&#39;</title>
      <link>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-classnotfoundexception-s3pinotfs/</link>
      <pubDate>Fri, 18 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-classnotfoundexception-s3pinotfs/</guid>
      <description>I recently wrote a post on the StarTre blog describing the inserts from SQL feature that was added in Apache Pinot 0.11, and while writing it I came across some interesting exceptions due to configuration mistakes I’d made. In this post we’re going to describe one of those exceptions.
To recap, I was trying to ingest a bunch of JSON files from an S3 bucket using the following SQL query:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Inserts from SQL - Unable to get tasks states map - No task is generated for table</title>
      <link>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-no-task-generated-for-table/</link>
      <pubDate>Fri, 18 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-no-task-generated-for-table/</guid>
      <description>I recently wrote a post on the StarTre blog describing the inserts from SQL feature that was added in Apache Pinot 0.11, and while writing it I came across some interesting exceptions due to configuration mistakes I’d made. In this post we’re going to describe one of those exceptions.
To recap, I was trying to ingest a bunch of JSON files from an S3 bucket using the following SQL query:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Inserts from SQL - Unable to get tasks states map - NullPointerException</title>
      <link>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-nullpointerexception/</link>
      <pubDate>Fri, 18 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/18/apache-pinot-inserts-sql-unable-get-tasks-states-map-nullpointerexception/</guid>
      <description>I recently wrote a post on the StarTre blog describing the inserts from SQL feature that was added in Apache Pinot 0.11, and while writing it I came across some interesting exceptions due to configuration mistakes I’d made. In this post we’re going to describe one of those exceptions.
To recap, I was trying to ingest a bunch of JSON files from an S3 bucket using the following SQL query:</description>
    </item>
    
    <item>
      <title>Diffing Apache Parquet schemas with DuckDB</title>
      <link>http://localhost:8000/blog/2022/11/17/duckdb-diff-parquet-schema/</link>
      <pubDate>Thu, 17 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/17/duckdb-diff-parquet-schema/</guid>
      <description>I’ve been playing around with DuckDB, the new hotness in the analytics space, over the last month, and my friend Michael Hunger asked whether you could use it to compute a diff of Apache Parquet schemas.
Challenge accepted!
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Unable to render templates on ingestion job spec template file</title>
      <link>http://localhost:8000/blog/2022/11/14/apache-pinot-unable-to-render-templates/</link>
      <pubDate>Mon, 14 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/14/apache-pinot-unable-to-render-templates/</guid>
      <description>I was recently trying to ingest some JSON files into Apache Pinot from an S3 bucket and came across an exception when trying to pass a variable to the LaunchDataIngestionJob command
I was using the following ingestion job specification:
config/job-spec.yml executionFrameworkSpec: name: &amp;#39;standalone&amp;#39; segmentGenerationJobRunnerClassName: &amp;#39;org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner&amp;#39; segmentTarPushJobRunnerClassName: &amp;#39;org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner&amp;#39; segmentUriPushJobRunnerClassName: &amp;#39;org.apache.pinot.plugin.ingestion.batch.standalone.SegmentUriPushJobRunner&amp;#39; jobType: SegmentCreationAndTarPush inputDirURI: &amp;#39;s3://marks-st-cloud-bucket/events/&amp;#39; includeFileNamePattern: &amp;#39;glob:**/*.json&amp;#39; outputDirURI: &amp;#39;/data&amp;#39; overwriteOutput: true pinotFSSpecs: - scheme: s3 className: org.apache.pinot.plugin.filesystem.S3PinotFS configs: region: &amp;#39;eu-west-2&amp;#39; - scheme: file className: org.</description>
    </item>
    
    <item>
      <title>Java: FileSystems.getDefault().getPathMatcher: IllegalArgumentException</title>
      <link>http://localhost:8000/blog/2022/11/11/java-file-systems-path-matcher/</link>
      <pubDate>Fri, 11 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/11/11/java-file-systems-path-matcher/</guid>
      <description>I was debugging something in the Apache Pinot code earlier this week and came across the FileSystems.getDefault().getPathMatcher function, which didn’t work quite how I expected.
The function creates a PathMatcher that you can use to match against Paths. I was passing through a value of *.json, which was then resulting in code similar to this:
import java.nio.file.FileSystems; import java.nio.file.Path; import java.nio.file.PathMatcher; class Main { public static void main(String args[]) { PathMatcher matcher = FileSystems.</description>
    </item>
    
    <item>
      <title>Vercel: Redirect wildcard (nested) paths</title>
      <link>http://localhost:8000/blog/2022/07/27/vercel-redirect-wildcards-nested-paths/</link>
      <pubDate>Wed, 27 Jul 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/07/27/vercel-redirect-wildcards-nested-paths/</guid>
      <description>We’re deploying the StarTree developer site, dev.startree.ai, to Vercel, and recently needed to do some redirects of a few pages. I initially added individual redirects for each page, but there were eventually too many pages and I wanted to automate it. In this post we’ll learn how to do that.
Figure 1. Vercel: Redirect wildcard (nested) paths We wanted to redirect everything under https://dev.startree.ai/docs/thirdeye to https://dev.startree.ai/docs/startree-enterprise-edition/startree-thirdeye/ and started off by using wild card path matching, as seen in the vercel.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Import JSON data from a CSV file - Illegal Json Path: $[&#39;id&#39;] does not match document</title>
      <link>http://localhost:8000/blog/2022/07/21/apache-pinot-import-json-data-csv-file-illegal-json-path-does-not-match-document/</link>
      <pubDate>Thu, 21 Jul 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/07/21/apache-pinot-import-json-data-csv-file-illegal-json-path-does-not-match-document/</guid>
      <description>I’ve been working on an Apache Pinot dataset where I ingested a JSON document stored in a CSV file. I made a mistake with the representation of the JSON and it took me a while to figure out what I’d done wrong.
We’ll go through it in this blog post.
Figure 1. Apache Pinot: Import JSON data from a CSV file - Illegal Json Path: $[&amp;#39;id&amp;#39;] does not match document Setup We’re going to spin up a local instance of Pinot and Kafka using the following Docker compose config:</description>
    </item>
    
    <item>
      <title>Docusaurus: Side menu on custom page</title>
      <link>http://localhost:8000/blog/2022/07/11/docusaurus-side-menu-custom-page/</link>
      <pubDate>Mon, 11 Jul 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/07/11/docusaurus-side-menu-custom-page/</guid>
      <description>I’ve been working with Docusaurus to build the dev.startree.ai website over the last few months and I wanted to add a custom page with a sidebar similar to the one that gets automatically generated on documentation pages.
All the examples I could find showed you to create a splash page, so it took me a while to figure out how to do what I wanted, but in this post we’ll learn how to do it.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Skipping periodic task: Task: PinotTaskManager</title>
      <link>http://localhost:8000/blog/2022/06/23/apache-pinot-skipping-periodic-task-pinot-task-manager/</link>
      <pubDate>Thu, 23 Jun 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/06/23/apache-pinot-skipping-periodic-task-pinot-task-manager/</guid>
      <description>As I mentioned in my last blog post, I’ve been working on an Apache Pinot recipe showing how to ingest data from S3 and after I’d got that working I moved onto using the SegmentGenerationAndPushTask to poll S3 and ingest files automatically.
It took me longer than it should have to get this working and hopefully this blog post will help you avoid the problems that I had.
Figure 1.</description>
    </item>
    
    <item>
      <title>docker exec: Passing in environment variables</title>
      <link>http://localhost:8000/blog/2022/06/16/docker-exec-environment-variables/</link>
      <pubDate>Thu, 16 Jun 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/06/16/docker-exec-environment-variables/</guid>
      <description>I’ve been working on an Apache Pinot recipe showing how to ingest data from S3 and I needed to pass in my AWS credentials to the docker exec command that I was running. It wasn’t difficult to do, but took me a little while to figure out.
Figure 1. docker exec: Passing in environment variables The command that I was running looked like this:
docker exec \ -it pinot-controller bin/pinot-admin.</description>
    </item>
    
    <item>
      <title>Dash: Configurable dcc.Interval</title>
      <link>http://localhost:8000/blog/2022/04/23/dash-configurable-dcc-interval/</link>
      <pubDate>Sat, 23 Apr 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/04/23/dash-configurable-dcc-interval/</guid>
      <description>As I mentioned in my blog post about building a Real-Time Crypto Dashboard, I’ve recently been working with the Dash low-code framework for building interactive data apps.
I was using the dcc.Interval component to automatically refresh components on the page and wanted to make the refresh interval configurable. In this blog post we’ll learn how to do that.
Figure 1. Dash: Configurable dcc.Interval Setup Let’s first setup our Python environment:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Speeding up queries with IdSets</title>
      <link>http://localhost:8000/blog/2022/04/08/apache-pinot-speeding-up-queries-id-set/</link>
      <pubDate>Fri, 08 Apr 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/04/08/apache-pinot-speeding-up-queries-id-set/</guid>
      <description>As I continue to build an Apache Pinot demo using CryptoWatch data, I found myself needing to optimise some queries so that the real-time dashboard would render more quickly. I did this using IdSets and in this blog post we’ll learn about those and how to use them.
Figure 1. Apache Pinot: Speeding up queries with IdSets Pinot Schema For the purpose of this blog post we don’t need to know how to configure the Pinot schema and tables, but we do need to know that we’re working with trades and pairs tables, whose schemas are shown below:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Lookup Join - 500 Error - Unsupported function: lookup with 4 parameters</title>
      <link>http://localhost:8000/blog/2022/04/05/apache-pinot-lookup-join-internal-error-unsupported-function/</link>
      <pubDate>Tue, 05 Apr 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/04/05/apache-pinot-lookup-join-internal-error-unsupported-function/</guid>
      <description>I’m currently working on an Apache Pinot demo using data from Crypto Watch, in which I was using the lookup function and had a bug in my query that didn’t return the clearest error message. In this blog post we’ll have a look at the query and how to fix it.
Figure 1. Apache Pinot: Lookup Join - 500 Error - Unsupported function: lookup with 4 parameters The query that I was writing was using the lookup function to return the name of the base asset in a transaction:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Failed to generate segment - Input path {} does not exist</title>
      <link>http://localhost:8000/blog/2022/03/17/apache-pinot-failed-to-generated-segment-input-path-does-not-exist/</link>
      <pubDate>Thu, 17 Mar 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/03/17/apache-pinot-failed-to-generated-segment-input-path-does-not-exist/</guid>
      <description>In this blog post we’re going to learn how to work around a bug when trying to ingest CSV files with the same name into Apache Pinot. I came across this issue while writing a recipe showing how to import data files from different directories.
Figure 1. Apache Pinot: Failed to generate segment - Input path {} does not exist Setup We’re going to spin up a local instance of Pinot and Kafka using the following Docker compose config:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Deleting instances in a bad state</title>
      <link>http://localhost:8000/blog/2022/02/21/apache-pinot-delete-instances-bad-state/</link>
      <pubDate>Mon, 21 Feb 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/02/21/apache-pinot-delete-instances-bad-state/</guid>
      <description>Sometimes when I start up a local Pinot cluster after doing a hard shutdown (by restarting my computer) I noticed that the Pinot Data Explorer shows controllers, brokers, or servers in a bad state. In this blog post we’ll see how to get rid of those bad instances.
Figure 1. Apache Pinot: Deleting instances in a bad state The screenshot below shows several instances in the bad state.
Figure 2.</description>
    </item>
    
    <item>
      <title>Streamlit: Overwrite previous value in a loop</title>
      <link>http://localhost:8000/blog/2022/02/19/streamlit-overwrite-previous-value-loop/</link>
      <pubDate>Sat, 19 Feb 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/02/19/streamlit-overwrite-previous-value-loop/</guid>
      <description>I was recently building a Streamlit app in which I was looping through a stream of values and wanted to only print out the most recent value. In this blog post we’ll learn how to do that.
Figure 1. Streamlit: Overwrite previous value in a loop Setup If you want to play along you’ll need to create a virtual environment and install Streamlit:
python -m venv env source venv/bin/activate pip install streamlit Streamlit App Now, create a file app.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Resetting a segment after an invalid JSON Transformation</title>
      <link>http://localhost:8000/blog/2022/01/31/pinot-resetting-segment-invalid-json-transformation/</link>
      <pubDate>Mon, 31 Jan 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/31/pinot-resetting-segment-invalid-json-transformation/</guid>
      <description>I recently had a typo in a Pinot ingestion transformation function and wanted to have Pinot re-process the Kafka stream without having to restart all the things. In this blog post we’ll learn how to do that.
Figure 1. Apache Pinot: Resetting a segment after an invalid JSON Transformation Setup We’re going to spin up a local instance of Pinot and Kafka using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.</description>
    </item>
    
    <item>
      <title>Kafka: Writing data to a topic from the command line</title>
      <link>http://localhost:8000/blog/2022/01/22/kafka-writing-data-topic-command-line/</link>
      <pubDate>Sat, 22 Jan 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/22/kafka-writing-data-topic-command-line/</guid>
      <description>I’ve been doing more Apache Pinot documentation - this time covering the JSON functions - and I needed to quickly write some data into Kafka to test things out. I’d normally do that using the Python Kafka client, but this time I wanted to do it using only command line tools. So that’s what we’ll be doing in this blog post and it’s more for future me than anyone else!</description>
    </item>
    
    <item>
      <title>Apache Pinot: Sorted indexes on real-time tables</title>
      <link>http://localhost:8000/blog/2022/01/20/apache-pinot-sorted-indexes-realtime-tables/</link>
      <pubDate>Thu, 20 Jan 2022 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/20/apache-pinot-sorted-indexes-realtime-tables/</guid>
      <description>I’ve recently been learning all about Apache Pinot’s sorted forward indexes, and in my first blog post I explained how they work for offline tables. In this blog post we’ll learn how sorted indexes work with real-time tables.
Figure 1. Apache Pinot: Sorted indexes on real-time tables Launch Components We’re going to spin up a local instance of Pinot and Kafka using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Sorted indexes on offline tables</title>
      <link>http://localhost:8000/blog/2022/01/19/apache-pinot-sorted-indexes-offline-tables/</link>
      <pubDate>Wed, 19 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/19/apache-pinot-sorted-indexes-offline-tables/</guid>
      <description>I’ve recently been learning all about Apache Pinot’s sorted forward indexes. I was initially going to explain how they work for offline and real-time tables, but the post got a bit long, so instead we’ll have two blog posts. In this one we’ll learn how sorted indexes are applied for offline tables.
Figure 1. Apache Pinot: Sorted indexes on offline tables Launch Components We’re going to spin up a local instance of Pinot using the following Docker compose config:</description>
    </item>
    
    <item>
      <title>Strava: Export and interpolate lat/long points for an activity</title>
      <link>http://localhost:8000/blog/2022/01/18/strava-export-interpolate-lat-long-points-activity/</link>
      <pubDate>Tue, 18 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/18/strava-export-interpolate-lat-long-points-activity/</guid>
      <description>I’ve been working with Strava data again recently and wanted to extract all the lat/long coordinates recorded for my runs. Having done this, I realised that my running watch hadn’t recorded as many points as I expected, so I needed to interpolate the missing points. In this blog post we’ll learn how to do that.
Figure 1. Strava: Export and interpolate lat/long points for an activity Setup Let’s first install a few libraries that we’ll be using:</description>
    </item>
    
    <item>
      <title>Python: Generate WKT from Lat Long Coordinates</title>
      <link>http://localhost:8000/blog/2022/01/14/python-generate-wkt-lat-long-coordinates/</link>
      <pubDate>Fri, 14 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/14/python-generate-wkt-lat-long-coordinates/</guid>
      <description>Recently I’ve been playing around with geometry objects in WKT format while documenting Apache Pinot’s Geospatial functions. I then wanted to figure out how to generate a WKT string from a list of lat long coordinates, which we’ll learn how to do in this blog post.
Figure 1. Python: Generate WKT from Lat Long Coordinates We’re going to do all this using Python’s Shapely library, so let’s first install that library:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Checking which indexes are defined</title>
      <link>http://localhost:8000/blog/2022/01/13/apache-pinot-which-indexes-are-defined/</link>
      <pubDate>Thu, 13 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2022/01/13/apache-pinot-which-indexes-are-defined/</guid>
      <description>One of the most common questions in the Apache Pinot community Slack is how to work out which indexes are defined on columns in Pinot segments. This blog post will attempt to answer that question.
Figure 1. Apache Pinot: Checking which indexes are defined Setup First, we’re going to spin up a local instance of Pinot using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: zookeeper: image: zookeeper:3.5.6 hostname: zookeeper container_name: zookeeper-indexes ports: - &amp;#34;2181:2181&amp;#34; environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 pinot-controller: image: apachepinot/pinot:0.</description>
    </item>
    
  </channel>
</rss>
