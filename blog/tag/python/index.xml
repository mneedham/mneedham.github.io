<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/python/</link>
    <description>Recent content in Python on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 May 2018 00:31:34 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week I&amp;#8217;ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiri&amp;#8217;s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
 I&amp;#8217;ll use the same data as Sidath, so if you want to follow along you&amp;#8217;ll need to download these files:
   iris_training.csv
  iris_test.csv
   Loading data The way we load data will remain exactly the same - we&amp;#8217;ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Python via virtualenv on Mac OS X: RuntimeError: Python is not installed as a framework.</title>
      <link>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</link>
      <pubDate>Fri, 04 May 2018 22:03:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</guid>
      <description>I&amp;#8217;ve previously written a couple of blog posts about my troubles getting matplotlib to play nicely and I run into a slightly different variant today while following Sidath Asiri&amp;#8217;s Hello World in TensorFlow tutorial.
 When I ran the script using a version of Python installed via virtualenv I got the following exception:
 Traceback (most recent call last): File &#34;iris.py&#34;, line 4, in &amp;lt;module&amp;gt; from matplotlib import pyplot as plt File &#34;</description>
    </item>
    
    <item>
      <title>PyData London 2018 Conference Experience Report</title>
      <link>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</link>
      <pubDate>Sun, 29 Apr 2018 11:54:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/29/pydata-london-2018/</guid>
      <description>Over the last few days I attended PyData London 2018 and wanted to share my experience. The PyData series of conferences aim to bring together users and developers of data analysis tools to share ideas and learn from each other. I presented a talk on building a recommendation with Python and Neo4j at the 2016 version but didn&amp;#8217;t attend last year.
 The organisers said there were ~ 550 attendees spread over 1 day of tutorials and 2 days of talks.</description>
    </item>
    
    <item>
      <title>Python: Serialize and Deserialize Numpy 2D arrays</title>
      <link>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</link>
      <pubDate>Sat, 07 Apr 2018 19:38:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</guid>
      <description>I&amp;#8217;ve been playing around with saving and loading scikit-learn models and needed to serialize and deserialize Numpy arrays as part of the process.
 I could use pickle but that seems a bit overkill so I decided instead to save the byte representation of the array. We can get that representation by calling the tobytes method on a Numpy array:
 import numpy as np &amp;gt;&amp;gt;&amp;gt; np.array([ [1,2,3], [4,5,6], [7,8,9] ]) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Python 3: Converting a list to a dictionary with dictionary comprehensions</title>
      <link>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</link>
      <pubDate>Mon, 02 Apr 2018 04:20:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</guid>
      <description>When coding in Python I often find myself with lists containing key/value pairs that I want to convert to a dictionary.
 In a recent example I had the following code:
 values = [{&#39;key&#39;: &#39;name&#39;, &#39;value&#39;: &#39;Mark&#39;}, {&#39;key&#39;: &#39;age&#39;, &#39;value&#39;: 34}]   And I wanted to create a dictionary that had the keys name and age and their respective values. The easiest way to convert this list to a dictionary is to iterate over the list and construct the dictionary key by key:</description>
    </item>
    
    <item>
      <title>Strava: Calculating the similarity of two runs</title>
      <link>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</link>
      <pubDate>Thu, 18 Jan 2018 23:35:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</guid>
      <description>I go running several times a week and wanted to compare my runs against each other to see how similar they are.  I record my runs with the Strava app and it has an API that returns lat/long coordinates for each run in the Google encoded polyline algorithm format.  We can use the polyline library to decode these values into a list of lat/long tuples. For example: import polyline polyline.</description>
    </item>
    
    <item>
      <title>scikit-learn: Using GridSearch to tune the hyper-parameters of VotingClassifier</title>
      <link>https://markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</link>
      <pubDate>Sun, 10 Dec 2017 07:55:43 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</guid>
      <description>In my last blog post I showed how to create a multi class classification ensemble using scikit-learn&#39;s VotingClassifier and finished mentioning that I didn&#39;t know which classifiers should be part of the ensemble.  We need to get a better score with each of the classifiers in the ensemble otherwise they can be excluded.  We have a TF/IDF based classifier as well as well as the classifiers I wrote about in the last post.</description>
    </item>
    
    <item>
      <title>Python: Combinations of values on and off</title>
      <link>https://markhneedham.com/blog/2017/12/03/python-combinations-values-off/</link>
      <pubDate>Sun, 03 Dec 2017 17:23:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/03/python-combinations-values-off/</guid>
      <description>In my continued exploration of Kaggle&#39;s Spooky Authors competition, I wanted to run a GridSearch turning on and off different classifiers to work out the best combination. I therefore needed to generate combinations of 1s and 0s enabling different classifiers. e.g. if we had 3 classifiers we&#39;d generate these combinations
0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1   where.</description>
    </item>
    
    <item>
      <title>Python: Learning about defaultdict&#39;s handling of missing keys</title>
      <link>https://markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</link>
      <pubDate>Fri, 01 Dec 2017 15:26:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</guid>
      <description>While reading the scikit-learn code I came across a bit of code that I didn&#39;t understand for a while but in retrospect is quite neat.  This is the code snippet that intrigued me: vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__   Let&#39;s quickly see how it works by adapting an example from scikit-learn: &amp;gt;&amp;gt;&amp;gt; from collections import defaultdict &amp;gt;&amp;gt;&amp;gt; vocabulary = defaultdict() &amp;gt;&amp;gt;&amp;gt; vocabulary.default_factory = vocabulary.__len__ &amp;gt;&amp;gt;&amp;gt; vocabulary[&amp;quot;foo&amp;quot;] 0 &amp;gt;&amp;gt;&amp;gt; vocabulary.</description>
    </item>
    
    <item>
      <title>Python: polyglot - ModuleNotFoundError: No module named &#39;icu&#39;</title>
      <link>https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</link>
      <pubDate>Tue, 28 Nov 2017 19:52:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</guid>
      <description>I wanted to use the polyglot NLP library that my colleague Will Lyon mentioned in his analysis of Russian Twitter Trolls but had installation problems which I thought I&#39;d share in case anyone else experiences the same issues.  I started by trying to install polyglot: $ pip install polyglot ImportError: No module named &#39;icu&#39;   Hmmm I&#39;m not sure what icu is but luckily there&#39;s a GitHub issue covering this problem.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: unsupported format string passed to numpy.ndarray.__format__</title>
      <link>https://markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</link>
      <pubDate>Sun, 19 Nov 2017 07:16:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</guid>
      <description>This post explains how to work around a change in how Python string formatting works for numpy arrays between Python 2 and Python 3.  I&#39;ve been going through Kevin Markham&#39;s scikit-learn Jupyter notebooks and ran into a problem on the Cross Validation one, which was throwing this error when attempting to print the KFold example: Iteration Training set observations Testing set observations --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-28-007cbab507e3&amp;gt; in &amp;lt;module&amp;gt;() 6 print(&#39;{} {:^61} {}&#39;.</description>
    </item>
    
    <item>
      <title>Python 3: Create sparklines using matplotlib</title>
      <link>https://markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</link>
      <pubDate>Sat, 23 Sep 2017 06:51:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</guid>
      <description>I recently wanted to create sparklines to show how some values were changing over time. In addition, I wanted to generate them as images on the server rather than introducing a JavaScript library.  Chris Seymour&#39;s excellent gist which shows how to create sparklines inside a Pandas dataframe got me most of the way there, but I had to tweak his code a bit to get it to play nicely with Python 3.</description>
    </item>
    
    <item>
      <title>Serverless: Python - virtualenv - { &#34;errorMessage&#34;: &#34;Unable to import module &#39;handler&#39;&#34; }</title>
      <link>https://markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</link>
      <pubDate>Sun, 06 Aug 2017 19:03:30 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</guid>
      <description>I&#39;ve been using the Serverless library to deploy and run some Python functions on AWS lambda recently and was initially confused about how to handle my dependencies.  I tend to create a new virtualenv for each of my project so let&#39;s get that setup first: Prerequisites $ npm install serverless  $ virtualenv -p python3 a $ . a/bin/activate   Now let&#39;s create our Serverless project. I&#39;m going to install the requests library so that I can use it in my function.</description>
    </item>
    
    <item>
      <title>PHP vs Python: Generating a HMAC</title>
      <link>https://markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</link>
      <pubDate>Wed, 02 Aug 2017 06:09:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</guid>
      <description>I&#39;ve been writing a bit of code to integrate with a ClassMarker webhook, and you&#39;re required to check that an incoming request actually came from ClassMarker by checking the value of a base64 hash using HMAC SHA256.  The example in the documentation is written in PHP which I haven&#39;t done for about 10 years so I had to figure out how to do the same thing in Python.  This is the PHP version: $ php -a php &amp;gt; echo base64_encode(hash_hmac(&amp;quot;sha256&amp;quot;, &amp;quot;my data&amp;quot;, &amp;quot;my_secret&amp;quot;, true)); vyniKpNSlxu4AfTgSJImt+j+pRx7v6m+YBobfKsoGhE=   The Python equivalent is a bit more code but it&#39;s not too bad.</description>
    </item>
    
    <item>
      <title>Pandas: ValueError: The truth value of a Series is ambiguous.</title>
      <link>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Wed, 26 Jul 2017 21:41:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>I&#39;ve been playing around with Kaggle in my spare time over the last few weeks and came across an unexpected behaviour when trying to add a column to a dataframe.  First let&#39;s get Panda&#39;s into our program scope: Prerequisites import pandas as pd   Now we&#39;ll create a data frame to play with for the duration of this post: &amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame({&amp;quot;a&amp;quot;: [1,2,3,4,5], &amp;quot;b&amp;quot;: [2,3,4,5,6]}) &amp;gt;&amp;gt;&amp;gt; df a b 0 5 2 1 6 6 2 0 8 3 3 2 4 1 6   Let&#39;s say we want to create a new column which returns True if either of the numbers are odd.</description>
    </item>
    
    <item>
      <title>Pandas/scikit-learn: get_dummies test/train sets - ValueError: shapes not aligned</title>
      <link>https://markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</link>
      <pubDate>Wed, 05 Jul 2017 15:42:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</guid>
      <description>I&#39;ve been using panda&#39;s get_dummies function to generate dummy columns for categorical variables to use with scikit-learn, but noticed that it sometimes doesn&#39;t work as I expect.
Prerequisites import pandas as pd import numpy as np from sklearn import linear_model  Let&#39;s say we have the following training and test sets:
Training set train = pd.DataFrame({&amp;quot;letter&amp;quot;:[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;], &amp;quot;value&amp;quot;: [1, 2, 3, 4]}) X_train = train.drop([&amp;quot;value&amp;quot;], axis=1) X_train = pd.</description>
    </item>
    
    <item>
      <title>Pandas: Find rows where column/field is null</title>
      <link>https://markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</link>
      <pubDate>Wed, 05 Jul 2017 14:31:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</guid>
      <description>In my continued playing around with the Kaggle house prices dataset I wanted to find any columns/fields that have null values in.  If we want to get a count of the number of null fields by column we can use the following code, adapted from Poonam Ligade&#39;s kernel: Prerequisites import pandas as pd  Count the null columns train = pd.read_csv(&amp;quot;train.csv&amp;quot;) null_columns=train.columns[train.isnull().any()] train[null_columns].isnull().sum()  LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   So there are lots of different columns containing null values.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, I&#39;ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.  Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant I&#39;d overfitted the model.  I wasn&#39;t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albon&#39;s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>I&#39;ve been playing around with the data in Kaggle&#39;s House Prices: Advanced Regression Techniques and while replicating Poonam Ligade&#39;s exploratory analysis I wanted to see if I could create a model to fill in some of the missing values.  Poonam wrote the following code to identify which columns in the dataset had the most missing values: import pandas as pd train = pd.read_csv(&#39;train.csv&#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   The one that I&#39;m most interested in is LotFrontage, which describes &#39;Linear feet of street connected to property&#39;.</description>
    </item>
    
    <item>
      <title>Loading and analysing Strava runs using PostgreSQL JSON data type</title>
      <link>https://markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</link>
      <pubDate>Mon, 01 May 2017 19:11:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</guid>
      <description>In my last post I showed how to map Strava runs using data that I&#39;d extracted from their /activities API, but the API returns a lot of other data that I discarded because I wasn&#39;t sure what I should keep.  The API returns a nested JSON structure so the easiest solution would be to save each run as an individual file but I&#39;ve always wanted to try out PostgreSQL&#39;s JSON data type and this seemed like a good opportunity.</description>
    </item>
    
    <item>
      <title>Leaflet: Mapping Strava runs/polylines on Open Street Map</title>
      <link>https://markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</link>
      <pubDate>Sat, 29 Apr 2017 15:36:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</guid>
      <description>I&#39;m a big Strava user and spent a bit of time last weekend playing around with their API to work out how to map all my runs.  Strava API and polylines  This is a two step process:   Call the /athlete/activities/ endpoint to get a list of all my activities For each of those activities call /activities/[activityId] endpoint to get more detailed information for each activity   That second API returns a &#39;polyline&#39; property which the documentation describes as follows:  Activity and segment API requests may include summary polylines of their respective routes.</description>
    </item>
    
    <item>
      <title>Python: Flask - Generating a static HTML page</title>
      <link>https://markhneedham.com/blog/2017/04/27/python-flask-generating-a-static-html-page/</link>
      <pubDate>Thu, 27 Apr 2017 20:59:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/27/python-flask-generating-a-static-html-page/</guid>
      <description>Whenever I need to quickly spin up a web application Python&#39;s Flask library is my go to tool but I recently found myself wanting to generate a static HTML to upload to S3 and wondered if I could use it for that as well.
 It&#39;s actually not too tricky. If we&#39;re in the scope of the app context then we have access to the template rendering that we&#39;d normally use when serving the response to a web request.</description>
    </item>
    
    <item>
      <title>Luigi: Defining dynamic requirements (on output files)</title>
      <link>https://markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</link>
      <pubDate>Tue, 28 Mar 2017 05:39:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</guid>
      <description>In my last blog post I showed how to convert a JSON document containing meetup groups into a CSV file using Luigi, the Python library for building data pipelines. As well as creating that CSV file I wanted to go back to the meetup.com API and download all the members of those groups.
This was a rough flow of what i wanted to do:
  Take JSON document containing all groups   Parse that document and for each group:    Call the /members endpoint   Save each one of those files as a JSON file    Iterate over all those JSON files and create a members CSV file   In the previous post we created the GroupsToJSON task which calls the /groups endpoint on the meetup API and creates the file /tmp/groups.</description>
    </item>
    
    <item>
      <title>Luigi: An ExternalProgramTask example - Converting JSON to CSV</title>
      <link>https://markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</link>
      <pubDate>Sat, 25 Mar 2017 14:09:59 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</guid>
      <description>I&#39;ve been playing around with the Python library Luigi which is used to build pipelines of batch jobs and I struggled to find an example of an ExternalProgramTask so this is my attempt at filling that void.   I&#39;m building a little data pipeline to get data from the meetup.com API and put it into CSV files that can be loaded into Neo4j using the LOAD CSV command.  The first task I created calls the /groups endpoint and saves the result into a JSON file: import luigi import requests import json from collections import Counter class GroupsToJSON(luigi.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: Object of type &#39;dict_values&#39; is not JSON serializable</title>
      <link>https://markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</link>
      <pubDate>Sun, 19 Mar 2017 16:40:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</guid>
      <description>I&#39;ve recently upgraded to Python 3 (I know, took me a while!) and realised that one of my scripts that writes JSON to a file no longer works!  This is a simplified version of what I&#39;m doing: &amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; x = {&amp;quot;mark&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Mark&amp;quot;}, &amp;quot;michael&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Michael&amp;quot;} } &amp;gt;&amp;gt;&amp;gt; json.dumps(x.values()) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py&amp;quot;, line 231, in dumps return _default_encoder.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:  Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>A couple of months ago Praveena and I created a Game of Thrones dataset to use in a workshop and I thought it&#39;d be fun to run it through some machine learning algorithms and hopefully find some interesting insights.  The dataset is available as CSV files but for this analysis I&#39;m assuming that it&#39;s already been imported into neo4j. If you want to import the data you can run the tutorial by typing the following into the query bar of the neo4j browser: :play http://guides.</description>
    </item>
    
    <item>
      <title>Python: matplotlib, seaborn, virtualenv - Python is not installed as a framework</title>
      <link>https://markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</link>
      <pubDate>Sun, 14 Aug 2016 18:56:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</guid>
      <description>Over the weekend I was following The Marketing Technologist&#39;s content based recommender tutorial but ran into the following exception when trying to import the seaborn library: $ python 5_content_based_recommender/run.py Traceback (most recent call last): File &amp;quot;5_content_based_recommender/run.py&amp;quot;, line 14, in &amp;lt;module&amp;gt; import seaborn as sns File &amp;quot;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/__init__.py&amp;quot;, line 6, in &amp;lt;module&amp;gt; from .rcmod import * File &amp;quot;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/rcmod.py&amp;quot;, line 8, in &amp;lt;module&amp;gt; from . import palettes, _orig_rc_params File &amp;quot;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/palettes.py&amp;quot;, line 12, in &amp;lt;module&amp;gt; from .</description>
    </item>
    
    <item>
      <title>scikit-learn: TF/IDF and cosine similarity for computer science papers</title>
      <link>https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</link>
      <pubDate>Wed, 27 Jul 2016 02:45:28 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</guid>
      <description>A couple of months ago I downloaded the meta data for a few thousand computer science papers so that I could try and write a mini recommendation engine to tell me what paper I should read next.  Since I don&#39;t have any data on which people read each paper a collaborative filtering approach is ruled out, so instead I thought I could try content based filtering instead.  Let&#39;s quickly check the Wikipedia definition of content based filtering:  In a content-based recommender system, keywords are used to describe the items and a user profile is built to indicate the type of item this user likes.</description>
    </item>
    
    <item>
      <title>Python: Scraping elements relative to each other with BeautifulSoup</title>
      <link>https://markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</link>
      <pubDate>Mon, 11 Jul 2016 06:01:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</guid>
      <description>Last week we hosted a Game of Thrones based intro to Cypher at the Women Who Code London meetup and in preparation had to scrape the wiki to build a dataset.  I&#39;ve built lots of datasets this way and it&#39;s a painless experience as long as the pages make liberal use of CSS classes and/or IDs.  Unfortunately the Game of Thrones wiki doesn&#39;t really do that so I had to find another way to extract the data I wanted - extracting elements based on their position to more prominent elements on the page.</description>
    </item>
    
    <item>
      <title>Python: BeautifulSoup - Insert tag</title>
      <link>https://markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</link>
      <pubDate>Thu, 30 Jun 2016 21:28:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</guid>
      <description>I&#39;ve been scraping the Game of Thrones wiki in preparation for a meetup at Women Who Code next week and while attempting to extract character allegiances I wanted to insert missing line breaks to separate different allegiances.  I initially tried creating a line break like this: &amp;gt;&amp;gt;&amp;gt; from bs4 import BeautifulSoup &amp;gt;&amp;gt;&amp;gt; tag = BeautifulSoup(&amp;quot;&amp;lt;br /&amp;gt;&amp;quot;, &amp;quot;html.parser&amp;quot;) &amp;gt;&amp;gt;&amp;gt; tag &amp;lt;br/&amp;gt;   It looks like it should work but later on in my script I check the &#39;name&#39; attribute to work out whether I&#39;ve got a line break and it doesn&#39;t return the value I expected it to: &amp;gt;&amp;gt;&amp;gt; tag.</description>
    </item>
    
    <item>
      <title>Python: Regex - matching foreign characters/unicode letters</title>
      <link>https://markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</link>
      <pubDate>Sat, 18 Jun 2016 07:38:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</guid>
      <description>I&#39;ve been back in the land of screen scrapping this week extracting data from the Game of Thrones wiki and needed to write a regular expression to pull out characters and actors.  Here are some examples of the format of the data:
Peter Dinklage as Tyrion Lannister Daniel Naprous as Oznak zo Pahl(credited as Stunt Performer) Filip Lozić as Young Nobleman Morgan C. Jones as a Braavosi captain Adewale Akinnuoye-Agbaje as Malko  So the pattern is:</description>
    </item>
    
    <item>
      <title>Python: Squashing &#39;duplicate&#39; pairs together</title>
      <link>https://markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</link>
      <pubDate>Sun, 20 Dec 2015 12:12:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</guid>
      <description>As part of a data cleaning pipeline I had pairs of ids of duplicate addresses that I wanted to group together.
I couldn&#39;t work out how to solve the problem immediately so I simplified the problem into pairs of letters i.e. A	B	(A is the same as B) B	C	(B is the same as C) C	D	... E	F	(E is the same as F) F	G	.</description>
    </item>
    
    <item>
      <title>Python: Parsing a JSON HTTP chunking stream</title>
      <link>https://markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</link>
      <pubDate>Sat, 28 Nov 2015 13:56:59 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</guid>
      <description>I&#39;ve been playing around with meetup.com&#39;s API again and this time wanted to consume the chunked HTTP RSVP stream and filter RSVPs for events I&#39;m interested in.  I use Python for most of my hacking these days and if HTTP requests are required the requests library is my first port of call. I started out with the following script import requests import json def stream_meetup_initial(): uri = &amp;quot;http://stream.meetup.com/2/rsvps&amp;quot; response = requests.</description>
    </item>
    
    <item>
      <title>Python: Extracting Excel spreadsheet into CSV files</title>
      <link>https://markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:42 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</guid>
      <description>I&#39;ve been playing around with the Road Safety open data set and the download comes with several CSV files and an excel spreadsheet containing the legend.  There are 45 sheets in total and each of them looks like this:   I wanted to create a CSV file for each sheet so that I can import the data set into Neo4j using the LOAD CSV command. I came across the Python Excel website which pointed me at the xlrd library since I&#39;m working with a pre 2010 Excel file.</description>
    </item>
    
    <item>
      <title>Python: Difference between two datetimes in milliseconds</title>
      <link>https://markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</link>
      <pubDate>Tue, 28 Jul 2015 20:05:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</guid>
      <description>I&#39;ve been doing a bit of adhoc measurement of some cypher queries executed via py2neo and wanted to work out how many milliseconds each query was taking end to end.   I thought there&#39;d be an obvious way of doing this but if there is it&#39;s evaded me so far and I ended up calculating the different between two datetime objects which gave me the following timedelta object: &amp;gt;&amp;gt;&amp;gt; import datetime &amp;gt;&amp;gt;&amp;gt; start = datetime.</description>
    </item>
    
    <item>
      <title>Python: UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 0: ordinal not in range(128)</title>
      <link>https://markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</link>
      <pubDate>Wed, 15 Jul 2015 06:20:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</guid>
      <description>I was recently doing some text scrubbing and had difficulty working out how to remove the &#39;†&#39; character from strings. e.g. I had a string like this:
&amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39; u&#39;foo \u2020&#39;  I wanted to get rid of the &#39;†&#39; character and then strip any trailing spaces so I&#39;d end up with the string &#39;foo&#39;. I tried to do this in one call to &#39;replace&#39;:
&amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39;.replace(&amp;quot; †&amp;quot;, &amp;quot;&amp;quot;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 1: ordinal not in range(128)   It took me a while to work out that &#34;</description>
    </item>
    
    <item>
      <title>Python: Converting WordPress posts in CSV format</title>
      <link>https://markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</link>
      <pubDate>Tue, 07 Jul 2015 06:28:01 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</guid>
      <description>Over the weekend I wanted to look into the Wordpress data behind this blog (very meta!) and wanted to get the data in CSV format so I could do some analysis in R.   I found a couple of WordPress CSV plugins but unfortunately I couldn&#39;t get any of them to work and ended up working with the raw XML data that WordPress produces when you &#39;export&#39; a blog.</description>
    </item>
    
    <item>
      <title>Python: CSV writing - TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;__getitem__&#39;</title>
      <link>https://markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</link>
      <pubDate>Sun, 31 May 2015 22:33:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</guid>
      <description>When I&#39;m working in Python I often find myself writing to CSV files using the in built library and every now and then make a mistake when calling writerow: import csv writer = csv.writer(file, delimiter=&amp;quot;,&amp;quot;) writer.writerow[&amp;quot;player&amp;quot;, &amp;quot;team&amp;quot;]  This results in the following error message:
TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;__getitem__&#39;  The error message is a bit weird at first but it&#39;s basically saying that I&#39;ve tried to do an associative lookup on an object which doesn&#39;t support that operation.</description>
    </item>
    
    <item>
      <title>Python: Look ahead multiple elements in an iterator/generator</title>
      <link>https://markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</link>
      <pubDate>Thu, 28 May 2015 20:56:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</guid>
      <description>As part of the BBC live text scraping code I&#39;ve been working on I needed to take an iterator of raw events created by a generator and transform this into an iterator of cards shown in a match.  The structure of the raw events I&#39;m interested in is as follows:  Line 1: Player booked Line 2: Player fouled Line 3: Information about the foul   e.g. events = [ {&#39;event&#39;: u&#39;Booking Pedro (Barcelona) is shown the yellow card for a bad foul.</description>
    </item>
    
    <item>
      <title>Python: Joining multiple generators/iterators</title>
      <link>https://markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</link>
      <pubDate>Sun, 24 May 2015 23:51:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</guid>
      <description>In my previous blog post I described how I&#39;d refactored some scraping code I&#39;ve been working on to use iterators and ended up with a function which returned a generator containing all the events for one BBC live text match: match_id = &amp;quot;32683310&amp;quot; events = extract_events(&amp;quot;data/raw/%s&amp;quot; % (match_id)) &amp;gt;&amp;gt;&amp;gt; print type(events) &amp;lt;type &#39;generator&#39;&amp;gt;   The next thing I wanted to do is get the events for multiple matches which meant I needed to glue together multiple generators into one big generator.</description>
    </item>
    
    <item>
      <title>Python: Refactoring to iterator</title>
      <link>https://markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</link>
      <pubDate>Sat, 23 May 2015 10:14:38 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</guid>
      <description>Over the last week I&#39;ve been building a set of scripts to scrape the events from the Bayern Munich/Barcelona game and I&#39;ve ended up with a few hundred lines of nested for statements, if statements and mutated lists. I thought it was about time I did a bit of refactoring. The following is a function which takes in a match file and spits out a collection of maps containing times &amp; events.</description>
    </item>
    
    <item>
      <title>Python: UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\xfc&#39; in position 11: ordinal not in range(128)</title>
      <link>https://markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</link>
      <pubDate>Thu, 21 May 2015 06:14:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</guid>
      <description>I&#39;ve been trying to write some Python code to extract the players and the team they represented in the Bayern Munich/Barcelona match into a CSV file and had much more difficulty than I expected.  I have some scraping code (which is beyond the scope of this article) which gives me a list of (player, team) pairs that I want to write to disk. The contents of the list is as follows: $ python extract_players.</description>
    </item>
    
    <item>
      <title>Python: Selecting certain indexes in an array</title>
      <link>https://markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</link>
      <pubDate>Tue, 05 May 2015 21:39:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</guid>
      <description>A couple of days ago I was scrapping the UK parliament constituencies from Wikipedia in preparation for the Graph Connect hackathon and had got to the point where I had an array with one entry per column in the table.   import requests from bs4 import BeautifulSoup from soupselect import select page = open(&amp;quot;constituencies.html&amp;quot;, &#39;r&#39;) soup = BeautifulSoup(page.read()) for row in select(soup, &amp;quot;table.wikitable tr&amp;quot;): if select(row, &amp;quot;th&amp;quot;): print [cell.text for cell in select(row, &amp;quot;th&amp;quot;)] if select(row, &amp;quot;td&amp;quot;): print [cell.</description>
    </item>
    
    <item>
      <title>Python: Creating a skewed random discrete distribution</title>
      <link>https://markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</link>
      <pubDate>Mon, 30 Mar 2015 22:28:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</guid>
      <description>I&#39;m planning to write a variant of the TF/IDF algorithm over the HIMYM corpus which weights in favour of term that appear in a medium number of documents and as a prerequisite needed a function that when given a number of documents would return a weighting.  It should return a higher value when a term appears in a medium number of documents i.e. if I pass in 10 I should get back a higher value than 200 as a term that appears in 10 episodes is likely to be more interesting than one which appears in almost every episode.</description>
    </item>
    
    <item>
      <title>Python: matplotlib hangs and shows nothing (Mac OS X)</title>
      <link>https://markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</link>
      <pubDate>Thu, 26 Mar 2015 00:02:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</guid>
      <description>I&#39;ve been playing around with some of the matplotlib demos recently and discovered that simply copying one of the examples didn&#39;t actually work for me. I was following the bar chart example and had the following code:
import numpy as np import matplotlib.pyplot as plt N = 5 ind = np.arange(N) fig, ax = plt.subplots() menMeans = (20, 35, 30, 35, 27) menStd = (2, 3, 4, 1, 2) width = 0.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>In my continued exploration of topic modelling I came across The Programming Historian blog and a post showing how to derive topics from a corpus using the Java library mallet.  The instructions on the blog make it very easy to get up and running but as with other libraries I&#39;ve used, you have to specify how many topics the corpus consists of. I&#39;m never sure what value to select but the authors make the following suggestion:  How do you know the number of topics to search for?</description>
    </item>
    
    <item>
      <title>Python: Equivalent to flatMap for flattening an array of arrays</title>
      <link>https://markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</link>
      <pubDate>Mon, 23 Mar 2015 00:45:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</guid>
      <description>I found myself wanting to flatten an array of arrays while writing some Python code earlier this afternoon and being lazy my first attempt involved building the flattened array manually:
episodes = [ {&amp;quot;id&amp;quot;: 1, &amp;quot;topics&amp;quot;: [1,2,3]}, {&amp;quot;id&amp;quot;: 2, &amp;quot;topics&amp;quot;: [4,5,6]} ] flattened_episodes = [] for episode in episodes: for topic in episode[&amp;quot;topics&amp;quot;]: flattened_episodes.append({&amp;quot;id&amp;quot;: episode[&amp;quot;id&amp;quot;], &amp;quot;topic&amp;quot;: topic}) for episode in flattened_episodes: print episode  If we run that we&#39;ll see this output:</description>
    </item>
    
    <item>
      <title>Python: Simplifying the creation of a stop word list with defaultdict</title>
      <link>https://markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</link>
      <pubDate>Sun, 22 Mar 2015 01:51:52 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</guid>
      <description>I&#39;ve been playing around with topics models again and recently read a paper by David Mimno which suggested the following heuristic for working out which words should go onto the stop list:  A good heuristic for identifying such words is to remove those that occur in more than 5-10% of documents (most common) and those that occur fewer than 5-10 times in the entire corpus (least common).   I decided to try this out on the HIMYM dataset that I&#39;ve been working on over the last couple of months.</description>
    </item>
    
    <item>
      <title>Python: Forgetting to use enumerate</title>
      <link>https://markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</link>
      <pubDate>Sun, 22 Mar 2015 01:28:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</guid>
      <description>Earlier this evening I found myself writing the equivalent of the following Python code while building a stop list for a topic model... words = [&amp;quot;mark&amp;quot;, &amp;quot;neo4j&amp;quot;, &amp;quot;michael&amp;quot;] word_position = 0 for word in words: print word_position, word word_position +=1   ...which is very foolish given that there&#39;s already a function that makes it really easy to grab the position of an item in a list:
for word_position, word in enumerate(words): print word_position, word  Python does make things extremely easy at times - you&#39;re welcome future Mark!</description>
    </item>
    
    <item>
      <title>Python: Transforming Twitter datetime string to timestamp (z&#39; is a bad directive in format)</title>
      <link>https://markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</link>
      <pubDate>Sun, 15 Mar 2015 22:43:17 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</guid>
      <description>I&#39;ve been playing around with importing Twitter data into Neo4j and since Neo4j can&#39;t store dates natively just yet I needed to convert a date string to timestamp. I started with the following which unfortunately throws an exception:
from datetime import datetime date = &amp;quot;Sat Mar 14 18:43:19 +0000 2015&amp;quot; &amp;gt;&amp;gt;&amp;gt; datetime.strptime(date, &amp;quot;%a %b %d %H:%M:%S %z %Y&amp;quot;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/System/Library/Frameworks/Python.</description>
    </item>
    
    <item>
      <title>Python: Checking any value in a list exists in a line of text</title>
      <link>https://markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</link>
      <pubDate>Sat, 14 Mar 2015 02:52:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</guid>
      <description>I&#39;ve been doing some log file analysis to see what cypher queries were being run on a Neo4j instance and I wanted to narrow down the lines I looked at to only contain ones which had mutating operations i.e. those containing the words MERGE, DELETE, SET or CREATE Here&#39;s an example of the text file I was parsing:
$ cat blog.txt MATCH n RETURN n MERGE (n:Person {name: &amp;quot;Mark&amp;quot;}) RETURN n MATCH (n:Person {name: &amp;quot;Mark&amp;quot;}) ON MATCH SET n.</description>
    </item>
    
    <item>
      <title>Python/Neo4j: Finding interesting computer sciency people to follow on Twitter</title>
      <link>https://markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</link>
      <pubDate>Wed, 11 Mar 2015 21:13:26 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</guid>
      <description>At the beginning of this year I moved from Neo4j&#39;s field team to dev team and since the code we write there is much lower level than I&#39;m used to I thought I should find some people to follow on twitter whom I can learn from.  My technique for finding some of those people was to pick a person from the Neo4j kernel team who&#39;s very good at systems programming and uses twitter which led me to Mr Chris Vest.</description>
    </item>
    
    <item>
      <title>Python: Streaming/Appending to a file</title>
      <link>https://markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</link>
      <pubDate>Mon, 09 Mar 2015 23:00:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</guid>
      <description>I&#39;ve been playing around with Twitter&#39;s API (via the tweepy library) and due to the rate limiting it imposes I wanted to stream results to a CSV file rather than waiting until my whole program had finished.  I wrote the following program to simulate what I was trying to do: import csv import time with open(&amp;quot;rows.csv&amp;quot;, &amp;quot;a&amp;quot;) as file: writer = csv.writer(file, delimiter = &amp;quot;,&amp;quot;) end = time.time() + 10 while True: if time.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>Following on from Rik van Bruggen&#39;s blog post on a QCon graph he&#39;s created ahead of this week&#39;s conference, I was curious whether we could extract any interesting relationships between talks based on their abstracts.  Talks are already grouped by their hosting track but there&#39;s likely to be some overlap in topics even for talks on different tracks. I therefore wanted to extract topics and connect each talk to the topic that describes it best.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn - Training a classifier with non numeric features</title>
      <link>https://markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</link>
      <pubDate>Mon, 02 Mar 2015 07:48:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</guid>
      <description>Following on from my previous posts on training a classifier to pick out the speaker in sentences of HIMYM transcripts the next thing to do was train a random forest of decision trees to see how that fared.  I&#39;ve used scikit-learn for this before so I decided to use that. However, before building a random forest I wanted to check that I could build an equivalent decision tree.  I initially thought that scikit-learn&#39;s DecisionTree classifier would take in data in the same format as nltk&#39;s so I started out with the following code: import json import nltk import collections from himymutil.</description>
    </item>
    
    <item>
      <title>Python: Detecting the speaker in HIMYM using Parts of Speech (POS) tagging</title>
      <link>https://markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</link>
      <pubDate>Sun, 01 Mar 2015 02:36:06 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</guid>
      <description>Over the last couple of weeks I&#39;ve been experimenting with different classifiers to detect speakers in HIMYM transcripts and in all my attempts so far the only features I&#39;ve used have been words.  This led to classifiers that were overfitted to the training data so I wanted to generalise them by introducing parts of speech of the words in sentences which are more generic.  First I changed the function which generates the features for each word to also contain the parts of speech of the previous and next words as well as the word itself: def pos_features(sentence, sentence_pos, i): features = {} features[&amp;quot;word&amp;quot;] = sentence[i] features[&amp;quot;word-pos&amp;quot;] = sentence_pos[i][1] if i == 0: features[&amp;quot;prev-word&amp;quot;] = &amp;quot;&amp;lt;START&amp;gt;&amp;quot; features[&amp;quot;prev-word-pos&amp;quot;] = &amp;quot;&amp;lt;START&amp;gt;&amp;quot; else: features[&amp;quot;prev-word&amp;quot;] = sentence[i-1] features[&amp;quot;prev-word-pos&amp;quot;] = sentence_pos[i-1][1] if i == len(sentence) - 1: features[&amp;quot;next-word&amp;quot;] = &amp;quot;&amp;lt;END&amp;gt;&amp;quot; features[&amp;quot;next-word-pos&amp;quot;] = &amp;quot;&amp;lt;END&amp;gt;&amp;quot; else: features[&amp;quot;next-word&amp;quot;] = sentence[i+1] features[&amp;quot;next-word-pos&amp;quot;] = sentence_pos[i+1][1] return features   Next we need to tweak our calling code to calculate the parts of speech tags for each sentence and pass it in: featuresets = [] for tagged_sent in tagged_sents: untagged_sent = nltk.</description>
    </item>
    
    <item>
      <title>Python/nltk: Naive vs Naive Bayes vs Decision Tree</title>
      <link>https://markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</link>
      <pubDate>Tue, 24 Feb 2015 22:39:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</guid>
      <description>Last week I wrote a blog post describing a decision tree I&#39;d trained to detect the speakers in a How I met your mother transcript and after writing the post I wondered whether a simple classifier would do the job.  The simple classifier will work on the assumption that any word followed by a &#34;:&#34; is a speaker and anything else isn&#39;t. Here&#39;s the definition of a NaiveClassifier: import nltk from nltk import ClassifierI class NaiveClassifier(ClassifierI): def classify(self, featureset): if featureset[&#39;next-word&#39;] == &amp;quot;:&amp;quot;: return True else: return False  As you can see it only implements the classify method and executes a static check.</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Detecting which sentences in a transcript contain a speaker</title>
      <link>https://markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</link>
      <pubDate>Fri, 20 Feb 2015 22:42:59 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</guid>
      <description>Over the past couple of months I&#39;ve been playing around with How I met your mother transcripts and the most recent thing I&#39;ve been working on is how to extract the speaker for a particular sentence. This initially seemed like a really simple problem as most of the initial sentences I looked at weere structured like this: &amp;lt;speaker&amp;gt;: &amp;lt;sentence&amp;gt;   If there were all in that format then we could write a simple regular expression and then move on but unfortunately they aren&#39;t.</description>
    </item>
    
    <item>
      <title>Python&#39;s pandas vs Neo4j&#39;s cypher: Exploring popular phrases in How I met your mother transcripts</title>
      <link>https://markhneedham.com/blog/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 19 Feb 2015 00:52:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</guid>
      <description>I&#39;ve previously written about extracting TF/IDF scores for phrases in documents using scikit-learn and the final step in that post involved writing the words into a CSV file for analysis later on.  I wasn&#39;t sure what the most appropriate tool of choice for that analysis was so I decided to explore the data using Python&#39;s pandas library and load it into Neo4j and write some Cypher queries. To do anything with Neo4j we need to first load the CSV file into the database.</description>
    </item>
    
    <item>
      <title>Python/pandas: Column value in list (ValueError: The truth value of a Series is ambiguous.)</title>
      <link>https://markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Mon, 16 Feb 2015 21:39:16 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>I&#39;ve been using Python&#39;s pandas library while exploring some CSV files and although for the most part I&#39;ve found it intuitive to use, I had trouble filtering a data frame based on checking whether a column value was in a list. A subset of one of the CSV files I&#39;ve been working with looks like this:
$ cat foo.csv &amp;quot;Foo&amp;quot; 1 2 3 4 5 6 7 8 9 10  Loading it into a pandas data frame is reasonably simple:</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Calculating TF/IDF on How I met your mother transcripts</title>
      <link>https://markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</link>
      <pubDate>Sun, 15 Feb 2015 15:56:09 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</guid>
      <description>Over the past few weeks I&#39;ve been playing around with various NLP techniques to find interesting insights into How I met your mother from its transcripts and one technique that kept coming up is TF/IDF.
The Wikipedia definition reads like this:
 tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</description>
    </item>
    
    <item>
      <title>Python/gensim: Creating bigrams over How I met your mother transcripts</title>
      <link>https://markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 12 Feb 2015 23:45:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</guid>
      <description>As part of my continued playing around with How I met your mother transcripts I wanted to identify plot arcs and as a first step I wrote some code using the gensim and nltk libraries to identify bigrams (two word phrases).  There&#39;s an easy to follow tutorial in the gensim docs showing how to go about this but I needed to do a couple of extra steps to get my text data from a CSV file into the structure gensim expects.</description>
    </item>
    
    <item>
      <title>Python/matpotlib: Plotting occurrences of the main characters in How I Met Your Mother</title>
      <link>https://markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</link>
      <pubDate>Fri, 30 Jan 2015 21:29:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</guid>
      <description>Normally when I&#39;m playing around with data sets in R I get out ggplot2 to plot some charts to get a feel for the data but having spent quite a bit of time with Python and How I met your mother transcripts I haven&#39;t created a single plot. I thought I&#39;d better change change that.  After a bit of searching around it seems that matplotlib is the go to library for this job and I thought an interesting thing to plot would be how often each of the main characters appear in each episode across the show.</description>
    </item>
    
    <item>
      <title>Python: Find the highest value in a group</title>
      <link>https://markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</link>
      <pubDate>Sun, 25 Jan 2015 12:47:01 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</guid>
      <description>In my continued playing around with a How I met your mother data set I needed to find out the last episode that happened in a season so that I could use it in a chart I wanted to plot.  I had this CSV file containing each of the episodes: $ head -n 10 data/import/episodes.csv NumberOverall,NumberInSeason,Episode,Season,DateAired,Timestamp 1,1,/wiki/Pilot,1,&amp;quot;September 19, 2005&amp;quot;,1127084400 2,2,/wiki/Purple_Giraffe,1,&amp;quot;September 26, 2005&amp;quot;,1127689200 3,3,/wiki/Sweet_Taste_of_Liberty,1,&amp;quot;October 3, 2005&amp;quot;,1128294000 4,4,/wiki/Return_of_the_Shirt,1,&amp;quot;October 10, 2005&amp;quot;,1128898800 5,5,/wiki/Okay_Awesome,1,&amp;quot;October 17, 2005&amp;quot;,1129503600 6,6,/wiki/Slutty_Pumpkin,1,&amp;quot;October 24, 2005&amp;quot;,1130108400 7,7,/wiki/Matchmaker,1,&amp;quot;November 7, 2005&amp;quot;,1131321600 8,8,/wiki/The_Duel,1,&amp;quot;November 14, 2005&amp;quot;,1131926400 9,9,/wiki/Belly_Full_of_Turkey,1,&amp;quot;November 21, 2005&amp;quot;,1132531200   I started out by parsing the CSV file into a dictionary of (seasons - episode ids): import csv from collections import defaultdict seasons = defaultdict(list) with open(&amp;quot;data/import/episodes.</description>
    </item>
    
    <item>
      <title>Python/pdfquery: Scraping the FIFA World Player of the Year votes PDF into shape</title>
      <link>https://markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</link>
      <pubDate>Thu, 22 Jan 2015 00:25:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</guid>
      <description>Last week the FIFA Ballon d&#39;Or 2014 was announced and along with the announcement of the winner the individual votes were also made available.  Unfortunately they weren&#39;t made open in a way that Ben Wellington (of IQuantNY fame) would approve of - the choice of format for the data is a PDF file!  I wanted to extract this data to play around with it but I wanted to automate the extraction as I&#39;d done when working with Google Trends data.</description>
    </item>
    
    <item>
      <title>Python/NLTK: Finding the most common phrases in How I Met Your Mother</title>
      <link>https://markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</link>
      <pubDate>Mon, 19 Jan 2015 00:24:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</guid>
      <description>Following on from last week&#39;s blog post where I found the most popular words in How I met your mother transcripts, in this post we&#39;ll have a look at how we can pull out sentences and then phrases from our corpus. The first thing I did was tweak the scraping script to pull out the sentences spoken by characters in the transcripts.
Each dialogue is separated by two line breaks so we use that as our separator.</description>
    </item>
    
    <item>
      <title>Python: Counter - ValueError: too many values to unpack</title>
      <link>https://markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</link>
      <pubDate>Mon, 12 Jan 2015 23:16:58 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</guid>
      <description>I recently came across Python&#39;s Counter tool which makes it really easy to count the number of occurrences of items in a list.  In my case I was trying to work out how many times words occurred in a corpus so I had something like the following: &amp;gt;&amp;gt; from collections import Counter &amp;gt;&amp;gt; counter = Counter([&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;, &amp;quot;word3&amp;quot;, &amp;quot;word1&amp;quot;]) &amp;gt;&amp;gt; print counter Counter({&#39;word1&#39;: 2, &#39;word3&#39;: 1, &#39;word2&#39;: 1})   I wanted to write a for loop to iterate over the counter and print the (key, value) pairs and started with the following: &amp;gt;&amp;gt;&amp;gt; for key, value in counter: .</description>
    </item>
    
    <item>
      <title>Python: scikit-learn: ImportError: cannot import name __check_build</title>
      <link>https://markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</link>
      <pubDate>Sat, 10 Jan 2015 08:48:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</guid>
      <description>In part 3 of Kaggle&#39;s series on text analytics I needed to install scikit-learn and having done so ran into the following error when trying to use one of its classes: &amp;gt;&amp;gt;&amp;gt; from sklearn.feature_extraction.text import CountVectorizer Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/sklearn/__init__.py&amp;quot;, line 37, in &amp;lt;module&amp;gt; from . import __check_build ImportError: cannot import name __check_build  This error doesn&#39;t reveal very much but I found that when I exited the REPL and tried the same command again I got a different error which was a bit more useful:</description>
    </item>
    
    <item>
      <title>Python: gensim - clang: error: unknown argument: &#39;-mno-fused-madd&#39; [-Wunused-command-line-argument-hard-error-in-future]</title>
      <link>https://markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</link>
      <pubDate>Sat, 10 Jan 2015 08:39:15 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</guid>
      <description>While working through part 2 of Kaggle&#39;s bag of words tutorial I needed to install the gensim library and initially ran into the following error: $ pip install gensim ... cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/Users/markneedham/projects/neo4j-himym/himym/build/gensim/gensim/models -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.</description>
    </item>
    
    <item>
      <title>Python NLTK/Neo4j: Analysing the transcripts of How I Met Your Mother</title>
      <link>https://markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</link>
      <pubDate>Sat, 10 Jan 2015 01:22:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</guid>
      <description>After reading Emil&#39;s blog post about dark data a few weeks ago I became intrigued about trying to find some structure in free text data and I thought How I met your mother&#39;s transcripts would be a good place to start. I found a website which has the transcripts for all the episodes and then having manually downloaded the two pages which listed all the episodes, wrote a script to grab each of the transcripts so I could use them on my machine.</description>
    </item>
    
    <item>
      <title>Python: Converting a date string to timestamp</title>
      <link>https://markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</link>
      <pubDate>Mon, 20 Oct 2014 15:53:51 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</guid>
      <description>I&#39;ve been playing around with Python over the last few days while cleaning up a data set and one thing I wanted to do was translate date strings into a timestamp.
I started with a date in this format:
date_text = &amp;quot;13SEP2014&amp;quot;  So the first step is to translate that into a Python date - the strftime section of the documentation is useful for figuring out which format code is needed:</description>
    </item>
    
    <item>
      <title>Python: Making scikit-learn and pandas play nice</title>
      <link>https://markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</link>
      <pubDate>Sat, 09 Nov 2013 13:58:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</guid>
      <description>In the last post I wrote about Nathan and my attempts at the Kaggle Titanic Problem I mentioned that we our next step was to try out scikit-learn so I thought I should summarise where we&#39;ve got up to.
We needed to write a classification algorithm to work out whether a person onboard the Titanic survived and luckily scikit-learn has extensive documentation on each of the algorithms.
Unfortunately almost all those examples use numpy data structures and we&#39;d loaded the data using pandas and didn&#39;t particularly want to switch back!</description>
    </item>
    
    <item>
      <title>Python: Scoping variables to use with timeit</title>
      <link>https://markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</link>
      <pubDate>Sat, 09 Nov 2013 11:01:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</guid>
      <description>I&#39;ve been playing around with Python&#39;s timeit library to help benchmark some Neo4j cypher queries but I ran into some problems when trying to give it accessible to variables in my program.
I had the following python script which I would call from the terminal using python top-away-scorers.py:
import query_profiler as qp attempts = [ {&amp;quot;query&amp;quot;: &#39;&#39;&#39;MATCH (player:Player)-[:played]-&amp;gt;stats-[:in]-&amp;gt;game, stats-[:for]-&amp;gt;team WHERE game&amp;lt;-[:away_team]-team RETURN player.name, SUM(stats.goals) AS goals ORDER BY goals DESC LIMIT 10&#39;&#39;&#39;} ] qp.</description>
    </item>
    
    <item>
      <title>Python: Generate all combinations of a list</title>
      <link>https://markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</link>
      <pubDate>Wed, 06 Nov 2013 07:25:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</guid>
      <description>Nathan and I have been playing around with different scikit-learn machine learning classifiers and we wanted to run different combinations of features through each one and work out which gave the best result.
We started with a list of features:
all_columns = [&amp;quot;Fare&amp;quot;, &amp;quot;Sex&amp;quot;, &amp;quot;Pclass&amp;quot;, &#39;Embarked&#39;]  itertools#combinations allows us to create combinations with a length of our choice:
&amp;gt;&amp;gt;&amp;gt; import itertools as it &amp;gt;&amp;gt;&amp;gt; list(it.combinations(all_columns, 3)) [(&#39;Fare&#39;, &#39;Sex&#39;, &#39;Pclass&#39;), (&#39;Fare&#39;, &#39;Sex&#39;, &#39;Embarked&#39;), (&#39;Fare&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;), (&#39;Sex&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;)]  We wanted to create combinations of arbitrary length so we wanted to combine a few invocations of that functions like this:</description>
    </item>
    
    <item>
      <title>Python: matplotlib -  Import error ft2font Symbol not found: _FT_Attach_File (Mac OS X 10.8.3/Mountain Lion)</title>
      <link>https://markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</link>
      <pubDate>Sun, 03 Nov 2013 11:14:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</guid>
      <description>As I mentioned at the end of my last post about the Titanic Kaggle problem our next step was to do some proper machine learning&amp;trade; using scikit-learn so I started by looking at the Decision Tree example.
Unfortunately I ended up on the mother of all yak shaving missions while trying to execute the code which draws a chart using matplotlib.
I ran the following line from the tutorial:
import pylab as pl  which lead to this exception:</description>
    </item>
    
    <item>
      <title>pandas: Adding a column to a DataFrame (based on another DataFrame)</title>
      <link>https://markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</link>
      <pubDate>Wed, 30 Oct 2013 06:12:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</guid>
      <description>Nathan and I have been working on the Titanic Kaggle problem using the pandas data analysis library and one thing we wanted to do was add a column to a DataFrame indicating if someone survived.
We had the following (simplified) DataFrame containing some information about customers on board the Titanic:
def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) customers = pd.DataFrame(columns=[&#39;PassengerId&#39;,&#39;Pclass&#39;,&#39;Name&#39;,&#39;Sex&#39;,&#39;Fare&#39;]) customers = addrow(customers, [dict(PassengerId=892, Pclass=3, Name=&amp;quot;Kelly, Mr. James&amp;quot;, Sex=&amp;quot;male&amp;quot;, Fare=7.8292)]) customers = addrow(customers, [dict(PassengerId=893, Pclass=3, Name=&amp;quot;Wilkes, Mrs.</description>
    </item>
    
    <item>
      <title>Python: for/list comprehensions and dictionaries</title>
      <link>https://markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</link>
      <pubDate>Tue, 13 Aug 2013 22:59:52 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</guid>
      <description>I&#39;ve been working through Coursera&#39;s Linear Algebra course and since all of the exercises are in Python I&#39;ve been playing around with it again.
One interesting thing I learnt is that you can construct dictionaries using a list comprehension type syntax.
For example, if we start with the following dictionaries:
&amp;gt;&amp;gt;&amp;gt; x = { &amp;quot;a&amp;quot;: 1, &amp;quot;b&amp;quot;:2 } &amp;gt;&amp;gt;&amp;gt; y = {1: &amp;quot;mark&amp;quot;, 2: &amp;quot;will&amp;quot;} &amp;gt;&amp;gt;&amp;gt; x {&#39;a&#39;: 1, &#39;b&#39;: 2} &amp;gt;&amp;gt;&amp;gt; y {1: &#39;mark&#39;, 2: &#39;will&#39;}  We might want to create a new dictionary which links from the keys in x to the values in y.</description>
    </item>
    
    <item>
      <title>Ruby/Python: Constructing a taxonomy from an array using zip</title>
      <link>https://markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</link>
      <pubDate>Sun, 19 May 2013 22:44:40 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</guid>
      <description>As I mentioned in my previous blog post I&#39;ve been hacking on a product taxonomy and I wanted to create a &#39;CHILD&#39; relationship between a collection of categories.
For example, I had the following array and I wanted to transform it into an array of &#39;SubCategory, Category&#39; pairs:
taxonomy = [&amp;quot;Cat&amp;quot;, &amp;quot;SubCat&amp;quot;, &amp;quot;SubSubCat&amp;quot;] # I wanted this to become [(&amp;quot;Cat&amp;quot;, &amp;quot;SubCat&amp;quot;), (&amp;quot;SubCat&amp;quot;, &amp;quot;SubSubCat&amp;quot;)  In order to do this we need to zip the first 2 items with the last which I found reasonably easy to do using Python:</description>
    </item>
    
    <item>
      <title>Python: Reading a JSON file</title>
      <link>https://markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</link>
      <pubDate>Tue, 09 Apr 2013 07:23:59 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</guid>
      <description>I&#39;ve been playing around with some code to spin up AWS instances using Fabric and Boto and one thing that I wanted to do was define a bunch of default properties in a JSON file and then load this into a script.
I found it harder to work out how to do this than I expected to so I thought I&#39;d document it for future me!
My JSON file looks like this:</description>
    </item>
    
    <item>
      <title>Python: (Conceptually) removing an item from a tuple</title>
      <link>https://markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</link>
      <pubDate>Sun, 27 Jan 2013 02:30:05 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</guid>
      <description>As part of some code I&#39;ve been playing around I wanted to remove an item from a tuple which wasn&#39;t particularly easy because Python&#39;s tuple data structure is immutable.
I therefore needed to create a new tuple excluding the value which I wanted to remove.
I ended up writing the following function to do this but I imagine there might be an easier way because it&#39;s quite verbose:
def tuple_without(original_tuple, element_to_remove): new_tuple = [] for s in list(original_tuple): if not s == element_to_remove: new_tuple.</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting values by multiple indices</title>
      <link>https://markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</link>
      <pubDate>Sun, 27 Jan 2013 02:21:39 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</guid>
      <description>As I mentioned in my previous post I&#39;ve been playing around with numpy and I wanted to get the values of a collection of different indices in a 2D array.
If we had a 2D array that looked like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])  I knew that it was possible to retrieve the first 3 rows by using the following code:</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting specific column in 2D array</title>
      <link>https://markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</link>
      <pubDate>Sun, 27 Jan 2013 02:10:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</guid>
      <description>I&#39;ve been playing around with numpy this evening in an attempt to improve the performance of a Travelling Salesman Problem implementation and I wanted to get every value in a specific column of a 2D array.
The array looked something like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])  I wanted to get the values for the 2nd column of each row which would return an array containing 1, 6, 11 and 16.</description>
    </item>
    
    <item>
      <title>Fabric: Tailing log files on multiple machines</title>
      <link>https://markhneedham.com/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</link>
      <pubDate>Tue, 15 Jan 2013 00:20:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</guid>
      <description>We wanted to tail one of the log files simultaneously on 12 servers this afternoon to try and see if a particular event was being logged and rather than opening 12 SSH sessions decided to get Fabric to help us out.
My initial attempt to do this was the following:
fab -H host1,host2,host3 -- tail -f /var/www/awesome/current/log/production.log  It works but the problem is that by default Fabric runs the specified command one machine after the other so we&#39;ve actually managed to block Fabric with the tail command on &#39;host1&#39;.</description>
    </item>
    
    <item>
      <title>Knapsack Problem: Python vs Ruby</title>
      <link>https://markhneedham.com/blog/2013/01/07/knapsack-problem-python-vs-ruby/</link>
      <pubDate>Mon, 07 Jan 2013 00:47:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/07/knapsack-problem-python-vs-ruby/</guid>
      <description>The latest algorithm that we had to code in Algorithms 2 was the Knapsack problem which is as follows:
 The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.</description>
    </item>
    
  </channel>
</rss>