<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/python/</link>
    <description>Recent content in python on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Aug 2023 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python: TypeError: Instance and class checks can only be used with @runtime_checkable protocols</title>
      <link>https://www.markhneedham.com/blog/2023/08/21/python-typeerrr-instance-class-check-runtime-checkable/</link>
      <pubDate>Mon, 21 Aug 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/08/21/python-typeerrr-instance-class-check-runtime-checkable/</guid>
      <description>Iâ€™ve been playing around with ChromaDB and I wanted to programatically get a list of the embedding functions, which was a little trickier thna I expected. In this blog post, weâ€™ll explore how I failed and then succeeded at this task.
But first, letâ€™s install ChromaDB:
pip install chromadb The embedding functions live in the chromadb.utils.embedding_functions module. So my first thought was that I could list all the things defined in that module and then check which ones were a sub class of EmbeddingFunction:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Experimenting with the StarTree Index</title>
      <link>https://www.markhneedham.com/blog/2023/07/28/apache-pinot-experimenting-with-startree-index/</link>
      <pubDate>Fri, 28 Jul 2023 11:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/28/apache-pinot-experimenting-with-startree-index/</guid>
      <description>My colleagues Sandeep Dabade and Kulbir Nijjer recently wrote a three part blog post series about the StarTree index, an Apache Pinot indexing technique that dynamically builds a tree structure to maintain aggregates across a group of dimensions. Iâ€™ve not used this index before and wanted to give it a try and in this blog post, Iâ€™ll share what I learned.
Iâ€™ve put all the code in the startreedata/pinot-recipes GitHub repository in case you want to try it out yourself.</description>
    </item>
    
    <item>
      <title>Python/Poetry: Library not loaded: no such file, not in dyld cache</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</link>
      <pubDate>Thu, 27 Jul 2023 11:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</guid>
      <description>As I mentioned in a previous blog post, Iâ€™ve been using Pythonâ€™s Poetry library, but today it stopped working! In this blog post, Iâ€™ll explain what happened and how I got it working again.
It started off innocent enough, with me trying to create a new project:
poetry init But instead of seeing the usual interactive wizard, I got the following error:
Output dyld[20269]: Library not loaded: /opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/Python Referenced from: &amp;lt;1B2377F9-2187-39A9-AA98-20E438024DE2&amp;gt; /Users/markhneedham/Library/Application Support/pypoetry/venv/bin/python Reason: tried: &amp;#39;/opt/homebrew/Cellar/python@3.</description>
    </item>
    
    <item>
      <title>OpenAI/GPT: Returning consistent/valid JSON from a prompt</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</link>
      <pubDate>Thu, 27 Jul 2023 01:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</guid>
      <description>When using OpenAI it can be tricky to get it to return a consistent response for a prompt. In this blog post, weâ€™re going to learn how to use functions to return a consistent JSON format for a basic sentiment analysis prompt.
Iâ€™ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, Iâ€™ve embedded it below:</description>
    </item>
    
    <item>
      <title>Confluent Kafka: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.</title>
      <link>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</link>
      <pubDate>Tue, 25 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</guid>
      <description>Iâ€™ve been creating a demo showing how to ingest Avro-encoded data from Apache Kafka into Apache Pinot and ran into a deprecation warning. In this blog post, Iâ€™ll show how to update code using the Confluent Kafka Python client to get rid of that warning.
I started by installing the following libraries:
pip install confluent-kafka avro urllib3 requests And then my code to publish an Avro encoded event to Kafka looked like this:</description>
    </item>
    
    <item>
      <title>VSCode: Adding Poetry Python Interpreter</title>
      <link>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</link>
      <pubDate>Mon, 24 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</guid>
      <description>Iâ€™ve been trying out Pythonâ€™s Poetry dependency management tool recently and I really like it, but couldnâ€™t figure out how to get it setup as VSCodeâ€™s Python interpreter. In this blog post, weâ€™ll learn how to do that.
One way to add the Python interpreter in VSCode is to press Cmd+Shift+p and then type Python Interpreter. If you select the first result, youâ€™ll see something like the following:
Figure 1.</description>
    </item>
    
    <item>
      <title>Quix Streams: Consuming and Producing JSON messages</title>
      <link>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</link>
      <pubDate>Tue, 11 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</guid>
      <description>Iâ€™ve been meaning to take Quix Streams for a spin for a while and got the chance while building a recent demo. Quix Streams is a library for building streaming applications on time-series data, but I wanted to use it to do some basic consuming and producing of JSON messages. Thatâ€™s what weâ€™re going to do in this blog post.
Weâ€™re going to use Redpanda to store our messages. Weâ€™ll launch a Redpanda instance using the following Docker Compose file:</description>
    </item>
    
    <item>
      <title>Python: Re-import module</title>
      <link>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</link>
      <pubDate>Fri, 07 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</guid>
      <description>I often write little Python scripts that import code from other local modules and a common problem I have when using the Python REPL is that I update the code in the other modules and then canâ€™t use the new functionality without restarting the REPL and re-importing everything. At least so I thought! It turns out there is a way to refresh those modules and thatâ€™s what weâ€™ll be exploring in this blog post.</description>
    </item>
    
    <item>
      <title>Python: All about the next function</title>
      <link>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</link>
      <pubDate>Wed, 28 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</guid>
      <description>Yesterday I wrote a blog post about some different ways to take the first element from a Python list. Afterward I was chatting to my new rubber duck, ChatGPT, which suggested the next function on an iterator as an alternative approach. And so thatâ€™s what weâ€™re going to explore in this blog post.
The next function gets the first value from an iterator and optionally returns a provided default value if the iterator is empty.</description>
    </item>
    
    <item>
      <title>Python: Get the first item from a collection, ignore the rest</title>
      <link>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</link>
      <pubDate>Tue, 27 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</guid>
      <description>When writing Python scripts, I often find myself wanting to take the first item from a collection and ignore the rest of the values. I usually use something like values[0] to take the first value from the list, but I was curious whether I could do better by using destructuring. Thatâ€™s what weâ€™re going to explore in this blog post.
Weâ€™ll start with a list that contains some names:</description>
    </item>
    
    <item>
      <title>Python: Working with tuples in lambda expressions</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</guid>
      <description>Iâ€™m still playing around with data returned by Apache Pinotâ€™s HTTP API and I wanted to sort a dictionary of segment names by partition id and index. In this blog post weâ€™re going to look into how to do that.
Weâ€™ll start with the following dictionary:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__13__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } As I mentioned above, I want to sort the dictionaryâ€™s items by partition id and index, which are embedded inside the key name.</description>
    </item>
    
    <item>
      <title>Python: Padding a string</title>
      <link>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</link>
      <pubDate>Mon, 05 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</guid>
      <description>Iâ€™ve been writing some scripts to parse data from Apache Pinotâ€™s HTTP API and I wanted to format the values stored in a map to make them more readable. In this blog post, weâ€™ll look at some ways that I did that.
I started with a map that looked a bit like this:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } And then I iterated over and printed each item like this:</description>
    </item>
    
    <item>
      <title>Python: Naming slices</title>
      <link>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</link>
      <pubDate>Sat, 13 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</guid>
      <description>Another gem from Fluent Python is that you can name slices. How did I not know that?!
Letâ€™s have a look how it works using an example of a Vehicle Identification Number, which has 17 characters that act as a unique identifier for a vehicle. Different parts of that string mean different things.
So given the following VIN:
vin = &amp;#34;2B3HD46R02H210893&amp;#34; We can extract components like this:
print(f&amp;#34;&amp;#34;&amp;#34; World manufacturer identifier: {vin[0:3]} Vehicle Descriptor: {vin[3:9]} Vehicle Identifier: {vin[9:17]} &amp;#34;&amp;#34;&amp;#34;.</description>
    </item>
    
    <item>
      <title>Python 3.10: Pattern matching with match/case</title>
      <link>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</link>
      <pubDate>Tue, 09 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</guid>
      <description>Iâ€™ve been reading Fluent Python and learnt about pattern matching with the match/case statement, introduced in Python 3.10. You can use it instead of places where youâ€™d otherwise use if, elif, else statements.
I created a small example to understand how it works. The following function takes in a list where the first argument should be foo, followed by a variable number of arguments, which we print to the console:</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post weâ€™re going to walk through some options for exporting big CSV files to Parquet format.
Note Iâ€™ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, Iâ€™ve embedded it below:</description>
    </item>
    
    <item>
      <title>Python: Sorting lists of dictionaries with sortedcontainers</title>
      <link>https://www.markhneedham.com/blog/2022/12/02/python-sorting-lists-dictionaries-sortedcontainers/</link>
      <pubDate>Fri, 02 Dec 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/12/02/python-sorting-lists-dictionaries-sortedcontainers/</guid>
      <description>I was recently working on a Kafka streams data generator, where I only wanted to publish events once the time on those events had been reached. To solve this problem I needed a sorted list and in this blog post weâ€™re going to explore how I went about doing this.
Note Iâ€™ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, Iâ€™ve embedded it below:</description>
    </item>
    
    <item>
      <title>Strava: Export and interpolate lat/long points for an activity</title>
      <link>https://www.markhneedham.com/blog/2022/01/18/strava-export-interpolate-lat-long-points-activity/</link>
      <pubDate>Tue, 18 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/01/18/strava-export-interpolate-lat-long-points-activity/</guid>
      <description>Iâ€™ve been working with Strava data again recently and wanted to extract all the lat/long coordinates recorded for my runs. Having done this, I realised that my running watch hadnâ€™t recorded as many points as I expected, so I needed to interpolate the missing points. In this blog post weâ€™ll learn how to do that.
Figure 1. Strava: Export and interpolate lat/long points for an activity Setup Letâ€™s first install a few libraries that weâ€™ll be using:</description>
    </item>
    
    <item>
      <title>Python: Generate WKT from Lat Long Coordinates</title>
      <link>https://www.markhneedham.com/blog/2022/01/14/python-generate-wkt-lat-long-coordinates/</link>
      <pubDate>Fri, 14 Jan 2022 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/01/14/python-generate-wkt-lat-long-coordinates/</guid>
      <description>Recently Iâ€™ve been playing around with geometry objects in WKT format while documenting Apache Pinotâ€™s Geospatial functions. I then wanted to figure out how to generate a WKT string from a list of lat long coordinates, which weâ€™ll learn how to do in this blog post.
Figure 1. Python: Generate WKT from Lat Long Coordinates Weâ€™re going to do all this using Pythonâ€™s Shapely library, so letâ€™s first install that library:</description>
    </item>
    
    <item>
      <title>Pandas: Add row to DataFrame</title>
      <link>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</link>
      <pubDate>Thu, 13 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</guid>
      <description>Usually when Iâ€™m working with Pandas DataFrames I want to add new columns of data, but I recently wanted to add a row to an existing DataFrame. It turns out there are more than one ways to do that, which weâ€™ll explore in this blog post.
Letâ€™s start by importing Pandas into our Python script:
import pandas as pd Weâ€™ll start from a DataFrame that has two rows and the columns name and age:</description>
    </item>
    
    <item>
      <title>Altair/Pandas: TypeError: Cannot interpret &#39;Float64Dtype()&#39; as a data type</title>
      <link>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</link>
      <pubDate>Wed, 28 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</guid>
      <description>I ran into an interesting problem when trying to use Altair to visualise a Pandas DataFrame containing vaccination rates of different parts of England. In this blog post weâ€™ll look at how to work around this issue.
First, letâ€™s install Pandas, numpy, and altair:
pip install pandas altair numpy And now weâ€™ll import those modules into a Python script or Jupyter notebook:
import pandas as pd import altair as alt import numpy as np Next, weâ€™ll create a DataFrame containing the vaccinations rates of a couple of regions:</description>
    </item>
    
    <item>
      <title>Pandas: Compare values in DataFrame to previous days</title>
      <link>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</link>
      <pubDate>Wed, 21 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</guid>
      <description>Iâ€™m still playing around with Covid vaccine data, this time exploring how the number of doses varies week by week. I want to know how many more (or less) vaccines have been done on a given day compared to that same day last week.
Weâ€™ll be using Pandas in this blog post, so letâ€™s first install that library and import it:
Install Pandas pip install pandas Import module import pandas as pd And now letâ€™s create a DataFrame containing a subset of the data that Iâ€™m working with:</description>
    </item>
    
    <item>
      <title>Vaccinating England: The Data (cleanup)</title>
      <link>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</link>
      <pubDate>Sat, 17 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</guid>
      <description>Over the last 13 months Iâ€™ve spent countless hours looking at dashboards that showed Coronavirus infection rates, death rates, and numbers of people vaccinated. The UK government host a dashboard at coronavirus.data.gov.uk, which contains charts and tables showing all of the above.
One thing I havenâ€™t been able to find, however, is a drill down of vaccinations by local area and age group. So Iâ€™m going to try to build my own!</description>
    </item>
    
    <item>
      <title>Pandas - Format DataFrame numbers with commas and control decimal places</title>
      <link>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</link>
      <pubDate>Sun, 11 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</guid>
      <description>Iâ€™m still playing around with the UKâ€™s COVID-19 vaccination data and in this blog post weâ€™ll learn how to format a DataFrame that contains a mix of string and numeric values.
Note On 10th November 2022 I created a video that covers the same content as this blog post. Let me know if itâ€™s helpful ðŸ˜Š
Weâ€™ll be using Pandas&amp;#39; styling functionality, which generates CSS and HTML, so if you want to follow along youâ€™ll need to install Pandas and Jupyter:</description>
    </item>
    
    <item>
      <title>Pandas - Dividing two DataFrames (TypeError: unsupported operand type(s) for /: &#39;str&#39; and &#39;str&#39;)</title>
      <link>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</link>
      <pubDate>Thu, 08 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</guid>
      <description>Iâ€™ve been doing some more exploration of the UK Coronavirus vaccine data, this time looking at the number of people vaccinated by Local Tier Local Authority. The government publish data showing the number of people vaccinated in each authority by age group, as well as population estimates for each cohort.
Having loaded that data into two Pandas DataFrames, I wanted to work out the % of people vaccinated per age group per local area.</description>
    </item>
    
    <item>
      <title>Altair - Remove margin/padding on discrete X axis</title>
      <link>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</link>
      <pubDate>Fri, 02 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</guid>
      <description>One of the Altair charts on my Covid Vaccine Dashboards Streamlit app shows the % of first doses, but when I first created it there was some padding on the X axis that I wanted to remove. In this blog post weâ€™ll learn how to do that.
Pre requisites Letâ€™s start by installing the following libraries:
pip install pandas altair altair_viewer Next letâ€™s import them, as shown below:
import pandas as pd import altair as alt Visualising % of first doses Now weâ€™re going to create a DataFrame that contains two columns - one contains the year and week number, the other the percentage of 1st doses administered.</description>
    </item>
    
    <item>
      <title>Pandas: Filter column value in array/list - ValueError: The truth value of a Series is ambiguous</title>
      <link>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</link>
      <pubDate>Sun, 28 Mar 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</guid>
      <description>The UK government publishes Coronavirus vaccinations data on coronavirus.data.gov.uk, but I wanted to create some different visualisations so I downloaded the data and have been playing with it in the mneedham/covid-vaccines GitHub repository.
I massaged the data so that I have rows in a Pandas DataFrame representing the numbers of first doses, second doses, and total doses done each day. I then wanted to filter this DataFrame based on the type of dose, but initially got a bit stuck.</description>
    </item>
    
    <item>
      <title>Strava: Export all activities to JSON file</title>
      <link>https://www.markhneedham.com/blog/2020/12/20/strava-export-all-activities-json/</link>
      <pubDate>Sun, 20 Dec 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/12/20/strava-export-all-activities-json/</guid>
      <description>In my continued playing around with the Strava API, I wanted to write a script to download all of my Strava activities to a JSON file.
As I mentioned in a previous blog post, the approach to authenticating requests has changed in the last two years, so we first need to generate an access token via the OAuth endpoint. Luckily Odd Eirik Igland shared a script showing how to solve most of the problem, and Iâ€™ve adapted it to do what I want.</description>
    </item>
    
    <item>
      <title>pipenv: ImportError: No module named &#39;virtualenv.seed.via_app_data&#39;</title>
      <link>https://www.markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</link>
      <pubDate>Fri, 07 Aug 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</guid>
      <description>Iâ€™ve been trying to install pipenv on a new computer and ran into a frustrating issue. After installing pipenv using pip, I tried to run the command below:
$ /home/markhneedham/.local/bin/pipenv shell Creating a virtualenv for this projectâ€¦ Pipfile: /tmp/Pipfile Using /usr/bin/python3.8 (3.8.2) to create virtualenvâ€¦ â ™ Creating virtual environment...ModuleNotFoundError: No module named &amp;#39;virtualenv.seed.via_app_data&amp;#39; âœ˜ Failed creating virtual environment [pipenv.exceptions.VirtualenvCreationException]: Failed to create virtual environment. Hmmm, for some reason itâ€™s unable to find one of the virtualenv modules.</description>
    </item>
    
    <item>
      <title>Python: Select keys from map/dictionary</title>
      <link>https://www.markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</link>
      <pubDate>Mon, 27 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</guid>
      <description>In this post weâ€™re going to learn how to filter a Python map/dictionary to return a subset of keys or values. I needed to do this recently while logging some maps that had a lot of keys that I wasnâ€™t interested in.
Weâ€™ll start with the following map:
x = {&amp;#34;a&amp;#34;: 1, &amp;#34;b&amp;#34;: 2, &amp;#34;c&amp;#34;: 3, &amp;#34;d&amp;#34;: 4} {&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 2, &amp;#39;c&amp;#39;: 3, &amp;#39;d&amp;#39;: 4} We want to filter this map so that we only have the keys a and c.</description>
    </item>
    
    <item>
      <title>Python: Find the starting Sunday for all the weeks in a month</title>
      <link>https://www.markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</link>
      <pubDate>Sat, 18 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</guid>
      <description>In this post weâ€™re going to learn how to find the dates of all the Sundays in a given month, as well as the Sunday immediately preceding the 1st day in the month, assuming that day isnâ€™t a Sunday.
Letâ€™s start by importing some libraries that weâ€™re going to use in this blog post:
from dateutil import parser import datetime import calendar Next we need to find the first day of the current month, which we can do with the following code:</description>
    </item>
    
    <item>
      <title>Creating an Interactive UK Official Charts Data App with Streamlit and Neo4j</title>
      <link>https://www.markhneedham.com/blog/2020/01/16/interactive-uk-charts-quickgraph-neo4j-streamlit/</link>
      <pubDate>Thu, 16 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/16/interactive-uk-charts-quickgraph-neo4j-streamlit/</guid>
      <description>I recently came across Streamlit, a tool that makes it easy to build data based single page web applications. I wanted to give it a try, and the UK Charts QuickGraph that I recently wrote about seemed like a good opportunity for that.
This blog post starts from where we left off. The data is loaded into Neo4j and weâ€™ve written some queries to explore different aspects of the dataset.</description>
    </item>
    
    <item>
      <title>Python: Altair - Setting the range of Date values for an axis</title>
      <link>https://www.markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</link>
      <pubDate>Tue, 14 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</guid>
      <description>In my continued experiments with the Altair visualisation library, I wanted to set a custom range of data values on the x axis of a chart. In this blog post weâ€™ll learn how to do that.
Weâ€™ll start where we left off in the last blog post, with the following code that renders a scatterplot containing the chart position of a song on a certain date:
import altair as alt import pandas as pd import datetime df = pd.</description>
    </item>
    
    <item>
      <title>Python: Altair - TypeError: Object of type date is not JSON serializable</title>
      <link>https://www.markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</link>
      <pubDate>Fri, 10 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</guid>
      <description>Iâ€™ve been playing with the Altair statistical visualisation library and recently ran into an error while trying to render a DataFrame that contained dates.
I was trying to render a scatterplot containing the chart position of a song on a certain date, as seen in the code below:
# pip install altair pandas import altair as alt import pandas as pd import datetime df = pd.DataFrame( [ {&amp;#34;position&amp;#34;: 2, &amp;#34;date&amp;#34;: datetime.</description>
    </item>
    
    <item>
      <title>Spotify API: Making my first call</title>
      <link>https://www.markhneedham.com/blog/2020/01/02/spotify-api-making-my-first-call/</link>
      <pubDate>Thu, 02 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/02/spotify-api-making-my-first-call/</guid>
      <description>I wanted to enrich the data for a little music application Iâ€™m working on and realised it would be a perfect opportunity to try out the Spotify API. I want to extract data about individual tracks (via the Tracks API), but before we do that weâ€™ll need to create an app and have it approved for access to the Spotify API.
Registering an application After logging into the Spotify Dashboard using my usual Spotify credentials, I was prompted to create an application:</description>
    </item>
    
    <item>
      <title>Elasticsearch: Importing data into App Search</title>
      <link>https://www.markhneedham.com/blog/2019/11/24/elasticsearch-import-data-appsearch/</link>
      <pubDate>Sun, 24 Nov 2019 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/11/24/elasticsearch-import-data-appsearch/</guid>
      <description>For a side project that Iâ€™m working on I wanted to create a small React application that can query data stored in Elasticsearch, and most of the tutorials I found suggested using a tool called Elastic App Search.
Iâ€™d not heard of App Search before, and it took me a while to figure out that itâ€™s the mid level product in between Elasticsearch Service and Elastic Site Search Service, as described on elastic.</description>
    </item>
    
    <item>
      <title>Python: Click - Handling Date Parameter</title>
      <link>https://www.markhneedham.com/blog/2019/07/29/python-click-date-parameter-type/</link>
      <pubDate>Mon, 29 Jul 2019 11:08:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/07/29/python-click-date-parameter-type/</guid>
      <description>Iâ€™ve been building a little CLI application using the Python Click Library, and I wanted to pass in a Date as a parameter. Thereâ€™s more than one way to do this.
Letâ€™s first install the Click library:
pip install click And now weâ€™ll import our required libraries:
from datetime import date import click Now weâ€™ll create a sub command that takes two parameters: date-start and date-end. These parameters have the type DateTime, and we can pass a string in the format yyyy-mm-dd from the command line:</description>
    </item>
    
    <item>
      <title>Kafka: Python Consumer - No messages with group id/consumer group</title>
      <link>https://www.markhneedham.com/blog/2019/06/03/kafka-python-consumer-no-messages-group-id-consumer-group/</link>
      <pubDate>Mon, 03 Jun 2019 11:08:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/06/03/kafka-python-consumer-no-messages-group-id-consumer-group/</guid>
      <description>When Iâ€™m learning a new technology, I often come across things that are incredibly confusing when I first come across them, but make complete sense afterwards. In this post Iâ€™ll explain my experience writing a Kafka consumer that wasnâ€™t finding any messages when using consumer groups .
Setting up Kafka infrastructure Weâ€™ll set up the Kafka infrastructure locally using the Docker Compose Template that I describe in my Kafka: A Basic Tutorial blog post.</description>
    </item>
    
    <item>
      <title>Kafka: A basic tutorial</title>
      <link>https://www.markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</link>
      <pubDate>Thu, 16 May 2019 10:02:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</guid>
      <description>In this post weâ€™re going to learn how to launch Kafka locally and write to and read from a topic using one of the Python drivers.
To make things easy for myself, Iâ€™ve created a Docker Compose template that launches 3 containers:
broker - our Kafka broker
zookeeper - used by Kafka for leader election
jupyter - notebooks for connecting to our Kafka broker
This template can be downloaded from the mneedham/basic-kafka-tutorial repository, and reads as follows:</description>
    </item>
    
    <item>
      <title>Jupyter: RuntimeError: This event loop is already running</title>
      <link>https://www.markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</link>
      <pubDate>Fri, 10 May 2019 23:00:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</guid>
      <description>Iâ€™ve been using the twint library to explore the Neo4j twitter community, and ran into an initially confusing error when I moved the code Iâ€™d written into a Jupyter notebook.
The first three cells of my notebook contain the following code:
Cell 1:
! pip install twint Cell 2:
import json import twint Cell 3:
users = [&amp;#34;vikatakavi11&amp;#34;, &amp;#34;tee_mars3&amp;#34;] for username in users[:10]: c = twint.Config() c.Username = username c.</description>
    </item>
    
    <item>
      <title>Python: Getting GitHub download count from the GraphQL API using requests</title>
      <link>https://www.markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</link>
      <pubDate>Sun, 07 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</guid>
      <description>I was recently trying to use some code I shared just over a year ago to compute GitHub Project download numbers from the GraphQL API, and wanted to automate this in a Python script.
It was more fiddly than I expected, so I thought Iâ€™d share the code for the benefit of future me more than anything else!
Pre requisites Weâ€™re going to use the popular requests library to query the API, so we need to import that.</description>
    </item>
    
    <item>
      <title>Finding famous MPs based on their Wikipedia Page Views</title>
      <link>https://www.markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</link>
      <pubDate>Mon, 01 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</guid>
      <description>As part of the Graphing Brexit series of blog posts, I wanted to work out who were the most important Members of the UK parliament, and after a bit of Googling I realised that views of their Wikipedia pages would do the trick.
I initially found my way to tools.wmflabs.org, which is great for exploring the popularity of an individual MP, but not so good if you want to extract the data for 600 of them.</description>
    </item>
    
    <item>
      <title>Python: Add query parameters to a URL</title>
      <link>https://www.markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</link>
      <pubDate>Fri, 11 Jan 2019 09:42:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</guid>
      <description>I was recently trying to automate adding a query parameter to a bunch of URLS and came across a neat approach a long way down this StackOverflow answer, that uses the PreparedRequest class from the requests library.
Letâ€™s first get the class imported:
from requests.models import PreparedRequest req = PreparedRequest() And now letâ€™s use use this class to add a query parameter to a URL. We can do this with the following code:</description>
    </item>
    
    <item>
      <title>Python: Pandas - DataFrame plotting ignoring figure</title>
      <link>https://www.markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</link>
      <pubDate>Tue, 25 Dec 2018 21:09:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</guid>
      <description>In my continued use of matplotlib I wanted to change the size of the chart I was plotting and struggled a bit to start with. Weâ€™ll use the same DataFrame as before:
df = pd.DataFrame({ &amp;#34;name&amp;#34;: [&amp;#34;Mark&amp;#34;, &amp;#34;Arya&amp;#34;, &amp;#34;Praveena&amp;#34;], &amp;#34;age&amp;#34;: [34, 1, 31] }) df In my last blog post I showed how we can create a bar chart by executing the following code:
df.plot.bar(x=&amp;#34;name&amp;#34;) plt.tight_layout() plt.show() plt.close() But how do we make it bigger?</description>
    </item>
    
    <item>
      <title>Pandas: Create matplotlib plot with x-axis label not index</title>
      <link>https://www.markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</link>
      <pubDate>Fri, 21 Dec 2018 16:57:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</guid>
      <description>Iâ€™ve been using matplotlib a bit recently, and wanted to share a lesson I learnt about choosing the label of the x-axis. Letâ€™s first import the libraries weâ€™ll use in this post:
import pandas as pd import matplotlib.pyplot as plt And now weâ€™ll create a DataFrame of values that we want to chart:
df = pd.DataFrame({ &amp;#34;name&amp;#34;: [&amp;#34;Mark&amp;#34;, &amp;#34;Arya&amp;#34;, &amp;#34;Praveena&amp;#34;], &amp;#34;age&amp;#34;: [34, 1, 31] }) df This is what our DataFrame looks like:</description>
    </item>
    
    <item>
      <title>PySpark: Creating DataFrame with one column - TypeError: Can not infer schema for type: &lt;type &#39;int&#39;&gt;</title>
      <link>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</link>
      <pubDate>Sun, 09 Dec 2018 10:25:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</guid>
      <description>Iâ€™ve been playing with PySpark recently, and wanted to create a DataFrame containing only one column. I tried to do this by writing the following code:
spark.createDataFrame([(1)], [&amp;#34;count&amp;#34;]) If we run that code weâ€™ll get the following error message:
Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&amp;#34;, line 748, in createDataFrame rdd, schema = self._createFromLocal(map(prepare, data), schema) File &amp;#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&amp;#34;, line 416, in _createFromLocal struct = self.</description>
    </item>
    
    <item>
      <title>matplotlib - Create a histogram/bar chart for ratings/full numbers</title>
      <link>https://www.markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</link>
      <pubDate>Mon, 24 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</guid>
      <description>In my continued work with matplotlib I wanted to plot a histogram (or bar chart) for a bunch of star ratings to see how they were distributed.
Before we do anything letâ€™s import matplotlib as well as pandas:
import random import pandas as pd import matplotlib matplotlib.use(&amp;#39;TkAgg&amp;#39;) import matplotlib.pyplot as plt plt.style.use(&amp;#39;fivethirtyeight&amp;#39;) Next weâ€™ll create an array of randomly chosen star ratings between 1 and 5:
stars = pd.Series([random.randint(1, 5) for _ in range(0, 100)]) We want to plot a histogram showing the proportion for each rating.</description>
    </item>
    
    <item>
      <title>matplotlib - MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.</title>
      <link>https://www.markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</link>
      <pubDate>Tue, 18 Sep 2018 07:56:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</guid>
      <description>In my last post I showed how to remove axes legends from a matplotlib chart, and while writing the post I actually had the change the code I used as my initial approach is now deprecated.
As in the previous post, weâ€™ll first import pandas and matplotlib:
import pandas as pd import matplotlib matplotlib.use(&amp;#39;TkAgg&amp;#39;) import matplotlib.pyplot as plt plt.style.use(&amp;#39;fivethirtyeight&amp;#39;) And weâ€™ll still use this DataFrame:
df = pd.DataFrame({&amp;#34;label&amp;#34;: [&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;, &amp;#34;C&amp;#34;, &amp;#34;D&amp;#34;], &amp;#34;count&amp;#34;: [12, 19, 5, 10]}) My initial approach to remove all legends was this:</description>
    </item>
    
    <item>
      <title>matplotlib - Remove axis legend</title>
      <link>https://www.markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</link>
      <pubDate>Tue, 18 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</guid>
      <description>Iâ€™ve been working with matplotlib a bit recently, and I wanted to remove all axis legends from my chart. It took me a bit longer than I expected to figure it out so I thought Iâ€™d write it up.
Before we do anything letâ€™s import matplotlib as well as pandas, since weâ€™re going to plot data from a pandas DataFrame.
import pandas as pd import matplotlib matplotlib.use(&amp;#39;TkAgg&amp;#39;) import matplotlib.pyplot as plt plt.</description>
    </item>
    
    <item>
      <title>QuickGraph #1: Analysing Python Dependency Graph with PageRank, Closeness Centrality, and Betweenness Centrality</title>
      <link>https://www.markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</link>
      <pubDate>Mon, 16 Jul 2018 05:25:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</guid>
      <description>Iâ€™ve always wanted to build a dependency graph of libraries in the Python ecosytem but I never quite got around to itâ€¦â€‹until now! I thought I might be able to get a dump of all the libraries and their dependencies, but while searching I came across this article which does a good job of explaining why thatâ€™s not possible.
Finding Python Dependencies The best we can do is generate a dependency graph of our locally installed packages using the excellent pipdeptree tool.</description>
    </item>
    
    <item>
      <title>Python: Parallel download files using requests</title>
      <link>https://www.markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</link>
      <pubDate>Sun, 15 Jul 2018 15:10:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</guid>
      <description>I often find myself downloading web pages with Pythonâ€™s requests library to do some local scrapping when building datasets but Iâ€™ve never come up with a good way for downloading those pages in parallel.
Below is the code that I use. First weâ€™ll import the required libraries:
import os import requests from time import time as timer And now a function that streams a response into a local file:</description>
    </item>
    
    <item>
      <title>Neo4j: Querying the Strava Graph using Py2neo</title>
      <link>https://www.markhneedham.com/blog/2018/06/15/neo4j-querying-strava-graph-py2neo/</link>
      <pubDate>Fri, 15 Jun 2018 13:45:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/06/15/neo4j-querying-strava-graph-py2neo/</guid>
      <description>Last week Nigel released v4 of Py2neo and given I was just getting ready to write some queries against my Strava activity graph I thought Iâ€™d give it a try.
If you want to learn how to create your own Strava graph you should read my previous post, but just to recap, this is the graph model that we created:
Letâ€™s get to it!
tl;dr the code in this post is available as a Jupyter notebook so if you want the code and nothing but the code head over there!</description>
    </item>
    
    <item>
      <title>Interpreting Word2vec or GloVe embeddings using scikit-learn and Neo4j graph algorithms</title>
      <link>https://www.markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</link>
      <pubDate>Sat, 19 May 2018 09:47:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</guid>
      <description>A couple of weeks I came across a paper titled Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings via Abigail See&amp;#39;s blog post about ACL 2017.
The paper explains an algorithm that helps to make sense of word embeddings generated by algorithms such as Word2vec and GloVe.
Iâ€™m fascinated by how graphs can be used to interpret seemingly black box data, so I was immediately intrigued and wanted to try and reproduce their findings using Neo4j.</description>
    </item>
    
    <item>
      <title>Predicting movie genres with node2Vec and Tensorflow</title>
      <link>https://www.markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</link>
      <pubDate>Fri, 11 May 2018 08:12:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</guid>
      <description>In my previous post we looked at how to get up and running with the node2Vec algorithm, and in this post weâ€™ll learn how we can feed graph embeddings into a simple Tensorflow model.
Recall that node2Vec takes in a list of edges (or relationships) and gives us back an embedding (array of numbers) for each node.
This time weâ€™re going to run the algorithm over a movies recommendation dataset from the Neo4j Sandbox.</description>
    </item>
    
    <item>
      <title>Exploring node2vec - a graph embedding algorithm</title>
      <link>https://www.markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</link>
      <pubDate>Fri, 11 May 2018 08:08:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</guid>
      <description>In my explorations of graph based machine learning, one algorithm I came across is called node2Vec. The paper describes it as &amp;#34;an algorithmic framework for learning continuous feature representations for nodes in networks&amp;#34;.
So what does the algorithm do? From the website:
The node2vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective. The objective is flexible, and the algorithm accommodates for various definitions of network neighborhoods by simulating biased random walks.</description>
    </item>
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://www.markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week Iâ€™ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiriâ€™s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
Iâ€™ll use the same data as Sidath, so if you want to follow along youâ€™ll need to download these files:
iris_training.csv
iris_test.csv
Loading data The way we load data will remain exactly the same - weâ€™ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Python via virtualenv on Mac OS X: RuntimeError: Python is not installed as a framework.</title>
      <link>https://www.markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</link>
      <pubDate>Fri, 04 May 2018 22:03:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</guid>
      <description>Iâ€™ve previously written a couple of blog posts about my troubles getting matplotlib to play nicely and I run into a slightly different variant today while following Sidath Asiriâ€™s Hello World in TensorFlow tutorial.
When I ran the script using a version of Python installed via virtualenv I got the following exception:
Traceback (most recent call last): File &amp;#34;iris.py&amp;#34;, line 4, in &amp;lt;module&amp;gt; from matplotlib import pyplot as plt File &amp;#34;/Users/markneedham/projects/tensorflow-playground/a/lib/python3.</description>
    </item>
    
    <item>
      <title>PyData London 2018 Conference Experience Report</title>
      <link>https://www.markhneedham.com/blog/2018/04/29/pydata-london-2018/</link>
      <pubDate>Sun, 29 Apr 2018 11:54:02 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/29/pydata-london-2018/</guid>
      <description>Over the last few days I attended PyData London 2018 and wanted to share my experience. The PyData series of conferences aim to bring together users and developers of data analysis tools to share ideas and learn from each other. I presented a talk on building a recommendation with Python and Neo4j at the 2016 version but didnâ€™t attend last year.
The organisers said there were ~ 550 attendees spread over 1 day of tutorials and 2 days of talks.</description>
    </item>
    
    <item>
      <title>Python: Serialize and Deserialize Numpy 2D arrays</title>
      <link>https://www.markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</link>
      <pubDate>Sat, 07 Apr 2018 19:38:36 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</guid>
      <description>Iâ€™ve been playing around with saving and loading scikit-learn models and needed to serialize and deserialize Numpy arrays as part of the process.
I could use pickle but that seems a bit overkill so I decided instead to save the byte representation of the array. We can get that representation by calling the tobytes method on a Numpy array:
import numpy as np &amp;gt;&amp;gt;&amp;gt; np.array([ [1,2,3], [4,5,6], [7,8,9] ]) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Python 3: Converting a list to a dictionary with dictionary comprehensions</title>
      <link>https://www.markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</link>
      <pubDate>Mon, 02 Apr 2018 04:20:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</guid>
      <description>When coding in Python I often find myself with lists containing key/value pairs that I want to convert to a dictionary.
In a recent example I had the following code:
values = [{&amp;#39;key&amp;#39;: &amp;#39;name&amp;#39;, &amp;#39;value&amp;#39;: &amp;#39;Mark&amp;#39;}, {&amp;#39;key&amp;#39;: &amp;#39;age&amp;#39;, &amp;#39;value&amp;#39;: 34}] And I wanted to create a dictionary that had the keys name and age and their respective values. The easiest way to convert this list to a dictionary is to iterate over the list and construct the dictionary key by key:</description>
    </item>
    
    <item>
      <title>Strava: Calculating the similarity of two runs</title>
      <link>https://www.markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</link>
      <pubDate>Thu, 18 Jan 2018 23:35:25 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/01/18/strava-calculating-similarity-two-runs/</guid>
      <description>I go running several times a week and wanted to compare my runs against each other to see how similar they are.
I record my runs with the Strava app and it has an API that returns lat/long coordinates for each run in the Google encoded polyline algorithm format.
We can use the polyline library to decode these values into a list of lat/long tuples. For example:
import polyline polyline.</description>
    </item>
    
    <item>
      <title>scikit-learn: Using GridSearch to tune the hyper-parameters of VotingClassifier</title>
      <link>https://www.markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</link>
      <pubDate>Sun, 10 Dec 2017 07:55:43 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</guid>
      <description>In my last blog post I showed how to create a multi class classification ensemble using scikit-learnâ€™s http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier and finished mentioning that I didnâ€™t know which classifiers should be part of the ensemble.
We need to get a better score with each of the classifiers in the ensemble otherwise they can be excluded.
We have a TF/IDF based classifier as well as well as the classifiers I wrote about in the last post.</description>
    </item>
    
    <item>
      <title>Python: Combinations of values on and off</title>
      <link>https://www.markhneedham.com/blog/2017/12/03/python-combinations-values-off/</link>
      <pubDate>Sun, 03 Dec 2017 17:23:14 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/03/python-combinations-values-off/</guid>
      <description>In my continued exploration of Kaggleâ€™s Spooky Authors competition, I wanted to run a GridSearch turning on and off different classifiers to work out the best combination.
I therefore needed to generate combinations of 1s and 0s enabling different classifiers.
e.g. if we had 3 classifiers weâ€™d generate these combinations
0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 where.</description>
    </item>
    
    <item>
      <title>Python: Learning about defaultdict&#39;s handling of missing keys</title>
      <link>https://www.markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</link>
      <pubDate>Fri, 01 Dec 2017 15:26:36 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</guid>
      <description>While reading the scikit-learn code I came across a bit of code that I didnâ€™t understand for a while but in retrospect is quite neat.
This is the code snippet that intrigued me:
vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__ Letâ€™s quickly see how it works by adapting an example from scikit-learn:
&amp;gt;&amp;gt;&amp;gt; from collections import defaultdict &amp;gt;&amp;gt;&amp;gt; vocabulary = defaultdict() &amp;gt;&amp;gt;&amp;gt; vocabulary.default_factory = vocabulary.__len__ &amp;gt;&amp;gt;&amp;gt; vocabulary[&amp;#34;foo&amp;#34;] 0 &amp;gt;&amp;gt;&amp;gt; vocabulary.items() dict_items([(&amp;#39;foo&amp;#39;, 0)]) &amp;gt;&amp;gt;&amp;gt; vocabulary[&amp;#34;bar&amp;#34;] 1 &amp;gt;&amp;gt;&amp;gt; vocabulary.</description>
    </item>
    
    <item>
      <title>Python: polyglot - ModuleNotFoundError: No module named &#39;icu&#39;</title>
      <link>https://www.markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</link>
      <pubDate>Tue, 28 Nov 2017 19:52:13 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</guid>
      <description>I wanted to use the polyglot NLP library that my colleague Will Lyon mentioned in his analysis of Russian Twitter Trolls but had installation problems which I thought Iâ€™d share in case anyone else experiences the same issues.
I started by trying to install polyglot:
$ pip install polyglot ImportError: No module named &amp;#39;icu&amp;#39; Hmmm Iâ€™m not sure what icu is but luckily thereâ€™s a GitHub issue covering this problem.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: unsupported format string passed to numpy.ndarray.*format*</title>
      <link>https://www.markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</link>
      <pubDate>Sun, 19 Nov 2017 07:16:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</guid>
      <description>This post explains how to work around a change in how Python string formatting works for numpy arrays between Python 2 and Python 3.
Iâ€™ve been going through Kevin Markham&amp;#39;s scikit-learn Jupyter notebooks and ran into a problem on the Cross Validation one, which was throwing this error when attempting to print the KFold example:
Iteration Training set observations Testing set observations --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-28-007cbab507e3&amp;gt; in &amp;lt;module&amp;gt;() 6 print(&amp;#39;{} {:^61} {}&amp;#39;.</description>
    </item>
    
    <item>
      <title>Python 3: Create sparklines using matplotlib</title>
      <link>https://www.markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</link>
      <pubDate>Sat, 23 Sep 2017 06:51:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</guid>
      <description>I recently wanted to create sparklines to show how some values were changing over time. In addition, I wanted to generate them as images on the server rather than introducing a JavaScript library.
Chris Seymourâ€™s excellent gist which shows how to create sparklines inside a Pandas dataframe got me most of the way there, but I had to tweak his code a bit to get it to play nicely with Python 3.</description>
    </item>
    
    <item>
      <title>Serverless: Python - virtualenv - { &#34;errorMessage&#34;: &#34;Unable to import module &#39;handler&#39;&#34; }</title>
      <link>https://www.markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</link>
      <pubDate>Sun, 06 Aug 2017 19:03:30 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</guid>
      <description>Iâ€™ve been using the Serverless library to deploy and run some Python functions on AWS lambda recently and was initially confused about how to handle my dependencies.
I tend to create a new virtualenv for each of my project so letâ€™s get that setup first:
Prerequisites $ npm install serverless $ virtualenv -p python3 a $ . a/bin/activate Now letâ€™s create our Serverless project. Iâ€™m going to install the requests library so that I can use it in my function.</description>
    </item>
    
    <item>
      <title>PHP vs Python: Generating a HMAC</title>
      <link>https://www.markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</link>
      <pubDate>Wed, 02 Aug 2017 06:09:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</guid>
      <description>Iâ€™ve been writing a bit of code to integrate with a ClassMarker webhook, and youâ€™re required to check that an incoming request actually came from ClassMarker by checking the value of a base64 hash using HMAC SHA256.
The example in the documentation is written in PHP which I havenâ€™t done for about 10 years so I had to figure out how to do the same thing in Python.
This is the PHP version:</description>
    </item>
    
    <item>
      <title>Pandas: ValueError: The truth value of a Series is ambiguous.</title>
      <link>https://www.markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Wed, 26 Jul 2017 21:41:55 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>Iâ€™ve been playing around with Kaggle in my spare time over the last few weeks and came across an unexpected behaviour when trying to add a column to a dataframe.
First letâ€™s get Pandaâ€™s into our program scope:
Prerequisites import pandas as pd Now weâ€™ll create a data frame to play with for the duration of this post:
&amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame({&amp;#34;a&amp;#34;: [1,2,3,4,5], &amp;#34;b&amp;#34;: [2,3,4,5,6]}) &amp;gt;&amp;gt;&amp;gt; df a b 0 5 2 1 6 6 2 0 8 3 3 2 4 1 6 Letâ€™s say we want to create a new column which returns True if either of the numbers are odd.</description>
    </item>
    
    <item>
      <title>Pandas/scikit-learn: get_dummies test/train sets - ValueError: shapes not aligned</title>
      <link>https://www.markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</link>
      <pubDate>Wed, 05 Jul 2017 15:42:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</guid>
      <description>Iâ€™ve been using pandaâ€™s https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html function to generate dummy columns for categorical variables to use with scikit-learn, but noticed that it sometimes doesnâ€™t work as I expect.
Prerequisites import pandas as pd import numpy as np from sklearn import linear_model Letâ€™s say we have the following training and test sets:
Training set train = pd.DataFrame({&amp;#34;letter&amp;#34;:[&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;, &amp;#34;C&amp;#34;, &amp;#34;D&amp;#34;], &amp;#34;value&amp;#34;: [1, 2, 3, 4]}) X_train = train.drop([&amp;#34;value&amp;#34;], axis=1) X_train = pd.</description>
    </item>
    
    <item>
      <title>Pandas: Find rows where column/field is null</title>
      <link>https://www.markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</link>
      <pubDate>Wed, 05 Jul 2017 14:31:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</guid>
      <description>In my continued playing around with the Kaggle house prices dataset I wanted to find any columns/fields that have null values in.
If we want to get a count of the number of null fields by column we can use the following code, adapted from Poonam Ligadeâ€™s kernel:
Prerequisites import pandas as pd Count the null columns train = pd.read_csv(&amp;#34;train.csv&amp;#34;) null_columns=train.columns[train.isnull().any()] train[null_columns].isnull().sum() LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64 So there are lots of different columns containing null values.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://www.markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, Iâ€™ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.
Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant Iâ€™d overfitted the model.
I wasnâ€™t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albonâ€™s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://www.markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>Iâ€™ve been playing around with the data in Kaggleâ€™s House Prices: Advanced Regression Techniques and while replicating Poonam Ligadeâ€™s exploratory analysis I wanted to see if I could create a model to fill in some of the missing values.
Poonam wrote the following code to identify which columns in the dataset had the most missing values:
import pandas as pd train = pd.read_csv(&amp;#39;train.csv&amp;#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64 The one that Iâ€™m most interested in is LotFrontage, which describes &amp;#39;Linear feet of street connected to property&amp;#39;.</description>
    </item>
    
    <item>
      <title>Loading and analysing Strava runs using PostgreSQL JSON data type</title>
      <link>https://www.markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</link>
      <pubDate>Mon, 01 May 2017 19:11:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</guid>
      <description>In my last post I showed how to map Strava runs using data that Iâ€™d extracted from their https://strava.github.io/api/v3/activities/ API, but the API returns a lot of other data that I discarded because I wasnâ€™t sure what I should keep.
The API returns a nested JSON structure so the easiest solution would be to save each run as an individual file but Iâ€™ve always wanted to try out PostgreSQLâ€™s JSON data type and this seemed like a good opportunity.</description>
    </item>
    
    <item>
      <title>Leaflet: Mapping Strava runs/polylines on Open Street Map</title>
      <link>https://www.markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</link>
      <pubDate>Sat, 29 Apr 2017 15:36:36 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</guid>
      <description>Iâ€™m a big Strava user and spent a bit of time last weekend playing around with their API to work out how to map all my runs.
Strava API and polylines This is a two step process:
Call the /athlete/activities/ endpoint to get a list of all my activities
For each of those activities call /activities/ endpoint to get more detailed information for each activity&amp;lt;/cite&amp;gt;
That second API returns a &amp;#39;polyline&amp;#39; property which the documentation describes as follows:</description>
    </item>
    
    <item>
      <title>Luigi: Defining dynamic requirements (on output files)</title>
      <link>https://www.markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</link>
      <pubDate>Tue, 28 Mar 2017 05:39:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</guid>
      <description>In my last blog post I showed how to convert a JSON document containing meetup groups into a CSV file using Luigi, the Python library for building data pipelines. As well as creating that CSV file I wanted to go back to the meetup.com API and download all the members of those groups.
This was a rough flow of what i wanted to do:
Take JSON document containing all groups</description>
    </item>
    
    <item>
      <title>Luigi: An ExternalProgramTask example - Converting JSON to CSV</title>
      <link>https://www.markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</link>
      <pubDate>Sat, 25 Mar 2017 14:09:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</guid>
      <description>Iâ€™ve been playing around with the Python library Luigi which is used to build pipelines of batch jobs and I struggled to find an example of an ExternalProgramTask so this is my attempt at filling that void.
Iâ€™m building a little data pipeline to get data from the meetup.com API and put it into CSV files that can be loaded into Neo4j using the LOAD CSV command.
The first task I created calls the /groups endpoint and saves the result into a JSON file:</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: Object of type &#39;dict_values&#39; is not JSON serializable</title>
      <link>https://www.markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</link>
      <pubDate>Sun, 19 Mar 2017 16:40:03 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</guid>
      <description>Iâ€™ve recently upgraded to Python 3 (I know, took me a while!) and realised that one of my scripts that writes JSON to a file no longer works!
This is a simplified version of what Iâ€™m doing:
&amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; x = {&amp;#34;mark&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;Mark&amp;#34;}, &amp;#34;michael&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;Michael&amp;#34;} } &amp;gt;&amp;gt;&amp;gt; json.dumps(x.values()) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py&amp;#34;, line 231, in dumps return _default_encoder.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:
Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called â€œcurse of dimensionalityâ€).</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>A couple of months ago Praveena and I created a Game of Thrones dataset to use in a workshop and I thought itâ€™d be fun to run it through some machine learning algorithms and hopefully find some interesting insights.
The dataset is available as CSV files but for this analysis Iâ€™m assuming that itâ€™s already been imported into neo4j. If you want to import the data you can run the tutorial by typing the following into the query bar of the neo4j browser:</description>
    </item>
    
    <item>
      <title>Python: matplotlib, seaborn, virtualenv - Python is not installed as a framework</title>
      <link>https://www.markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</link>
      <pubDate>Sun, 14 Aug 2016 18:56:35 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</guid>
      <description>Over the weekend I was following The Marketing Technologistâ€™s content based recommender tutorial but ran into the following exception when trying to import the seaborn library:
$ python 5_content_based_recommender/run.py Traceback (most recent call last): File &amp;#34;5_content_based_recommender/run.py&amp;#34;, line 14, in &amp;lt;module&amp;gt; import seaborn as sns File &amp;#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/__init__.py&amp;#34;, line 6, in &amp;lt;module&amp;gt; from .rcmod import * File &amp;#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/rcmod.py&amp;#34;, line 8, in &amp;lt;module&amp;gt; from . import palettes, _orig_rc_params File &amp;#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/palettes.py&amp;#34;, line 12, in &amp;lt;module&amp;gt; from .</description>
    </item>
    
    <item>
      <title>scikit-learn: TF/IDF and cosine similarity for computer science papers</title>
      <link>https://www.markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</link>
      <pubDate>Wed, 27 Jul 2016 02:45:28 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</guid>
      <description>A couple of months ago I downloaded the meta data for a few thousand computer science papers so that I could try and write a mini recommendation engine to tell me what paper I should read next.
Since I donâ€™t have any data on which people read each paper a collaborative filtering approach is ruled out, so instead I thought I could try content based filtering instead.
Letâ€™s quickly check the Wikipedia definition of content based filtering:</description>
    </item>
    
    <item>
      <title>Python: Scraping elements relative to each other with BeautifulSoup</title>
      <link>https://www.markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</link>
      <pubDate>Mon, 11 Jul 2016 06:01:22 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</guid>
      <description>Last week we hosted a Game of Thrones based intro to Cypher at the Women Who Code London meetup and in preparation had to scrape the wiki to build a dataset.
Iâ€™ve built lots of datasets this way and itâ€™s a painless experience as long as the pages make liberal use of CSS classes and/or IDs.
Unfortunately the Game of Thrones wiki doesnâ€™t really do that so I had to find another way to extract the data I wanted - extracting elements based on their position to more prominent elements on the page.</description>
    </item>
    
    <item>
      <title>Python: BeautifulSoup - Insert tag</title>
      <link>https://www.markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</link>
      <pubDate>Thu, 30 Jun 2016 21:28:35 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</guid>
      <description>Iâ€™ve been scraping the Game of Thrones wiki in preparation for a meetup at Women Who Code next week and while attempting to extract character allegiances I wanted to insert missing line breaks to separate different allegiances.
I initially tried creating a line break like this:
&amp;gt;&amp;gt;&amp;gt; from bs4 import BeautifulSoup &amp;gt;&amp;gt;&amp;gt; tag = BeautifulSoup(&amp;#34;&amp;lt;br /&amp;gt;&amp;#34;, &amp;#34;html.parser&amp;#34;) &amp;gt;&amp;gt;&amp;gt; tag &amp;lt;br/&amp;gt; It looks like it should work but later on in my script I check the &amp;#39;name&amp;#39; attribute to work out whether Iâ€™ve got a line break and it doesnâ€™t return the value I expected it to:</description>
    </item>
    
    <item>
      <title>Python: Regex - matching foreign characters/unicode letters</title>
      <link>https://www.markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</link>
      <pubDate>Sat, 18 Jun 2016 07:38:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</guid>
      <description>Iâ€™ve been back in the land of screen scrapping this week extracting data from the Game of Thrones wiki and needed to write a regular expression to pull out characters and actors.
Here are some examples of the format of the data: ~text Peter Dinklage as Tyrion Lannister Daniel Naprous as Oznak zo Pahl(credited as Stunt Performer) Filip LoziÄ‡ as Young Nobleman Morgan C. Jones as a Braavosi captain Adewale Akinnuoye-Agbaje as Malko ~</description>
    </item>
    
    <item>
      <title>Python: Squashing &#39;duplicate&#39; pairs together</title>
      <link>https://www.markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</link>
      <pubDate>Sun, 20 Dec 2015 12:12:46 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</guid>
      <description>As part of a data cleaning pipeline I had pairs of ids of duplicate addresses that I wanted to group together.
I couldnâ€™t work out how to solve the problem immediately so I simplified the problem into pairs of letters i.e.
A	B	(A is the same as B) B	C	(B is the same as C) C	D	... E	F	(E is the same as F) F	G	.</description>
    </item>
    
    <item>
      <title>Python: Parsing a JSON HTTP chunking stream</title>
      <link>https://www.markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</link>
      <pubDate>Sat, 28 Nov 2015 13:56:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</guid>
      <description>Iâ€™ve been playing around with meetup.comâ€™s API again and this time wanted to consume the chunked HTTP RSVP stream and filter RSVPs for events Iâ€™m interested in.
I use Python for most of my hacking these days and if HTTP requests are required the requests library is my first port of call.
I started out with the following script
import requests import json def stream_meetup_initial(): uri = &amp;#34;http://stream.meetup.com/2/rsvps&amp;#34; response = requests.</description>
    </item>
    
    <item>
      <title>Python: Extracting Excel spreadsheet into CSV files</title>
      <link>https://www.markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</guid>
      <description>Iâ€™ve been playing around with the Road Safety open data set and the download comes with several CSV files and an excel spreadsheet containing the legend.
There are 45 sheets in total and each of them looks like this:
I wanted to create a CSV file for each sheet so that I can import the data set into Neo4j using the LOAD CSV command.
I came across the Python Excel website which pointed me at the xlrd library since Iâ€™m working with a pre 2010 Excel file.</description>
    </item>
    
    <item>
      <title>Python: Difference between two datetimes in milliseconds</title>
      <link>https://www.markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</link>
      <pubDate>Tue, 28 Jul 2015 20:05:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</guid>
      <description>Iâ€™ve been doing a bit of adhoc measurement of some cypher queries executed via py2neo and wanted to work out how many milliseconds each query was taking end to end.
I thought thereâ€™d be an obvious way of doing this but if there is itâ€™s evaded me so far and I ended up calculating the different between two datetime objects which gave me the following timedelta object: ~python &amp;gt;&amp;gt;&amp;gt; import datetime &amp;gt;&amp;gt;&amp;gt; start = datetime.</description>
    </item>
    
    <item>
      <title>Python: UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 0: ordinal not in range(128)</title>
      <link>https://www.markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</link>
      <pubDate>Wed, 15 Jul 2015 06:20:07 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</guid>
      <description>I was recently doing some text scrubbing and had difficulty working out how to remove the &amp;#39;â€ &amp;#39; character from strings.
e.g. I had a string like this:
&amp;gt;&amp;gt;&amp;gt; u&amp;#39;foo â€ &amp;#39; u&amp;#39;foo \u2020&amp;#39; I wanted to get rid of the &amp;#39;â€ &amp;#39; character and then strip any trailing spaces so Iâ€™d end up with the string &amp;#39;foo&amp;#39;. I tried to do this in one call to &amp;#39;replace&amp;#39;:
&amp;gt;&amp;gt;&amp;gt; u&amp;#39;foo â€ &amp;#39;.replace(&amp;#34; â€ &amp;#34;, &amp;#34;&amp;#34;) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; UnicodeDecodeError: &amp;#39;ascii&amp;#39; codec can&amp;#39;t decode byte 0xe2 in position 1: ordinal not in range(128) It took me a while to work out that &amp;#34;â€  &amp;#34; was being treated as ASCII rather than UTF-8.</description>
    </item>
    
    <item>
      <title>Python: Converting WordPress posts in CSV format</title>
      <link>https://www.markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</link>
      <pubDate>Tue, 07 Jul 2015 06:28:01 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</guid>
      <description>Over the weekend I wanted to look into the Wordpress data behind this blog (very meta!) and wanted to get the data in CSV format so I could do some analysis in R.
I found a couple of WordPress CSV plugins but unfortunately I couldnâ€™t get any of them to work and ended up working with the raw XML data that WordPress produces when you &amp;#39;export&amp;#39; a blog.
I had the problem of the export being incomplete which I &amp;#39;solved&amp;#39; by importing the posts in two parts of a few years each.</description>
    </item>
    
    <item>
      <title>Python: CSV writing - TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;*getitem*&#39;</title>
      <link>https://www.markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</link>
      <pubDate>Sun, 31 May 2015 22:33:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</guid>
      <description>When Iâ€™m working in Python I often find myself writing to CSV files using the in built library and every now and then make a mistake when calling writerow:
import csv writer = csv.writer(file, delimiter=&amp;#34;,&amp;#34;) writer.writerow[&amp;#34;player&amp;#34;, &amp;#34;team&amp;#34;] This results in the following error message:
TypeError: &amp;#39;builtin_function_or_method&amp;#39; object has no attribute &amp;#39;__getitem__&amp;#39; The error message is a bit weird at first but itâ€™s basically saying that Iâ€™ve tried to do an associative lookup on an object which doesnâ€™t support that operation.</description>
    </item>
    
    <item>
      <title>Python: Look ahead multiple elements in an iterator/generator</title>
      <link>https://www.markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</link>
      <pubDate>Thu, 28 May 2015 20:56:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</guid>
      <description>As part of the BBC live text scraping code Iâ€™ve been working on I needed to take an iterator of raw events created by a generator and transform this into an iterator of cards shown in a match.
The structure of the raw events Iâ€™m interested in is as follows:
Line 1: Player booked
Line 2: Player fouled
Line 3: Information about the foul
e.g.
events = [ {&amp;#39;event&amp;#39;: u&amp;#39;Booking Pedro (Barcelona) is shown the yellow card for a bad foul.</description>
    </item>
    
    <item>
      <title>Python: Joining multiple generators/iterators</title>
      <link>https://www.markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</link>
      <pubDate>Sun, 24 May 2015 23:51:25 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</guid>
      <description>In my previous blog post I described how Iâ€™d refactored some scraping code Iâ€™ve been working on to use iterators and ended up with a function which returned a generator containing all the events for one BBC live text match:
match_id = &amp;#34;32683310&amp;#34; events = extract_events(&amp;#34;data/raw/%s&amp;#34; % (match_id)) &amp;gt;&amp;gt;&amp;gt; print type(events) &amp;lt;type &amp;#39;generator&amp;#39;&amp;gt; The next thing I wanted to do is get the events for multiple matches which meant I needed to glue together multiple generators into one big generator.</description>
    </item>
    
    <item>
      <title>Python: Refactoring to iterator</title>
      <link>https://www.markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</link>
      <pubDate>Sat, 23 May 2015 10:14:38 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</guid>
      <description>Over the last week Iâ€™ve been building a set of scripts to scrape the events from the Bayern Munich/Barcelona game and Iâ€™ve ended up with a few hundred lines of nested for statements, if statements and mutated lists. I thought it was about time I did a bit of refactoring.
The following is a function which takes in a match file and spits out a collection of maps containing times &amp;amp; events.</description>
    </item>
    
    <item>
      <title>Python: UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\xfc&#39; in position 11: ordinal not in range(128)</title>
      <link>https://www.markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</link>
      <pubDate>Thu, 21 May 2015 06:14:32 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</guid>
      <description>Iâ€™ve been trying to write some Python code to extract the players and the team they represented in the Bayern Munich/Barcelona match into a CSV file and had much more difficulty than I expected.
I have some scraping code (which is beyond the scope of this article) which gives me a list of (player, team) pairs that I want to write to disk. The contents of the list is as follows:</description>
    </item>
    
    <item>
      <title>Python: Selecting certain indexes in an array</title>
      <link>https://www.markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</link>
      <pubDate>Tue, 05 May 2015 21:39:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</guid>
      <description>A couple of days ago I was scrapping the UK parliament constituencies from Wikipedia in preparation for the Graph Connect hackathon and had got to the point where I had an array with one entry per column in the table.
import requests from bs4 import BeautifulSoup from soupselect import select page = open(&amp;#34;constituencies.html&amp;#34;, &amp;#39;r&amp;#39;) soup = BeautifulSoup(page.read()) for row in select(soup, &amp;#34;table.wikitable tr&amp;#34;): if select(row, &amp;#34;th&amp;#34;): print [cell.text for cell in select(row, &amp;#34;th&amp;#34;)] if select(row, &amp;#34;td&amp;#34;): print [cell.</description>
    </item>
    
    <item>
      <title>Python: Creating a skewed random discrete distribution</title>
      <link>https://www.markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</link>
      <pubDate>Mon, 30 Mar 2015 22:28:23 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</guid>
      <description>Iâ€™m planning to write a variant of the TF/IDF algorithm over the HIMYM corpus which weights in favour of term that appear in a medium number of documents and as a prerequisite needed a function that when given a number of documents would return a weighting.
It should return a higher value when a term appears in a medium number of documents i.e. if I pass in 10 I should get back a higher value than 200 as a term that appears in 10 episodes is likely to be more interesting than one which appears in almost every episode.</description>
    </item>
    
    <item>
      <title>Python: matplotlib hangs and shows nothing (Mac OS X)</title>
      <link>https://www.markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</link>
      <pubDate>Thu, 26 Mar 2015 00:02:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</guid>
      <description>Iâ€™ve been playing around with some of the matplotlib demos recently and discovered that simply copying one of the examples didnâ€™t actually work for me.
I was following the bar chart example and had the following code:
import numpy as np import matplotlib.pyplot as plt N = 5 ind = np.arange(N) fig, ax = plt.subplots() menMeans = (20, 35, 30, 35, 27) menStd = (2, 3, 4, 1, 2) width = 0.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>In my continued exploration of topic modelling I came across The Programming Historian blog and a post showing how to derive topics from a corpus using the Java library mallet.
The instructions on the blog make it very easy to get up and running but as with other libraries Iâ€™ve used, you have to specify how many topics the corpus consists of. Iâ€™m never sure what value to select but the authors make the following suggestion:</description>
    </item>
    
    <item>
      <title>Python: Equivalent to flatMap for flattening an array of arrays</title>
      <link>https://www.markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</link>
      <pubDate>Mon, 23 Mar 2015 00:45:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</guid>
      <description>I found myself wanting to flatten an array of arrays while writing some Python code earlier this afternoon and being lazy my first attempt involved building the flattened array manually:
episodes = [ {&amp;#34;id&amp;#34;: 1, &amp;#34;topics&amp;#34;: [1,2,3]}, {&amp;#34;id&amp;#34;: 2, &amp;#34;topics&amp;#34;: [4,5,6]} ] flattened_episodes = [] for episode in episodes: for topic in episode[&amp;#34;topics&amp;#34;]: flattened_episodes.append({&amp;#34;id&amp;#34;: episode[&amp;#34;id&amp;#34;], &amp;#34;topic&amp;#34;: topic}) for episode in flattened_episodes: print episode If we run that weâ€™ll see this output:</description>
    </item>
    
    <item>
      <title>Python: Simplifying the creation of a stop word list with defaultdict</title>
      <link>https://www.markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</link>
      <pubDate>Sun, 22 Mar 2015 01:51:52 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</guid>
      <description>Iâ€™ve been playing around with topics models again and recently read a paper by David Mimno which suggested the following heuristic for working out which words should go onto the stop list:
A good heuristic for identifying such words is to remove those that occur in more than 5-10% of documents (most common) and those that occur fewer than 5-10 times in the entire corpus (least common).
I decided to try this out on the HIMYM dataset that Iâ€™ve been working on over the last couple of months.</description>
    </item>
    
    <item>
      <title>Python: Forgetting to use enumerate</title>
      <link>https://www.markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</link>
      <pubDate>Sun, 22 Mar 2015 01:28:33 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</guid>
      <description>Earlier this evening I found myself writing the equivalent of the following Python code while building a stop list for a topic model...
words = [&amp;#34;mark&amp;#34;, &amp;#34;neo4j&amp;#34;, &amp;#34;michael&amp;#34;] word_position = 0 for word in words: print word_position, word word_position +=1 ...which is very foolish given that thereâ€™s already a function that makes it really easy to grab the position of an item in a list:
for word_position, word in enumerate(words): print word_position, word Python does make things extremely easy at times - youâ€™re welcome future Mark!</description>
    </item>
    
    <item>
      <title>Python: Transforming Twitter datetime string to timestamp (z&#39; is a bad directive in format)</title>
      <link>https://www.markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</link>
      <pubDate>Sun, 15 Mar 2015 22:43:17 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</guid>
      <description>Iâ€™ve been playing around with importing Twitter data into Neo4j and since Neo4j canâ€™t store dates natively just yet I needed to convert a date string to timestamp.
I started with the following which unfortunately throws an exception:
from datetime import datetime date = &amp;#34;Sat Mar 14 18:43:19 +0000 2015&amp;#34; &amp;gt;&amp;gt;&amp;gt; datetime.strptime(date, &amp;#34;%a %b %d %H:%M:%S %z %Y&amp;#34;) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/System/Library/Frameworks/Python.</description>
    </item>
    
    <item>
      <title>Python: Checking any value in a list exists in a line of text</title>
      <link>https://www.markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</link>
      <pubDate>Sat, 14 Mar 2015 02:52:02 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</guid>
      <description>Iâ€™ve been doing some log file analysis to see what cypher queries were being run on a Neo4j instance and I wanted to narrow down the lines I looked at to only contain ones which had mutating operations i.e. those containing the words MERGE, DELETE, SET or CREATE
Hereâ€™s an example of the text file I was parsing:
$ cat blog.txt MATCH n RETURN n MERGE (n:Person {name: &amp;#34;Mark&amp;#34;}) RETURN n MATCH (n:Person {name: &amp;#34;Mark&amp;#34;}) ON MATCH SET n.</description>
    </item>
    
    <item>
      <title>Python/Neo4j: Finding interesting computer sciency people to follow on Twitter</title>
      <link>https://www.markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</link>
      <pubDate>Wed, 11 Mar 2015 21:13:26 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</guid>
      <description>At the beginning of this year I moved from Neo4jâ€™s field team to dev team and since the code we write there is much lower level than Iâ€™m used to I thought I should find some people to follow on twitter whom I can learn from.
My technique for finding some of those people was to pick a person from the Neo4j kernel team whoâ€™s very good at systems programming and uses twitter which led me to Mr Chris Vest.</description>
    </item>
    
    <item>
      <title>Python: Streaming/Appending to a file</title>
      <link>https://www.markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</link>
      <pubDate>Mon, 09 Mar 2015 23:00:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</guid>
      <description>Iâ€™ve been playing around with Twitterâ€™s API (via the tweepy library) and due to the rate limiting it imposes I wanted to stream results to a CSV file rather than waiting until my whole program had finished.
I wrote the following program to simulate what I was trying to do:
import csv import time with open(&amp;#34;rows.csv&amp;#34;, &amp;#34;a&amp;#34;) as file: writer = csv.writer(file, delimiter = &amp;#34;,&amp;#34;) end = time.time() + 10 while True: if time.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>Following on from Rik van Bruggenâ€™s blog post on a QCon graph heâ€™s created ahead of this weekâ€™s conference, I was curious whether we could extract any interesting relationships between talks based on their abstracts.
Talks are already grouped by their hosting track but thereâ€™s likely to be some overlap in topics even for talks on different tracks. I therefore wanted to extract topics and connect each talk to the topic that describes it best.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn - Training a classifier with non numeric features</title>
      <link>https://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</link>
      <pubDate>Mon, 02 Mar 2015 07:48:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</guid>
      <description>Following on from my previous posts on training a classifier to pick out the speaker in sentences of HIMYM transcripts the next thing to do was train a random forest of decision trees to see how that fared.
Iâ€™ve used scikit-learn for this before so I decided to use that. However, before building a random forest I wanted to check that I could build an equivalent decision tree.
I initially thought that scikit-learnâ€™s DecisionTree classifier would take in data in the same format as nltkâ€™s so I started out with the following code:</description>
    </item>
    
    <item>
      <title>Python: Detecting the speaker in HIMYM using Parts of Speech (POS) tagging</title>
      <link>https://www.markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</link>
      <pubDate>Sun, 01 Mar 2015 02:36:06 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</guid>
      <description>Over the last couple of weeks Iâ€™ve been experimenting with different classifiers to detect speakers in HIMYM transcripts and in all my attempts so far the only features Iâ€™ve used have been words.
This led to classifiers that were overfitted to the training data so I wanted to generalise them by introducing parts of speech of the words in sentences which are more generic.
First I changed the function which generates the features for each word to also contain the parts of speech of the previous and next words as well as the word itself:</description>
    </item>
    
    <item>
      <title>Python/nltk: Naive vs Naive Bayes vs Decision Tree</title>
      <link>https://www.markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</link>
      <pubDate>Tue, 24 Feb 2015 22:39:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</guid>
      <description>Last week I wrote a blog post describing a decision tree Iâ€™d trained to detect the speakers in a How I met your mother transcript and after writing the post I wondered whether a simple classifier would do the job.
The simple classifier will work on the assumption that any word followed by a &amp;#34;:&amp;#34; is a speaker and anything else isnâ€™t. Hereâ€™s the definition of a NaiveClassifier:
import nltk from nltk import ClassifierI class NaiveClassifier(ClassifierI): def classify(self, featureset): if featureset[&amp;#39;next-word&amp;#39;] == &amp;#34;:&amp;#34;: return True else: return False As you can see it only implements the classify method and executes a static check.</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Detecting which sentences in a transcript contain a speaker</title>
      <link>https://www.markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</link>
      <pubDate>Fri, 20 Feb 2015 22:42:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</guid>
      <description>Over the past couple of months Iâ€™ve been playing around with How I met your mother transcripts and the most recent thing Iâ€™ve been working on is how to extract the speaker for a particular sentence.
This initially seemed like a really simple problem as most of the initial sentences I looked at weere structured like this:
&amp;lt;speaker&amp;gt;: &amp;lt;sentence&amp;gt; If there were all in that format then we could write a simple regular expression and then move on but unfortunately they arenâ€™t.</description>
    </item>
    
    <item>
      <title>Python&#39;s pandas vs Neo4j&#39;s cypher: Exploring popular phrases in How I met your mother transcripts</title>
      <link>https://www.markhneedham.com/blog/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 19 Feb 2015 00:52:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/19/pythons-pandas-vs-neo4js-cypher-exploring-popular-phrases-in-how-i-met-your-mother-transcripts/</guid>
      <description>Iâ€™ve previously written about extracting TF/IDF scores for phrases in documents using scikit-learn and the final step in that post involved writing the words into a CSV file for analysis later on.
I wasnâ€™t sure what the most appropriate tool of choice for that analysis was so I decided to explore the data using Pythonâ€™s pandas library and load it into Neo4j and write some Cypher queries.
To do anything with Neo4j we need to first load the CSV file into the database.</description>
    </item>
    
    <item>
      <title>Python/pandas: Column value in list (ValueError: The truth value of a Series is ambiguous.)</title>
      <link>https://www.markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Mon, 16 Feb 2015 21:39:16 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>Iâ€™ve been using Pythonâ€™s pandas library while exploring some CSV files and although for the most part Iâ€™ve found it intuitive to use, I had trouble filtering a data frame based on checking whether a column value was in a list.
A subset of one of the CSV files Iâ€™ve been working with looks like this:
$ cat foo.csv &amp;#34;Foo&amp;#34; 1 2 3 4 5 6 7 8 9 10 Loading it into a pandas data frame is reasonably simple:</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Calculating TF/IDF on How I met your mother transcripts</title>
      <link>https://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</link>
      <pubDate>Sun, 15 Feb 2015 15:56:09 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</guid>
      <description>Over the past few weeks Iâ€™ve been playing around with various NLP techniques to find interesting insights into How I met your mother from its transcripts and one technique that kept coming up is TF/IDF.
The Wikipedia definition reads like this:
tfâ€”â€‹idf, short for term frequencyâ€”â€‹inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</description>
    </item>
    
    <item>
      <title>Python/gensim: Creating bigrams over How I met your mother transcripts</title>
      <link>https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 12 Feb 2015 23:45:03 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</guid>
      <description>As part of my continued playing around with How I met your mother transcripts I wanted to identify plot arcs and as a first step I wrote some code using the gensim and nltk libraries to identify bigrams (two word phrases).
Thereâ€™s an easy to follow tutorial in the gensim docs showing how to go about this but I needed to do a couple of extra steps to get my text data from a CSV file into the structure gensim expects.</description>
    </item>
    
    <item>
      <title>Python/matpotlib: Plotting occurrences of the main characters in How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</link>
      <pubDate>Fri, 30 Jan 2015 21:29:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</guid>
      <description>Normally when Iâ€™m playing around with data sets in R I get out ggplot2 to plot some charts to get a feel for the data but having spent quite a bit of time with Python and How I met your mother transcripts I havenâ€™t created a single plot. I thought Iâ€™d better change change that.
After a bit of searching around it seems that matplotlib is the go to library for this job and I thought an interesting thing to plot would be how often each of the main characters appear in each episode across the show.</description>
    </item>
    
    <item>
      <title>Python: Find the highest value in a group</title>
      <link>https://www.markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</link>
      <pubDate>Sun, 25 Jan 2015 12:47:01 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</guid>
      <description>In my continued playing around with a How I met your mother data set I needed to find out the last episode that happened in a season so that I could use it in a chart I wanted to plot.
I had this CSV file containing each of the episodes:
$ head -n 10 data/import/episodes.csv NumberOverall,NumberInSeason,Episode,Season,DateAired,Timestamp 1,1,/wiki/Pilot,1,&amp;#34;September 19, 2005&amp;#34;,1127084400 2,2,/wiki/Purple_Giraffe,1,&amp;#34;September 26, 2005&amp;#34;,1127689200 3,3,/wiki/Sweet_Taste_of_Liberty,1,&amp;#34;October 3, 2005&amp;#34;,1128294000 4,4,/wiki/Return_of_the_Shirt,1,&amp;#34;October 10, 2005&amp;#34;,1128898800 5,5,/wiki/Okay_Awesome,1,&amp;#34;October 17, 2005&amp;#34;,1129503600 6,6,/wiki/Slutty_Pumpkin,1,&amp;#34;October 24, 2005&amp;#34;,1130108400 7,7,/wiki/Matchmaker,1,&amp;#34;November 7, 2005&amp;#34;,1131321600 8,8,/wiki/The_Duel,1,&amp;#34;November 14, 2005&amp;#34;,1131926400 9,9,/wiki/Belly_Full_of_Turkey,1,&amp;#34;November 21, 2005&amp;#34;,1132531200 I started out by parsing the CSV file into a dictionary of (seasons -&amp;gt; episode ids):</description>
    </item>
    
    <item>
      <title>Python/pdfquery: Scraping the FIFA World Player of the Year votes PDF into shape</title>
      <link>https://www.markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</link>
      <pubDate>Thu, 22 Jan 2015 00:25:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</guid>
      <description>Last week the FIFA Ballon dâ€™Or 2014 was announced and along with the announcement of the winner the individual votes were also made available.
Unfortunately they werenâ€™t made open in a way that Ben Wellington (of IQuantNY fame) would approve of - the choice of format for the data is a PDF file!
I wanted to extract this data to play around with it but I wanted to automate the extraction as Iâ€™d done when working with Google Trends data.</description>
    </item>
    
    <item>
      <title>Python/NLTK: Finding the most common phrases in How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</link>
      <pubDate>Mon, 19 Jan 2015 00:24:23 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</guid>
      <description>Following on from last weekâ€™s blog post where I found the most popular words in How I met your mother transcripts, in this post weâ€™ll have a look at how we can pull out sentences and then phrases from our corpus.
The first thing I did was tweak the scraping script to pull out the sentences spoken by characters in the transcripts.&amp;lt;/p&amp;gt;
Each dialogue is separated by two line breaks so we use that as our separator.</description>
    </item>
    
    <item>
      <title>Python: Counter - ValueError: too many values to unpack</title>
      <link>https://www.markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</link>
      <pubDate>Mon, 12 Jan 2015 23:16:58 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</guid>
      <description>I recently came across Pythonâ€™s Counter tool which makes it really easy to count the number of occurrences of items in a list.
In my case I was trying to work out how many times words occurred in a corpus so I had something like the following:
&amp;gt;&amp;gt; from collections import Counter &amp;gt;&amp;gt; counter = Counter([&amp;#34;word1&amp;#34;, &amp;#34;word2&amp;#34;, &amp;#34;word3&amp;#34;, &amp;#34;word1&amp;#34;]) &amp;gt;&amp;gt; print counter Counter({&amp;#39;word1&amp;#39;: 2, &amp;#39;word3&amp;#39;: 1, &amp;#39;word2&amp;#39;: 1}) I wanted to write a for loop to iterate over the counter and print the (key, value) pairs and started with the following:</description>
    </item>
    
    <item>
      <title>Python: scikit-learn: ImportError: cannot import name __check_build</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</link>
      <pubDate>Sat, 10 Jan 2015 08:48:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</guid>
      <description>In part 3 of Kaggleâ€™s series on text analytics I needed to install scikit-learn and having done so ran into the following error when trying to use one of its classes:
&amp;gt;&amp;gt;&amp;gt; from sklearn.feature_extraction.text import CountVectorizer Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/sklearn/__init__.py&amp;#34;, line 37, in &amp;lt;module&amp;gt; from . import __check_build ImportError: cannot import name __check_build This error doesnâ€™t reveal very much but I found that when I exited the REPL and tried the same command again I got a different error which was a bit more useful:</description>
    </item>
    
    <item>
      <title>Python: gensim - clang: error: unknown argument: &#39;-mno-fused-madd&#39; [-Wunused-command-line-argument-hard-error-in-future]</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</link>
      <pubDate>Sat, 10 Jan 2015 08:39:15 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</guid>
      <description>While working through part 2 of Kaggleâ€™s bag of words tutorial I needed to install the gensim library and initially ran into the following error:
$ pip install gensim ... cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/Users/markneedham/projects/neo4j-himym/himym/build/gensim/gensim/models -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.</description>
    </item>
    
    <item>
      <title>Python NLTK/Neo4j: Analysing the transcripts of How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</link>
      <pubDate>Sat, 10 Jan 2015 01:22:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</guid>
      <description>After reading Emilâ€™s blog post about dark data a few weeks ago I became intrigued about trying to find some structure in free text data and I thought How I met your motherâ€™s transcripts would be a good place to start.
I found a website which has the transcripts for all the episodes and then having manually downloaded the two pages which listed all the episodes, wrote a script to grab each of the transcripts so I could use them on my machine.</description>
    </item>
    
    <item>
      <title>Python: Converting a date string to timestamp</title>
      <link>https://www.markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</link>
      <pubDate>Mon, 20 Oct 2014 15:53:51 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</guid>
      <description>Iâ€™ve been playing around with Python over the last few days while cleaning up a data set and one thing I wanted to do was translate date strings into a timestamp.
I started with a date in this format:
date_text = &amp;#34;13SEP2014&amp;#34; So the first step is to translate that into a Python date - the strftime section of the documentation is useful for figuring out which format code is needed:</description>
    </item>
    
    <item>
      <title>Python: Making scikit-learn and pandas play nice</title>
      <link>https://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</link>
      <pubDate>Sat, 09 Nov 2013 13:58:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</guid>
      <description>In the last post I wrote about Nathan and my http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/[attempts at the http://www.kaggle.com/c/titanic-gettingStarted[Kaggle Titanic Problem\] I mentioned that we our next step was to try out http://scikit-learn.org/stable/tutorial/[scikit-learn\] so I thought I should summarise where weâ€™ve got up to.&amp;lt;/p&amp;gt;
We needed to write a classification algorithm to work out whether a person onboard the Titanic survived and luckily scikit-learn has http://scikit-learn.org/stable/supervised_learning.html#supervised-learning[extensive documentation on each of the algorithms\].
Unfortunately almost all those examples use http://www.</description>
    </item>
    
    <item>
      <title>Python: Scoping variables to use with timeit</title>
      <link>https://www.markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</link>
      <pubDate>Sat, 09 Nov 2013 11:01:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</guid>
      <description>Iâ€™ve been playing around with Pythonâ€™s timeit library to help benchmark some Neo4j cypher queries but I ran into some problems when trying to give it accessible to variables in my program.
I had the following python script which I would call from the terminal using python top-away-scorers.py:
import query_profiler as qp attempts = [ {&amp;#34;query&amp;#34;: &amp;#39;&amp;#39;&amp;#39;MATCH (player:Player)-[:played]-&amp;gt;stats-[:in]-&amp;gt;game, stats-[:for]-&amp;gt;team WHERE game&amp;lt;-[:away_team]-team RETURN player.name, SUM(stats.goals) AS goals ORDER BY goals DESC LIMIT 10&amp;#39;&amp;#39;&amp;#39;} ] qp.</description>
    </item>
    
    <item>
      <title>Python: Generate all combinations of a list</title>
      <link>https://www.markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</link>
      <pubDate>Wed, 06 Nov 2013 07:25:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</guid>
      <description>Nathan and I have been playing around with different scikit-learn machine learning classifiers and we wanted to run different combinations of features through each one and work out which gave the best result.
We started with a list of features:
all_columns = [&amp;#34;Fare&amp;#34;, &amp;#34;Sex&amp;#34;, &amp;#34;Pclass&amp;#34;, &amp;#39;Embarked&amp;#39;] itertools#combinations allows us to create combinations with a length of our choice:
&amp;gt;&amp;gt;&amp;gt; import itertools as it &amp;gt;&amp;gt;&amp;gt; list(it.combinations(all_columns, 3)) [(&amp;#39;Fare&amp;#39;, &amp;#39;Sex&amp;#39;, &amp;#39;Pclass&amp;#39;), (&amp;#39;Fare&amp;#39;, &amp;#39;Sex&amp;#39;, &amp;#39;Embarked&amp;#39;), (&amp;#39;Fare&amp;#39;, &amp;#39;Pclass&amp;#39;, &amp;#39;Embarked&amp;#39;), (&amp;#39;Sex&amp;#39;, &amp;#39;Pclass&amp;#39;, &amp;#39;Embarked&amp;#39;)] We wanted to create combinations of arbitrary length so we wanted to combine a few invocations of that functions like this:</description>
    </item>
    
    <item>
      <title>Python: matplotlib -  Import error ft2font Symbol not found: _FT_Attach_File (Mac OS X 10.8.3/Mountain Lion)</title>
      <link>https://www.markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</link>
      <pubDate>Sun, 03 Nov 2013 11:14:48 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</guid>
      <description>As I mentioned at the end of my last post about the Titanic Kaggle problem our next step was to do some proper machine learningâ„¢ using scikit-learn so I started by looking at the Decision Tree example.
Unfortunately I ended up on the mother of all yak shaving missions while trying to execute the code which draws a chart using matplotlib.
I ran the following line from the tutorial:</description>
    </item>
    
    <item>
      <title>pandas: Adding a column to a DataFrame (based on another DataFrame)</title>
      <link>https://www.markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</link>
      <pubDate>Wed, 30 Oct 2013 06:12:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</guid>
      <description>Nathan and I have been working on the Titanic Kaggle problem using the pandas data analysis library and one thing we wanted to do was add a column to a DataFrame indicating if someone survived.
We had the following (simplified) DataFrame containing some information about customers on board the Titanic:
def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) customers = pd.DataFrame(columns=[&amp;#39;PassengerId&amp;#39;,&amp;#39;Pclass&amp;#39;,&amp;#39;Name&amp;#39;,&amp;#39;Sex&amp;#39;,&amp;#39;Fare&amp;#39;]) customers = addrow(customers, [dict(PassengerId=892, Pclass=3, Name=&amp;#34;Kelly, Mr. James&amp;#34;, Sex=&amp;#34;male&amp;#34;, Fare=7.8292)]) customers = addrow(customers, [dict(PassengerId=893, Pclass=3, Name=&amp;#34;Wilkes, Mrs.</description>
    </item>
    
    <item>
      <title>Python: for/list comprehensions and dictionaries</title>
      <link>https://www.markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</link>
      <pubDate>Tue, 13 Aug 2013 22:59:52 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</guid>
      <description>Iâ€™ve been working through Courseraâ€™s Linear Algebra course and since all of the exercises are in Python Iâ€™ve been playing around with it again.
One interesting thing I learnt is that you can construct dictionaries using a list comprehension type syntax.
For example, if we start with the following dictionaries:
&amp;gt;&amp;gt;&amp;gt; x = { &amp;#34;a&amp;#34;: 1, &amp;#34;b&amp;#34;:2 } &amp;gt;&amp;gt;&amp;gt; y = {1: &amp;#34;mark&amp;#34;, 2: &amp;#34;will&amp;#34;} &amp;gt;&amp;gt;&amp;gt; x {&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 2} &amp;gt;&amp;gt;&amp;gt; y {1: &amp;#39;mark&amp;#39;, 2: &amp;#39;will&amp;#39;} We might want to create a new dictionary which links from the keys in x to the values in y.</description>
    </item>
    
    <item>
      <title>Ruby/Python: Constructing a taxonomy from an array using zip</title>
      <link>https://www.markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</link>
      <pubDate>Sun, 19 May 2013 22:44:40 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</guid>
      <description>As I mentioned in my previous blog post Iâ€™ve been hacking on a product taxonomy and I wanted to create a &amp;#39;CHILD&amp;#39; relationship between a collection of categories.
For example, I had the following array and I wanted to transform it into an array of &amp;#39;SubCategory, Category&amp;#39; pairs:
taxonomy = [&amp;#34;Cat&amp;#34;, &amp;#34;SubCat&amp;#34;, &amp;#34;SubSubCat&amp;#34;] # I wanted this to become [(&amp;#34;Cat&amp;#34;, &amp;#34;SubCat&amp;#34;), (&amp;#34;SubCat&amp;#34;, &amp;#34;SubSubCat&amp;#34;) In order to do this we need to zip the first 2 items with the last which I found reasonably easy to do using Python:</description>
    </item>
    
    <item>
      <title>Python: Reading a JSON file</title>
      <link>https://www.markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</link>
      <pubDate>Tue, 09 Apr 2013 07:23:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</guid>
      <description>Iâ€™ve been playing around with some code to spin up AWS instances using Fabric and Boto and one thing that I wanted to do was define a bunch of default properties in a JSON file and then load this into a script.
I found it harder to work out how to do this than I expected to so I thought Iâ€™d document it for future me!
My JSON file looks like this:</description>
    </item>
    
    <item>
      <title>Python: (Conceptually) removing an item from a tuple</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</link>
      <pubDate>Sun, 27 Jan 2013 02:30:05 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</guid>
      <description>As part of some code Iâ€™ve been playing around I wanted to remove an item from a tuple which wasnâ€™t particularly easy because Pythonâ€™s tuple data structure is immutable.
I therefore needed to create a new tuple excluding the value which I wanted to remove.
I ended up writing the following function to do this but I imagine there might be an easier way because itâ€™s quite verbose:
def tuple_without(original_tuple, element_to_remove): new_tuple = [] for s in list(original_tuple): if not s == element_to_remove: new_tuple.</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting values by multiple indices</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</link>
      <pubDate>Sun, 27 Jan 2013 02:21:39 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</guid>
      <description>As I mentioned in my previous post Iâ€™ve been playing around with numpy and I wanted to get the values of a collection of different indices in a 2D array.
If we had a 2D array that looked like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]) I knew that it was possible to retrieve the first 3 rows by using the following code:</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting specific column in 2D array</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</link>
      <pubDate>Sun, 27 Jan 2013 02:10:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</guid>
      <description>Iâ€™ve been playing around with numpy this evening in an attempt to improve the performance of a Travelling Salesman Problem implementation and I wanted to get every value in a specific column of a 2D array.
The array looked something like this:
&amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]) I wanted to get the values for the 2nd column of each row which would return an array containing 1, 6, 11 and 16.</description>
    </item>
    
    <item>
      <title>Fabric: Tailing log files on multiple machines</title>
      <link>https://www.markhneedham.com/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</link>
      <pubDate>Tue, 15 Jan 2013 00:20:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</guid>
      <description>We wanted to tail one of the log files simultaneously on 12 servers this afternoon to try and see if a particular event was being logged and rather than opening 12 SSH sessions decided to get Fabric to help us out.
My initial attempt to do this was the following:
fab -H host1,host2,host3 -- tail -f /var/www/awesome/current/log/production.log It works but the problem is that by default Fabric runs the specified command one machine after the other so weâ€™ve actually managed to block Fabric with the tail command on &amp;#39;host1&amp;#39;.</description>
    </item>
    
    <item>
      <title>Knapsack Problem: Python vs Ruby</title>
      <link>https://www.markhneedham.com/blog/2013/01/07/knapsack-problem-python-vs-ruby/</link>
      <pubDate>Mon, 07 Jan 2013 00:47:34 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/07/knapsack-problem-python-vs-ruby/</guid>
      <description>The latest algorithm that we had to code in Algorithms 2 was the Knapsack problem which is as follows:
The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.</description>
    </item>
    
  </channel>
</rss>
