<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>generative-ai on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/generative-ai/</link>
    <description>Recent content in generative-ai on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Jun 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/generative-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chroma/LangChain: &#39;NoneType&#39; object has no attribute &#39;info&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</link>
      <pubDate>Thu, 22 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</guid>
      <description>Following on from a blog post that I wrote yesterday about doing similarity search with ChromaDB, I noticed an odd error message being printed as the script was exiting. In this blog post, we’ll explore what was going on.
To recap, I have the following code to find chunks of YouTube transcripts that are most similar to an input query:
test_chroma.py from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma hf_embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;) store = Chroma(collection_name=&amp;#34;transcript&amp;#34;, persist_directory=&amp;#34;db&amp;#34;, embedding_function=hf_embeddings) result = store.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: Index not found, please create an instance before querying</title>
      <link>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</link>
      <pubDate>Wed, 21 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</guid>
      <description>Somewhat belatedly, I’ve been playing around with LangChain and HuggingFace to spike a tool that lets me ask question about Tim Berglund’s Real-Time Analytics podcast.
I’m using the Chroma database to store vectors of chunks of the transcript so that I can find appropriate sections to feed to the Large Language Model to help with answering my questions. I ran into an initially perplexing error while building this out, which we’re going to explore in this blog post.</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
  </channel>
</rss>
