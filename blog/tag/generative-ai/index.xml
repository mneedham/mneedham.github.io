<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>generative-ai on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/generative-ai/</link>
    <description>Recent content in generative-ai on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 May 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/generative-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mistral 7B function calling with llama.cpp</title>
      <link>https://www.markhneedham.com/blog/2024/06/23/mistral-7b-function-calling-llama-cpp/</link>
      <pubDate>Thu, 23 May 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/06/23/mistral-7b-function-calling-llama-cpp/</guid>
      <description>Mistral AI recently released version 3 of their popular 7B model and this one is fine-tuned for function calling. Function calling is a confusing name because the LLM isn’t doing any function calling itself. Instead, it takes a prompt and can then tell you which function you should call in your code and with which parameters.
In this blog post, we’re going to learn how to use this functionality with llama.</description>
    </item>
    
    <item>
      <title>Side by side LLMs with Ollama and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</link>
      <pubDate>Sat, 11 May 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</guid>
      <description>The recent 0.1.33 release of Ollama added experimental support for running multiple LLMs or the same LLM in parallel. But, to compare models on the same prompt we need a UI and that’s what we’re going to build in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Semantic Router: Stop LLM chatbots going rogue</title>
      <link>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</link>
      <pubDate>Sun, 14 Apr 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</guid>
      <description>A tricky problem when deploying LLM-based chatbots is working out how to stop them from talking about topics that you don’t want them to talk about. Even with the cleverest prompts, with enough effort and ingenuity, users will figure a way around the guard rails.
However, I recently came across a library called Semantic Router, which amongst other things, seems to provide a solution to this problem. In this blog post, we’re going to explore Semantic Router and see if we can create a chatbot that only talks about a pre-defined set of topics.</description>
    </item>
    
    <item>
      <title>llama.cpp - ValueError: Failed to create llama_context - ggml-common.h file not found</title>
      <link>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</link>
      <pubDate>Sun, 31 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</guid>
      <description>I’ve been playing around with the outlines library and needed to install llama.cpp as a result. I ran into trouble when trying to offload model layers to the GPU and in this post, I’ll explain how to install llama.cpp so that you don’t have the same issues.
This was how I installed the library initially:
CMAKE_ARGS=&amp;#34;-DLLAMA_METAL=on&amp;#34; pip install llama-cpp-python And then let’s try to load a GGUF model with some layers offloaded to the GPU:</description>
    </item>
    
    <item>
      <title>LLaVA 1.5 vs. 1.6</title>
      <link>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</link>
      <pubDate>Sun, 04 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</guid>
      <description>LLaVA (or Large Language and Vision Assistant), an open-source large multi-modal model, just released version 1.6. It claims to have improvements over version 1.5, which was released a few months ago:
Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.
Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.</description>
    </item>
    
    <item>
      <title>Ollama is on PyPi</title>
      <link>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</link>
      <pubDate>Sun, 28 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</guid>
      <description>This week Ollama released a Python/PyPi library to go with their awesome tool for running LLMs on your own machine. You still need to download and run Ollama, but after that you can do almost everything from the library. In this blog post, we’re going to take it for a spin.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>An introduction to Retrieval Augmented Generation</title>
      <link>https://www.markhneedham.com/blog/2024/01/12/intro-to-retrieval-augmented-generation/</link>
      <pubDate>Fri, 12 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/12/intro-to-retrieval-augmented-generation/</guid>
      <description>Retrieval Augmented Generation (RAG) is a technique used with Large Language Models (LLM) where you augment the prompt with data retrieved from a data store so that the LLM can generate a better answer to the question that is being asked. In this blog post, we’re going to learn the basics of RAG by creating a Question and Answer system on top of the 2023 Wimbledon Championships Wikipedia page.</description>
    </item>
    
    <item>
      <title>Running Mistral AI on my machine with Ollama</title>
      <link>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</link>
      <pubDate>Tue, 03 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</guid>
      <description>Last week Mistral AI announced the release of their first Large Language Model (LLM), trained with 7 billion parameters, and better than Meta’s Llama 2 model with 13 billion parameters. For those keeping track, Mistral AI was founded in the summer of 2023 and raised $113m in their seed round.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Running a Hugging Face Large Language Model (LLM) locally on my laptop</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</link>
      <pubDate>Fri, 23 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</guid>
      <description>I’ve been playing around with a bunch of Large Language Models (LLMs) on Hugging Face and while the free inference API is cool, it can sometimes be busy, so I wanted to learn how to run the models locally. That’s what we’ll be doing in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>LangChain: 1 validation error for LLMChain - value is not a valid dict (type=type_error.dict)</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</link>
      <pubDate>Fri, 23 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</guid>
      <description>I surely can’t be the first to make the mistake that I’m about to describe and I expect I won’t be the last! I’m still swimming in the LLM waters and I was trying to get GPT4All to play nicely with LangChain.
I wrote the following code to create an LLM chain in LangChain so that every question would use the same prompt template:
from langchain import PromptTemplate, LLMChain from gpt4all import GPT4All llm = GPT4All( model_name=&amp;#34;ggml-gpt4all-j-v1.</description>
    </item>
    
    <item>
      <title>GPT4All/LangChain: Model.__init__() got an unexpected keyword argument &#39;ggml_model&#39; (type=type_error)</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</link>
      <pubDate>Thu, 22 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</guid>
      <description>I’m starting to realise that things move insanely fast in the world of LLMs (Large Language Models) and you will run into issues because you aren’t using the latest version of libraries. I say this because I’ve been following Sami Maameri’s blog post which explains how to run an LLM on your own machine and ran into an error, which we’ll explore in this blog post.
Sami’s post is based around a library called GPT4All, but he also uses LangChain to glue things together.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: &#39;NoneType&#39; object has no attribute &#39;info&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</link>
      <pubDate>Thu, 22 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</guid>
      <description>Following on from a blog post that I wrote yesterday about doing similarity search with ChromaDB, I noticed an odd error message being printed as the script was exiting. In this blog post, we’ll explore what was going on.
To recap, I have the following code to find chunks of YouTube transcripts that are most similar to an input query:
test_chroma.py from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma hf_embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;) store = Chroma(collection_name=&amp;#34;transcript&amp;#34;, persist_directory=&amp;#34;db&amp;#34;, embedding_function=hf_embeddings) result = store.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: Index not found, please create an instance before querying</title>
      <link>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</link>
      <pubDate>Wed, 21 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</guid>
      <description>Somewhat belatedly, I’ve been playing around with LangChain and HuggingFace to spike a tool that lets me ask question about Tim Berglund’s Real-Time Analytics podcast.
I’m using the Chroma database to store vectors of chunks of the transcript so that I can find appropriate sections to feed to the Large Language Model to help with answering my questions. I ran into an initially perplexing error while building this out, which we’re going to explore in this blog post.</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
  </channel>
</rss>
