<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>generative-ai on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/generative-ai/</link>
    <description>Recent content in generative-ai on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jun 2023 04:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/generative-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running a Hugging Face Large Language Model (LLM) locally on my laptop</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</link>
      <pubDate>Fri, 23 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</guid>
      <description>I’ve been playing around with a bunch of Large Language Models (LLMs) on Hugging Face and while the free inference API is cool, it can sometimes be busy, so I wanted to learn how to run the models locally. That’s what we’ll be doing in this blog post.
You’ll need to install the following libraries if you want to follow along:
pip install &amp;#39;langchain[llms]&amp;#39; huggingface-hub langchain transformers The first step is to choose a model that you want to download.</description>
    </item>
    
    <item>
      <title>LangChain: 1 validation error for LLMChain - value is not a valid dict (type=type_error.dict)</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</link>
      <pubDate>Fri, 23 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</guid>
      <description>I surely can’t be the first to make the mistake that I’m about to describe and I expect I won’t be the last! I’m still swimming in the LLM waters and I was trying to get GPT4All to play nicely with LangChain.
I wrote the following code to create an LLM chain in LangChain so that every question would use the same prompt template:
from langchain import PromptTemplate, LLMChain from gpt4all import GPT4All llm = GPT4All( model_name=&amp;#34;ggml-gpt4all-j-v1.</description>
    </item>
    
    <item>
      <title>GPT4All/LangChain: Model.__init__() got an unexpected keyword argument &#39;ggml_model&#39; (type=type_error)</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</link>
      <pubDate>Thu, 22 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</guid>
      <description>I’m starting to realise that things move insanely fast in the world of LLMs (Large Language Models) and you will run into issues because you aren’t using the latest version of libraries. I say this because I’ve been following Sami Maameri’s blog post which explains how to run an LLM on your own machine and ran into an error, which we’ll explore in this blog post.
Sami’s post is based around a library called GPT4All, but he also uses LangChain to glue things together.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: &#39;NoneType&#39; object has no attribute &#39;info&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</link>
      <pubDate>Thu, 22 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</guid>
      <description>Following on from a blog post that I wrote yesterday about doing similarity search with ChromaDB, I noticed an odd error message being printed as the script was exiting. In this blog post, we’ll explore what was going on.
To recap, I have the following code to find chunks of YouTube transcripts that are most similar to an input query:
test_chroma.py from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma hf_embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;) store = Chroma(collection_name=&amp;#34;transcript&amp;#34;, persist_directory=&amp;#34;db&amp;#34;, embedding_function=hf_embeddings) result = store.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: Index not found, please create an instance before querying</title>
      <link>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</link>
      <pubDate>Wed, 21 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</guid>
      <description>Somewhat belatedly, I’ve been playing around with LangChain and HuggingFace to spike a tool that lets me ask question about Tim Berglund’s Real-Time Analytics podcast.
I’m using the Chroma database to store vectors of chunks of the transcript so that I can find appropriate sections to feed to the Large Language Model to help with answering my questions. I ran into an initially perplexing error while building this out, which we’re going to explore in this blog post.</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
  </channel>
</rss>
