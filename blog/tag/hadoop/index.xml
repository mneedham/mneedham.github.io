<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/hadoop/</link>
    <description>Recent content in hadoop on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Jul 2016 13:55:14 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mahout/Hadoop: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>https://markhneedham.com/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Fri, 22 Jul 2016 13:55:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mahout-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Exception in thread &amp;#34;main&amp;#34; org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4 at org.apache.hadoop.ipc.Client.call(Client.java:1113) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229) at com.sun.proxy.$Proxy1.getProtocolVersion(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62) at com.sun.proxy.$Proxy1.getProtocolVersion(Unknown Source) at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422) at org.apache.hadoop.hdfs.DFSClient.createNamenode(DFSClient.java:183) at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:281) at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:245) at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1446) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1464) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:263) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:124) at com.markhneedham.mahout.Similarity.getDocIndex(Similarity.java:86) at com.markhneedham.mahout.Similarity.main(Similarity.java:25) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.</description>
    </item>
    
    <item>
      <title>Hadoop: DataNode not starting</title>
      <link>https://markhneedham.com/blog/2016/07/22/hadoop-datanode-not-starting/</link>
      <pubDate>Fri, 22 Jul 2016 13:31:15 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/07/22/hadoop-datanode-not-starting/</guid>
      <description>java.io.IOException: File /user/markneedham/book2.txt could only be replicated to 0 nodes, instead of 1 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1448) at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:690) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:342) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1350) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1346) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:742) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1344) at org.apache.hadoop.ipc.Client.call(Client.java:905) at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198) at $Proxy0.addBlock(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59) at $Proxy0.addBlock(Unknown Source) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:928) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:811) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:427) 2016-07-21 18:58:00,496 WARN org.</description>
    </item>
    
    <item>
      <title>Hadoop: HDFS - ava.lang.NoSuchMethodError: org.apache.hadoop.fs.FSOutputSummer.&lt;init&gt;(Ljava/util/zip/Checksum;II)V</title>
      <link>https://markhneedham.com/blog/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</link>
      <pubDate>Sat, 31 Oct 2015 23:58:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</guid>
      <description>package org.playground; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; public class HadoopDFSFileReadWrite { static void printAndExit(String str) { System.err.println( str ); System.exit(1); } public static void main (String[] argv) throws IOException { Configuration conf = new Configuration(); conf.addResource(new Path(&amp;#34;/Users/markneedham/Downloads/core-site.xml&amp;#34;)); FileSystem fs = FileSystem.get(conf); Path inFile = new Path(&amp;#34;hdfs://192.168.0.11/user/markneedham/explore.R&amp;#34;); Path outFile = new Path(&amp;#34;hdfs://192.168.0.11/user/markneedham/output-&amp;#34; + System.currentTimeMillis()); // Check if input/output are valid  if (!fs.exists(inFile)) printAndExit(&amp;#34;Input file not found&amp;#34;); if (!</description>
    </item>
    
  </channel>
</rss>