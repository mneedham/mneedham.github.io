<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on Mark Needham</title>
    <link>http://localhost:8000/blog/tag/hadoop/</link>
    <description>Recent content in hadoop on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Jul 2016 13:55:14 +0000</lastBuildDate><atom:link href="http://localhost:8000/blog/tag/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mahout/Hadoop: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>http://localhost:8000/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Fri, 22 Jul 2016 13:55:14 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>I’ve been working my way through Dragan Milcevski’s mini tutorial on using Mahout to do content based filtering on documents and reached the final step where I needed to read in the generated item-similarity files.
I got the example compiling by using the following Maven dependency:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mahout-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Unfortunately when I ran the code I ran into a version incompatibility problem:
Exception in thread &amp;#34;main&amp;#34; org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4 at org.</description>
    </item>
    
    <item>
      <title>Hadoop: DataNode not starting</title>
      <link>http://localhost:8000/blog/2016/07/22/hadoop-datanode-not-starting/</link>
      <pubDate>Fri, 22 Jul 2016 13:31:15 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2016/07/22/hadoop-datanode-not-starting/</guid>
      <description>In my continued playing with Mahout I eventually decided to give up using my local file system and use a local Hadoop instead since that seems to have much less friction when following any examples.
Unfortunately all my attempts to upload any files from my local file system to HDFS were being met with the following exception:
java.io.IOException: File /user/markneedham/book2.txt could only be replicated to 0 nodes, instead of 1 at org.</description>
    </item>
    
    <item>
      <title>Hadoop: HDFS - java.lang.NoSuchMethodError: org.apache.hadoop.fs.FSOutputSummer.&lt;init&gt;(Ljava/util/zip/Checksum;II)V</title>
      <link>http://localhost:8000/blog/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</link>
      <pubDate>Sat, 31 Oct 2015 23:58:22 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2015/10/31/hadoop-hdfs-ava-lang-nosuchmethoderror-org-apache-hadoop-fs-fsoutputsummer-ljavautilzipchecksumiiv/</guid>
      <description>I wanted to write a little program to check that one machine could communicate a HDFS server running on the other and adapted some code from the Hadoop wiki as follows:
package org.playground; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; public class HadoopDFSFileReadWrite { static void printAndExit(String str) { System.err.println( str ); System.exit(1); } public static void main (String[] argv) throws IOException { Configuration conf = new Configuration(); conf.</description>
    </item>
    
  </channel>
</rss>
