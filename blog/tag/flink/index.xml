<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>flink on Mark Needham</title>
    <link>http://localhost:8000/blog/tag/flink/</link>
    <description>Recent content in flink on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jan 2023 02:44:37 +0000</lastBuildDate><atom:link href="http://localhost:8000/blog/tag/flink/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink SQL: Could not execute SQL statement. Reason: java.io.IOException: Corrupt Debezium JSON message</title>
      <link>http://localhost:8000/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</guid>
      <description>As part of a JFokus workshop that I’m working on I wanted to create a Flink table around a Kafka stream that I’d populated from MySQL with help from Debezium. In this blog post I want to show how to do this and explain an error that I encountered along the way.
To start, we have a products table in MySQL that’s publishing events to Apache Kafka. We can see the fields in this event by running the following command:</description>
    </item>
    
    <item>
      <title>Flink SQL: Exporting nested JSON to a Kafka topic</title>
      <link>http://localhost:8000/blog/2023/01/24/flink-sql-export-nested-json-kafka/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2023/01/24/flink-sql-export-nested-json-kafka/</guid>
      <description>I’ve been playing around with Flink as part of a workshop that I’m doing at JFokus in a couple of weeks and I wanted to export some data from Flink to Apache Kafka in a nested format. In this blog post we’ll learn how to do that.
Setup We’re going to be using the following Docker Compose config:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:latest container_name: zookeeper hostname: zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOO_MY_ID: 1 ZOO_PORT: 2181 ZOO_SERVERS: server.</description>
    </item>
    
  </channel>
</rss>
