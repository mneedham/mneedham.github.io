<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pyspark on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/pyspark/</link>
    <description>Recent content in pyspark on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2019 09:00:00 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/pyspark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>pyspark: Py4JJavaError: An error occurred while calling o138.loadClass.: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI</title>
      <link>https://www.markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</link>
      <pubDate>Wed, 17 Apr 2019 09:00:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</guid>
      <description>I’ve been building a Docker Container that has support for Jupyter, Spark, GraphFrames, and Neo4j, and ran into a problem that had me pulling my (metaphorical) hair out!
The pyspark-notebook container gets us most of the way there, but it doesn’t have GraphFrames or Neo4j support. Adding Neo4j is as simple as pulling in the Python Driver from Conda Forge, which leaves us with GraphFrames.
When I’m using GraphFrames with pyspark locally I would pull it in via the --packages config parameter, like this:</description>
    </item>
    
    <item>
      <title>PySpark: Creating DataFrame with one column - TypeError: Can not infer schema for type: &lt;type &#39;int&#39;&gt;</title>
      <link>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</link>
      <pubDate>Sun, 09 Dec 2018 10:25:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</guid>
      <description>I’ve been playing with PySpark recently, and wanted to create a DataFrame containing only one column. I tried to do this by writing the following code:
spark.createDataFrame([(1)], [&amp;#34;count&amp;#34;]) If we run that code we’ll get the following error message:
Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&amp;#34;, line 748, in createDataFrame rdd, schema = self._createFromLocal(map(prepare, data), schema) File &amp;#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&amp;#34;, line 416, in _createFromLocal struct = self.</description>
    </item>
    
  </channel>
</rss>
