<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science 2 on Mark Needham</title>
    <link>http://markhneedham.com/blog/tag/data-science-2/</link>
    <description>Recent content in Data Science 2 on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Oct 2015 10:03:57 +0000</lastBuildDate>
    
	<atom:link href="http://markhneedham.com/blog/tag/data-science-2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring (potential) data entry errors in the Land Registry data set</title>
      <link>http://markhneedham.com/blog/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</link>
      <pubDate>Sun, 18 Oct 2015 10:03:57 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</guid>
      <description>I&#39;ve previously written a couple of blog posts describing the mechanics of analysing the Land Registry data set and I thought it was about time I described some of the queries I&#39;ve been running the discoveries I&#39;ve made. To recap, the land registry provides a 3GB, 20 million line CSV file containing all the property sales in the UK since 1995. We&#39;ll be loading and query the data in R using the data.</description>
    </item>
    
    <item>
      <title>Data Science: Mo&#39; Data Mo&#39; Problems</title>
      <link>http://markhneedham.com/blog/2014/06/28/data-science-mo-data-mo-problems/</link>
      <pubDate>Sat, 28 Jun 2014 23:35:25 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/06/28/data-science-mo-data-mo-problems/</guid>
      <description>Over the last couple of years I&#39;ve worked on several proof of concept style Neo4j projects and on a lot of them people have wanted to work with their entire data set which I don&#39;t think makes sense so early on.
In the early parts of a project we&#39;re trying to prove out our approach rather than prove we can handle big data - something that Ashok taught me a couple of years ago on a project we worked on together.</description>
    </item>
    
    <item>
      <title>Data Science: Don&#39;t build a crawler (if you can avoid it!)</title>
      <link>http://markhneedham.com/blog/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</link>
      <pubDate>Thu, 19 Sep 2013 06:55:19 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</guid>
      <description>On Tuesday I spoke at the Data Science London meetup about football data and I started out by covering some lessons I&#39;ve learnt about building data sets for personal use when open data isn&#39;t available.
When that&#39;s the case you often end up scraping HTML pages to extract the data that you&#39;re interested in and then storing that in files or in a database if you want to be more fancy.</description>
    </item>
    
    <item>
      <title>Micro Services Style Data Work Flow</title>
      <link>http://markhneedham.com/blog/2013/02/18/micro-services-style-data-work-flow/</link>
      <pubDate>Mon, 18 Feb 2013 22:16:39 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/02/18/micro-services-style-data-work-flow/</guid>
      <description>Having worked on a few data related applications over the last ten months or so Ashok and I were recently discussing some of the things that we&#39;ve learnt
One of the things he pointed out is that it&#39;s very helpful to separate the different stages of a data work flow into their own applications/scripts.
I decided to try out this idea with some football data that I&#39;m currently trying to model and I ended up with the following stages:</description>
    </item>
    
    <item>
      <title>Data Science: Discovery work</title>
      <link>http://markhneedham.com/blog/2012/12/09/data-science-discovery-work/</link>
      <pubDate>Sun, 09 Dec 2012 10:36:39 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/12/09/data-science-discovery-work/</guid>
      <description>Aaron Erickson recently wrote a blog post where he talks through some of the problems he&#39;s seen with big data initiatives where organisations end up buying a product and expecting it to magically produce results.
 […] corporate IT departments are suddenly are looking at their long running “Business Intelligence” initiatives and wondering why they are not seeing the same kinds of return on investment. They are thinking… if only we tweaked that “BI” initiative and somehow mix in some “Big Data”, maybe *we* could become the next Amazon.</description>
    </item>
    
    <item>
      <title>Nygard Big Data Model: The Investigation Stage</title>
      <link>http://markhneedham.com/blog/2012/10/10/nygard-big-data-model-the-investigation-stage/</link>
      <pubDate>Wed, 10 Oct 2012 00:00:36 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/10/10/nygard-big-data-model-the-investigation-stage/</guid>
      <description>Earlier this year Michael Nygard wrote an extremely detailed post about his experiences in the world of big data projects and included in the post was the following diagram which I&amp;rsquo;ve found very useful.
 Nygard&amp;rsquo;s Big Data Model (shamelessly borrowed by me because it&amp;rsquo;s awesome) Ashok and I have been doing some work in this area helping one of our clients make sense of and visualise some of their data and we realised retrospectively that we were very acting very much in the investigation stage of the model.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 2 Wrap Up</title>
      <link>http://markhneedham.com/blog/2012/10/03/strata-conf-london-day-2-wrap-up/</link>
      <pubDate>Wed, 03 Oct 2012 06:46:13 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/10/03/strata-conf-london-day-2-wrap-up/</guid>
      <description>Yesterday I attended the second day of Strata Conf London and these are the some of the things I learned from the talks I attended:
 John Graham Cunningham opened the series of keynotes with a talk describing the problems British Rail had in 1955 when trying to calculate the distances between all train stations and comparing them to the problems we have today. British Rail were trying to solve a graph problem when people didn&#39;t know about graphs and Dijkstra&#39;s algorithm hadn&#39;t been invented and it was effectively invented on this project but never publicised.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 1 Wrap Up</title>
      <link>http://markhneedham.com/blog/2012/10/02/strata-conf-london-day-1-wrap-up/</link>
      <pubDate>Tue, 02 Oct 2012 23:42:58 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/10/02/strata-conf-london-day-1-wrap-up/</guid>
      <description>For the past couple of days I attended the first Strata Conf to be held in London - a conference which seems to bring together people from the data science and big data worlds to talk about the stuff they&amp;rsquo;re doing.
Since I&amp;rsquo;ve been playing around with a couple of different things in this area over the last 4&amp;frasl;5 months I thought it&amp;rsquo;d be interesting to come along and see what people much more experienced in this area had to say!</description>
    </item>
    
    <item>
      <title>Data Science: Making sense of the data</title>
      <link>http://markhneedham.com/blog/2012/09/30/data-science-making-sense-of-the-data/</link>
      <pubDate>Sun, 30 Sep 2012 14:58:11 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/09/30/data-science-making-sense-of-the-data/</guid>
      <description>Over the past month or so Ashok and I have been helping one of our clients explore and visualise some of their data and one of the first things we needed to do was make sense of the data that was available.
Start small Ashok suggested that we work with a subset of our eventual data set so that we could get a feel for the data and quickly see whether what we were planning to do made sense.</description>
    </item>
    
    <item>
      <title>Data Science: Scrapping the data together</title>
      <link>http://markhneedham.com/blog/2012/09/30/data-science-scrapping-the-data-together/</link>
      <pubDate>Sun, 30 Sep 2012 13:44:18 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/09/30/data-science-scrapping-the-data-together/</guid>
      <description>On Friday Martin, Darren and I were discussing the ThoughtWorks graph that I was working on earlier in the year and Martin pointed out that an interesting aspect of this type of work is that the data you want to work with isn&amp;rsquo;t easily available.
You therefore need to find a way to scrap the data together to make some headway and then maybe at a later stage once some progress has been made it will become easier to replace that with a cleaner solution.</description>
    </item>
    
  </channel>
</rss>