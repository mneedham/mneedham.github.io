<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/kubernetes/</link>
    <description>Recent content in kubernetes on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Nov 2017 12:44:37 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubernetes: Copy a dataset to a StatefulSet&#39;s PersistentVolume</title>
      <link>https://markhneedham.com/blog/2017/11/18/kubernetes-copy-a-dataset-to-a-statefulsets-persistentvolume/</link>
      <pubDate>Sat, 18 Nov 2017 12:44:37 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/18/kubernetes-copy-a-dataset-to-a-statefulsets-persistentvolume/</guid>
      <description>In this post we&amp;#8217;ll learn how to copy an existing dataset to the PersistentVolumes used by a Neo4j cluster running on Kubernetes.
 Neo4j Clusters on Kubernetes This posts assumes that we&amp;#8217;re familiar with deploying Neo4j on Kubernetes. I wrote an article on the Neo4j blog explaining this in more detail.
 The StatefulSet we create for our core servers require persistent storage, achieved via the PersistentVolumeClaim (PVC) primitive. A Neo4j cluster containing 3 core servers would have the following PVCs:</description>
    </item>
    
    <item>
      <title>Kubernetes 1.8: Using Cronjobs to take Neo4j backups</title>
      <link>https://markhneedham.com/blog/2017/11/17/kubernetes-1-8-using-cronjobs-take-neo4j-backups/</link>
      <pubDate>Fri, 17 Nov 2017 18:10:28 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/17/kubernetes-1-8-using-cronjobs-take-neo4j-backups/</guid>
      <description>minikube start --memory 8192 helm init &amp;amp;&amp;amp; kubectl rollout status -w deployment/tiller-deploy --namespace=kube-system helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/ helm install incubator/neo4j --name neo-helm --wait --set authEnabled=false,core.extraVars.NEO4J_dbms_backup_address=0.0.0.0:6362 kubectl exec neo-helm-neo4j-core-0 \  -- bin/cypher-shell --format verbose \  &amp;#34;CALL dbms.cluster.overview() YIELD id, role RETURN id, role&amp;#34; +-----------------------------------------------------+ | id | role | +-----------------------------------------------------+ | &amp;#34;0b3bfe6c-6a68-4af5-9dd2-e96b564df6e5&amp;#34; | &amp;#34;LEADER&amp;#34; | | &amp;#34;09e9bee8-bdc5-4e95-926c-16ea8213e6e7&amp;#34; | &amp;#34;FOLLOWER&amp;#34; | | &amp;#34;859b9b56-9bfc-42ae-90c3-02cedacfe720&amp;#34; | &amp;#34;FOLLOWER&amp;#34; | +-----------------------------------------------------+ kubectl exec neo-helm-neo4j-core-0 \  -- bin/cypher-shell --format verbose \  &amp;#34;UNWIND range(0,1000) AS id CREATE (:Person {id: id})&amp;#34; 0 rows available after 653 ms, consumed after another 0 ms Added 1001 nodes, Set 1001 properties, Added 1001 labels kind: PersistentVolumeClaim apiVersion: v1 metadata: name: backupdir-neo4j labels: app: neo4j-backup spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi --- apiVersion: batch/v1beta1 kind: CronJob metadata: name: neo4j-backup spec: schedule: &amp;#34;*/1 * * * *&amp;#34; jobTemplate: spec: template: spec: volumes: - name: backupdir-neo4j persistentVolumeClaim: claimName: backupdir-neo4j containers: - name: neo4j-backup image: neo4j:3.</description>
    </item>
    
    <item>
      <title>Kubernetes: Simple example of pod running</title>
      <link>https://markhneedham.com/blog/2017/10/21/kubernetes-simple-example-pod-running/</link>
      <pubDate>Sat, 21 Oct 2017 10:06:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/10/21/kubernetes-simple-example-pod-running/</guid>
      <description>kind: Pod apiVersion: v1 metadata: name: marks-dummy-pod spec: containers: - name: marks-dummy-pod image: ubuntu restartPolicy: Never $ kubectl apply -f pod_no_while.yaml pod &amp;#34;marks-dummy-pod&amp;#34; created $ kubectl get pods No resources found, use --show-all to see completed objects. $ kubectl get pods --show-all NAME READY STATUS RESTARTS AGE marks-dummy-pod 0/1 Completed 0 1m kind: Pod apiVersion: v1 metadata: name: marks-dummy-pod spec: containers: - name: marks-dummy-pod image: ubuntu command: [&amp;#34;/bin/bash&amp;#34;, &amp;#34;-ec&amp;#34;, &amp;#34;while :; do echo &amp;#39;.</description>
    </item>
    
    <item>
      <title>Kubernetes: Which node is a pod on?</title>
      <link>https://markhneedham.com/blog/2017/06/14/kubernetes-node-pod/</link>
      <pubDate>Wed, 14 Jun 2017 08:49:06 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/14/kubernetes-node-pod/</guid>
      <description>$ kubectl get pod NAME READY STATUS RESTARTS AGE neo4j-core-0 1/1 Running 0 6m neo4j-core-1 1/1 Running 0 6m neo4j-core-2 1/1 Running 0 2m $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE neo4j-core-0 1/1 Running 0 6m 10.32.3.6 gke-neo4j-cluster-default-pool-ded394fa-0kpw neo4j-core-1 1/1 Running 0 6m 10.32.3.7 gke-neo4j-cluster-default-pool-ded394fa-0kpw neo4j-core-2 1/1 Running 0 2m 10.32.0.10 gke-neo4j-cluster-default-pool-ded394fa-kp68 </description>
    </item>
    
    <item>
      <title>Kubernetes: Simulating a network partition</title>
      <link>https://markhneedham.com/blog/2016/12/04/kubernetes-simulating-a-network-partition/</link>
      <pubDate>Sun, 04 Dec 2016 12:37:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/12/04/kubernetes-simulating-a-network-partition/</guid>
      <description>$ kubectl get pods NAME READY STATUS RESTARTS AGE neo4j-0 1/1 Running 0 6h neo4j-1 1/1 Running 0 6h neo4j-2 1/1 Running 0 6h $ kubectl exec neo4j-0 -- ls -alh /sbin/route lrwxrwxrwx 1 root root 12 Oct 18 18:58 /sbin/route -&amp;gt; /bin/busybox $ kubectl exec neo4j-0 -- bin/cypher-shell &amp;#34;CALL dbms.cluster.role()&amp;#34; role &amp;#34;FOLLOWER&amp;#34; Bye! $ kubectl exec neo4j-1 -- bin/cypher-shell &amp;#34;CALL dbms.cluster.role()&amp;#34; role &amp;#34;FOLLOWER&amp;#34; Bye! $ kubectl exec neo4j-2 -- bin/cypher-shell &amp;#34;CALL dbms.</description>
    </item>
    
    <item>
      <title>Kubernetes: Spinning up a Neo4j 3.1 Causal Cluster</title>
      <link>https://markhneedham.com/blog/2016/11/25/kubernetes-spinning-up-a-neo4j-3-1-causal-cluster/</link>
      <pubDate>Fri, 25 Nov 2016 16:55:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/11/25/kubernetes-spinning-up-a-neo4j-3-1-causal-cluster/</guid>
      <description>for i in $(seq 0 2); do cat &amp;lt;&amp;lt;EOF | kubectl create -f - kind: PersistentVolume apiVersion: v1 metadata: name: pv${i} labels: type: local app: neo4j spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: &amp;#34;/tmp/${i}&amp;#34; EOF cat &amp;lt;&amp;lt;EOF | kubectl create -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: datadir-neo4j-${i} labels: app: neo4j spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi EOF done; $ kubectl get pv NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE pv0 1Gi RWO Bound default/datadir-neo4j-0 7s pv1 1Gi RWO Bound default/datadir-neo4j-1 7s pv2 1Gi RWO Bound default/datadir-neo4j-2 7s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE datadir-neo4j-0 Bound pv0 1Gi RWO 26s datadir-neo4j-1 Bound pv1 1Gi RWO 26s datadir-neo4j-2 Bound pv2 1Gi RWO 25s # Headless service to provide DNS lookup apiVersion: v1 kind: Service metadata: labels: app: neo4j name: neo4j spec: clusterIP: None ports: - port: 7474 selector: app: neo4j ---- # new API name apiVersion: &amp;#34;apps/v1alpha1&amp;#34; kind: PetSet metadata: name: neo4j spec: serviceName: neo4j replicas: 3 template: metadata: annotations: pod.</description>
    </item>
    
    <item>
      <title>Kubernetes: Writing hostname to a file</title>
      <link>https://markhneedham.com/blog/2016/11/22/kubernetes-writing-hostname-to-a-file/</link>
      <pubDate>Tue, 22 Nov 2016 19:56:31 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/11/22/kubernetes-writing-hostname-to-a-file/</guid>
      <description>$ minikube start Starting local Kubernetes cluster... Kubectl is now configured to use the cluster. apiVersion: v1 kind: Pod metadata: name: mark-super-simple-test-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox:1.24 command: [ &amp;#34;/bin/sh&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;env&amp;#34; ] dnsPolicy: Default restartPolicy: Never $ kubectl create -f hostname_super_simple.yaml pod &amp;#34;mark-super-simple-test-pod&amp;#34; created $ kubectl logs mark-super-simple-test-pod KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.0.0.1:443 HOSTNAME=mark-super-simple-test-pod SHLVL=1 HOME=/root KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443 PWD=/ KUBERNETES_SERVICE_HOST=10.0.0.1 apiVersion: v1 kind: Pod metadata: name: mark-test-pod spec: containers: - name: test-container image: gcr.</description>
    </item>
    
  </channel>
</rss>