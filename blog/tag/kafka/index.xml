<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafka on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/kafka/</link>
    <description>Recent content in kafka on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jan 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink SQL: Could not execute SQL statement. Reason: java.io.IOException: Corrupt Debezium JSON message</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/</guid>
      <description>As part of a JFokus workshop that I’m working on I wanted to create a Flink table around a Kafka stream that I’d populated from MySQL with help from Debezium. In this blog post I want to show how to do this and explain an error that I encountered along the way.
To start, we have a products table in MySQL that’s publishing events to Apache Kafka. We can see the fields in this event by running the following command:</description>
    </item>
    
    <item>
      <title>Flink SQL: Exporting nested JSON to a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</link>
      <pubDate>Tue, 24 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/24/flink-sql-export-nested-json-kafka/</guid>
      <description>I’ve been playing around with Flink as part of a workshop that I’m doing at JFokus in a couple of weeks and I wanted to export some data from Flink to Apache Kafka in a nested format. In this blog post we’ll learn how to do that.
Setup We’re going to be using the following Docker Compose config:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:latest container_name: zookeeper hostname: zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOO_MY_ID: 1 ZOO_PORT: 2181 ZOO_SERVERS: server.</description>
    </item>
    
    <item>
      <title>kcat/jq: Reached end of topic at offset: exiting</title>
      <link>https://www.markhneedham.com/blog/2022/12/06/kcat-jq-reached-end-of-topic-exiting/</link>
      <pubDate>Tue, 06 Dec 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/12/06/kcat-jq-reached-end-of-topic-exiting/</guid>
      <description>I’ve recently been working with Debezium to get the Pizza Shop product catalogue from MySQL into Apache Kafka and ran into an issue when querying the resulting stream using kcat and jq. In this blog I’ll show how I worked around that problem.
I configured Debezium to write any changes to the products table into the mysql.pizzashop.products topic. I then queriesthis topic to find the changes for just one of the products:</description>
    </item>
    
    <item>
      <title>Kafka: Writing data to a topic from the command line</title>
      <link>https://www.markhneedham.com/blog/2022/01/22/kafka-writing-data-topic-command-line/</link>
      <pubDate>Sat, 22 Jan 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/01/22/kafka-writing-data-topic-command-line/</guid>
      <description>I’ve been doing more Apache Pinot documentation - this time covering the JSON functions - and I needed to quickly write some data into Kafka to test things out. I’d normally do that using the Python Kafka client, but this time I wanted to do it using only command line tools. So that’s what we’ll be doing in this blog post and it’s more for future me than anyone else!</description>
    </item>
    
    <item>
      <title>Kafka: Python Consumer - No messages with group id/consumer group</title>
      <link>https://www.markhneedham.com/blog/2019/06/03/kafka-python-consumer-no-messages-group-id-consumer-group/</link>
      <pubDate>Mon, 03 Jun 2019 11:08:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/06/03/kafka-python-consumer-no-messages-group-id-consumer-group/</guid>
      <description>When I’m learning a new technology, I often come across things that are incredibly confusing when I first come across them, but make complete sense afterwards. In this post I’ll explain my experience writing a Kafka consumer that wasn’t finding any messages when using consumer groups .
Setting up Kafka infrastructure We’ll set up the Kafka infrastructure locally using the Docker Compose Template that I describe in my Kafka: A Basic Tutorial blog post.</description>
    </item>
    
    <item>
      <title>Twint: Loading tweets into Kafka and Neo4j</title>
      <link>https://www.markhneedham.com/blog/2019/05/29/loading-tweets-twint-kafka-neo4j/</link>
      <pubDate>Wed, 29 May 2019 06:50:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/29/loading-tweets-twint-kafka-neo4j/</guid>
      <description>In this post we’re going to load tweets via the twint library into Kafka, and once we’ve got them in there we’ll use the Kafka Connect Neo4j Sink Plugin to get them into Neo4j.
What is twint? Twitter data has always been some of the most fun to play with, but over the years the official API has become more and more restritive, and it now takes a really long time to download enough data to do anything interesting.</description>
    </item>
    
    <item>
      <title>Processing Neo4j Transaction Events with KSQL and Kafka Streams</title>
      <link>https://www.markhneedham.com/blog/2019/05/23/processing-neo4j-transaction-events-ksql-kafka-streams/</link>
      <pubDate>Thu, 23 May 2019 12:46:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/23/processing-neo4j-transaction-events-ksql-kafka-streams/</guid>
      <description>The Neo4j Streams Library lets users send transaction events to a Kafka topic, and in this post we’re going to learn how to explore these events using the KSQL streaming SQL Engine.
All the infrastructure used in this post can be launched locally from Docker compose, using the following command:
git clone git@github.com:mneedham/ksql-kafka-neo4j-streams.git &amp;amp;&amp;amp; cd ksql-kafka-neo4j-streams docker-compose-up Running this command will create four containers:
Starting zookeeper-blog ... Starting broker-blog .</description>
    </item>
    
    <item>
      <title>Deleting Kafka Topics on Docker</title>
      <link>https://www.markhneedham.com/blog/2019/05/23/deleting-kafka-topics-on-docker/</link>
      <pubDate>Thu, 23 May 2019 07:58:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/23/deleting-kafka-topics-on-docker/</guid>
      <description>In this post we’re going to learn how to delete a Kafka Topic when running a Kafka Broker on Docker.
We’ll spin up a local Kafka environment using the Docker Compose template from the Kafka Basic Tutorial blog post that I wrote last week. Let’s open a terminal window and run the following commands to set up our environment:
git clone git@github.com:mneedham/basic-kafka-tutorial.git &amp;amp;&amp;amp; cd basic-kafka-tutorial docker-compose up On another terminal window, run the following command to see the list of Docker containers that are running:</description>
    </item>
    
    <item>
      <title>KSQL: Create Stream - extraneous input &#39;properties&#39;</title>
      <link>https://www.markhneedham.com/blog/2019/05/20/kql-create-stream-extraneous-input/</link>
      <pubDate>Mon, 20 May 2019 11:43:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/20/kql-create-stream-extraneous-input/</guid>
      <description>In my continued playing with the KSQL streaming engine for Kafka, I came across another interesting error while trying to put a stream on top of a topic generated by the Neo4j Streams Library.
We’ll simplify the events being posted on the topic for this blog post, so this is what the events on the topic look like:
{ &amp;#34;id&amp;#34;:&amp;#34;ABCDEFGHI&amp;#34;, &amp;#34;properties&amp;#34;: { &amp;#34;name&amp;#34;:&amp;#34;Mark&amp;#34;, &amp;#34;location&amp;#34;:&amp;#34;London&amp;#34; } } We then create a stream on that topic:</description>
    </item>
    
    <item>
      <title>KSQL: Create Stream - Failed to prepare statement: name is null</title>
      <link>https://www.markhneedham.com/blog/2019/05/19/ksql-create-stream-failed-to-prepare-statement-name-is-null/</link>
      <pubDate>Sun, 19 May 2019 19:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/19/ksql-create-stream-failed-to-prepare-statement-name-is-null/</guid>
      <description>I’ve been playing with KSQL over the weekend and ran into a basic error message that took me a little while to solve.
I was trying to create a stream over a topic dummy1, which is the simplest possible thing you can do with KSQL. The events posted to dummy1 are JSON messages containing only an id key. Below is an example of a message posted to the topic:</description>
    </item>
    
    <item>
      <title>Kafka: A basic tutorial</title>
      <link>https://www.markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</link>
      <pubDate>Thu, 16 May 2019 10:02:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</guid>
      <description>In this post we’re going to learn how to launch Kafka locally and write to and read from a topic using one of the Python drivers.
To make things easy for myself, I’ve created a Docker Compose template that launches 3 containers:
broker - our Kafka broker
zookeeper - used by Kafka for leader election
jupyter - notebooks for connecting to our Kafka broker
This template can be downloaded from the mneedham/basic-kafka-tutorial repository, and reads as follows:</description>
    </item>
    
  </channel>
</rss>
