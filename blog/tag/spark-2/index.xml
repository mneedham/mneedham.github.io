<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark-2 on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/spark-2/</link>
    <description>Recent content in spark-2 on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Oct 2015 23:10:47 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/spark-2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark: MatchError (of class org.apache.spark.sql.catalyst.expressions.GenericRow) spark</title>
      <link>https://markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</link>
      <pubDate>Tue, 27 Oct 2015 23:10:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</guid>
      <description>$ head -n5 pp-complete.csv &amp;#34;{0C7ADEF5-878D-4066-B785-0000003ED74A}&amp;#34;,&amp;#34;163000&amp;#34;,&amp;#34;2003-02-21 00:00&amp;#34;,&amp;#34;UB5 4PJ&amp;#34;,&amp;#34;T&amp;#34;,&amp;#34;N&amp;#34;,&amp;#34;F&amp;#34;,&amp;#34;106&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;READING ROAD&amp;#34;,&amp;#34;NORTHOLT&amp;#34;,&amp;#34;NORTHOLT&amp;#34;,&amp;#34;EALING&amp;#34;,&amp;#34;GREATER LONDON&amp;#34;,&amp;#34;A&amp;#34; &amp;#34;{35F67271-ABD4-40DA-AB09-00000085B9D3}&amp;#34;,&amp;#34;247500&amp;#34;,&amp;#34;2005-07-15 00:00&amp;#34;,&amp;#34;TA19 9DD&amp;#34;,&amp;#34;D&amp;#34;,&amp;#34;N&amp;#34;,&amp;#34;F&amp;#34;,&amp;#34;58&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;ADAMS MEADOW&amp;#34;,&amp;#34;ILMINSTER&amp;#34;,&amp;#34;ILMINSTER&amp;#34;,&amp;#34;SOUTH SOMERSET&amp;#34;,&amp;#34;SOMERSET&amp;#34;,&amp;#34;A&amp;#34; &amp;#34;{B20B1C74-E8E1-4137-AB3E-0000011DF342}&amp;#34;,&amp;#34;320000&amp;#34;,&amp;#34;2010-09-10 00:00&amp;#34;,&amp;#34;W4 1DZ&amp;#34;,&amp;#34;F&amp;#34;,&amp;#34;N&amp;#34;,&amp;#34;L&amp;#34;,&amp;#34;58&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;WHELLOCK ROAD&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;LONDON&amp;#34;,&amp;#34;EALING&amp;#34;,&amp;#34;GREATER LONDON&amp;#34;,&amp;#34;A&amp;#34; &amp;#34;{7D6B0915-C56B-4275-AF9B-00000156BCE7}&amp;#34;,&amp;#34;104000&amp;#34;,&amp;#34;1997-08-27 00:00&amp;#34;,&amp;#34;NE61 2BH&amp;#34;,&amp;#34;D&amp;#34;,&amp;#34;N&amp;#34;,&amp;#34;F&amp;#34;,&amp;#34;17&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;WESTGATE&amp;#34;,&amp;#34;MORPETH&amp;#34;,&amp;#34;MORPETH&amp;#34;,&amp;#34;CASTLE MORPETH&amp;#34;,&amp;#34;NORTHUMBERLAND&amp;#34;,&amp;#34;A&amp;#34; &amp;#34;{47B60101-B64C-413D-8F60-000002F1692D}&amp;#34;,&amp;#34;147995&amp;#34;,&amp;#34;2003-05-02 00:00&amp;#34;,&amp;#34;PE33 0RU&amp;#34;,&amp;#34;D&amp;#34;,&amp;#34;N&amp;#34;,&amp;#34;F&amp;#34;,&amp;#34;4&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;MASON GARDENS&amp;#34;,&amp;#34;WEST WINCH&amp;#34;,&amp;#34;KING&amp;#39;S LYNN&amp;#34;,&amp;#34;KING&amp;#39;S LYNN AND WEST NORFOLK&amp;#34;,&amp;#34;NORFOLK&amp;#34;,&amp;#34;A&amp;#34; import org.apache.spark.sql.{SQLContext, _} import org.apache.spark.{SparkConf, SparkContext} case class BlogTransaction(price: Integer, date: String, postCode:String, paon:String, saon:String, street:String, locality:String, city:String, district:String, county:String) object BlogApp { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&amp;#34;Simple Application&amp;#34;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) import sqlContext.</description>
    </item>
    
    <item>
      <title>SparkR: Error in invokeJava(isStatic = TRUE, className, methodName, ...) :  java.lang.ClassNotFoundException: Failed to load class for data source: csv.</title>
      <link>https://markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</link>
      <pubDate>Mon, 21 Sep 2015 22:06:44 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</guid>
      <description>./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0 &amp;gt; sales &amp;lt;- read.df(sqlContext, &amp;#34;pp-complete.csv&amp;#34;, &amp;#34;csv&amp;#34;) 15/09/20 19:13:02 ERROR RBackendHandler: loadDF on org.apache.spark.sql.api.r.SQLUtils failed Error in invokeJava(isStatic = TRUE, className, methodName, ...) : java.lang.ClassNotFoundException: Failed to load class for data source: csv. at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:67) at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:87) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114) at org.apache.spark.sql.api.r.SQLUtils$.loadDF(SQLUtils.scala:156) at org.apache.spark.sql.api.r.SQLUtils.loadDF(SQLUtils.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132) at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79) at org.apache.spark.api.r.RBackendH &amp;gt; sales &amp;lt;- read.df(sqlContext, &amp;#34;pp-complete.csv&amp;#34;, &amp;#34;com.databricks.spark.csv&amp;#34;, header=&amp;#34;false&amp;#34;) &amp;gt; sales DataFrame[C0:string, C1:string, C2:string, C3:string, C4:string, C5:string, C6:string, C7:string, C8:string, C9:string, C10:string, C11:string, C12:string, C13:string, C14:string] </description>
    </item>
    
    <item>
      <title>Spark: Convert RDD to DataFrame</title>
      <link>https://markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</link>
      <pubDate>Thu, 06 Aug 2015 21:11:44 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</guid>
      <description>import org.apache.spark.sql.{SQLContext, Row, DataFrame} val sqlContext = new SQLContext(sc) val crimeFile = &amp;#34;Crimes_-_2001_to_present.csv&amp;#34; sqlContext.load(&amp;#34;com.databricks.spark.csv&amp;#34;, Map(&amp;#34;path&amp;#34; -&amp;gt; crimeFile, &amp;#34;header&amp;#34; -&amp;gt; &amp;#34;true&amp;#34;)).registerTempTable(&amp;#34;crimes&amp;#34;) private def createFile(df: DataFrame, file: String, header: String): Unit = { FileUtil.fullyDelete(new File(file)) val tmpFile = &amp;#34;tmp/&amp;#34; + System.currentTimeMillis() + &amp;#34;-&amp;#34; + file df.distinct.save(tmpFile, &amp;#34;com.databricks.spark.csv&amp;#34;) } val rows = sqlContext.sql(&amp;#34;select `Primary Type` as primaryType FROM crimes LIMIT 10&amp;#34;) rows.collect() res4: Array[org.apache.spark.sql.Row] = Array([ASSAULT], [ROBBERY], [CRIMINAL DAMAGE], [THEFT], [THEFT], [BURGLARY], [THEFT], [BURGLARY], [THEFT], [CRIMINAL DAMAGE]) rows.</description>
    </item>
    
    <item>
      <title>Spark: pyspark/Hadoop - py4j.protocol.Py4JJavaError: An error occurred while calling o23.load.: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>https://markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Tue, 04 Aug 2015 06:35:40 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>from pyspark import SparkContext from pyspark.sql import SQLContext sc = SparkContext(&amp;#34;local&amp;#34;, &amp;#34;Simple App&amp;#34;) sqlContext = SQLContext(sc) file = &amp;#34;hdfs://localhost:9000/user/markneedham/Crimes_-_2001_to_present.csv&amp;#34; sqlContext.load(source=&amp;#34;com.databricks.spark.csv&amp;#34;, header=&amp;#34;true&amp;#34;, path = file).registerTempTable(&amp;#34;crimes&amp;#34;) rows = sqlContext.sql(&amp;#34;select `FBI Code` AS fbiCode, COUNT(*) AS times FROM crimes GROUP BY `FBI Code` ORDER BY times DESC&amp;#34;).collect() for row in rows: print(&amp;#34;{0} -&amp;gt; {1}&amp;#34;.format(row.fbiCode, row.times)) $ ./spark-1.3.0-bin-hadoop1/bin/spark-submit --driver-memory 5g --packages com.databricks:spark-csv_2.10:1.1.0 fbi_spark.py ... Traceback (most recent call last): File &amp;#34;/Users/markneedham/projects/neo4j-spark-chicago/fbi_spark.py&amp;#34;, line 11, in &amp;lt;module&amp;gt; sqlContext.</description>
    </item>
    
    <item>
      <title>Spark: Processing CSV files using Databricks Spark CSV Library</title>
      <link>https://markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</link>
      <pubDate>Sun, 02 Aug 2015 18:08:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</guid>
      <description>$ ./spark-1.3.0-bin-hadoop1/bin/spark-shell scala&amp;gt; import org.apache.spark.sql.SQLContext import org.apache.spark.sql.SQLContext scala&amp;gt; val crimeFile = &amp;#34;/Users/markneedham/Downloads/Crimes_-_2001_to_present.csv&amp;#34; crimeFile: String = /Users/markneedham/Downloads/Crimes_-_2001_to_present.csv scala&amp;gt; val sqlContext = new SQLContext(sc) sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@9746157 scala&amp;gt; sqlContext.load(&amp;#34;com.databricks.spark.csv&amp;#34;, Map(&amp;#34;path&amp;#34; -&amp;gt; crimeFile, &amp;#34;header&amp;#34; -&amp;gt; &amp;#34;true&amp;#34;)).registerTempTable(&amp;#34;crimes&amp;#34;) java.lang.RuntimeException: Failed to load class for data source: com.databricks.spark.csv at scala.sys.package$.error(package.scala:27) at org.apache.spark.sql.sources.ResolvedDataSource$.lookupDataSource(ddl.scala:268) at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:279) at org.apache.spark.sql.SQLContext.load(SQLContext.scala:679) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813) at org.</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file with header using saveAsFile</title>
      <link>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</link>
      <pubDate>Sun, 30 Nov 2014 08:21:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</guid>
      <description>import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.*; import org.apache.hadoop.io.IOUtils; import java.io.IOException; public class MyFileUtil { public static boolean copyMergeWithHeader(FileSystem srcFS, Path srcDir, FileSystem dstFS, Path dstFile, boolean deleteSource, Configuration conf, String header) throws IOException { dstFile = checkDest(srcDir.getName(), dstFS, dstFile, false); if(!srcFS.getFileStatus(srcDir).isDir()) { return false; } else { FSDataOutputStream out = dstFS.create(dstFile); if(header != null) { out.write((header + &amp;#34;\n&amp;#34;).getBytes(&amp;#34;UTF-8&amp;#34;)); } try { FileStatus[] contents = srcFS.listStatus(srcDir); for(int i = 0; i &amp;lt; contents.length; ++i) { if(!</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file</title>
      <link>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</link>
      <pubDate>Sun, 30 Nov 2014 07:40:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</guid>
      <description>import au.com.bytecode.opencsv.CSVParser import org.apache.spark.rdd.RDD import org.apache.spark.SparkContext._ def dropHeader(data: RDD[String]): RDD[String] = { data.mapPartitionsWithIndex((idx, lines) =&amp;gt; { if (idx == 0) { lines.drop(1) } lines }) } // https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2 val crimeFile = &amp;#34;/Users/markneedham/Downloads/Crimes_-_2001_to_present.csv&amp;#34; val crimeData = sc.textFile(crimeFile).cache() val withoutHeader: RDD[String] = dropHeader(crimeData) val file = &amp;#34;/tmp/primaryTypes.csv&amp;#34; FileUtil.fullyDelete(new File(file)) val partitions: RDD[(String, Int)] = withoutHeader.mapPartitions(lines =&amp;gt; { val parser = new CSVParser(&amp;#39;,&amp;#39;) lines.map(line =&amp;gt; { val columns = parser.parseLine(line) (columns(5), 1) }) }) val counts = partitions.</description>
    </item>
    
    <item>
      <title>Spark: Parse CSV file and group by column value</title>
      <link>https://markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</link>
      <pubDate>Sun, 16 Nov 2014 22:53:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</guid>
      <description>$ ls -alh ~/Downloads/Crimes_-_2001_to_present.csv -rw-r--r--@ 1 markneedham staff 1.0G 16 Nov 12:14 /Users/markneedham/Downloads/Crimes_-_2001_to_present.csv $ wc -l ~/Downloads/Crimes_-_2001_to_present.csv 4193441 /Users/markneedham/Downloads/Crimes_-_2001_to_present.csv $ head -n 2 ~/Downloads/Crimes_-_2001_to_present.csv ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location 9464711,HX114160,01/14/2014 05:00:00 AM,028XX E 80TH ST,0560,ASSAULT,SIMPLE,APARTMENT,false,true,0422,004,7,46,08A,1196652,1852516,2014,01/20/2014 12:40:05 AM,41.75017626412204,-87.55494559131228,&amp;#34;(41.75017626412204, -87.55494559131228)&amp;#34; $ time tail +2 ~/Downloads/Crimes_-_2001_to_present.csv | cut -d, -f6 | sort | uniq -c | sort -rn 859197 THEFT 757530 BATTERY 489528 NARCOTICS 488209 CRIMINAL DAMAGE 257310 BURGLARY 253964 OTHER OFFENSE 247386 ASSAULT 197404 MOTOR VEHICLE THEFT 157706 ROBBERY 137538 DECEPTIVE PRACTICE 124974 CRIMINAL TRESPASS 47245 PROSTITUTION 40361 WEAPONS VIOLATION 31585 PUBLIC PEACE VIOLATION 26524 OFFENSE INVOLVING CHILDREN 14788 CRIM SEXUAL ASSAULT 14283 SEX OFFENSE 10632 GAMBLING 8847 LIQUOR LAW VIOLATION 6443 ARSON 5178 INTERFERE WITH PUBLIC OFFICER 4846 HOMICIDE 3585 KIDNAPPING 3147 INTERFERENCE WITH PUBLIC OFFICER 2471 INTIMIDATION 1985 STALKING 355 OFFENSES INVOLVING CHILDREN 219 OBSCENITY 86 PUBLIC INDECENCY 80 OTHER NARCOTIC VIOLATION 12 RITUALISM 12 NON-CRIMINAL 6 OTHER OFFENSE 2 NON-CRIMINAL (SUBJECT SPECIFIED) 2 NON - CRIMINAL real	2m37.</description>
    </item>
    
  </channel>
</rss>