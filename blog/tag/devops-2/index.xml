<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Devops 2 on Mark Needham</title>
    <link>https://mneedham.github.io/blog/tag/devops-2/</link>
    <description>Recent content in Devops 2 on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 Apr 2013 14:22:10 +0000</lastBuildDate>
    
	<atom:link href="https://mneedham.github.io/blog/tag/devops-2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Treat servers as cattle: Spin them up, tear them down</title>
      <link>https://mneedham.github.io/blog/2013/04/27/treat-servers-as-cattle-spin-them-up-tear-them-down/</link>
      <pubDate>Sat, 27 Apr 2013 14:22:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/27/treat-servers-as-cattle-spin-them-up-tear-them-down/</guid>
      <description>A few agos I wrote a post about treating servers as cattle, not as pets in which I described an approach to managing virtual machines at uSwitch whereby we frequently spin up new ones and delete the existing ones.
I&#39;ve worked on teams previously where we&#39;ve also talked about this mentality but ended up not doing it because it was difficult, usually for one of two reasons:
 Slow spin up - this might be due to the cloud providers infrastructure, doing too much on spin up or I&#39;m sure a variety of other reasons.</description>
    </item>
    
    <item>
      <title>Puppet: Installing Oracle Java - oracle-license-v1-1 license could not be presented</title>
      <link>https://mneedham.github.io/blog/2013/04/18/puppet-installing-oracle-java-oracle-license-v1-1-license-could-not-be-presented/</link>
      <pubDate>Thu, 18 Apr 2013 23:36:32 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/18/puppet-installing-oracle-java-oracle-license-v1-1-license-could-not-be-presented/</guid>
      <description>In order to run the neo4j server on my Ubuntu 12.04 Vagrant VM I needed to install the Oracle/Sun JDK which proved to be more difficult than I&#39;d expected.
I initially tried to install it via the OAB-Java script but was running into some dependency problems and eventually came across a post which specified a PPA that had an installer I could use.
I wrote a little puppet Java module to wrap the commands in:</description>
    </item>
    
    <item>
      <title>dpkg/apt-cache: Useful commands</title>
      <link>https://mneedham.github.io/blog/2013/04/18/dpkgapt-cache-useful-commands/</link>
      <pubDate>Thu, 18 Apr 2013 21:54:10 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/18/dpkgapt-cache-useful-commands/</guid>
      <description>As I&#39;ve mentioned in a couple of previous posts I&#39;ve been playing around with creating a Vagrant VM that I can use for my neo4j hacking which has involved a lot of messing around with installing apt packages.
There are loads of different ways of working out what&#39;s going on when packages aren&#39;t installing as you&#39;d expect so I thought it&#39;d be good to document the ones I&#39;ve been using so I can find them more easily next time.</description>
    </item>
    
    <item>
      <title>Puppet Debt</title>
      <link>https://mneedham.github.io/blog/2013/04/16/puppet-debt/</link>
      <pubDate>Tue, 16 Apr 2013 20:57:53 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/16/puppet-debt/</guid>
      <description>I&#39;ve been playing around with a puppet configuration to run a neo4j server on an Ubuntu VM and one thing that has been quite tricky is getting the Sun/Oracle Java JDK to install repeatably.
I adapted Julian&#39;s Java module which uses OAB-Java and although it was certainly working cleanly at one stage I somehow ended up with it not working because of failed dependencies:
[2013-04-12 07:03:10] Notice: /Stage[main]/Java/Exec[install OAB repo]/returns: [x] Installing Java build requirements Ofailed [2013-04-12 07:03:10] Notice: /Stage[main]/Java/Exec[install OAB repo]/returns: ^[[m^O [i] Showing the last 5 lines from the logfile (/root/oab-java.</description>
    </item>
    
    <item>
      <title>Capistrano: Deploying to a Vagrant VM</title>
      <link>https://mneedham.github.io/blog/2013/04/13/capistrano-deploying-to-a-vagrant-vm/</link>
      <pubDate>Sat, 13 Apr 2013 11:17:37 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/13/capistrano-deploying-to-a-vagrant-vm/</guid>
      <description>I&#39;ve been working on a tutorial around thinking through problems in graphs using my football graph and I wanted to deploy it on a local vagrant VM as a stepping stone to deploying it in a live environment.
My Vagrant file for the VM looks like this:
# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant::Config.run do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.define :neo01 do |neo| neo.vm.network :hostonly, &amp;quot;192.168.33.101&amp;quot; neo.</description>
    </item>
    
    <item>
      <title>Treating servers as cattle, not as pets</title>
      <link>https://mneedham.github.io/blog/2013/04/07/treating-servers-as-cattle-not-as-pets/</link>
      <pubDate>Sun, 07 Apr 2013 11:41:34 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/04/07/treating-servers-as-cattle-not-as-pets/</guid>
      <description>Although I didn&#39;t go to Dev Ops Days London earlier in the year I was following the hash tag on twitter and one of my favourites things that I read was the following:
 “Treating servers as cattle, not as pets” #DevOpsDays  I think this is particularly applicable now that a lot of the time we&#39;re using virtualised production environments via AWS, Rackspace or .
At uSwitch we use AWS and over the last week Sid and I spent some time investigating a memory leak by running our applications against two different versions of Ruby.</description>
    </item>
    
    <item>
      <title>Incrementally rolling out machines with a new puppet role</title>
      <link>https://mneedham.github.io/blog/2013/03/24/incrementally-rolling-out-machines-with-a-new-puppet-role/</link>
      <pubDate>Sun, 24 Mar 2013 22:52:19 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/03/24/incrementally-rolling-out-machines-with-a-new-puppet-role/</guid>
      <description>Last week Jason and I with (a lot of) help from Tim have been working on moving several of our applications from Passenger to Unicorn and decided that the easiest way to do this was to create a new set of nodes with this setup.
The architecture we&#39;re working with looks like this at a VM level:
 The &#39;nginx LB&#39; nodes are responsible for routing all the requests to their appropriate application servers and the &#39;web&#39; nodes serve the different applications initially using Passenger.</description>
    </item>
    
    <item>
      <title>Understanding what lsof socket/port aliases refer to</title>
      <link>https://mneedham.github.io/blog/2013/03/17/understanding-what-lsof-socketport-aliases-refer-to/</link>
      <pubDate>Sun, 17 Mar 2013 14:00:35 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/03/17/understanding-what-lsof-socketport-aliases-refer-to/</guid>
      <description>Earlier in the week we wanted to check which ports were being listened on and by what processes which we can do with the following command on Mac OS X:
$ lsof -ni | grep LISTEN idea 2398 markhneedham 58u IPv6 0xac8f13f77b903331 0t0 TCP *:49410 (LISTEN) idea 2398 markhneedham 65u IPv6 0xac8f13f7799a4af1 0t0 TCP *:58741 (LISTEN) idea 2398 markhneedham 122u IPv6 0xac8f13f7799a4711 0t0 TCP 127.0.0.1:6942 (LISTEN) idea 2398 markhneedham 249u IPv6 0xac8f13f777586711 0t0 TCP *:63342 (LISTEN) idea 2398 markhneedham 253u IPv6 0xac8f13f777586331 0t0 TCP 127.</description>
    </item>
    
    <item>
      <title>telnet/netcat: Waiting for a port to be open</title>
      <link>https://mneedham.github.io/blog/2013/01/20/waiting-for-a-port-to-be-open/</link>
      <pubDate>Sun, 20 Jan 2013 15:53:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/01/20/waiting-for-a-port-to-be-open/</guid>
      <description>On Friday Nathan and I were setting up a new virtual machine and we needed a firewall rule to be created to allow us to connect to another machine which had some JAR files we wanted to download.
We wanted to know when it had been done by one of our operations team and I initially thought we might be able to do that using telnet:
$ telnet 10.0.0.1 8081 Trying 10.</description>
    </item>
    
    <item>
      <title>Fabric/Boto: boto.exception.NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. [&#39;QuerySignatureV2AuthHandler&#39;] Check your credentials</title>
      <link>https://mneedham.github.io/blog/2013/01/15/fabricboto-boto-exception-noauthhandlerfound-no-handler-was-ready-to-authenticate-1-handlers-were-checked-querysignaturev2authhandler-check-your-credentials/</link>
      <pubDate>Tue, 15 Jan 2013 00:37:01 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/01/15/fabricboto-boto-exception-noauthhandlerfound-no-handler-was-ready-to-authenticate-1-handlers-were-checked-querysignaturev2authhandler-check-your-credentials/</guid>
      <description>In our Fabric code we make use of Boto to connect to the EC2 API and pull back various bits of information and the first time anyone tries to use it they end up with the following stack trace:
File &amp;quot;/Library/Python/2.7/site-packages/fabric/main.py&amp;quot;, line 717, in main *args, **kwargs File &amp;quot;/Library/Python/2.7/site-packages/fabric/tasks.py&amp;quot;, line 332, in execute results[&#39;&amp;lt;local-only&amp;gt;&#39;] = task.run(*args, **new_kwargs) File &amp;quot;/Library/Python/2.7/site-packages/fabric/tasks.py&amp;quot;, line 112, in run return self.wrapped(*args, **kwargs) File &amp;quot;/Users/mark/projects/forward-puppet/ec2.py&amp;quot;, line 131, in running instances = instances_by_zones(running_instances(region, role_name)) File &amp;quot;/Users/mark/projects/forward-puppet/ec2.</description>
    </item>
    
    <item>
      <title>Fabric: Tailing log files on multiple machines</title>
      <link>https://mneedham.github.io/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</link>
      <pubDate>Tue, 15 Jan 2013 00:20:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2013/01/15/fabric-tailing-log-files-on-multiple-machines/</guid>
      <description>We wanted to tail one of the log files simultaneously on 12 servers this afternoon to try and see if a particular event was being logged and rather than opening 12 SSH sessions decided to get Fabric to help us out.
My initial attempt to do this was the following:
fab -H host1,host2,host3 -- tail -f /var/www/awesome/current/log/production.log  It works but the problem is that by default Fabric runs the specified command one machine after the other so we&#39;ve actually managed to block Fabric with the tail command on &#39;host1&#39;.</description>
    </item>
    
    <item>
      <title>The Tracer Bullet Approach: An example</title>
      <link>https://mneedham.github.io/blog/2012/12/24/the-tracer-bullet-approach-an-example/</link>
      <pubDate>Mon, 24 Dec 2012 09:09:44 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2012/12/24/the-tracer-bullet-approach-an-example/</guid>
      <description>A few weeks ago my former colleague Kief Morris wrote a blog post describing the tracer bullet approach he&#39;s used to setup a continuous delivery pipeline on his current project.
 The idea is to get the simplest implementation of a pipeline in place, prioritizing a fully working skeleton that stretches across the full path to production over a fully featured, final-design functionality for each stage of the pipeline.  Kief goes on to explain in detail how we can go about executing this and it reminded of a project I worked on almost 3 years ago where we took a similar approach.</description>
    </item>
    
    <item>
      <title>There&#39;s No such thing as a &#39;DevOps Team&#39;: Some thoughts</title>
      <link>https://mneedham.github.io/blog/2012/11/30/theres-no-such-thing-as-a-devops-team-some-thoughts/</link>
      <pubDate>Fri, 30 Nov 2012 16:56:16 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2012/11/30/theres-no-such-thing-as-a-devops-team-some-thoughts/</guid>
      <description>A few weeks ago Jez Humble wrote a blog post titled &#34;There&#39;s no such thing as a &#39;DevOps team&#39;&#34; where he explains what DevOps is actually supposed to be about and describes a model of how developers and operations folk can work together.
Jez&#39;s suggestion is for developers to take responsibility for the systems they create but he notes that:
[...] they need support from operations to understand how to build reliable software that can be continuous deployed to an unreliable platform that scales horizontally.</description>
    </item>
    
    <item>
      <title>Web Operations: Feature flags to turn off failing parts of infrastructure</title>
      <link>https://mneedham.github.io/blog/2012/11/13/web-operations-feature-flags-to-turn-off-failing-parts-of-infrastructure/</link>
      <pubDate>Tue, 13 Nov 2012 12:19:30 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2012/11/13/web-operations-feature-flags-to-turn-off-failing-parts-of-infrastructure/</guid>
      <description>On most of the projects I&amp;rsquo;ve worked on over the last couple of years we&amp;rsquo;ve made use of feature toggles that we used to turn pending features on and off while they were still being built but while reading Web Operations I came across another usage.
In the chapter titled &amp;lsquo;Dev and Ops Collaboration and Cooperation&amp;rsquo; Paul Hammond suggests the following:
 Eventually some of your infrastructure will fail in an unexpected way.</description>
    </item>
    
    <item>
      <title>Configuration in DNS</title>
      <link>https://mneedham.github.io/blog/2012/10/24/configuration-in-dns/</link>
      <pubDate>Wed, 24 Oct 2012 17:40:49 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2012/10/24/configuration-in-dns/</guid>
      <description>In the latest version of the ThoughtWorks Technology Radar one of the areas covered is &amp;lsquo;configuration in DNS&amp;rsquo;, a term which I first came across earlier in the year from a mailing list post by my former colleague Daniel Worthington-Bodart.
The radar describes it like so:
 Application deployments often suffer from an excess of environment-specific configuration settings, including the hostnames of dependent services. Configuration in DNS is a valuable technique to reduce this complexity by using standard hostnames like ‘mail’ or ‘db’ and have DNS resolve to the correct host for that environment.</description>
    </item>
    
    <item>
      <title>Environment agnostic machines and applications</title>
      <link>https://mneedham.github.io/blog/2012/10/14/environment-agnostic-machines-and-applications/</link>
      <pubDate>Sun, 14 Oct 2012 18:49:02 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2012/10/14/environment-agnostic-machines-and-applications/</guid>
      <description>On my current project we&amp;rsquo;ve been setting up production and staging environments and Shodhan came up with the idea of making staging and production identical to the point that a machine wouldn&amp;rsquo;t even know what environment it was in.
Identical in this sense means:
 Puppet doesn&#39;t know which environment the machine is in. Our factor variables suggest the environment is production. We set the RACK_ENV variable to production so applications don&#39;t know what environment they&#39;re in.</description>
    </item>
    
    <item>
      <title>Database configuration: Just like any other change</title>
      <link>https://mneedham.github.io/blog/2010/08/18/database-configuration-just-like-any-other-change/</link>
      <pubDate>Wed, 18 Aug 2010 10:07:42 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2010/08/18/database-configuration-just-like-any-other-change/</guid>
      <description>I&amp;rsquo;ve been flicking through Continuous Deployment and one section early on about changing configuration information in our applications particularly caught my eye:
 In our experience, it is an enduring myth that configuration information is somehow less risky to change than source code. Our bet is that, given access to both, we can stop your system at least as easily by changing the configuration as by changing the source code.  In many organisations where I&amp;rsquo;ve worked this is generally adhered to except when it comes to configuration which is controlled from the database!</description>
    </item>
    
    <item>
      <title>Can we always release to production incrementally?</title>
      <link>https://mneedham.github.io/blog/2010/08/16/can-we-always-release-to-production-incrementally/</link>
      <pubDate>Mon, 16 Aug 2010 04:22:40 +0000</pubDate>
      
      <guid>https://mneedham.github.io/blog/2010/08/16/can-we-always-release-to-production-incrementally/</guid>
      <description>Jez recently linked to a post written by Timothy Fitz about a year ago where he talks about the way his team use continuous delivery which means that every change made to the code base goes into production immediately as long as it passes their test suite.
I&amp;rsquo;ve become fairly convinced recently that it should always be possible to deploy to production frequently but we recently came across a situation where it seemed like doing that wouldn&amp;rsquo;t make much sense.</description>
    </item>
    
  </channel>
</rss>