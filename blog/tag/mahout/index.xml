<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mahout on Mark Needham</title>
    <link>http://localhost:8000/blog/tag/mahout/</link>
    <description>Recent content in mahout on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Jul 2016 13:55:14 +0000</lastBuildDate><atom:link href="http://localhost:8000/blog/tag/mahout/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mahout/Hadoop: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>http://localhost:8000/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Fri, 22 Jul 2016 13:55:14 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2016/07/22/mahouthadoop-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>I’ve been working my way through Dragan Milcevski’s mini tutorial on using Mahout to do content based filtering on documents and reached the final step where I needed to read in the generated item-similarity files.
I got the example compiling by using the following Maven dependency:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mahout-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Unfortunately when I ran the code I ran into a version incompatibility problem:
Exception in thread &amp;#34;main&amp;#34; org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4 at org.</description>
    </item>
    
    <item>
      <title>Mahout: Exception in thread &#34;main&#34; java.lang.IllegalArgumentException: Wrong FS: file:/... expected: hdfs://</title>
      <link>http://localhost:8000/blog/2016/07/21/mahout-exception-in-thread-main-java-lang-illegalargumentexception-wrong-fs-file-expected-hdfs/</link>
      <pubDate>Thu, 21 Jul 2016 17:57:41 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2016/07/21/mahout-exception-in-thread-main-java-lang-illegalargumentexception-wrong-fs-file-expected-hdfs/</guid>
      <description>I’ve been playing around with Mahout over the last couple of days to see how well it works for content based filtering.
I started following a mini tutorial from Stack Overflow but ran into trouble on the first step:
bin/mahout seqdirectory \ --input file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo \ --output file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo-out \ -c UTF-8 \ -chunk 64 \ -prefix mah 16/07/21 21:19:20 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[file:///Users/markneedham/Downloads/apache-mahout-distribution-0.12.2/foo], --keyPrefix=[mah], --method=[mapreduce], --output=[file:///Users/markneedham/Downloads/apache-mahout-distribution-0.</description>
    </item>
    
    <item>
      <title>Mahout: Parallelising the creation of DecisionTrees</title>
      <link>http://localhost:8000/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</link>
      <pubDate>Thu, 27 Dec 2012 00:08:01 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</guid>
      <description>A couple of months ago I wrote a blog post describing our use of Mahout random forests for the Kaggle Digit Recogniser Problem and after seeing how long it took to create forests with 500+ trees I wanted to see if this could be sped up by parallelising the process.
From looking at the https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java it seemed like it should be possible to create lots of small forests and then combine them together.</description>
    </item>
    
    <item>
      <title>Clojure: Mahout&#39;s &#39;entropy&#39; function</title>
      <link>http://localhost:8000/blog/2012/10/30/clojure-mahouts-entropy-function/</link>
      <pubDate>Tue, 30 Oct 2012 22:46:34 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2012/10/30/clojure-mahouts-entropy-function/</guid>
      <description>As I mentioned in a couple of previous posts Jen and I have been playing around with Mahout random forests and for a few hours last week we spent some time looking through the code to see how it worked.
In particular we came across an entropy function which is used to determine how good a particular &amp;#39;split&amp;#39; point in a decision tree is going to be.
I quite like the following definition:</description>
    </item>
    
    <item>
      <title>Mahout: Using a saved Random Forest/DecisionTree</title>
      <link>http://localhost:8000/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</link>
      <pubDate>Sat, 27 Oct 2012 22:03:30 +0000</pubDate>
      
      <guid>http://localhost:8000/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</guid>
      <description>One of the things that I wanted to do while playing around with random forests using Mahout was to save the random forest and then use use it again which is something Mahout does cater for.
It was actually much easier to do this than I’d expected and assuming that we already have a https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java built we’d just need the following code to save it to disc:
int numberOfTrees = 1; Data data = loadData(.</description>
    </item>
    
  </channel>
</rss>
