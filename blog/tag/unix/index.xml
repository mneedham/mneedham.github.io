<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>unix on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/unix/</link>
    <description>Recent content in unix on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Aug 2020 00:21:00 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/unix/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Unix: Get file name without extension from file path</title>
      <link>https://markhneedham.com/blog/2020/08/24/unix-get-file-name-without-extension-from-file-path/</link>
      <pubDate>Mon, 24 Aug 2020 00:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2020/08/24/unix-get-file-name-without-extension-from-file-path/</guid>
      <description>I recently found myself needing to extract the file name but not file extension from a bunch of file paths and wanted to share a neat technique that I learnt to do it.
 I started with a bunch of Jupyter notebook files, which I listed usign the following command;
 $ find notebooks/ -maxdepth 1 -iname *ipynb notebooks/09_Predictions_sagemaker.ipynb notebooks/00_Environment.ipynb notebooks/05_Train_Evaluate_Model.ipynb notebooks/01_DataLoading.ipynb notebooks/05_SageMaker.ipynb notebooks/09_Predictions_sagemaker-Copy2.ipynb notebooks/09_Predictions_sagemaker-Copy1.ipynb notebooks/02_Co-Author_Graph.ipynb notebooks/04_Model_Feature_Engineering.ipynb notebooks/09_Predictions_scikit.ipynb notebooks/03_Train_Test_Split.ipynb   If we pick one of those files:</description>
    </item>
    
    <item>
      <title>Unix: Find files greater than date</title>
      <link>https://markhneedham.com/blog/2016/06/24/unix-find-files-greater-than-date/</link>
      <pubDate>Fri, 24 Jun 2016 16:56:17 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/24/unix-find-files-greater-than-date/</guid>
      <description>For the latter part of the week I&amp;#8217;ve been running some tests against Neo4j which generate a bunch of log files and I wanted to filter those files based on the time they were created to do some further analysis.
 This is an example of what the directory listing looks like:
 $ ls -alh foo/database-agent-* -rw-r--r-- 1 markneedham wheel 2.5K 23 Jun 14:00 foo/database-agent-mac17f73-1-logs-archive-201606231300176.tar.gz -rw-r--r-- 1 markneedham wheel 8.</description>
    </item>
    
    <item>
      <title>Unix: Find all text below string in a file</title>
      <link>https://markhneedham.com/blog/2016/06/19/unix-find-all-text-below-string-in-a-file/</link>
      <pubDate>Sun, 19 Jun 2016 08:36:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/19/unix-find-all-text-below-string-in-a-file/</guid>
      <description>I recently wanted to parse some text out of a bunch of files so that I could do some sentiment analysis on it. Luckily the text I want is at the end of the file and doesn&amp;#8217;t have anything after it but there is text before it that I want to get rid.
 The files look like this:
 # text I don&#39;t care about = Heading of the bit I care about # text I care about   In other words I want to find the line that contains the Heading and then get all the text after that point.</description>
    </item>
    
    <item>
      <title>Unix: Split string using separator</title>
      <link>https://markhneedham.com/blog/2016/06/19/unix-split-string-using-separator/</link>
      <pubDate>Sun, 19 Jun 2016 07:22:57 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/19/unix-split-string-using-separator/</guid>
      <description>I recently found myself needing to iterate over a bunch of &#39;/&#39; separated strings on the command line and extract just the text after the last &#39;/&#39;.
 e.g. an example of one of the strings
 A/B/C   I wanted to write some code that could split on &#39;/&#39; and then pick the 3rd item in the resulting collection.
 One way of doing this is to echo the string and then pipe it through cut:</description>
    </item>
    
    <item>
      <title>Unix: Stripping first n bytes in a file / Byte Order Mark (BOM)</title>
      <link>https://markhneedham.com/blog/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:28 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</guid>
      <description>I&amp;#8217;ve previously written a couple of blog posts showing how to strip out the byte order mark (BOM) from CSV files to make loading them into Neo4j easier and today I came across another way to clean up the file using tail.
 The BOM is 3 bytes long at the beginning of the file so if we know that a file contains it then we can strip out those first 3 bytes tail like this:</description>
    </item>
    
    <item>
      <title>Unix: Redirecting stderr to stdout</title>
      <link>https://markhneedham.com/blog/2015/08/15/unix-redirecting-stderr-to-stdout/</link>
      <pubDate>Sat, 15 Aug 2015 15:55:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/15/unix-redirecting-stderr-to-stdout/</guid>
      <description>I&amp;#8217;ve been trying to optimise some Neo4j import queries over the last couple of days and as part of the script I&amp;#8217;ve been executed I wanted to redirect the output of a couple of commands into a file to parse afterwards.
 I started with the following script which doesn&amp;#8217;t do any explicit redirection of the output:
 #!/bin/sh ./neo4j-community-2.2.3/bin/neo4j start   Now let&amp;#8217;s run that script and redirect the output to a file:</description>
    </item>
    
    <item>
      <title>Mac OS X: GNU sed -  Hex string replacement / replacing new line characters</title>
      <link>https://markhneedham.com/blog/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</link>
      <pubDate>Thu, 11 Jun 2015 21:38:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</guid>
      <description>Recently I was working with a CSV file which contained both Windows and Unix line endings which was making it difficult to work with.
 The actual line endings were HEX &#39;0A0D&#39; i.e. Windows line breaks but there were also HEX &#39;OA&#39; i.e. Unix line breaks within one of the columns.
 I wanted to get rid of the Unix line breaks and discovered that you can do HEX sequence replacement using the GNU version of sed - unfortunately the Mac ships with the BSD version which doesn&amp;#8217;t have this functionaltiy.</description>
    </item>
    
    <item>
      <title>Unix: Converting a file of values into a comma separated list</title>
      <link>https://markhneedham.com/blog/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</link>
      <pubDate>Mon, 08 Jun 2015 22:23:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</guid>
      <description>I recently had a bunch of values in a file that I wanted to paste into a Java program which required a comma separated list of strings.
 This is what the file looked like:
 $ cat foo2.txt | head -n 5 1.0 1.0 1.0 1.0 1.0   And the idea is that we would end up with something like this:
 &#34;1.0&#34;,&#34;1.0&#34;,&#34;1.0&#34;,&#34;1.0&#34;,&#34;1.0&#34;   The first thing we need to do is quote each of the values.</description>
    </item>
    
    <item>
      <title>cURL: POST/Upload multi part form</title>
      <link>https://markhneedham.com/blog/2013/09/23/curl-postupload-multi-part-form/</link>
      <pubDate>Mon, 23 Sep 2013 22:16:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/09/23/curl-postupload-multi-part-form/</guid>
      <description>I&amp;#8217;ve been doing some work which involved uploading a couple of files from a HTML form and I wanted to check that the server side code was working by executing a cURL command rather than using the browser.
 The form looks like this:
 &amp;lt;form action=&#34;http://foobar.com&#34; method=&#34;POST&#34; enctype=&#34;multipart/form-data&#34;&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&#34;nodes&#34;&amp;gt;File 1:&amp;lt;/label&amp;gt; &amp;lt;input type=&#34;file&#34; name=&#34;file1&#34; id=&#34;file1&#34;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&#34;relationships&#34;&amp;gt;File 2:&amp;lt;/label&amp;gt; &amp;lt;input type=&#34;file&#34; name=&#34;file2&#34; id=&#34;file2&#34;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;input type=&#34;submit&#34; name=&#34;submit&#34; value=&#34;</description>
    </item>
    
    <item>
      <title>Unix: tar - Extracting, creating and viewing archives</title>
      <link>https://markhneedham.com/blog/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</link>
      <pubDate>Thu, 22 Aug 2013 22:56:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</guid>
      <description>I&amp;#8217;ve been playing around with the Unix tar command a bit this week and realised that I&amp;#8217;d memorised some of the flag combinations but didn&amp;#8217;t actually know what each of them meant.
 For example, one of the most common things that I want to do is extract a gripped neo4j archive:
 $ wget http://dist.neo4j.org/neo4j-community-1.9.2-unix.tar.gz $ tar -xvf neo4j-community-1.9.2-unix.tar.gz   where:
   -x means extract
  -v means produce verbose output i.</description>
    </item>
    
    <item>
      <title>Unix: find, xargs, zipinfo and the &#39;caution: filename not matched:&#39; error</title>
      <link>https://markhneedham.com/blog/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</link>
      <pubDate>Sun, 09 Jun 2013 23:10:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</guid>
      <description>As I mentioned in my previous post last week I needed to scan all the jar files included with the neo4j-enterprise gem and I started out by finding out where it&amp;#8217;s located on my machine:
 $ bundle show neo4j-enterprise /Users/markhneedham/.rbenv/versions/jruby-1.7.1/lib/ruby/gems/shared/gems/neo4j-enterprise-1.8.2-java   I then thought I could get a list of all the jar files using http://unixhelp.ed.ac.uk/CGI/man-cgi?find and pipe it into http://linux.about.com/library/cmd/blcmdl1_zipinfo.htm via xargs to get all the file names and then search for HighlyAvailableGraphDatabaseFactory:</description>
    </item>
    
    <item>
      <title>Unix: Working with parts of large files</title>
      <link>https://markhneedham.com/blog/2013/05/19/unix-working-with-parts-of-large-files/</link>
      <pubDate>Sun, 19 May 2013 21:44:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/05/19/unix-working-with-parts-of-large-files/</guid>
      <description>Chris and I were looking at the neo4j log files of a client earlier in the week and wanted to do some processing of the file so we could ask the client to send us some further information.
 The log file was over 10,000 lines long but the bit of the file we were interesting in was only a few hundred lines.
 I usually use Vim and the &#39;:set number&#39; when I want to refer to line numbers in a file but Chris showed me that we can achieve the same thing with e.</description>
    </item>
    
    <item>
      <title>Unix: Checking for open sockets on nginx</title>
      <link>https://markhneedham.com/blog/2013/04/23/unix-checking-for-open-sockets-on-nginx/</link>
      <pubDate>Tue, 23 Apr 2013 23:59:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/04/23/unix-checking-for-open-sockets-on-nginx/</guid>
      <description>Tim and I were investigating a weird problem we were having with nginx where it was getting in a state where it had exceeded the number of open files allowed on the system and started rejecting requests.
 We can find out the maximum number of open files that we&amp;#8217;re allowed on a system with the following command:
 $ ulimit -n 1024   Our hypothesis was that some socket connections were never being closed and therefore the number of open files was climbing slowly upwards until it exceeded the limit.</description>
    </item>
    
    <item>
      <title>awk: Parsing &#39;free -m&#39; output to get memory usage/consumption</title>
      <link>https://markhneedham.com/blog/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</link>
      <pubDate>Wed, 10 Apr 2013 07:03:15 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</guid>
      <description>Although I know this problem is already solved by collectd and New Relic I wanted to write a little shell script that showed me the memory usage on a bunch of VMs by parsing the output of http://linux.about.com/library/cmd/blcmdl1_free.htm.
 The output I was playing with looks like this:
 $ free -m total used free shared buffers cached Mem: 365 360 5 0 59 97 -/+ buffers/cache: 203 161 Swap: 767 13 754   I wanted to find out what % of the memory on the machine was being used and as I understand it the numbers that we would use to calculate this are the &#39;total&#39; value on the &#39;Mem&#39; line and the &#39;used&#39; value on the &#39;buffers/cache&#39; line.</description>
    </item>
    
    <item>
      <title>Unix: Counting the number of commas on a line</title>
      <link>https://markhneedham.com/blog/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</link>
      <pubDate>Sat, 10 Nov 2012 16:30:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</guid>
      <description>A few weeks ago I was playing around with some data stored in a CSV file and wanted to do a simple check on the quality of the data by making sure that each line had the same number of fields.
 One way this can be done is with awk:
 awk -F &#34;,&#34; &#39; { print NF-1 } &#39; file.csv   Here we&amp;#8217;re specifying the file separator -F as &#39;,&#39; and then using the NF (number of fields) variable to print how many commas there are on the line.</description>
    </item>
    
    <item>
      <title>Unix: tee</title>
      <link>https://markhneedham.com/blog/2012/07/29/unix-tee/</link>
      <pubDate>Sun, 29 Jul 2012 19:11:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/07/29/unix-tee/</guid>
      <description>I&amp;#8217;ve read about the Unix &#39;http://en.wikipedia.org/wiki/Tee_(command)[tee]&#39; command before but never found a reason to use it until the last few weeks.
 One of the things I repeatedly do by mistake is open /etc/hosts without sudo and then try to make changes to it:
 $ vi /etc/hosts # Editing it leads to the dreaded &#39;W10: Changing a readonly file&#39;   I always used to close the file and then re-open it with sudo but I recently came across an approach which allows us to use &#39;tee&#39; to get around the problem.</description>
    </item>
    
    <item>
      <title>Learning Unix find: Searching in/Excluding certain folders</title>
      <link>https://markhneedham.com/blog/2011/10/21/learning-unix-find-searching-inexcluding-certain-folders/</link>
      <pubDate>Fri, 21 Oct 2011 21:25:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/21/learning-unix-find-searching-inexcluding-certain-folders/</guid>
      <description>I love playing around with commands on the Unix shell but one of the ones that I&amp;#8217;ve found the most difficult to learn beyond the very basics is http://unixhelp.ed.ac.uk/CGI/man-cgi?find.
 I think this is partially because I find the find man page quite difficult to read and partially because it&amp;#8217;s usually quicker to work out how to solve my problem with a command I already know than to learn another one.</description>
    </item>
    
    <item>
      <title>Bash: Reusing previous commands</title>
      <link>https://markhneedham.com/blog/2011/10/13/bash-reusing-previous-commands/</link>
      <pubDate>Thu, 13 Oct 2011 19:46:20 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/13/bash-reusing-previous-commands/</guid>
      <description>A lot of the time when I&amp;#8217;m using the bash shell I want to re-use commands that I&amp;#8217;ve previously entered and I&amp;#8217;ve recently learnt some neat ways to do this from my colleagues Tom and Kief.
 If we want to list the history of all the commands we&amp;#8217;ve entered in a shell session then the following command does the trick:
 &amp;gt; history ... 761 sudo port search pdfinfo 762 to_ipad andersen-phd-thesis.</description>
    </item>
    
    <item>
      <title>Unix: Getting the page count of a linearized PDF</title>
      <link>https://markhneedham.com/blog/2011/10/09/unix-getting-the-page-count-of-a-linearized-pdf/</link>
      <pubDate>Sun, 09 Oct 2011 11:34:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/09/unix-getting-the-page-count-of-a-linearized-pdf/</guid>
      <description>We were doing some work last week to rasterize a PDF document into a sequence of images and wanted to get a rough idea of how many pages we&amp;#8217;d be dealing with if we created an image per page.
 The PDFs we&amp;#8217;re dealing with are linearized since they&amp;#8217;re available for viewing on the web:
  A LINEARIZED PDF FILE is one that has been organized in a special way to enable efÔ¨Åcient incremental access in a network environment.</description>
    </item>
    
    <item>
      <title>Unix: Summing the total time from a log file</title>
      <link>https://markhneedham.com/blog/2011/07/27/unix-summing-the-total-time-from-a-log-file/</link>
      <pubDate>Wed, 27 Jul 2011 23:02:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/07/27/unix-summing-the-total-time-from-a-log-file/</guid>
      <description>As I mentioned in my last post we&amp;#8217;ve been doing some profiling of a data ingestion job and as a result have been putting some logging into our code to try and work out where we need to work on.
 We end up with a log file peppered with different statements which looks a bit like the following:
 18:50:08.086 [akka:event-driven:dispatcher:global-5] DEBUG - Imported document. /Users/mneedham/foo.xml in: 1298 18:50:09.064 [akka:event-driven:dispatcher:global-1] DEBUG - Imported document.</description>
    </item>
    
  </channel>
</rss>