<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/aws/</link>
    <description>Recent content in aws on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Searching through AWS Icons</title>
      <link>https://www.markhneedham.com/blog/2024/08/23/searching-aws-icons/</link>
      <pubDate>Fri, 23 Aug 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/08/23/searching-aws-icons/</guid>
      <description>I recently needed to search for an icon in the AWS [asset package](https://aws.amazon.com/architecture/icons/) and wanted to share a little script that I wrote. You wouldn’t think that searching for icons should be that hard, but they’re spread across so many folders and sub-folders that you can spend forever trying to find what you want.
First, let’s import some modules:
import base64 import sys import glob import os And then I’m using the following function to render images in the terminal:</description>
    </item>
    
    <item>
      <title>AWS: Spinning up a Neo4j instance with APOC installed</title>
      <link>https://www.markhneedham.com/blog/2017/09/30/aws-spinning-up-a-neo4j-instance-with-apoc-installed/</link>
      <pubDate>Sat, 30 Sep 2017 21:23:11 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/09/30/aws-spinning-up-a-neo4j-instance-with-apoc-installed/</guid>
      <description>One of the first things I do after installing Neo4j is install the APOC library, but I find it’s a bit of a manual process when spinning up a server on AWS so I wanted to simplify it a bit.
There’s already a Neo4j AMI which installs Neo4j 3.2.0 and my colleague Michael pointed out that we could download APOC into the correct folder by writing a script and sending it as UserData.</description>
    </item>
    
    <item>
      <title>Serverless: Building a mini producer/consumer data pipeline with AWS SNS</title>
      <link>https://www.markhneedham.com/blog/2017/09/30/serverless-building-mini-producerconsumer-data-pipeline-aws-sns/</link>
      <pubDate>Sat, 30 Sep 2017 07:51:29 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/09/30/serverless-building-mini-producerconsumer-data-pipeline-aws-sns/</guid>
      <description>I wanted to create a little data pipeline with Serverless whose main use would be to run once a day, call an API, and load that data into a database.
It’s mostly used to pull in recent data from that API, but I also wanted to be able to invoke it manually and specify a date range.
I created the following pair of lambdas that communicate with each other via an SNS topic.</description>
    </item>
    
    <item>
      <title>Serverless: S3 - S3BucketPermissions - Action does not apply to any resource(s) in statement</title>
      <link>https://www.markhneedham.com/blog/2017/09/29/serverless-s3-s3bucketpermissions-action-does-not-apply-to-any-resources-in-statement/</link>
      <pubDate>Fri, 29 Sep 2017 06:09:58 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/09/29/serverless-s3-s3bucketpermissions-action-does-not-apply-to-any-resources-in-statement/</guid>
      <description>I’ve been playing around with S3 buckets with Serverless, and recently wrote the following code to create an S3 bucket and put a file into that bucket:
const AWS = require(&amp;#34;aws-sdk&amp;#34;); let regionParams = { &amp;#39;region&amp;#39;: &amp;#39;us-east-1&amp;#39; } let s3 = new AWS.S3(regionParams); let s3BucketName = &amp;#34;marks-blog-bucket&amp;#34;; console.log(&amp;#34;Creating bucket: &amp;#34; + s3BucketName); let bucketParams = { Bucket: s3BucketName, ACL: &amp;#34;public-read&amp;#34; }; s3.createBucket(bucketParams).promise() .then(console.log) .catch(console.error); var putObjectParams = { Body: &amp;#34;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello blog!</description>
    </item>
    
    <item>
      <title>AWS Lambda: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory&#39;</title>
      <link>https://www.markhneedham.com/blog/2017/08/03/aws-lambda-libld-linux-2-bad-elf-interpreter-no-file-directory/</link>
      <pubDate>Thu, 03 Aug 2017 17:24:16 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/08/03/aws-lambda-libld-linux-2-bad-elf-interpreter-no-file-directory/</guid>
      <description>I’ve been working on an AWS lambda job to convert a HTML page to PDF using a Python wrapper around the wkhtmltopdf library but ended up with the following error when I tried to execute it:
b&amp;#39;/bin/sh: ./binary/wkhtmltopdf: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory\n&amp;#39;: Exception Traceback (most recent call last): File &amp;#34;/var/task/handler.py&amp;#34;, line 33, in generate_certificate wkhtmltopdf(local_html_file_name, local_pdf_file_name) File &amp;#34;/var/task/lib/wkhtmltopdf.py&amp;#34;, line 64, in wkhtmltopdf wkhp.render() File &amp;#34;/var/task/lib/wkhtmltopdf.</description>
    </item>
    
    <item>
      <title>AWS Lambda: Programmatically scheduling a CloudWatchEvent</title>
      <link>https://www.markhneedham.com/blog/2017/04/05/aws-lambda-programatically-scheduling-a-cloudwatchevent/</link>
      <pubDate>Wed, 05 Apr 2017 23:49:45 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/04/05/aws-lambda-programatically-scheduling-a-cloudwatchevent/</guid>
      <description>I recently wrote a blog post showing how to create a Python &amp;#39;Hello World&amp;#39; AWS lambda function and manually invoke it, but what I really wanted to do was have it run automatically every hour.
To achieve that in AWS Lambda land we need to create a CloudWatch Event. The documentation describes them as follows:
Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</description>
    </item>
    
    <item>
      <title>AWS Lambda: Programatically create a Python &#39;Hello World&#39; function</title>
      <link>https://www.markhneedham.com/blog/2017/04/02/aws-lambda-programatically-create-a-python-hello-world-function/</link>
      <pubDate>Sun, 02 Apr 2017 22:11:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/04/02/aws-lambda-programatically-create-a-python-hello-world-function/</guid>
      <description>I’ve been playing around with AWS Lambda over the last couple of weeks and I wanted to automate the creation of these functions and all their surrounding config.
Let’s say we have the following Hello World function: ~python def lambda_handler(event, context): print(&amp;#34;Hello world&amp;#34;) ~
To upload it to AWS we need to put it inside a zip file so let’s do that: ~bash $ zip HelloWorld.zip HelloWorld.py ~ ~bash $ unzip -l HelloWorld.</description>
    </item>
    
    <item>
      <title>s3cmd: put fails with &#34;`Connection reset by peer`&#34; for large files</title>
      <link>https://www.markhneedham.com/blog/2013/07/30/s3cmd-put-fails-with-connection-reset-by-peer-for-large-files/</link>
      <pubDate>Tue, 30 Jul 2013 16:20:16 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/07/30/s3cmd-put-fails-with-connection-reset-by-peer-for-large-files/</guid>
      <description>I recently wanted to copy some large files from an AWS instance into an S3 bucket using s3cmd but ended up with the following error when trying to use the &amp;#39;put&amp;#39; command:
$ s3cmd put /mnt/ebs/myfile.tar s3://mybucket.somewhere.com /mnt/ebs/myfile.tar -&amp;gt; s3://mybucket.somewhere.com/myfile.tar [1 of 1] 1077248 of 12185313280 0% in 1s 937.09 kB/s failed WARNING: Upload failed: /myfile.tar ([Errno 104] Connection reset by peer) WARNING: Retrying on lower speed (throttle=0.00) WARNING: Waiting 3 sec.</description>
    </item>
    
    <item>
      <title>Wiring up an Amazon S3 bucket to a CNAME entry - The specified bucket does not exist</title>
      <link>https://www.markhneedham.com/blog/2013/03/21/wiring-up-an-amazon-s3-bucket-to-a-cname-entry-the-specified-bucket-does-not-exist/</link>
      <pubDate>Thu, 21 Mar 2013 22:39:02 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/03/21/wiring-up-an-amazon-s3-bucket-to-a-cname-entry-the-specified-bucket-does-not-exist/</guid>
      <description>Jason and I were setting up an internal static website using an S3 bucket a couple of days ago and wanted to point a more friendly domain name at it.
We initially called our bucket &amp;#39;static-site&amp;#39; and then created a CNAME entry using zerigo to point our sub domain at the bucket.
The mapping was something like this:
our-subdomain.somedomain.com -&amp;gt; static-site.s3-website-eu-west-1.amazonaws.com When we tried to access the site through our-subdomain.</description>
    </item>
    
  </channel>
</rss>
