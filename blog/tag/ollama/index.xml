<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ollama on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/ollama/</link>
    <description>Recent content in ollama on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Jan 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/ollama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ollama is on PyPi</title>
      <link>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</link>
      <pubDate>Sun, 28 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</guid>
      <description>This week Ollama released a Python/PyPi library to go with their awesome tool for running LLMs on your own machine. You still need to download and run Ollama, but after that you can do almost everything from the library. In this blog post, we’re going to take it for a spin.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Ollama: Running GGUF Models from Hugging Face</title>
      <link>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</link>
      <pubDate>Wed, 18 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</guid>
      <description>GGUF (GPT-Generated Unified Format) has emerged as the de facto standard file format for storing large language models for inference. We are starting to see a lot of models in this format on Hugging Face, many of them uploaded by The Bloke.
One cool thing about GGUF models is that it’s super easy to get them running on your own machine using Ollama. In this blog post, we’re going to look at how to download a GGUF model from Hugging Face and run it locally.</description>
    </item>
    
    <item>
      <title>Ollama: Experiments with few-shot prompting on Llama2 7B</title>
      <link>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</link>
      <pubDate>Wed, 11 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</guid>
      <description>A problem that I’m currently trying to solve is how to work out whether a given sentence is a question. If there’s a question mark on the end we can assume it is a question, but what about if the question mark’s been left off?
Few-shot prompting is a technique where we provide some examples in our prompt to try to guide the LLM to do what we want. And, this seemed like a good opportunity to try it out on Meta’s Llama2 7B Large Language Model using Ollama.</description>
    </item>
    
    <item>
      <title>Running Mistral AI on my machine with Ollama</title>
      <link>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</link>
      <pubDate>Tue, 03 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</guid>
      <description>Last week Mistral AI announced the release of their first Large Language Model (LLM), trained with 7 billion parameters, and better than Meta’s Llama 2 model with 13 billion parameters. For those keeping track, Mistral AI was founded in the summer of 2023 and raised $113m in their seed round.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
