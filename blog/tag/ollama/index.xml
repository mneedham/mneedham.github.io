<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ollama on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/ollama/</link>
    <description>Recent content in ollama on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/ollama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ollama: Multiple prompts on vision models</title>
      <link>https://www.markhneedham.com/blog/2024/10/06/ollama-multi-prompts-vision-models/</link>
      <pubDate>Wed, 02 Oct 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/10/06/ollama-multi-prompts-vision-models/</guid>
      <description>In this blog post, we’re going to learn how to send multiple prompts to vision models when using Ollama. This isn’t super well documented, but it is possible!
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
Let’s import Ollam:
import ollama We’re going to call the ollama.</description>
    </item>
    
    <item>
      <title>Side by side LLMs with Ollama and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</link>
      <pubDate>Sat, 11 May 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</guid>
      <description>The recent 0.1.33 release of Ollama added experimental support for running multiple LLMs or the same LLM in parallel. But, to compare models on the same prompt we need a UI and that’s what we’re going to build in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Clustering YouTube comments using Ollama Embeddings</title>
      <link>https://www.markhneedham.com/blog/2024/02/27/clustering-youtube-comments-ollama-embeddings-nomic/</link>
      <pubDate>Tue, 27 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/27/clustering-youtube-comments-ollama-embeddings-nomic/</guid>
      <description>One of my favourite tools in the LLM space is Ollama and if you want to learn how to use it, there’s no better place than Matt Williams&amp;#39; YouTube channel. His videos get a lot of comments and they tend to contain a treasure trove of the things that people are thinking about and the questions that they have. Matt recently did a video about embeddings in Ollama and I thought it’d be fun to try to get a high-level overview of what’s happening in the comments section.</description>
    </item>
    
    <item>
      <title>LLaVA 1.5 vs. 1.6</title>
      <link>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</link>
      <pubDate>Sun, 04 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</guid>
      <description>LLaVA (or Large Language and Vision Assistant), an open-source large multi-modal model, just released version 1.6. It claims to have improvements over version 1.5, which was released a few months ago:
Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.
Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.</description>
    </item>
    
    <item>
      <title>Ollama is on PyPi</title>
      <link>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</link>
      <pubDate>Sun, 28 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</guid>
      <description>This week Ollama released a Python/PyPi library to go with their awesome tool for running LLMs on your own machine. You still need to download and run Ollama, but after that you can do almost everything from the library. In this blog post, we’re going to take it for a spin.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Ollama: Running GGUF Models from Hugging Face</title>
      <link>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</link>
      <pubDate>Wed, 18 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</guid>
      <description>GGUF (GPT-Generated Unified Format) has emerged as the de facto standard file format for storing large language models for inference. We are starting to see a lot of models in this format on Hugging Face, many of them uploaded by The Bloke.
One cool thing about GGUF models is that it’s super easy to get them running on your own machine using Ollama. In this blog post, we’re going to look at how to download a GGUF model from Hugging Face and run it locally.</description>
    </item>
    
    <item>
      <title>Ollama: Experiments with few-shot prompting on Llama2 7B</title>
      <link>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</link>
      <pubDate>Wed, 11 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</guid>
      <description>A problem that I’m currently trying to solve is how to work out whether a given sentence is a question. If there’s a question mark on the end we can assume it is a question, but what about if the question mark’s been left off?
Few-shot prompting is a technique where we provide some examples in our prompt to try to guide the LLM to do what we want. And, this seemed like a good opportunity to try it out on Meta’s Llama2 7B Large Language Model using Ollama.</description>
    </item>
    
    <item>
      <title>Running Mistral AI on my machine with Ollama</title>
      <link>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</link>
      <pubDate>Tue, 03 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</guid>
      <description>Last week Mistral AI announced the release of their first Large Language Model (LLM), trained with 7 billion parameters, and better than Meta’s Llama 2 model with 13 billion parameters. For those keeping track, Mistral AI was founded in the summer of 2023 and raised $113m in their seed round.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
