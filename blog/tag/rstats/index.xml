<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/rstats/</link>
    <description>Recent content in rstats on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Oct 2015 18:42:47 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/rstats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>R: data.table - Finding the maximum row</title>
      <link>https://markhneedham.com/blog/2015/10/02/r-data-table-finding-the-maximum-row/</link>
      <pubDate>Fri, 02 Oct 2015 18:42:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/10/02/r-data-table-finding-the-maximum-row/</guid>
      <description>In my continued playing around with the R data.table package I wanted to find the maximum row based on one of the columns, grouped by another column, and then return back the whole row.
 We&amp;#8217;ll use the following data table to illustrate:
 &amp;gt; blogDT = data.table(name = c(&#34;Property 1&#34;,&#34;Property 1&#34;,&#34;Property 1&#34;,&#34;Property 2&#34;,&#34;Property 2&#34;,&#34;Property 2&#34;), price = c(10000, 12500, 18000, 245000, 512000, 1000000), date = c(&#34;Day 1&#34;, &#34;Day 7&#34;, &#34;</description>
    </item>
    
    <item>
      <title>R: data.table - Comparing adjacent rows</title>
      <link>https://markhneedham.com/blog/2015/09/27/r-data-table-comparing-adjacent-rows/</link>
      <pubDate>Sun, 27 Sep 2015 22:02:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/09/27/r-data-table-comparing-adjacent-rows/</guid>
      <description>As part of my exploration of the Land Registry price paid data set I wanted to compare the difference between consecutive sales of properties.
 This means we need to group the sales by a property identifier and then get the previous sale price into a column on each row unless it&amp;#8217;s the first sale in which case we&amp;#8217;ll have &#39;NA&#39;. We can do this by creating a http://stackoverflow.com/questions/26291988/r-how-to-create-a-lag-variable-for-each-by-group variable.</description>
    </item>
    
    <item>
      <title>R: Date for given week/year</title>
      <link>https://markhneedham.com/blog/2015/07/10/r-date-for-given-weekyear/</link>
      <pubDate>Fri, 10 Jul 2015 22:01:58 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/07/10/r-date-for-given-weekyear/</guid>
      <description>As I mentioned in my last couple of blog posts I&amp;#8217;ve been looking at the data behind this blog and I wanted to plot a chart showing the number of posts per week since the blog started.
 I started out with a data frame with posts and publication date:
 &amp;gt; library(dplyr) &amp;gt; df = read.csv(&#34;posts.csv&#34;) &amp;gt; df$date = ymd_hms(df$date) &amp;gt; df %&amp;gt;% sample_n(10) title date 538 Nygard Big Data Model: The Investigation Stage 2012-10-10 00:00:36 341 The read-only database 2011-08-29 23:32:26 1112 CSS in Internet Explorer - Some lessons learned 2008-10-31 15:24:51 143 Coding: Mutating parameters 2010-08-26 07:47:23 433 Scala: Counting number of inversions (via merge sort) for an unsorted collection 2012-03-20 06:53:18 618 neo4j/cypher: SQL style GROUP BY functionality 2013-02-17 21:05:27 1111 Testing Hibernate mappings: Setting up test data 2008-10-30 13:24:14 462 neo4j: What question do you want to answer?</description>
    </item>
    
    <item>
      <title>R: dplyr - Error: cannot modify grouping variable</title>
      <link>https://markhneedham.com/blog/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</link>
      <pubDate>Thu, 09 Jul 2015 05:55:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</guid>
      <description>I&amp;#8217;ve been doing some exploration of the posts made on this blog and I thought I&amp;#8217;d start with answering a simple question - on which dates did I write the most posts?
 I started with a data frame containing each post and the date it was published:
 &amp;gt; library(dplyr) &amp;gt; df %&amp;gt;% sample_n(5) title date 1148 Taiichi Ohno&#39;s Workplace Management: Book Review 2008-12-08 14:14:48 158 Rails: Faking a delete method with &#39;form_for&#39; 2010-09-20 18:52:15 331 Retrospectives: The 4 L&#39;s Retrospective 2011-07-25 21:00:30 1035 msbuild - Use OutputPath instead of OutDir 2008-08-14 18:54:03 1181 The danger of commenting out code 2009-01-17 06:02:33   To find the most popular days for blog posts we can write the following aggregation function:</description>
    </item>
    
    <item>
      <title>R: ggplot geom_density - Error in exists(name, envir = env, mode = mode) : argument &#34;env&#34; is missing, with no default</title>
      <link>https://markhneedham.com/blog/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</link>
      <pubDate>Wed, 03 Jun 2015 05:52:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</guid>
      <description>Continuing on from yesterday&amp;#8217;s blog post where I worked out how to clean up the Think Bayes Price is Right data set, the next task was to plot a distribution of the prices of show case items.
 To recap, this is what the data frame we&amp;#8217;re working with looks like:
 library(dplyr) df2011 = read.csv(&#34;~/projects/rLearning/showcases.2011.csv&#34;, na.strings = c(&#34;&#34;, &#34;NA&#34;)) df2011 = df2011 %&amp;gt;% na.omit() &amp;gt; df2011 %&amp;gt;% head() X Sep.</description>
    </item>
    
    <item>
      <title>R: Think Bayes Euro Problem</title>
      <link>https://markhneedham.com/blog/2015/05/31/r-think-bayes-euro-problem/</link>
      <pubDate>Sun, 31 May 2015 23:11:50 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/31/r-think-bayes-euro-problem/</guid>
      <description>I&amp;#8217;ve got back to working my way through Think Bayes after a month&amp;#8217;s break and started out with the one euro coin problem in Chapter 4:
  A statistical statement appeared in &#34;`The Guardian&#34; on Friday January 4, 2002: When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. &amp;#8216;It looks very suspicious to me,&amp;#8217; said Barry Blight, a statistics lecturer at the London School of Economics.</description>
    </item>
    
    <item>
      <title>R: ggplot - Displaying multiple charts with a for loop</title>
      <link>https://markhneedham.com/blog/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</link>
      <pubDate>Thu, 14 May 2015 00:17:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</guid>
      <description>Continuing with my analysis of the Neo4j London user group I wanted to drill into some individual meetups and see the makeup of the people attending those meetups with respect to the cohort they belong to.
 I started by writing a function which would take in an event ID and output a bar chart showing the number of people who attended that event from each cohort. &amp;lt;?p&amp;gt;
 We can work out the cohort that a member belongs to by querying for the first event they attended.</description>
    </item>
    
    <item>
      <title>R: Cohort heatmap of Neo4j London meetup</title>
      <link>https://markhneedham.com/blog/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</link>
      <pubDate>Mon, 11 May 2015 23:16:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</guid>
      <description>A few months ago I had a go at doing some cohort analysis of the Neo4j London meetup group which was an interesting experiment but unfortunately resulted in a chart that was completely illegible.
   I wasn&amp;#8217;t sure how to progress from there but a few days ago I came across the cohort heatmap which seemed like a better way of visualising things over time.
 The underlying idea is still the same - we&amp;#8217;ve comparing different cohorts of users against each other to see whether a change or intervention we did at a certain time had any impact.</description>
    </item>
    
    <item>
      <title>R: Neo4j London meetup group - How many events do people come to?</title>
      <link>https://markhneedham.com/blog/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</link>
      <pubDate>Sat, 09 May 2015 22:33:05 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</guid>
      <description>Earlier this week the number of members in the Neo4j London meetup group creeped over the 2,000 mark and I thought it&amp;#8217;d be fun to re-explore the data that I previously imported into Neo4j.
 How often do people come to meetups?
 library(RNeo4j) library(dplyr) graph = startGraph(&#34;http://localhost:7474/db/data/&#34;) query = &#34;MATCH (g:Group {name: &#39;Neo4j - London User Group&#39;})-[:HOSTED_EVENT]-&amp;gt;(event)&amp;lt;-[:TO]-({response: &#39;yes&#39;})&amp;lt;-[:RSVPD]-(profile)-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g) WHERE (event.time + event.utc_offset) &amp;lt; timestamp() RETURN event.id, event.time + event.utc_offset AS eventTime, profile.</description>
    </item>
    
    <item>
      <title>R: dplyr - Error in (list: invalid subscript type &#39;double&#39;</title>
      <link>https://markhneedham.com/blog/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</link>
      <pubDate>Mon, 27 Apr 2015 22:34:43 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</guid>
      <description>In my continued playing around with R I wanted to find the minimum value for a specified percentile given a data frame representing a cumulative distribution function (CDF).
 e.g. imagine we have the following CDF represented in a data frame:
 library(dplyr) df = data.frame(score = c(5,7,8,10,12,20), percentile = c(0.05,0.1,0.15,0.20,0.25,0.5))   and we want to find the minimum value for the 0.05 percentile. We can use the filter function to do so:</description>
    </item>
    
    <item>
      <title>R: Creating an object with functions to calculate conditional probability</title>
      <link>https://markhneedham.com/blog/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</link>
      <pubDate>Sun, 12 Apr 2015 07:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</guid>
      <description>I&amp;#8217;ve been working through Alan Downey&amp;#8217;s Thinking Bayes and I thought it&amp;#8217;d be an interesting exercise to translate some of the code from Python to R.
 The first example is a simple one about conditional probablity and the author creates a class &#39;PMF&#39; (Probability Mass Function) to solve the following problem:
  Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.</description>
    </item>
    
    <item>
      <title>R: Cohort analysis of Neo4j meetup members</title>
      <link>https://markhneedham.com/blog/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</link>
      <pubDate>Tue, 24 Feb 2015 01:19:26 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</guid>
      <description>A few weeks ago I came across a blog post explaining how to apply cohort analysis to customer retention using R and I thought it&amp;#8217;d be a fun exercise to calculate something similar for meetup attendees.
 In the customer retention example we track customer purchases on a month by month basis and each customer is put into a cohort or bucket based on the first month they made a purchase in.</description>
    </item>
    
    <item>
      <title>R/dplyr: Extracting data frame column value for filtering with %in%</title>
      <link>https://markhneedham.com/blog/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</link>
      <pubDate>Sun, 22 Feb 2015 08:58:57 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</guid>
      <description>I&amp;#8217;ve been playing around with dplyr over the weekend and wanted to extract the values from a data frame column to use in a later filtering step.
 I had a data frame:
 library(dplyr) df = data.frame(userId = c(1,2,3,4,5), score = c(2,3,4,5,5))   And wanted to extract the userIds of those people who have a score greater than 3. I started with:
 highScoringPeople = df %&amp;gt;% filter(score &amp;gt; 3) %&amp;gt;% select(userId) &amp;gt; highScoringPeople userId 1 3 2 4 3 5   And then filtered the data frame expecting to get back those 3 people:</description>
    </item>
    
    <item>
      <title>R: Weather vs attendance at NoSQL meetups</title>
      <link>https://markhneedham.com/blog/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</link>
      <pubDate>Wed, 11 Feb 2015 07:09:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</guid>
      <description>A few weeks ago I came across a tweet by Sean Taylor asking for a weather data set with a few years worth of recording and I was surprised to learn that R already has such a thing - the weatherData package.
  Winner is: @UTVilla! library(weatherData)
df &amp;lt;- getWeatherForYear(&amp;quot;SFO&amp;quot;, 2013)
ggplot(df, aes(x=Date, y = Mean_TemperatureF)) + geom_line()
&amp;mdash; Sean J. Taylor (@seanjtaylor) January 22, 2015   weatherData provides a thin veneer around the wunderground API and was exactly what I&amp;#8217;d been looking for to compare meetup at London&amp;#8217;s NoSQL against weather conditions that day.</description>
    </item>
    
    <item>
      <title>R: ggplot2 - Each group consist of only one observation. Do you need to adjust the group aesthetic?</title>
      <link>https://markhneedham.com/blog/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</link>
      <pubDate>Fri, 30 Jan 2015 00:27:53 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</guid>
      <description>I&amp;#8217;ve been playing around with some weather data over the last couple of days which I aggregated down to the average temperature per month over the last 4 years and stored in a CSV file.
 This is what the file looks like:
 $ cat /tmp/averageTemperatureByMonth.csv &#34;month&#34;,&#34;aveTemperature&#34; &#34;January&#34;,6.02684563758389 &#34;February&#34;,5.89380530973451 &#34;March&#34;,7.54838709677419 &#34;April&#34;,10.875 &#34;May&#34;,13.3064516129032 &#34;June&#34;,15.9666666666667 &#34;July&#34;,18.8387096774194 &#34;August&#34;,18.3709677419355 &#34;September&#34;,16.2583333333333 &#34;October&#34;,13.4596774193548 &#34;November&#34;,9.19166666666667 &#34;December&#34;,7.01612903225806   I wanted to create a simple line chart which would show the months of the year in ascending order with the appropriate temperature.</description>
    </item>
    
    <item>
      <title>R: Vectorising all the things</title>
      <link>https://markhneedham.com/blog/2014/12/22/r-vectorising-all-the-things/</link>
      <pubDate>Mon, 22 Dec 2014 11:46:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/22/r-vectorising-all-the-things/</guid>
      <description>After my last post about finding the distance a date/time is from the weekend Hadley Wickham suggested I could improve the function by vectorising it...
  @markhneedham vectorise with pmin(pmax(dateToLookup - before, 0), pmax(after - dateToLookup, 0)) / dhours(1)
&amp;mdash; Hadley Wickham (@hadleywickham) December 14, 2014   &amp;#8230;&amp;#8203;so I thought I&amp;#8217;d try and vectorise some of the other functions I&amp;#8217;ve written recently and show the two versions.
 I found the following articles useful for explaining vectorisation and why you might want to do it:</description>
    </item>
    
    <item>
      <title>R: Time to/from the weekend</title>
      <link>https://markhneedham.com/blog/2014/12/13/r-time-tofrom-the-weekend/</link>
      <pubDate>Sat, 13 Dec 2014 20:38:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/13/r-time-tofrom-the-weekend/</guid>
      <description>In my last post I showed some examples using R&amp;#8217;s lubridate package and another problem it made really easy to solve was working out how close a particular date time was to the weekend.
 I wanted to write a function which would return the previous Sunday or upcoming Saturday depending on which was closer.
 lubridate&amp;#8217;s floor_date and ceiling_date functions make this quite simple.
 e.g. if we want to round the 18th December down to the beginning of the week and up to the beginning of the next week we could do the following:</description>
    </item>
    
    <item>
      <title>R: Numeric representation of date time</title>
      <link>https://markhneedham.com/blog/2014/12/13/r-numeric-representation-of-date-time/</link>
      <pubDate>Sat, 13 Dec 2014 19:58:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/13/r-numeric-representation-of-date-time/</guid>
      <description>I&amp;#8217;ve been playing around with date times in R recently and I wanted to derive a numeric representation for a given value to make it easier to see the correlation between time and another variable.
 e.g. December 13th 2014 17:30 should return 17.5 since it&amp;#8217;s 17.5 hours since midnight.
 Using the standard R libraries we would write the following code:
 &amp;gt; december13 = as.POSIXlt(&#34;2014-12-13 17:30:00&#34;) &amp;gt; as.numeric(december13 - trunc(december13, &#34;</description>
    </item>
    
    <item>
      <title>R: data.table/dplyr/lubridate - Error in wday(date, label = TRUE, abbr = FALSE) :  unused arguments (label = TRUE, abbr = FALSE)</title>
      <link>https://markhneedham.com/blog/2014/12/11/r-data-tabledplyrlubridate-error-in-wdaydate-label-true-abbr-false-unused-arguments-label-true-abbr-false/</link>
      <pubDate>Thu, 11 Dec 2014 19:03:06 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/11/r-data-tabledplyrlubridate-error-in-wdaydate-label-true-abbr-false-unused-arguments-label-true-abbr-false/</guid>
      <description>I spent a couple of hours playing around with data.table this evening and tried changing some code written using a data frame to use a data table instead.
 I started off by building a data frame which contains all the weekends between 2010 and 2015...
 &amp;gt; library(lubridate) &amp;gt; library(dplyr) &amp;gt; dates = data.frame(date = seq( dmy(&#34;01-01-2010&#34;), to=dmy(&#34;01-01-2015&#34;), by=&#34;day&#34; )) &amp;gt; dates = dates %&amp;gt;% filter(wday(date, label = TRUE, abbr = FALSE) %in% c(&#34;</description>
    </item>
    
    <item>
      <title>R: Cleaning up and plotting Google Trends data</title>
      <link>https://markhneedham.com/blog/2014/12/09/r-cleaning-up-plotting-google-trends-data/</link>
      <pubDate>Tue, 09 Dec 2014 18:14:45 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/09/r-cleaning-up-plotting-google-trends-data/</guid>
      <description>I recently came across an excellent article written by Stian Haklev in which he describes things he wishes he&amp;#8217;d been told before starting out with R, one being to do all data clean up in code which I thought I&amp;#8217;d give a try.
  My goal is to leave the raw data completely unchanged, and do all the transformation in code, which can be rerun at any time. While I&amp;#8217;m writing the scripts, I&amp;#8217;m often jumping around, selectively executing individual lines or code blocks, running commands to inspect the data in the REPL (read-evaluate-print-loop, where each command is executed as soon as you type enter, in the picture above it&amp;#8217;s the pane to the right), etc.</description>
    </item>
    
    <item>
      <title>R: dplyr - mutate with strptime (incompatible size/wrong result size)</title>
      <link>https://markhneedham.com/blog/2014/12/08/r-dplyr-mutate-with-strptime-incompatible-sizewrong-result-size/</link>
      <pubDate>Mon, 08 Dec 2014 19:02:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/08/r-dplyr-mutate-with-strptime-incompatible-sizewrong-result-size/</guid>
      <description>Having worked out how to translate a string into a date or NA if it wasn&amp;#8217;t the appropriate format the next thing I wanted to do was store the result of the transformation in my data frame.
 I started off with this:
 data = data.frame(x = c(&#34;2014-01-01&#34;, &#34;2014-02-01&#34;, &#34;foo&#34;)) &amp;gt; data x 1 2014-01-01 2 2014-02-01 3 foo   And when I tried to do the date translation ran into the following error:</description>
    </item>
    
    <item>
      <title>R: String to Date or NA</title>
      <link>https://markhneedham.com/blog/2014/12/07/r-string-to-date-or-na/</link>
      <pubDate>Sun, 07 Dec 2014 19:29:01 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/07/r-string-to-date-or-na/</guid>
      <description>I&amp;#8217;ve been trying to clean up a CSV file which contains some rows with dates and some not - I only want to keep the cells which do have dates so I&amp;#8217;ve been trying to work out how to do that.
 My first thought was that I&amp;#8217;d try and find a function which would convert the contents of the cell into a date if it was in date format and NA if not.</description>
    </item>
    
    <item>
      <title>R: Applying a function to every row of a data frame</title>
      <link>https://markhneedham.com/blog/2014/12/04/r-applying-a-function-to-every-row-of-a-data-frame/</link>
      <pubDate>Thu, 04 Dec 2014 06:31:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/12/04/r-applying-a-function-to-every-row-of-a-data-frame/</guid>
      <description>In my continued exploration of London&amp;#8217;s meetups I wanted to calculate the distance from meetup venues to a centre point in London.
 I&amp;#8217;ve created a gist containing the coordinates of some of the venues that host NoSQL meetups in London town if you want to follow along:
 library(dplyr) # https://gist.github.com/mneedham/7e926a213bf76febf5ed venues = read.csv(&#34;/tmp/venues.csv&#34;) venues %&amp;gt;% head() ## venue lat lon ## 1 Skills Matter 51.52482 -0.099109 ## 2 Skinkers 51.</description>
    </item>
    
    <item>
      <title>R: dplyr - Select &#39;random&#39; rows from a data frame</title>
      <link>https://markhneedham.com/blog/2014/11/26/r-dplyr-select-random-rows-from-a-data-frame/</link>
      <pubDate>Wed, 26 Nov 2014 00:01:12 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/26/r-dplyr-select-random-rows-from-a-data-frame/</guid>
      <description>Frequently I find myself wanting to take a sample of the rows in a data frame where just taking the head isn&amp;#8217;t enough.
 Let&amp;#8217;s say we start with the following data frame:
 data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) )   And we&amp;#8217;d like to sample 10 rows to see what it contains. We&amp;#8217;ll start by generating 10 random numbers to represent row numbers using the runif function:</description>
    </item>
    
    <item>
      <title>R: ggmap - Overlay shapefile with filled polygon of regions</title>
      <link>https://markhneedham.com/blog/2014/11/17/r-ggmap-overlay-shapefile-with-filled-polygon-of-regions/</link>
      <pubDate>Mon, 17 Nov 2014 00:53:11 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/17/r-ggmap-overlay-shapefile-with-filled-polygon-of-regions/</guid>
      <description>I&amp;#8217;ve been playing around with plotting maps in R over the last week and got to the point where I wanted to have a google map in the background with a filled polygon on a shapefile in the foreground.
 The first bit is reasonably simple - we can just import the ggmap library and make a call to get_map:
 &amp;gt; library(ggmap) &amp;gt; sfMap = map = get_map(location = &#39;San Francisco&#39;, zoom = 12)     Next I wanted to show the outlines of the different San Francisco zip codes and came across a blog post by Paul Bidanset on Baltimore neighbourhoods which I was able to adapt.</description>
    </item>
    
    <item>
      <title>R: Linear models with the lm function, NA values and Collinearity</title>
      <link>https://markhneedham.com/blog/2014/10/18/r-linear-models-with-the-lm-function-na-values-and-collinearity/</link>
      <pubDate>Sat, 18 Oct 2014 06:35:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/10/18/r-linear-models-with-the-lm-function-na-values-and-collinearity/</guid>
      <description>In my continued playing around with R I&amp;#8217;ve sometimes noticed &#39;NA&#39; values in the linear regression models I created but hadn&amp;#8217;t really thought about what that meant.
 On the advice of Peter Huber I recently started working my way through Coursera&amp;#8217;s Regression Models which has a whole slide explaining its meaning:
   So in this case &#39;z&#39; doesn&amp;#8217;t help us in predicting Fertility since it doesn&amp;#8217;t give us any more information that we can&amp;#8217;t already get from &#39;Agriculture&#39; and &#39;Education&#39;.</description>
    </item>
    
    <item>
      <title>R: A first attempt at linear regression</title>
      <link>https://markhneedham.com/blog/2014/09/30/r-a-first-attempt-at-linear-regression/</link>
      <pubDate>Tue, 30 Sep 2014 22:20:05 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/09/30/r-a-first-attempt-at-linear-regression/</guid>
      <description>I&amp;#8217;ve been working through the videos that accompany the Introduction to Statistical Learning with Applications in R book and thought it&amp;#8217;d be interesting to try out the linear regression algorithm against my meetup data set.
 I wanted to see how well a linear regression algorithm could predict how many people were likely to RSVP to a particular event. I started with the following code to build a data frame containing some potential predictors:</description>
    </item>
    
    <item>
      <title>R: Deriving a new data frame column based on containing string</title>
      <link>https://markhneedham.com/blog/2014/09/29/r-deriving-a-new-data-frame-column-based-on-containing-string/</link>
      <pubDate>Mon, 29 Sep 2014 21:37:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/09/29/r-deriving-a-new-data-frame-column-based-on-containing-string/</guid>
      <description>I&amp;#8217;ve been playing around with R data frames a bit more and one thing I wanted to do was derive a new column based on the text contained in the existing column.
 I started with something like this:
 &amp;gt; x = data.frame(name = c(&#34;Java Hackathon&#34;, &#34;Intro to Graphs&#34;, &#34;Hands on Cypher&#34;)) &amp;gt; x name 1 Java Hackathon 2 Intro to Graphs 3 Hands on Cypher   And I wanted to derive a new column based on whether or not the session was a practical one.</description>
    </item>
    
    <item>
      <title>R: Filtering data frames by column type (&#39;x&#39; must be numeric)</title>
      <link>https://markhneedham.com/blog/2014/09/29/r-filtering-data-frames-by-column-type-x-must-be-numeric/</link>
      <pubDate>Mon, 29 Sep 2014 05:46:43 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/09/29/r-filtering-data-frames-by-column-type-x-must-be-numeric/</guid>
      <description>I&amp;#8217;ve been working through the exercises from An Introduction to Statistical Learning and one of them required you to create a pair wise correlation matrix of variables in a data frame.
 The exercise uses the &#39;Carseats&#39; data set which can be imported like so:
 &amp;gt; install.packages(&#34;ISLR&#34;) &amp;gt; library(ISLR) &amp;gt; head(Carseats) Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 1 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 2 11.</description>
    </item>
    
    <item>
      <title>R: ggplot - Plotting multiple variables on a line chart</title>
      <link>https://markhneedham.com/blog/2014/09/16/r-ggplot-plotting-multiple-variables-on-a-line-chart/</link>
      <pubDate>Tue, 16 Sep 2014 16:59:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/09/16/r-ggplot-plotting-multiple-variables-on-a-line-chart/</guid>
      <description>In my continued playing around with meetup data I wanted to plot the number of members who join the Neo4j group over time.
 I started off with the variable &#39;byWeek&#39; which shows how many members joined the group each week:
 &amp;gt; head(byWeek) Source: local data frame [6 x 2] week n 1 2011-06-02 8 2 2011-06-09 4 3 2011-06-30 2 4 2011-07-14 1 5 2011-07-21 1 6 2011-08-18 1   I wanted to plot the actual count alongside a rolling average for which I created the following data frame:</description>
    </item>
    
    <item>
      <title>R: Calculating rolling or moving averages</title>
      <link>https://markhneedham.com/blog/2014/09/13/r-calculating-rolling-or-moving-averages/</link>
      <pubDate>Sat, 13 Sep 2014 08:15:26 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/09/13/r-calculating-rolling-or-moving-averages/</guid>
      <description>I&amp;#8217;ve been playing around with some time series data in R and since there&amp;#8217;s a bit of variation between consecutive points I wanted to smooth the data out by calculating the moving average.
 I struggled to find an in built function to do this but came across Didier Ruedin&amp;#8217;s blog post which described the following function to do the job:
 mav &amp;lt;- function(x,n=5){filter(x,rep(1/n,n), sides=2)}   I tried plugging in some numbers to understand how it works:</description>
    </item>
    
  </channel>
</rss>