<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>duckdb on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/duckdb/</link>
    <description>Recent content in duckdb on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Jun 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/duckdb/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB/SQL: Convert string in YYYYmmdd format to Date</title>
      <link>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</link>
      <pubDate>Tue, 20 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</guid>
      <description>I’ve been working with a data set that represents dates as strings in the format &amp;#39;YYYYmmdd&amp;#39; and I wanted to convert those values to Dates in DuckDB. In this blog post, we’ll learn how to do that.
Let’s create a small table with a single column that represents date of births:
create table players (dob VARCHAR); insert into players values(&amp;#39;20080203&amp;#39;), (&amp;#39;20230708&amp;#39;); We can write the following query to return the rows in the table:</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Transpose columns to rows with UNPIVOT</title>
      <link>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</link>
      <pubDate>Tue, 13 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</guid>
      <description>I’ve been playing around with the Kaggle European Soccer dataset, which contains, amongst other things, players and their stats in the FIFA video game. I wanted to compare the stats of Ronaldo and Messi, which is where this story begins.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Pivot - 0 if null</title>
      <link>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</link>
      <pubDate>Wed, 07 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</guid>
      <description>I’ve been learning all about the PIVOT function that was recently added in DuckDB and I ran into an issue where lots of the cells in my post PIVOT table were null values. In this blog post, we’ll learn how to replace those nulls with 0s (or indeed any other value).
Setup I’m working with Jeff Sackmann’s tennis dataset, which I loaded by running the following query:
CREATE OR REPLACE TABLE matches AS SELECT * FROM read_csv_auto( list_transform( range(1968, 2023), y -&amp;gt; &amp;#39;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB: Generate dummy data with user defined functions (UDFs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</link>
      <pubDate>Fri, 02 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</guid>
      <description>In the 0.8 release of DuckDB, they added functionality that lets you add your own functions when using the Python package I wanted to see if I could use it to generate dummy data so that’s what we’re going to do in this blog post.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB: Ingest a bunch of CSV files from GitHub</title>
      <link>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</link>
      <pubDate>Thu, 25 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</guid>
      <description>Jeff Sackmann’s tennis_atp repository is one of the best collections of tennis data and I wanted to ingest the ATP Tour singles matches using the DuckDB CLI. In this blog post we’ll learn how to do that.
Usually when I’m ingesting data into DuckDB I’ll specify the files that I want to ingest using the wildcard syntax. In this case that would mean running a query like this:
CREATE OR REPLACE TABLE matches AS SELECT * FROM &amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10); Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Get decade from date</title>
      <link>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</link>
      <pubDate>Thu, 20 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</guid>
      <description>Working with dates in SQL can sometimes be a bit tricky, especially when you need to extract specific information, like the decade a date belongs to. In this blog post, we’ll explore how to easily obtain the decade from a date using DuckDB, a lightweight and efficient SQL database engine.
First, install DuckDB and launch it:
./duckdb Next, we’re going to create a movies table that has columns for title and releaseDate:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert epoch to timestamp with timezone</title>
      <link>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</link>
      <pubDate>Wed, 05 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</guid>
      <description>I’ve been playing around with the Citi Bike Stations dataset on Kaggle with DuckDB and ran into trouble when trying to convert a column containing epoch timestamps to a timestamp with timezone support. In this blog we’ll learn how to do that, which will at least be helpful to future me, if noone else!
The dataset contains 4GB worth of CSV files, but I’ve just downloaded a few of them manually for now.</description>
    </item>
    
    <item>
      <title>Tennis Head to Head with DuckDB and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</link>
      <pubDate>Fri, 31 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</guid>
      <description>In this blog post we’re going to learn how to build an application to compare the matches between two ATP tennis players. DuckDB and Streamlit will be our partners in crime for this mission.
Set up To get started, let’s create a virtual environment:
python -m venv .venv source .venv/bin/activate And now install some libraries:
pip install duckdb streamlit streamlit-searchbox And now let’s open a file, app.py and import the packages:</description>
    </item>
    
    <item>
      <title>DuckDB/Python: Cannot combine LEFT and RIGHT relations of different connections!</title>
      <link>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</link>
      <pubDate>Mon, 20 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</guid>
      <description>I’ve been playing around with DuckDB over the weekend and ran into an interesting problem while using the Relational API in the Python package. We’re going to explore that in this blog post.
Set up To get started, let’s install DuckDB:
pip install duckdb And now let’s open a Python shell and import the package:
import duckdb Next, let’s create a DuckDB connection and import the httpfs module, which we’ll use in just a minute:</description>
    </item>
    
    <item>
      <title>DuckDB: Join based on maximum value in other table</title>
      <link>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</link>
      <pubDate>Wed, 01 Feb 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</guid>
      <description>In this blog post we’re going to learn how to write a SQL query to join two tables where one of the tables has multiple rows for each key. We want to select only the rows that contain the most recent (or maximum) value from that table.
Our story begins with a YouTube video that I created showing how to query the European Soccer SQLite database with DuckDB. This database contains lots of different tables, but we are only interested in Player and Player_Attributes.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Diffing Apache Parquet schemas with DuckDB</title>
      <link>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</link>
      <pubDate>Thu, 17 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</guid>
      <description>I’ve been playing around with DuckDB, the new hotness in the analytics space, over the last month, and my friend Michael Hunger asked whether you could use it to compute a diff of Apache Parquet schemas.
Challenge accepted!
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
