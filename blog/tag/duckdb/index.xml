<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>duckdb on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/duckdb/</link>
    <description>Recent content in duckdb on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/duckdb/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PIVOTing data in ClickHouse and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2024/11/15/pivot-clickhouse-duckdb/</link>
      <pubDate>Fri, 15 Nov 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/11/15/pivot-clickhouse-duckdb/</guid>
      <description>I really like DuckDB’s PIVOT clause and along with some others wish that ClickHouse supported it too. Sadly it doesn’t, but we can get pretty close to this functionality using ClickHouse’s aggregate function combinators. In this blog post, I’m going to go through each of the examples in the DuckDB documentation and show how to do the equivalent in ClickHouse.
Set up First, we need to setup the sample data.</description>
    </item>
    
    <item>
      <title>DuckDB 1.1: Dynamic Column Selection gets even better</title>
      <link>https://www.markhneedham.com/blog/2024/09/22/duckdb-dynamic-column-selection/</link>
      <pubDate>Sun, 22 Sep 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/09/22/duckdb-dynamic-column-selection/</guid>
      <description>DuckDB 1.1 was released a couple of weeks ago and there are a couple of features that make dynamic column selection even better. We’re going to explore those features in this blog.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
Kaggle’s FIFA 2022 Dataset To demonstrate dynamic column selection, we need a dataset that has a lot of columns, ideally one containing lots of numeric values as well.</description>
    </item>
    
    <item>
      <title>DuckDB: Chaining functions</title>
      <link>https://www.markhneedham.com/blog/2024/08/25/duckdb-chaining-functions/</link>
      <pubDate>Sun, 25 Aug 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/08/25/duckdb-chaining-functions/</guid>
      <description>One of my favourite things about DuckDB is the innovations it’s made in SQL. A recent discovery (for me at least) is that you can chain functions using the dot operator, in the same way you can in many general purpose programming languages. In this blog, we’re going to explore that functionality.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Hybrid Search in SQL with DuckDB</title>
      <link>https://www.markhneedham.com/blog/2024/07/28/hybrid-search-sql-duckdb/</link>
      <pubDate>Sun, 28 Jul 2024 01:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/07/28/hybrid-search-sql-duckdb/</guid>
      <description>I’ve been playing around with different approaches for Retrieval Augmented Generation (RAG) recently and came across a blog post describing Reciprocal Rank Fusion, a hybrid search technique. In this blog post, we’re going to explore how to apply this method in SQL using DuckDB.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB: Create a function in SQL</title>
      <link>https://www.markhneedham.com/blog/2024/07/28/duckdb-create-function-sql/</link>
      <pubDate>Sun, 28 Jul 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/07/28/duckdb-create-function-sql/</guid>
      <description>I’ve been learning about Hybrid Search via this blog post, which describes the Reciprocal Rank Fusion algorithm, and I wanted to implement and use it in a DuckDB query.
The formula for the function is shown below:
RRF(d) = Σ(r ∈ R) 1 / (k + r(d))
Where:
d is a document
R is the set of rankers (retrievers)
k is a constant (typically 60)
r(d) is the rank of document d in ranker r</description>
    </item>
    
    <item>
      <title>DuckDB 0.10: Binder Error: No function matches the given name and argument types</title>
      <link>https://www.markhneedham.com/blog/2024/03/09/duckdb-strptime-binder-error-no-function-matches/</link>
      <pubDate>Sat, 09 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/09/duckdb-strptime-binder-error-no-function-matches/</guid>
      <description>In the 0.10 version of DuckDB, a breaking change was made that stops implicit casting to VARCHAR during function binding. In this blog post, we’re going to look at some ways to work around this change when fixing our DuckDB code from 0.9 or earlier.
I have a CSV file that looks like this:
from &amp;#39;people.csv&amp;#39; select *; Output ┌─────────┬─────────────┐ │ name │ dateOfBirth │ │ varchar │ int64 │ ├─────────┼─────────────┤ │ John │ 19950105 │ └─────────┴─────────────┘ The dateOfBirth column isn’t an int64, but that’s how DuckDB has inferred it.</description>
    </item>
    
    <item>
      <title>DuckDB: Show a list of views</title>
      <link>https://www.markhneedham.com/blog/2023/10/02/duckdb-list-show-views/</link>
      <pubDate>Mon, 02 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/02/duckdb-list-show-views/</guid>
      <description>I recently wanted to get a list of the views that I’d created in a DuckDB database and it took me a while to figure out how to do it. So this blog post is for future Mark more than anyone else!
We’re going to start with the following CSV file:
data/sales.csv date,product_id,quantity,sales_amount 2021-01-01,101,5,50 2021-01-02,102,3,30 2021-02-01,101,4,40 2021-02-02,103,6,60 And now we’ll create a table from the DuckDB CLI:
CREATE TABLE sales AS SELECT * from &amp;#39;data/sales.</description>
    </item>
    
    <item>
      <title>dbt-duckdb: KeyError: &#34;&#39;winner_seed&#39;&#34;</title>
      <link>https://www.markhneedham.com/blog/2023/10/01/dbt-duckdb-key-error/</link>
      <pubDate>Sun, 01 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/01/dbt-duckdb-key-error/</guid>
      <description>I’ve been building a little demo with dbt and DuckDB to transform CSV files from Jeff Sackmann’s tennis dataset and ran into an error that initially puzzled me. In this blog post, we’ll learn how to deal with it.
But first things first, we’re going to install dbt-duckdb as well as the latest version of DuckDB, which at the time of writing is 0.9.0.
pip install dbt-duckdb duckdb I then cloned Mehdi Ouazza’s demo project and adjusted it to work with my dataset.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert string in YYYYmmdd format to Date</title>
      <link>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</link>
      <pubDate>Tue, 20 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</guid>
      <description>I’ve been working with a data set that represents dates as strings in the format &amp;#39;YYYYmmdd&amp;#39; and I wanted to convert those values to Dates in DuckDB. In this blog post, we’ll learn how to do that.
Let’s create a small table with a single column that represents date of births:
create table players (dob VARCHAR); insert into players values(&amp;#39;20080203&amp;#39;), (&amp;#39;20230708&amp;#39;); We can write the following query to return the rows in the table:</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Transpose columns to rows with UNPIVOT</title>
      <link>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</link>
      <pubDate>Tue, 13 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</guid>
      <description>I’ve been playing around with the Kaggle European Soccer dataset, which contains, amongst other things, players and their stats in the FIFA video game. I wanted to compare the stats of Ronaldo and Messi, which is where this story begins.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Pivot - 0 if null</title>
      <link>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</link>
      <pubDate>Wed, 07 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</guid>
      <description>I’ve been learning all about the PIVOT function that was recently added in DuckDB and I ran into an issue where lots of the cells in my post PIVOT table were null values. In this blog post, we’ll learn how to replace those nulls with 0s (or indeed any other value).
Setup I’m working with Jeff Sackmann’s tennis dataset, which I loaded by running the following query:
CREATE OR REPLACE TABLE matches AS SELECT * FROM read_csv_auto( list_transform( range(1968, 2023), y -&amp;gt; &amp;#39;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB: Generate dummy data with user defined functions (UDFs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</link>
      <pubDate>Fri, 02 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</guid>
      <description>In the 0.8 release of DuckDB, they added functionality that lets you add your own functions when using the Python package I wanted to see if I could use it to generate dummy data so that’s what we’re going to do in this blog post.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB: Ingest a bunch of CSV files from GitHub</title>
      <link>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</link>
      <pubDate>Thu, 25 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</guid>
      <description>Jeff Sackmann’s tennis_atp repository is one of the best collections of tennis data and I wanted to ingest the ATP Tour singles matches using the DuckDB CLI. In this blog post we’ll learn how to do that.
Usually when I’m ingesting data into DuckDB I’ll specify the files that I want to ingest using the wildcard syntax. In this case that would mean running a query like this:
CREATE OR REPLACE TABLE matches AS SELECT * FROM &amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10); Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Get decade from date</title>
      <link>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</link>
      <pubDate>Thu, 20 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/20/duckdb-sql-decade-from-date/</guid>
      <description>Working with dates in SQL can sometimes be a bit tricky, especially when you need to extract specific information, like the decade a date belongs to. In this blog post, we’ll explore how to easily obtain the decade from a date using DuckDB, a lightweight and efficient SQL database engine.
First, install DuckDB and launch it:
./duckdb Next, we’re going to create a movies table that has columns for title and releaseDate:</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert epoch to timestamp with timezone</title>
      <link>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</link>
      <pubDate>Wed, 05 Apr 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/04/05/duckdb-sql-convert-epoch-timestamp-timezone/</guid>
      <description>I’ve been playing around with the Citi Bike Stations dataset on Kaggle with DuckDB and ran into trouble when trying to convert a column containing epoch timestamps to a timestamp with timezone support. In this blog we’ll learn how to do that, which will at least be helpful to future me, if noone else!
The dataset contains 4GB worth of CSV files, but I’ve just downloaded a few of them manually for now.</description>
    </item>
    
    <item>
      <title>Tennis Head to Head with DuckDB and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</link>
      <pubDate>Fri, 31 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/31/tennis-head-to-head-duckdb-streamlit/</guid>
      <description>In this blog post we’re going to learn how to build an application to compare the matches between two ATP tennis players. DuckDB and Streamlit will be our partners in crime for this mission.
Set up To get started, let’s create a virtual environment:
python -m venv .venv source .venv/bin/activate And now install some libraries:
pip install duckdb streamlit streamlit-searchbox And now let’s open a file, app.py and import the packages:</description>
    </item>
    
    <item>
      <title>DuckDB/Python: Cannot combine LEFT and RIGHT relations of different connections!</title>
      <link>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</link>
      <pubDate>Mon, 20 Mar 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/03/20/duckdb-cannot-combine-left-right-relations/</guid>
      <description>I’ve been playing around with DuckDB over the weekend and ran into an interesting problem while using the Relational API in the Python package. We’re going to explore that in this blog post.
Set up To get started, let’s install DuckDB:
pip install duckdb And now let’s open a Python shell and import the package:
import duckdb Next, let’s create a DuckDB connection and import the httpfs module, which we’ll use in just a minute:</description>
    </item>
    
    <item>
      <title>DuckDB: Join based on maximum value in other table</title>
      <link>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</link>
      <pubDate>Wed, 01 Feb 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</guid>
      <description>In this blog post we’re going to learn how to write a SQL query to join two tables where one of the tables has multiple rows for each key. We want to select only the rows that contain the most recent (or maximum) value from that table.
Our story begins with a YouTube video that I created showing how to query the European Soccer SQLite database with DuckDB. This database contains lots of different tables, but we are only interested in Player and Player_Attributes.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Diffing Apache Parquet schemas with DuckDB</title>
      <link>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</link>
      <pubDate>Thu, 17 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</guid>
      <description>I’ve been playing around with DuckDB, the new hotness in the analytics space, over the last month, and my friend Michael Hunger asked whether you could use it to compute a diff of Apache Parquet schemas.
Challenge accepted!
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
