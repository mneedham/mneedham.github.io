<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>duckdb on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/duckdb/</link>
    <description>Recent content in duckdb on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Feb 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/duckdb/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB: Join based on maximum value in other table</title>
      <link>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</link>
      <pubDate>Wed, 01 Feb 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/02/01/duckdb-join-max-value-other-table/</guid>
      <description>In this blog post we’re going to learn how to write a SQL query to join two tables where one of the tables has multiple rows for each key. We want to select only the rows that contain the most recent (or maximum) value from that table.
Our story begins with a YouTube video that I created showing how to query the European Soccer SQLite database with DuckDB. This database contains lots of different tables, but we are only interested in Player and Player_Attributes.</description>
    </item>
    
    <item>
      <title>Exporting CSV files to Parquet file format with Pandas, Polars, and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</link>
      <pubDate>Fri, 06 Jan 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/01/06/export-csv-parquet-pandas-polars-duckdb/</guid>
      <description>I was recently trying to convert a CSV file to Parquet format and came across a StackOverflow post that described a collection of different options. My CSV file was bigger than the amount of memory I had available, which ruled out some of the methods. In this blog post we’re going to walk through some options for exporting big CSV files to Parquet format.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Diffing Apache Parquet schemas with DuckDB</title>
      <link>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</link>
      <pubDate>Thu, 17 Nov 2022 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2022/11/17/duckdb-diff-parquet-schema/</guid>
      <description>I’ve been playing around with DuckDB, the new hotness in the analytics space, over the last month, and my friend Michael Hunger asked whether you could use it to compute a diff of Apache Parquet schemas.
Challenge accepted!
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
  </channel>
</rss>
