<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gguf on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/gguf/</link>
    <description>Recent content in gguf on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Oct 2023 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/gguf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ollama: Running GGUF Models from Hugging Face</title>
      <link>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</link>
      <pubDate>Wed, 18 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</guid>
      <description>GGUF (GPT-Generated Unified Format) has emerged as the de facto standard file format for storing large language models for inference. We are starting to see a lot of models in this format on Hugging Face, many of them uploaded by The Bloke.
One cool thing about GGUF models is that it’s super easy to get them running on your own machine using Ollama. In this blog post, we’re going to look at how to download a GGUF model from Hugging Face and run it locally.</description>
    </item>
    
  </channel>
</rss>
