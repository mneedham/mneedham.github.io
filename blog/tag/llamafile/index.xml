<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llamafile on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/llamafile/</link>
    <description>Recent content in llamafile on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Dec 2023 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/llamafile/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>litellm and llamafile -  APIError: OpenAIException - File Not Found</title>
      <link>https://www.markhneedham.com/blog/2023/12/14/litellm-apierror-openaiexception-file-not-found/</link>
      <pubDate>Thu, 14 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/14/litellm-apierror-openaiexception-file-not-found/</guid>
      <description>I wanted to get two of my favourite tools in the LLM world - llmlite and llamafile - to play nicely and ran into an issue that I’ll explain in this blog post. This should be helpful if you’re trying to wire up other LLM servers to llmlite, it’s not specific to llamafile.
Setting up llamafile In case you want to follow along, I downloaded llamafile and MistralAI 7B weights from TheBloke/Mistral-7B-v0.</description>
    </item>
    
  </channel>
</rss>
