<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kaggle on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/kaggle/</link>
    <description>Recent content in kaggle on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Jan 2018 06:51:10 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/kaggle/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import pandas as pd import tensorflow as tf from sklearn import preprocessing from sklearn.model_selection import train_test_split EMBEDDING_SIZE = 50 MAX_LABEL = 3 WORDS_FEATURE = &amp;#39;words&amp;#39; # Name of the input words feature. def bag_of_words_model(features, labels, mode): bow_column = tf.feature_column.categorical_column_with_identity(WORDS_FEATURE, num_buckets=n_words) bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=EMBEDDING_SIZE) bow = tf.feature_column.input_layer(features, feature_columns=[bow_embedding_column]) logits = tf.layers.dense(bow, MAX_LABEL, activation=None) return create_estimator_spec(logits=logits, labels=labels, mode=mode) def create_estimator_spec(logits, labels, mode): predicted_classes = tf.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # We&amp;#39;ll use this library to make the display pretty from tabulate import tabulate train = pd.read_csv(&amp;#39;train.csv&amp;#39;) # the model can only handle numeric values so filter out the rest data = train.select_dtypes(include=[np.number]).interpolate().dropna() y = train.SalePrice X = data.drop([&amp;#34;SalePrice&amp;#34;, &amp;#34;Id&amp;#34;], axis=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33) clf = RandomForestRegressor(n_jobs=2, n_estimators=1000) model = clf.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>import pandas as pd train = pd.read_csv(&amp;#39;train.csv&amp;#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64 cols = [col for col in train.columns if col.startswith(&amp;#34;Lot&amp;#34;)] missing_frontage = train[cols][train[&amp;#34;LotFrontage&amp;#34;].isnull()] &amp;gt;&amp;gt;&amp;gt; print(missing_frontage.head()) LotFrontage LotArea LotShape LotConfig 7 NaN 10382 IR1 Corner 12 NaN 12968 IR2 Inside 14 NaN 10920 IR1 Corner 16 NaN 11241 IR1 CulDSac 24 NaN 8246 IR1 Inside sub_train = train[train.</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>import pandas as pd def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) def fare_in_bucket(fare, fare_bracket_size, bucket): return (fare &amp;gt; bucket * fare_bracket_size) &amp;amp; (fare &amp;lt;= ((bucket+1) * fare_bracket_size)) def build_survival_table(training_file): fare_ceiling = 40 train_df = pd.read_csv(training_file) train_df[train_df[&amp;#39;Fare&amp;#39;] &amp;gt;= 39.0] = 39.0 fare_bracket_size = 10 number_of_price_brackets = fare_ceiling / fare_bracket_size number_of_classes = 3 #There were 1st, 2nd and 3rd classes on board  survival_table = pd.DataFrame(columns=[&amp;#39;Sex&amp;#39;, &amp;#39;Pclass&amp;#39;, &amp;#39;PriceDist&amp;#39;, &amp;#39;Survived&amp;#39;, &amp;#39;NumberOfPeople&amp;#39;]) for pclass in range(1, number_of_classes + 1): # add 1 to handle 0 start for bucket in range(0, number_of_price_brackets): for sex in [&amp;#39;female&amp;#39;, &amp;#39;male&amp;#39;]: survival = train_df[(train_df[&amp;#39;Sex&amp;#39;] == sex) &amp;amp; (train_df[&amp;#39;Pclass&amp;#39;] == pclass) &amp;amp; fare_in_bucket(train_df[&amp;#34;Fare&amp;#34;], fare_bracket_size, bucket)] row = [dict(Pclass=pclass, Sex=sex, PriceDist = bucket, Survived = round(survival[&amp;#39;Survived&amp;#39;].</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I&amp;rsquo;ve written a few blog postsabout our attempts at the Kaggle Digit Recogniserproblem and one thing we haven&amp;rsquo;t yet tried is feature extraction.
Many people go straight from a data set to applying an algorithm. But there’s a huge space in between of important stuff. It’s easy to run a piece of code that predicts or classifies. That’s not the hard part. The hard part is doing it well.
One needs to conduct exploratory data analysis as I’ve emphasized; and conduct feature selection as Will Cukierski emphasized.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>Many people go straight from a data set to applying an algorithm. But there’s a huge space in between of important stuff. It’s easy to run a piece of code that predicts or classifies. That’s not the hard part. The hard part is doing it well.
initial &amp;lt;- read.csv(&amp;#34;train.csv&amp;#34;, nrows=10000, header = TRUE) # take a sample of 1000 rows of the input  sampleSet &amp;lt;- initial[sample(1:nrow(initial), 1000), ] # get all the labels sampleSet.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle&amp;rsquo;s Digit RecognizerJenand I decided to try out boostingon our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
We couldn&amp;rsquo;t find any documentation that it was possible to apply boosting to Mahout&amp;rsquo;s random forest algorithm but we knew it was possible with Wekaso we decided to use that instead!
As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it&amp;rsquo;s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jenand I used the K-means algorithmto classify digits in Kaggle&amp;rsquo;s Digit Recognizer problemand one of the things we&amp;rsquo;d read was that with this algorithm you often end up with situations where it&amp;rsquo;s difficult to classify a new item because if falls between two labels.
We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
  </channel>
</rss>