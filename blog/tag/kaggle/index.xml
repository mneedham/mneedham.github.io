<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kaggle on Mark Needham</title>
    <link>https://markhneedham.com/blog/tag/kaggle/</link>
    <description>Recent content in kaggle on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Jan 2018 06:51:10 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/tag/kaggle/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>I&amp;#8217;ve been playing around with some Tensorflow tutorials recently and wanted to see if I could create a submission for Kaggle&amp;#8217;s Spooky Author Identification competition that I&amp;#8217;ve written about recently.
 My model is based on one from the text classification tutorial. The tutorial shows how to create custom Estimators which we can learn more about in a post on the Google Developers blog.
 Imports Let&amp;#8217;s get started. First, our imports:</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, I&amp;#8217;ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.
 Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant I&amp;#8217;d overfitted the model.
 I wasn&amp;#8217;t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albon&amp;#8217;s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>I&amp;#8217;ve been playing around with the data in Kaggle&amp;#8217;s House Prices: Advanced Regression Techniques and while replicating Poonam Ligade&amp;#8217;s exploratory analysis I wanted to see if I could create a model to fill in some of the missing values.
 Poonam wrote the following code to identify which columns in the dataset had the most missing values:
 import pandas as pd train = pd.read_csv(&#39;train.csv&#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   The one that I&amp;#8217;m most interested in is LotFrontage, which describes &#39;Linear feet of street connected to property&#39;.</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>Nathan and I have been looking at Kaggle&amp;#8217;s Titanic problem and while working through the Python tutorial Nathan pointed out that we could greatly simplify the code if we used pandas instead.
 The problem we had with numpy is that you use integers to reference columns. We spent a lot of time being thoroughly confused as to why something wasn&amp;#8217;t working only to realise we were using the wrong column.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I&amp;#8217;ve written a few blog posts about our attempts at the Kaggle Digit Recogniser problem and one thing we haven&amp;#8217;t yet tried is feature extraction.
 Feature extraction in this context means that we&amp;#8217;d generate some other features to train a classifier with rather than relying on just the pixel values we were provided.
 Every week Jen would try and persuade me that we should try it out but it wasn&amp;#8217;t until I was flicking through the notes from the Columbia Data Science class that it struck home:</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>I&amp;#8217;ve written previously about our attempts at the Kaggle Digit Recogniser problem and our approach so far has been to use the data provided and plug it into different algorithms and see what we end up with.
 From browsing through the forums we saw others mentioning feature extraction - an approach where we transform the data into another format , the thinking being that we can train a better classifier with better data.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle&amp;#8217;s Digit Recognizer Jen and I decided to try out boosting on our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
 We couldn&amp;#8217;t find any documentation that it was possible to apply boosting to Mahout&amp;#8217;s random forest algorithm but we knew it was possible with Weka so we decided to use that instead!
 As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it&amp;#8217;s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jen and I used the K-means algorithm to classify digits in Kaggle&amp;#8217;s Digit Recognizer problem and one of the things we&amp;#8217;d read was that with this algorithm you often end up with situations where it&amp;#8217;s difficult to classify a new item because if falls between two labels.
 We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
  </channel>
</rss>