<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>til on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/til/</link>
    <description>Recent content in til on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jun 2023 02:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/til/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB/SQL: Pivot - 0 if null</title>
      <link>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</link>
      <pubDate>Wed, 07 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</guid>
      <description>I’ve been learning all about the PIVOT function that was recently added in DuckDB and I ran into an issue where lots of the cells in my post PIVOT table were null values. In this blog post, we’ll learn how to replace those nulls with 0s (or indeed any other value).
Setup I’m working with Jeff Sackmann’s tennis dataset, which I loaded by running the following query:
CREATE OR REPLACE TABLE matches AS SELECT * FROM read_csv_auto( list_transform( range(1968, 2023), y -&amp;gt; &amp;#39;https://raw.</description>
    </item>
    
    <item>
      <title>Kafka/Kubernetes: Failed to resolve: nodename nor servname provided, or not known</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</guid>
      <description>I’ve been trying out the Running Pinot in Kubernetes tutorial and ran into a problem trying to write data to Kafka. In this blog we’ll explore how I got around that problem.
I’m using Helm with Kubernetes and started a Kafka service by running the following:
helm repo add kafka https://charts.bitnami.com/bitnami helm install -n pinot-quickstart kafka kafka/kafka --set replicas=1,zookeeper.image.tag=latest I waited until the service had started and then ran the following command to port forward the Kafka service’s port 9092 to port 9092 on my host OS:</description>
    </item>
    
    <item>
      <title>Python: Working with tuples in lambda expressions</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</guid>
      <description>I’m still playing around with data returned by Apache Pinot’s HTTP API and I wanted to sort a dictionary of segment names by partition id and index. In this blog post we’re going to look into how to do that.
We’ll start with the following dictionary:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__13__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } As I mentioned above, I want to sort the dictionary’s items by partition id and index, which are embedded inside the key name.</description>
    </item>
    
    <item>
      <title>Python: Padding a string</title>
      <link>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</link>
      <pubDate>Mon, 05 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</guid>
      <description>I’ve been writing some scripts to parse data from Apache Pinot’s HTTP API and I wanted to format the values stored in a map to make them more readable. In this blog post, we’ll look at some ways that I did that.
I started with a map that looked a bit like this:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } And then I iterated over and printed each item like this:</description>
    </item>
    
    <item>
      <title>DuckDB: Generate dummy data with user defined functions (UDFs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</link>
      <pubDate>Fri, 02 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</guid>
      <description>In the 0.8 release of DuckDB, they added functionality that lets you add your own functions when using the Python package I wanted to see if I could use it to generate dummy data so that’s what we’re going to do in this blog post.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Debezium: Capture changes from MySQL</title>
      <link>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</link>
      <pubDate>Wed, 31 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</guid>
      <description>I’ve been working on a Real-Time Analytics workshop that I’m going to be presenting at the ODSC Europe conference in June 2023 and I wanted to have Debezium publish records from a MySQL database without including the schema.
I’m using the debezium/connect:2.3 Docker image to run Debezium locally and I have a MySQL database running with the hostname mysql on port 3306. Below is the way that I configured this:</description>
    </item>
    
    <item>
      <title>Node.js: Minifying JSON documents</title>
      <link>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</link>
      <pubDate>Tue, 30 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</guid>
      <description>I often need to minimise the schema and table config files that you use to configure Apache Pinot so that they don’t take up so much space. After doing this manually for ages, I came across the json-stringify-pretty-compact library, which speeds up the process.
We can install it like this:
npm install json-stringify-pretty-compact And then I have the following script:
minify.mjs import pretty from &amp;#39;json-stringify-pretty-compact&amp;#39;; let inputData = &amp;#39;&amp;#39;; process.</description>
    </item>
    
    <item>
      <title>DuckDB: Ingest a bunch of CSV files from GitHub</title>
      <link>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</link>
      <pubDate>Thu, 25 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</guid>
      <description>Jeff Sackmann’s tennis_atp repository is one of the best collections of tennis data and I wanted to ingest the ATP Tour singles matches using the DuckDB CLI. In this blog post we’ll learn how to do that.
Usually when I’m ingesting data into DuckDB I’ll specify the files that I want to ingest using the wildcard syntax. In this case that would mean running a query like this:
CREATE OR REPLACE TABLE matches AS SELECT * FROM &amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10); Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>Arc Browser: Building a plugin (Boost) with help from ChatGPT</title>
      <link>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</link>
      <pubDate>Tue, 23 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</guid>
      <description>I’ve been using the Arc Browser for a couple of months now and one of my favourite things is the simplicity of the plugin (or as they call it, &amp;#39;Boost&amp;#39;) functionality.
I wanted to port over a Chrome bookmark that I use to capture the podcasts that I’ve listened to on Player.FM. In this blog post I’ll show how ChatGPT helped me convert the bookmark code to an Arc Boost.</description>
    </item>
    
    <item>
      <title>Cropping a video using FFMPEG</title>
      <link>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</link>
      <pubDate>Mon, 15 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</guid>
      <description>I needed to crop a video that I used as part of a video on my YouTube channel, Learn Data With Mark, and Camtasia kept rendering a black screen. So I had to call for FFMPEG!
Cropping the bottom of a video My initial video was 2160 x 3840 but I didn’t need the bottom 1920 pixels because I’m using that part of the screen for a video of me.</description>
    </item>
    
    <item>
      <title>Python: Naming slices</title>
      <link>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</link>
      <pubDate>Sat, 13 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</guid>
      <description>Another gem from Fluent Python is that you can name slices. How did I not know that?!
Let’s have a look how it works using an example of a Vehicle Identification Number, which has 17 characters that act as a unique identifier for a vehicle. Different parts of that string mean different things.
So given the following VIN:
vin = &amp;#34;2B3HD46R02H210893&amp;#34; We can extract components like this:
print(f&amp;#34;&amp;#34;&amp;#34; World manufacturer identifier: {vin[0:3]} Vehicle Descriptor: {vin[3:9]} Vehicle Identifier: {vin[9:17]} &amp;#34;&amp;#34;&amp;#34;.</description>
    </item>
    
    <item>
      <title>Python 3.10: Pattern matching with match/case</title>
      <link>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</link>
      <pubDate>Tue, 09 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</guid>
      <description>I’ve been reading Fluent Python and learnt about pattern matching with the match/case statement, introduced in Python 3.10. You can use it instead of places where you’d otherwise use if, elif, else statements.
I created a small example to understand how it works. The following function takes in a list where the first argument should be foo, followed by a variable number of arguments, which we print to the console:</description>
    </item>
    
  </channel>
</rss>
