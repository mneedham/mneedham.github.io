<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>til on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/til/</link>
    <description>Recent content in til on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jul 2023 11:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/til/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python/Poetry: Library not loaded: no such file, not in dyld cache</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</link>
      <pubDate>Thu, 27 Jul 2023 11:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</guid>
      <description>As I mentioned in a previous blog post, I’ve been using Python’s Poetry library, but today it stopped working! In this blog post, I’ll explain what happened and how I got it working again.
It started off innocent enough, with me trying to create a new project:
poetry init But instead of seeing the usual interactive wizard, I got the following error:
Output dyld[20269]: Library not loaded: /opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/Python Referenced from: &amp;lt;1B2377F9-2187-39A9-AA98-20E438024DE2&amp;gt; /Users/markhneedham/Library/Application Support/pypoetry/venv/bin/python Reason: tried: &amp;#39;/opt/homebrew/Cellar/python@3.</description>
    </item>
    
    <item>
      <title>OpenAI/GPT: Returning consistent/valid JSON from a prompt</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</link>
      <pubDate>Thu, 27 Jul 2023 01:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</guid>
      <description>When using OpenAI it can be tricky to get it to return a consistent response for a prompt. In this blog post, we’re going to learn how to use functions to return a consistent JSON format for a basic sentiment analysis prompt.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>How to delete a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/07/26/how-to-delete-kafka-topic/</link>
      <pubDate>Wed, 26 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/26/how-to-delete-kafka-topic/</guid>
      <description>A few years ago I wrote a blog post showing how to delete a Kafka topic when running on Docker and while that approach still works, I think I’ve now got a better way. And that’s what we’re going to learn about in this blog post.
Spin up Kafka Cluster We’re going to spin up Kafka using the following Docker Compose file:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:3.8.0 hostname: zookeeper container_name: zookeeper-delete ports: - &amp;#34;2181:2181&amp;#34; environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 kafka: image: wurstmeister/kafka:latest restart: unless-stopped container_name: &amp;#34;kafka-delete&amp;#34; ports: - &amp;#34;9092:9092&amp;#34; expose: - &amp;#34;9093&amp;#34; depends_on: - zookeeper environment: KAFKA_ZOOKEEPER_CONNECT: zookeeper-delete:2181/kafka KAFKA_BROKER_ID: 0 KAFKA_ADVERTISED_HOST_NAME: kafka-delete KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-delete:9093,OUTSIDE://localhost:9092 KAFKA_LISTENERS: PLAINTEXT://0.</description>
    </item>
    
    <item>
      <title>Confluent Kafka: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.</title>
      <link>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</link>
      <pubDate>Tue, 25 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</guid>
      <description>I’ve been creating a demo showing how to ingest Avro-encoded data from Apache Kafka into Apache Pinot and ran into a deprecation warning. In this blog post, I’ll show how to update code using the Confluent Kafka Python client to get rid of that warning.
I started by installing the following libraries:
pip install confluent-kafka avro urllib3 requests And then my code to publish an Avro encoded event to Kafka looked like this:</description>
    </item>
    
    <item>
      <title>VSCode: Adding Poetry Python Interpreter</title>
      <link>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</link>
      <pubDate>Mon, 24 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</guid>
      <description>I’ve been trying out Python’s Poetry dependency management tool recently and I really like it, but couldn’t figure out how to get it setup as VSCode’s Python interpreter. In this blog post, we’ll learn how to do that.
One way to add the Python interpreter in VSCode is to press Cmd+Shift+p and then type Python Interpreter. If you select the first result, you’ll see something like the following:
Figure 1.</description>
    </item>
    
    <item>
      <title>Docker: Failed to create network: Error response from daemon: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</title>
      <link>https://www.markhneedham.com/blog/2023/07/20/docker-network-could-not-find-non-overlapping-address-pool/</link>
      <pubDate>Thu, 20 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/20/docker-network-could-not-find-non-overlapping-address-pool/</guid>
      <description>I use Docker for pretty much every demo I create and this sometimes results in me running out of IP addresses to serve all those networks. In this blog post, we’ll learn how to diagnose and solve this issue.
Our story starts with the following command on a new project:
docker compose up Usually this purs along nicely and all our components spin up just fine, but today is not our lucky day and we get the following error:</description>
    </item>
    
    <item>
      <title>Plotly: Visualising a normal distribution given average and standard deviation</title>
      <link>https://www.markhneedham.com/blog/2023/07/19/plotly-normal-distribution-average-stdev/</link>
      <pubDate>Wed, 19 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/19/plotly-normal-distribution-average-stdev/</guid>
      <description>I’ve been playing around with Microsoft’s TrueSkill algorithm, which attempts to quantify the skill of a player using the Bayesian inference algorithm. A rating in this system is a Gaussian distribution that starts with an average of 25 and a confidence of 8.333. I wanted to visualise various ratings using Plotly and that’s what we’ll be doing in this blog post.
To save you from having to install TrueSkill, we’re going to create a named tuple to simulate a TrueSkill Rating object:</description>
    </item>
    
    <item>
      <title>Redpanda: Configure pruning/retention of data</title>
      <link>https://www.markhneedham.com/blog/2023/07/18/redpanda-prune-retention/</link>
      <pubDate>Tue, 18 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/18/redpanda-prune-retention/</guid>
      <description>I wanted to test how Apache Pinot deals with data being truncated from the underlying stream from which it’s consuming, so I’ve been trying to work out how to prune data in Redpanda. In this blog post, I’ll share what I’ve learnt so far.
We’re going to spin up a Redpanda cluster using the following Docker Compose file:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: redpanda: container_name: &amp;#34;redpanda-pruning&amp;#34; image: docker.redpanda.com/vectorized/redpanda:v22.2.2 command: - redpanda start - --smp 1 - --overprovisioned - --node-id 0 - --kafka-addr PLAINTEXT://0.</description>
    </item>
    
    <item>
      <title>Puppeteer: Button click doesn&#39;t work when zoomed in</title>
      <link>https://www.markhneedham.com/blog/2023/07/17/puppeteer-button-click-not-working-after-zoom/</link>
      <pubDate>Mon, 17 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/17/puppeteer-button-click-not-working-after-zoom/</guid>
      <description>I’m still playing around with Puppeteer, a Nodejs library that provides an API to control Chrome/Chromium. I want to load the Pinot UI zoomed to 250% and then write and run some queries.
We can install Puppeteer by running the following command:
npm i puppeteer-core I then created the file drive_pinot.mjs and added the following code, which opens the Pinot query console and then clicks on the &amp;#39;Run Query&amp;#39; button:</description>
    </item>
    
    <item>
      <title>Puppeteer: Unsupported command-line flag: --enabled-blink-features=IdleDetection.</title>
      <link>https://www.markhneedham.com/blog/2023/07/13/puppeteer-unsupported-flag-enabled-blink-features-idledetection/</link>
      <pubDate>Thu, 13 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/13/puppeteer-unsupported-flag-enabled-blink-features-idledetection/</guid>
      <description>In many of the StarTree recipe videos that I’ve worked on, I show how to write queries in the Pinot UI. If I wrote these queries manually there’d be way too many typos, so I drive the UI using a script. I’ve recently been exploring whether I can do this using a Node.js library called Puppeteer and wanted to share a warning message that I ran into early doors.</description>
    </item>
    
    <item>
      <title>Redpanda: Viewing consumer group offsets from __consumer_offsets</title>
      <link>https://www.markhneedham.com/blog/2023/07/12/redpanda-consumer-group-offsets/</link>
      <pubDate>Wed, 12 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/12/redpanda-consumer-group-offsets/</guid>
      <description>Redpanda supports consumer groups, which are sets of consumers that cooperate to consume data from topics. The consumers in a group are assigned a partition and they keep track of the last consumed offset in the __consumer_offsets topic. I wanted to see how many messages had been consumed by a consumer group and that’s what we’ll explore in this post.
My first thought was to query the __consumer_offsets topic using rpk topic consume.</description>
    </item>
    
    <item>
      <title>Quix Streams: Consuming and Producing JSON messages</title>
      <link>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</link>
      <pubDate>Tue, 11 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</guid>
      <description>I’ve been meaning to take Quix Streams for a spin for a while and got the chance while building a recent demo. Quix Streams is a library for building streaming applications on time-series data, but I wanted to use it to do some basic consuming and producing of JSON messages. That’s what we’re going to do in this blog post.
We’re going to use Redpanda to store our messages. We’ll launch a Redpanda instance using the following Docker Compose file:</description>
    </item>
    
    <item>
      <title>Python: Re-import module</title>
      <link>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</link>
      <pubDate>Fri, 07 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</guid>
      <description>I often write little Python scripts that import code from other local modules and a common problem I have when using the Python REPL is that I update the code in the other modules and then can’t use the new functionality without restarting the REPL and re-importing everything. At least so I thought! It turns out there is a way to refresh those modules and that’s what we’ll be exploring in this blog post.</description>
    </item>
    
    <item>
      <title>ClickHouse: How to unpack or unnest an array</title>
      <link>https://www.markhneedham.com/blog/2023/07/03/clickhouse-unpack-unnest-array/</link>
      <pubDate>Mon, 03 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/03/clickhouse-unpack-unnest-array/</guid>
      <description>I recently came across clickhouse-local via this article in the MotherDuck monthly newsletter and I wanted to give it a try on my expected goals dataset. One of the first things that I wanted to do was unpack an array and in this blog post, we’ll learn how to do that.
I installed Clickhouse by running the following command:
curl https://clickhouse.com/ | sh And then launched the clickhouse-local CLI like this:</description>
    </item>
    
    <item>
      <title>Detecting and splitting scenes in a video</title>
      <link>https://www.markhneedham.com/blog/2023/06/30/detecting-splitting-scenes-video/</link>
      <pubDate>Fri, 30 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/30/detecting-splitting-scenes-video/</guid>
      <description>When editing videos for my YouTube channel, Learn Data with Mark, I spend a bunch of time each week chopping up a screencast into scenes that I then line up with a separately recorded voice-over. I was curious whether I could automate the chopping-up process and that’s what we’re going to explore in this blog post.
I started out by asking ChatGPT the following question:
ChatGPT Prompt I want to chop up a demo for a YouTube video into smaller segments.</description>
    </item>
    
    <item>
      <title>Python: All about the next function</title>
      <link>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</link>
      <pubDate>Wed, 28 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</guid>
      <description>Yesterday I wrote a blog post about some different ways to take the first element from a Python list. Afterward I was chatting to my new rubber duck, ChatGPT, which suggested the next function on an iterator as an alternative approach. And so that’s what we’re going to explore in this blog post.
The next function gets the first value from an iterator and optionally returns a provided default value if the iterator is empty.</description>
    </item>
    
    <item>
      <title>Python: Get the first item from a collection, ignore the rest</title>
      <link>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</link>
      <pubDate>Tue, 27 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</guid>
      <description>When writing Python scripts, I often find myself wanting to take the first item from a collection and ignore the rest of the values. I usually use something like values[0] to take the first value from the list, but I was curious whether I could do better by using destructuring. That’s what we’re going to explore in this blog post.
We’ll start with a list that contains some names:</description>
    </item>
    
    <item>
      <title>Running a Hugging Face Large Language Model (LLM) locally on my laptop</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</link>
      <pubDate>Fri, 23 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</guid>
      <description>I’ve been playing around with a bunch of Large Language Models (LLMs) on Hugging Face and while the free inference API is cool, it can sometimes be busy, so I wanted to learn how to run the models locally. That’s what we’ll be doing in this blog post.
You’ll need to install the following libraries if you want to follow along:
pip install langchain[all] huggingface-hub langchain transformers The first step is to choose a model that you want to download.</description>
    </item>
    
    <item>
      <title>LangChain: 1 validation error for LLMChain - value is not a valid dict (type=type_error.dict)</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</link>
      <pubDate>Fri, 23 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</guid>
      <description>I surely can’t be the first to make the mistake that I’m about to describe and I expect I won’t be the last! I’m still swimming in the LLM waters and I was trying to get GPT4All to play nicely with LangChain.
I wrote the following code to create an LLM chain in LangChain so that every question would use the same prompt template:
from langchain import PromptTemplate, LLMChain from gpt4all import GPT4All llm = GPT4All( model_name=&amp;#34;ggml-gpt4all-j-v1.</description>
    </item>
    
    <item>
      <title>GPT4All/LangChain: Model.__init__() got an unexpected keyword argument &#39;ggml_model&#39; (type=type_error)</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</link>
      <pubDate>Thu, 22 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</guid>
      <description>I’m starting to realise that things move insanely fast in the world of LLMs (Large Language Models) and you will run into issues because you aren’t using the latest version of libraries. I say this because I’ve been following Sami Maameri’s blog post which explains how to run an LLM on your own machine and ran into an error, which we’ll explore in this blog post.
Sami’s post is based around a library called GPT4All, but he also uses LangChain to glue things together.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: &#39;NoneType&#39; object has no attribute &#39;info&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</link>
      <pubDate>Thu, 22 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</guid>
      <description>Following on from a blog post that I wrote yesterday about doing similarity search with ChromaDB, I noticed an odd error message being printed as the script was exiting. In this blog post, we’ll explore what was going on.
To recap, I have the following code to find chunks of YouTube transcripts that are most similar to an input query:
test_chroma.py from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma hf_embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;) store = Chroma(collection_name=&amp;#34;transcript&amp;#34;, persist_directory=&amp;#34;db&amp;#34;, embedding_function=hf_embeddings) result = store.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: Index not found, please create an instance before querying</title>
      <link>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</link>
      <pubDate>Wed, 21 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</guid>
      <description>Somewhat belatedly, I’ve been playing around with LangChain and HuggingFace to spike a tool that lets me ask question about Tim Berglund’s Real-Time Analytics podcast.
I’m using the Chroma database to store vectors of chunks of the transcript so that I can find appropriate sections to feed to the Large Language Model to help with answering my questions. I ran into an initially perplexing error while building this out, which we’re going to explore in this blog post.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert string in YYYYmmdd format to Date</title>
      <link>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</link>
      <pubDate>Tue, 20 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</guid>
      <description>I’ve been working with a data set that represents dates as strings in the format &amp;#39;YYYYmmdd&amp;#39; and I wanted to convert those values to Dates in DuckDB. In this blog post, we’ll learn how to do that.
Let’s create a small table with a single column that represents date of births:
create table players (dob VARCHAR); insert into players values(&amp;#39;20080203&amp;#39;), (&amp;#39;20230708&amp;#39;); We can write the following query to return the rows in the table:</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Transpose columns to rows with UNPIVOT</title>
      <link>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</link>
      <pubDate>Tue, 13 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</guid>
      <description>I’ve been playing around with the Kaggle European Soccer dataset, which contains, amongst other things, players and their stats in the FIFA video game. I wanted to compare the stats of Ronaldo and Messi, which is where this story begins.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>GitHub: Get a CSV containing my pull requests (PRs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/12/github-list-pull-requests-csv/</link>
      <pubDate>Mon, 12 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/12/github-list-pull-requests-csv/</guid>
      <description>I wanted to get a list of my GitHub pull requests (PRs) and commits, which was surprisingly difficult to figure out how to do. I’m sure it must be possible to get this data from the API, but it was a lot easier to figure out how to do so with the GitHub CLI.
This blog post explains how to use the GitHub CLI on the Mac OS terminal. If you’re trying to do this on Windows, see Get a CSV of all my pull requests from Github using Github CLI and PowerShell.</description>
    </item>
    
    <item>
      <title>Creating LinkedIn Carousel/Slides</title>
      <link>https://www.markhneedham.com/blog/2023/06/08/linkedin-slides-carousel/</link>
      <pubDate>Thu, 08 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/08/linkedin-slides-carousel/</guid>
      <description>If you’ve been using LinkedIn recently, you’ve likely seen those posts where people post slides in a kind of carousel that you can horizontally scroll. I wanted to create one to explain Apache Pinot’s Upserts feature, but I wasn’t sure how to create one.
Since it’s 2023, I started by asking ChatGPT:
Figure 1. ChatGPT doesn’t know about LinkedIn Carousel Unfortunately ChatGPT doesn’t know how to do it, but Harrison Avisto does and was happy to teach me.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Pivot - 0 if null</title>
      <link>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</link>
      <pubDate>Wed, 07 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</guid>
      <description>I’ve been learning all about the PIVOT function that was recently added in DuckDB and I ran into an issue where lots of the cells in my post PIVOT table were null values. In this blog post, we’ll learn how to replace those nulls with 0s (or indeed any other value).
Setup I’m working with Jeff Sackmann’s tennis dataset, which I loaded by running the following query:
CREATE OR REPLACE TABLE matches AS SELECT * FROM read_csv_auto( list_transform( range(1968, 2023), y -&amp;gt; &amp;#39;https://raw.</description>
    </item>
    
    <item>
      <title>Kafka/Kubernetes: Failed to resolve: nodename nor servname provided, or not known</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</guid>
      <description>I’ve been trying out the Running Pinot in Kubernetes tutorial and ran into a problem trying to write data to Kafka. In this blog we’ll explore how I got around that problem.
I’m using Helm with Kubernetes and started a Kafka service by running the following:
helm repo add kafka https://charts.bitnami.com/bitnami helm install -n pinot-quickstart kafka kafka/kafka --set replicas=1,zookeeper.image.tag=latest I waited until the service had started and then ran the following command to port forward the Kafka service’s port 9092 to port 9092 on my host OS:</description>
    </item>
    
    <item>
      <title>Python: Working with tuples in lambda expressions</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</guid>
      <description>I’m still playing around with data returned by Apache Pinot’s HTTP API and I wanted to sort a dictionary of segment names by partition id and index. In this blog post we’re going to look into how to do that.
We’ll start with the following dictionary:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__13__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } As I mentioned above, I want to sort the dictionary’s items by partition id and index, which are embedded inside the key name.</description>
    </item>
    
    <item>
      <title>Python: Padding a string</title>
      <link>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</link>
      <pubDate>Mon, 05 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</guid>
      <description>I’ve been writing some scripts to parse data from Apache Pinot’s HTTP API and I wanted to format the values stored in a map to make them more readable. In this blog post, we’ll look at some ways that I did that.
I started with a map that looked a bit like this:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } And then I iterated over and printed each item like this:</description>
    </item>
    
    <item>
      <title>DuckDB: Generate dummy data with user defined functions (UDFs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</link>
      <pubDate>Fri, 02 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</guid>
      <description>In the 0.8 release of DuckDB, they added functionality that lets you add your own functions when using the Python package I wanted to see if I could use it to generate dummy data so that’s what we’re going to do in this blog post.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Debezium: Capture changes from MySQL</title>
      <link>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</link>
      <pubDate>Wed, 31 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</guid>
      <description>I’ve been working on a Real-Time Analytics workshop that I’m going to be presenting at the ODSC Europe conference in June 2023 and I wanted to have Debezium publish records from a MySQL database without including the schema.
I’m using the debezium/connect:2.3 Docker image to run Debezium locally and I have a MySQL database running with the hostname mysql on port 3306. Below is the way that I configured this:</description>
    </item>
    
    <item>
      <title>Node.js: Minifying JSON documents</title>
      <link>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</link>
      <pubDate>Tue, 30 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</guid>
      <description>I often need to minimise the schema and table config files that you use to configure Apache Pinot so that they don’t take up so much space. After doing this manually for ages, I came across the json-stringify-pretty-compact library, which speeds up the process.
We can install it like this:
npm install json-stringify-pretty-compact And then I have the following script:
minify.mjs import pretty from &amp;#39;json-stringify-pretty-compact&amp;#39;; let inputData = &amp;#39;&amp;#39;; process.</description>
    </item>
    
    <item>
      <title>DuckDB: Ingest a bunch of CSV files from GitHub</title>
      <link>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</link>
      <pubDate>Thu, 25 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</guid>
      <description>Jeff Sackmann’s tennis_atp repository is one of the best collections of tennis data and I wanted to ingest the ATP Tour singles matches using the DuckDB CLI. In this blog post we’ll learn how to do that.
Usually when I’m ingesting data into DuckDB I’ll specify the files that I want to ingest using the wildcard syntax. In this case that would mean running a query like this:
CREATE OR REPLACE TABLE matches AS SELECT * FROM &amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10); Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>Arc Browser: Building a plugin (Boost) with help from ChatGPT</title>
      <link>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</link>
      <pubDate>Tue, 23 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</guid>
      <description>I’ve been using the Arc Browser for a couple of months now and one of my favourite things is the simplicity of the plugin (or as they call it, &amp;#39;Boost&amp;#39;) functionality.
I wanted to port over a Chrome bookmark that I use to capture the podcasts that I’ve listened to on Player.FM. In this blog post I’ll show how ChatGPT helped me convert the bookmark code to an Arc Boost.</description>
    </item>
    
    <item>
      <title>Cropping a video using FFMPEG</title>
      <link>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</link>
      <pubDate>Mon, 15 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</guid>
      <description>I needed to crop a video that I used as part of a video on my YouTube channel, Learn Data With Mark, and Camtasia kept rendering a black screen. So I had to call for FFMPEG!
Cropping the bottom of a video My initial video was 2160 x 3840 but I didn’t need the bottom 1920 pixels because I’m using that part of the screen for a video of me.</description>
    </item>
    
    <item>
      <title>Python: Naming slices</title>
      <link>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</link>
      <pubDate>Sat, 13 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</guid>
      <description>Another gem from Fluent Python is that you can name slices. How did I not know that?!
Let’s have a look how it works using an example of a Vehicle Identification Number, which has 17 characters that act as a unique identifier for a vehicle. Different parts of that string mean different things.
So given the following VIN:
vin = &amp;#34;2B3HD46R02H210893&amp;#34; We can extract components like this:
print(f&amp;#34;&amp;#34;&amp;#34; World manufacturer identifier: {vin[0:3]} Vehicle Descriptor: {vin[3:9]} Vehicle Identifier: {vin[9:17]} &amp;#34;&amp;#34;&amp;#34;.</description>
    </item>
    
    <item>
      <title>Python 3.10: Pattern matching with match/case</title>
      <link>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</link>
      <pubDate>Tue, 09 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</guid>
      <description>I’ve been reading Fluent Python and learnt about pattern matching with the match/case statement, introduced in Python 3.10. You can use it instead of places where you’d otherwise use if, elif, else statements.
I created a small example to understand how it works. The following function takes in a list where the first argument should be foo, followed by a variable number of arguments, which we print to the console:</description>
    </item>
    
  </channel>
</rss>
