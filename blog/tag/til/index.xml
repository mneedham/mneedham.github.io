<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>til on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/til/</link>
    <description>Recent content in til on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/til/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PIVOTing data in ClickHouse and DuckDB</title>
      <link>https://www.markhneedham.com/blog/2024/11/15/pivot-clickhouse-duckdb/</link>
      <pubDate>Fri, 15 Nov 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/11/15/pivot-clickhouse-duckdb/</guid>
      <description>I really like DuckDB’s PIVOT clause and along with some others wish that ClickHouse supported it too. Sadly it doesn’t, but we can get pretty close to this functionality using ClickHouse’s aggregate function combinators. In this blog post, I’m going to go through each of the examples in the DuckDB documentation and show how to do the equivalent in ClickHouse.
Set up First, we need to setup the sample data.</description>
    </item>
    
    <item>
      <title>ClickHouse: A hacky way to default parameters in a view</title>
      <link>https://www.markhneedham.com/blog/2024/11/25/clickhouse-view-hacky-default-parameters/</link>
      <pubDate>Fri, 25 Oct 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/11/25/clickhouse-view-hacky-default-parameters/</guid>
      <description>ClickHouse recently added support for runtime provided parameters in views, so I wanted to try it when querying the MidJourney messages dataset. It worked pretty well, but I ran into problems when trying to define default parameters, which is what we’re going to explore in this blog post.
Let’s launch ClickHouse Local:
clickhouse -m --max_http_get_redirects=10 --output_format_pretty_row_numbers=0 We need to set max_http_get_redirects so that it can handle redirects in the Hugging Face URL, and output_format_pretty_row_numbers is so that it won’t put numbers in front of each result row.</description>
    </item>
    
    <item>
      <title>LLMs on the command line</title>
      <link>https://www.markhneedham.com/blog/2024/10/25/llms-on-command-line/</link>
      <pubDate>Fri, 25 Oct 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/10/25/llms-on-command-line/</guid>
      <description>I’ve been playing around with Simon Willison’s llm library over the last week and I have to say I love it! If you want to use LLMs on the command line, this is the tool you need.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
Installing llm Let’s have a look at how to use it, starting with installation.</description>
    </item>
    
    <item>
      <title>Ollama: Multiple prompts on vision models</title>
      <link>https://www.markhneedham.com/blog/2024/10/06/ollama-multi-prompts-vision-models/</link>
      <pubDate>Sun, 06 Oct 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/10/06/ollama-multi-prompts-vision-models/</guid>
      <description>In this blog post, we’re going to learn how to send multiple prompts to vision models when using Ollama. This isn’t super well documented, but it is possible!
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
Let’s import Ollam:
import ollama We’re going to call the ollama.</description>
    </item>
    
    <item>
      <title>Running OpenAI Whisper Turbo on a Mac with insanely-fast-whisper</title>
      <link>https://www.markhneedham.com/blog/2024/10/02/insanely-fast-whisper-running-openai-whisper-turbo-mac/</link>
      <pubDate>Wed, 02 Oct 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/10/02/insanely-fast-whisper-running-openai-whisper-turbo-mac/</guid>
      <description>A couple of days ago OpenAI released a new version of Whisper, their audio to text model. It’s called Turbo and we can run it on a Mac using the insanely-fast-whisper library.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
I like trying out this models on podcasts and a recent favourite is The AI Daily Brief, so we’re going to download an MP3 file from a recent episode about some executive departures at OpenAI.</description>
    </item>
    
    <item>
      <title>An intro to rerankers</title>
      <link>https://www.markhneedham.com/blog/2024/09/28/intro-to-rerankers/</link>
      <pubDate>Sat, 28 Sep 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/09/28/intro-to-rerankers/</guid>
      <description>rerankers provides a unified API for various reranking models, including any that use transformers, FlashRank, RankGPT, RankLLM, and more. In this blog, we’ll take it for a spin.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
But first, let’s remind ourselves what reranking is. A basic RAG pipeline would look like this:</description>
    </item>
    
    <item>
      <title>DuckDB 1.1: Dynamic Column Selection gets even better</title>
      <link>https://www.markhneedham.com/blog/2024/09/22/duckdb-dynamic-column-selection/</link>
      <pubDate>Sun, 22 Sep 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/09/22/duckdb-dynamic-column-selection/</guid>
      <description>DuckDB 1.1 was released a couple of weeks ago and there are a couple of features that make dynamic column selection even better. We’re going to explore those features in this blog.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:
Kaggle’s FIFA 2022 Dataset To demonstrate dynamic column selection, we need a dataset that has a lot of columns, ideally one containing lots of numeric values as well.</description>
    </item>
    
    <item>
      <title>DuckDB: Chaining functions</title>
      <link>https://www.markhneedham.com/blog/2024/08/25/duckdb-chaining-functions/</link>
      <pubDate>Sun, 25 Aug 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/08/25/duckdb-chaining-functions/</guid>
      <description>One of my favourite things about DuckDB is the innovations it’s made in SQL. A recent discovery (for me at least) is that you can chain functions using the dot operator, in the same way you can in many general purpose programming languages. In this blog, we’re going to explore that functionality.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Searching through AWS Icons</title>
      <link>https://www.markhneedham.com/blog/2024/08/23/searching-aws-icons/</link>
      <pubDate>Fri, 23 Aug 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/08/23/searching-aws-icons/</guid>
      <description>I recently needed to search for an icon in the AWS [asset package](https://aws.amazon.com/architecture/icons/) and wanted to share a little script that I wrote. You wouldn’t think that searching for icons should be that hard, but they’re spread across so many folders and sub-folders that you can spend forever trying to find what you want.
First, let’s import some modules:
import base64 import sys import glob import os And then I’m using the following function to render images in the terminal:</description>
    </item>
    
    <item>
      <title>ClickHouse: Specifying config settings</title>
      <link>https://www.markhneedham.com/blog/2024/08/05/clickhouse-config-settings/</link>
      <pubDate>Mon, 05 Aug 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/08/05/clickhouse-config-settings/</guid>
      <description>We recently had a question on ClickHouse Community Slack about configuring the network_compression_method on an individual query basis and across all requests. Let’s see how to do just that in this blog post.
Set up Let’s start by downloading and running the ClickHouse Server:
curl https://clickhouse.com/ | sh ./clickhouse server Output 2024.08.05 12:01:54.701406 [ 85587882 ] {} &amp;lt;Information&amp;gt; Application: Listening for http://[::1]:8123 2024.08.05 12:01:54.701426 [ 85587882 ] {} &amp;lt;Information&amp;gt; Application: Listening for native protocol (tcp): [::1]:9000 2024.</description>
    </item>
    
    <item>
      <title>Hybrid Search in SQL with DuckDB</title>
      <link>https://www.markhneedham.com/blog/2024/07/28/hybrid-search-sql-duckdb/</link>
      <pubDate>Sun, 28 Jul 2024 01:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/07/28/hybrid-search-sql-duckdb/</guid>
      <description>I’ve been playing around with different approaches for Retrieval Augmented Generation (RAG) recently and came across a blog post describing Reciprocal Rank Fusion, a hybrid search technique. In this blog post, we’re going to explore how to apply this method in SQL using DuckDB.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB: Create a function in SQL</title>
      <link>https://www.markhneedham.com/blog/2024/07/28/duckdb-create-function-sql/</link>
      <pubDate>Sun, 28 Jul 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/07/28/duckdb-create-function-sql/</guid>
      <description>I’ve been learning about Hybrid Search via this blog post, which describes the Reciprocal Rank Fusion algorithm, and I wanted to implement and use it in a DuckDB query.
The formula for the function is shown below:
RRF(d) = Σ(r ∈ R) 1 / (k + r(d))
Where:
d is a document
R is the set of rankers (retrievers)
k is a constant (typically 60)
r(d) is the rank of document d in ranker r</description>
    </item>
    
    <item>
      <title>ClickHouse: Unknown setting &#39;allow_nullable_key&#39;</title>
      <link>https://www.markhneedham.com/blog/2024/06/27/clickhouse-unknown-setting-allow_nullable_key/</link>
      <pubDate>Thu, 27 Jun 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/06/27/clickhouse-unknown-setting-allow_nullable_key/</guid>
      <description>I’ve been playing around with ClickHouse’s Amazon reviews dataset and ran into an interesting problem when trying to set the allow_nullable_key setting. In this blog post, we’ll learn how and why we might choose to set it.
I started off with the following SQL statement to create a table called reviews based on the structure of the Parquet file:
CREATE TABLE reviews ENGINE = MergeTree ORDER BY review_date EMPTY AS ( SELECT * FROM s3(concat( &amp;#39;https://datasets-documentation.</description>
    </item>
    
    <item>
      <title>Mistral 7B function calling with llama.cpp</title>
      <link>https://www.markhneedham.com/blog/2024/06/23/mistral-7b-function-calling-llama-cpp/</link>
      <pubDate>Sun, 23 Jun 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/06/23/mistral-7b-function-calling-llama-cpp/</guid>
      <description>Mistral AI recently released version 3 of their popular 7B model and this one is fine-tuned for function calling. Function calling is a confusing name because the LLM isn’t doing any function calling itself. Instead, it takes a prompt and can then tell you which function you should call in your code and with which parameters.
In this blog post, we’re going to learn how to use this functionality with llama.</description>
    </item>
    
    <item>
      <title>Side by side LLMs with Ollama and Streamlit</title>
      <link>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</link>
      <pubDate>Sat, 11 May 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/05/11/side-by-side-local-llms-ollama-streamlit/</guid>
      <description>The recent 0.1.33 release of Ollama added experimental support for running multiple LLMs or the same LLM in parallel. But, to compare models on the same prompt we need a UI and that’s what we’re going to build in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Semantic Router: Stop LLM chatbots going rogue</title>
      <link>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</link>
      <pubDate>Sun, 14 Apr 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</guid>
      <description>A tricky problem when deploying LLM-based chatbots is working out how to stop them from talking about topics that you don’t want them to talk about. Even with the cleverest prompts, with enough effort and ingenuity, users will figure a way around the guard rails.
However, I recently came across a library called Semantic Router, which amongst other things, seems to provide a solution to this problem. In this blog post, we’re going to explore Semantic Router and see if we can create a chatbot that only talks about a pre-defined set of topics.</description>
    </item>
    
    <item>
      <title>llama.cpp - ValueError: Failed to create llama_context - ggml-common.h file not found</title>
      <link>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</link>
      <pubDate>Sun, 31 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</guid>
      <description>I’ve been playing around with the outlines library and needed to install llama.cpp as a result. I ran into trouble when trying to offload model layers to the GPU and in this post, I’ll explain how to install llama.cpp so that you don’t have the same issues.
This was how I installed the library initially:
CMAKE_ARGS=&amp;#34;-DLLAMA_METAL=on&amp;#34; pip install llama-cpp-python And then let’s try to load a GGUF model with some layers offloaded to the GPU:</description>
    </item>
    
    <item>
      <title>DuckDB 0.10: Binder Error: No function matches the given name and argument types</title>
      <link>https://www.markhneedham.com/blog/2024/03/09/duckdb-strptime-binder-error-no-function-matches/</link>
      <pubDate>Sat, 09 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/09/duckdb-strptime-binder-error-no-function-matches/</guid>
      <description>In the 0.10 version of DuckDB, a breaking change was made that stops implicit casting to VARCHAR during function binding. In this blog post, we’re going to look at some ways to work around this change when fixing our DuckDB code from 0.9 or earlier.
I have a CSV file that looks like this:
from &amp;#39;people.csv&amp;#39; select *; Output ┌─────────┬─────────────┐ │ name │ dateOfBirth │ │ varchar │ int64 │ ├─────────┼─────────────┤ │ John │ 19950105 │ └─────────┴─────────────┘ The dateOfBirth column isn’t an int64, but that’s how DuckDB has inferred it.</description>
    </item>
    
    <item>
      <title>Clustering YouTube comments using Ollama Embeddings</title>
      <link>https://www.markhneedham.com/blog/2024/02/27/clustering-youtube-comments-ollama-embeddings-nomic/</link>
      <pubDate>Tue, 27 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/27/clustering-youtube-comments-ollama-embeddings-nomic/</guid>
      <description>One of my favourite tools in the LLM space is Ollama and if you want to learn how to use it, there’s no better place than Matt Williams&amp;#39; YouTube channel. His videos get a lot of comments and they tend to contain a treasure trove of the things that people are thinking about and the questions that they have. Matt recently did a video about embeddings in Ollama and I thought it’d be fun to try to get a high-level overview of what’s happening in the comments section.</description>
    </item>
    
    <item>
      <title>python-youtube: Retrieving multiple pages using page token</title>
      <link>https://www.markhneedham.com/blog/2024/02/26/python-youtube-data-page-token/</link>
      <pubDate>Mon, 26 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/26/python-youtube-data-page-token/</guid>
      <description>I’ve been playing around with the YouTube API to analyse comments on YouTube videos and needed to use pagination to get all the comments. In this blog post, we’ll learn how to do that.
But before we do anything, you’ll need to go to console.developers.google.com, create a project and enable YouTube Data API v3.
Figure 1. YouTube Data API Once you’ve done that, create an API key.
Figure 2. Creating an API key Create an environment variable that contains your API key:</description>
    </item>
    
    <item>
      <title>Using environment variables in ClickHouse queries</title>
      <link>https://www.markhneedham.com/blog/2024/02/23/clickhouse-environment-variables/</link>
      <pubDate>Fri, 23 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/23/clickhouse-environment-variables/</guid>
      <description>For quite some time I’ve been wondering how to get access to an environment variable in a ClickHouse Local and finally today I have a solution, which we’ll explore in this blog post.
My reason for wanting to do this is so that I can pass through a ClickHouse Cloud password to use in a remoteSecure function call. I wanted to do this as part of a blog post I wrote showing how to do Hybrid Query Execution with ClickHouse.</description>
    </item>
    
    <item>
      <title>Render a CSV across multiple columns on the terminal/shell</title>
      <link>https://www.markhneedham.com/blog/2024/02/20/shell-render-csv-multiple-columns/</link>
      <pubDate>Tue, 20 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/20/shell-render-csv-multiple-columns/</guid>
      <description>I was recently working with a CSV file that contained a bunch of words and I wanted to render them on the console so that you could see all of them at once without any scrolling. i.e. I wanted the rendering of the CSV file to wrap across columns.
I learned that we can do exactly this using the paste command, so let’s see how to do it.
Imagine we have the CSV file shown below:</description>
    </item>
    
    <item>
      <title>Qdrant/FastEmbed: Content discovery for my blog posts</title>
      <link>https://www.markhneedham.com/blog/2024/02/11/qdrant-fast-embed-content-discovery/</link>
      <pubDate>Sun, 11 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/11/qdrant-fast-embed-content-discovery/</guid>
      <description>I was recently reading Simon Willison’s blog post about embedding algorithms in which he described how he’d used them to create a &amp;#39;related posts&amp;#39; section on his blog post. So, of course, I wanted to see whether I could do the same for my blog as well.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>LLaVA 1.5 vs. 1.6</title>
      <link>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</link>
      <pubDate>Sun, 04 Feb 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/02/04/llava-large-multi-modal-model-v1.5-v1.6/</guid>
      <description>LLaVA (or Large Language and Vision Assistant), an open-source large multi-modal model, just released version 1.6. It claims to have improvements over version 1.5, which was released a few months ago:
Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.
Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.</description>
    </item>
    
    <item>
      <title>Ollama is on PyPi</title>
      <link>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</link>
      <pubDate>Sun, 28 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/28/ollama-now-on-pypi/</guid>
      <description>This week Ollama released a Python/PyPi library to go with their awesome tool for running LLMs on your own machine. You still need to download and run Ollama, but after that you can do almost everything from the library. In this blog post, we’re going to take it for a spin.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>ClickHouse: Configure default output format</title>
      <link>https://www.markhneedham.com/blog/2024/01/19/clickhouse-configure-output-format/</link>
      <pubDate>Fri, 19 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/19/clickhouse-configure-output-format/</guid>
      <description>When running queries with ClickHouse Local, the results are rendered back to the screen in a table format in blocks. This default format is called PrettyCompact and most of the time this works fine, but sometimes you can end up with multiple mini-tables. In this blog post, we’re going to learn how to change the default format so that all the results show in one table.
But first, let’s see how the problem manifests.</description>
    </item>
    
    <item>
      <title>An introduction to Retrieval Augmented Generation</title>
      <link>https://www.markhneedham.com/blog/2024/01/12/intro-to-retrieval-augmented-generation/</link>
      <pubDate>Fri, 12 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/12/intro-to-retrieval-augmented-generation/</guid>
      <description>Retrieval Augmented Generation (RAG) is a technique used with Large Language Models (LLM) where you augment the prompt with data retrieved from a data store so that the LLM can generate a better answer to the question that is being asked. In this blog post, we’re going to learn the basics of RAG by creating a Question and Answer system on top of the 2023 Wimbledon Championships Wikipedia page.</description>
    </item>
    
    <item>
      <title>Pandas: Exclude columns using regex</title>
      <link>https://www.markhneedham.com/blog/2024/01/05/pandas-exclude-columns-regex/</link>
      <pubDate>Fri, 05 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/05/pandas-exclude-columns-regex/</guid>
      <description>After a few months of using ClickHouse, I’ve got quite used to using the SELECT &amp;lt;expr&amp;gt; EXCEPT modifier, which lets you remove columns based on a regular expression. I wanted to do something similar when working with some data in Pandas and in this blog we’ll explore how to do that.
We’re gonna be working with a CSV file of UK energy and gas tariffs for one of the energy providers.</description>
    </item>
    
    <item>
      <title>ClickHouse: Float equality</title>
      <link>https://www.markhneedham.com/blog/2024/01/04/clickhouse-float-equality/</link>
      <pubDate>Thu, 04 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/04/clickhouse-float-equality/</guid>
      <description>I’ve been playing around with NumPy data in ClickHouse this week and wanted to share what I learnt when checking for equality of float values. Let’s get going!
Creating arrays We’re going to use Python’s NumPy library to create 5 arrays containing 10 values each:
import numpy as np rng = np.random.default_rng(seed=42) rng.random(size=(5, 5)) Output array([[0.28138389, 0.29359376, 0.66191651, 0.55703215, 0.78389821], [0.66431354, 0.40638686, 0.81402038, 0.16697292, 0.02271207], [0.09004786, 0.72235935, 0.46187723, 0.16127178, 0.</description>
    </item>
    
    <item>
      <title>nvim: Unable to create directory for swap file - recovery impossible: permission denied</title>
      <link>https://www.markhneedham.com/blog/2024/01/03/nvim-swap-file-permission-denied/</link>
      <pubDate>Wed, 03 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/03/nvim-swap-file-permission-denied/</guid>
      <description>I was playing around with neovim last week and despite installing it via Homebrew, ran into a weird permissions error. In this blog post, I’ll describe the problem I had and how to solve it.
I installed it like this:
brew install nvim And then tried to create a new file:
nvim foo.py Which resulted in the following error:
Output E303: Unable to create directory &amp;#34;/Users/markhneedham/.local/state/nvim&amp;#34; for swap file, recovery impossible: permission denied E303: Unable to open swap file for &amp;#34;foo.</description>
    </item>
    
    <item>
      <title>ClickHouse: How does a number have a set number of decimal places?</title>
      <link>https://www.markhneedham.com/blog/2024/01/02/clickhouse-set-number-decimal-places/</link>
      <pubDate>Tue, 02 Jan 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/01/02/clickhouse-set-number-decimal-places/</guid>
      <description>I’ve been working with a dataset in ClickHouse where I compute currency values and I really struggled to figure out how to get numbers whose decimal part is divisible by 10 to have a fixed number of decimal places. If you want to do that too, hopefully, this blog post will help.
Let’s start by seeing what happens if we output the number 12.40
SELECT 12.40 AS number; Output ┌─number─┐ │ 12.</description>
    </item>
    
    <item>
      <title>Experimenting with insanely-fast-whisper</title>
      <link>https://www.markhneedham.com/blog/2023/12/23/insanely-fast-whisper-experiments/</link>
      <pubDate>Sat, 23 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/23/insanely-fast-whisper-experiments/</guid>
      <description>I recently came across insanely-fast-whisper, a CLI tool that you can use to transcribe audio files using OpenAI’s whisper-large-v3 model or other smaller models. In this blog post, I’ll summarise my experience using it to transcribe one of Scott Galloway’s podcast episodes.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Generating sample JSON data in S3 with shadowtraffic.io</title>
      <link>https://www.markhneedham.com/blog/2023/12/22/sample-data-s3-shadowtraffic/</link>
      <pubDate>Fri, 22 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/22/sample-data-s3-shadowtraffic/</guid>
      <description>I needed to quickly generate some data to write to S3 for a recent video on the ClickHouse YouTube channel and it seemed like a good opportunity to try out ShadowTraffic.
ShadowTraffic is a tool being built by Michael Drogalis and it simulates production traffic based on a JSON file that you provide. Michael is documenting the process of building ShadowTraffic on his Substack newsletter.
Michael gave me a free license to use for a few months as a &amp;#39;thank you&amp;#39; for giving him some feedback on the product, but there is also a free version of the tool.</description>
    </item>
    
    <item>
      <title>litellm and llamafile -  APIError: OpenAIException - File Not Found</title>
      <link>https://www.markhneedham.com/blog/2023/12/14/litellm-apierror-openaiexception-file-not-found/</link>
      <pubDate>Thu, 14 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/14/litellm-apierror-openaiexception-file-not-found/</guid>
      <description>I wanted to get two of my favourite tools in the LLM world - llmlite and llamafile - to play nicely and ran into an issue that I’ll explain in this blog post. This should be helpful if you’re trying to wire up other LLM servers to llmlite, it’s not specific to llamafile.
Setting up llamafile In case you want to follow along, I downloaded llamafile and MistralAI 7B weights from TheBloke/Mistral-7B-v0.</description>
    </item>
    
    <item>
      <title>ClickHouse: S3Queue Table Engine -  DB::Exception: There is no Zookeeper configuration in server config</title>
      <link>https://www.markhneedham.com/blog/2023/12/13/clickhouse-s3queue-no-zookeeper-configuration/</link>
      <pubDate>Wed, 13 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/13/clickhouse-s3queue-no-zookeeper-configuration/</guid>
      <description>This week I’ve been making a video showing how to use ClickHouse’s S3Queue table engine, which allows streaming import of files in an S3 bucket. The S3Queue table engine was released in version 23.8, but only received &amp;#39;production-ready&amp;#39; status in version 23.11. In this blog post, we’ll walk through the steps to getting this to work locally and the mistakes that I made along the way.
I configured an S3 bucket, added 10 files containing 100,000 rows of JSON each, and made sure that I’d set the AWS_PROFILE environment variable so that ClickHouse Server could read from the bucket.</description>
    </item>
    
    <item>
      <title>Dask: Parallelising file downloads</title>
      <link>https://www.markhneedham.com/blog/2023/12/11/dash-parallelise-file-downloads/</link>
      <pubDate>Mon, 11 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/11/dash-parallelise-file-downloads/</guid>
      <description>Before a recent meetup talk that I did showing how to do analytics on your laptop with ClickHouse Local at Aiven’s Open Source Data Infrastructure Meetup, I needed to download a bunch of Parquet files from Hugging Face’s midjourney-messages dataset. I alternate between using wget/curl or a Python script to do this type of work.
This time I used Python’s requests library and I had the following script which downloads the Parquet files that I haven’t already downloaded.</description>
    </item>
    
    <item>
      <title>ClickHouse: Tuples - Code: 47. DB::Exception: Missing columns: while processing query:</title>
      <link>https://www.markhneedham.com/blog/2023/12/04/clickhouse-tuples-missing-columns/</link>
      <pubDate>Mon, 04 Dec 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/12/04/clickhouse-tuples-missing-columns/</guid>
      <description>I’ve been playing around with the Mid Journey Parquet metadata that I wrote about in my last blog post and struggled quite a bit to get the query to do what I wanted. Come along on a journey with me and we’ll figure it out together.
We’re querying the metadata of a Parquet file that contains the metadata (I know!) of images created by the Mid Journey generative AI service.</description>
    </item>
    
    <item>
      <title>Summing columns in remote Parquet files using ClickHouse</title>
      <link>https://www.markhneedham.com/blog/2023/11/15/clickhouse-summing-columns-remote-files/</link>
      <pubDate>Wed, 15 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/15/clickhouse-summing-columns-remote-files/</guid>
      <description>I’m an avid reader of Simon Willison’s TIL blog and enjoyed a recent post showing how to sum the size of all the Midjourney images stored on Discord. He did this by querying a bunch of Parquet files stored on Hugging Face with DuckDB. I was curious whether I could do the same thing using ClickHouse and in this blog post, we’re going to find out.
The dataset that we’re going to use is available at vivym/midjourney-messages.</description>
    </item>
    
    <item>
      <title>ClickHouse - How to get the first &#39;n&#39; values from an array</title>
      <link>https://www.markhneedham.com/blog/2023/11/09/clickhouse-array-first-n-values/</link>
      <pubDate>Thu, 09 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/09/clickhouse-array-first-n-values/</guid>
      <description>I was recently working with some very long arrays in ClickHouse and I wanted to select just a few values so that they didn’t take up the entire screen. The way I thought would &amp;#39;just work&amp;#39; ™ didn’t, so this blog documents how to do it.
If you want to follow along, you’ll need to install ClickHouse. On a Mac, Brew is a pretty good option:
brew install clickhouse Once you’ve done that, launch ClickHouse Local:</description>
    </item>
    
    <item>
      <title>ClickHouse - AttributeError: &#39;NoneType&#39; object has no attribute &#39;array&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/11/08/clickhouse-client-array-nonetype-no-attribute/</link>
      <pubDate>Wed, 08 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/08/clickhouse-client-array-nonetype-no-attribute/</guid>
      <description>I was querying a ClickHouse server from a Python script a couple of days ago and ran into an error message when trying to create a Pandas DataFrame. In this blog, we’ll see the error message and how to fix it.
I’m gonna assume that we have a ClickHouse Server running and we’re going to connect to it like this:
./clickhouse client Output ClickHouse client version 23.10.1.1709 (official build). Connecting to localhost:9000 as user default.</description>
    </item>
    
    <item>
      <title>ClickHouse - DB::Exception:: there is no writeable access storage in user directories (ACCESS_STORAGE_FOR_INSERTION_NOT_FOUND)</title>
      <link>https://www.markhneedham.com/blog/2023/11/07/clickhouse-no-writeable-access-storage/</link>
      <pubDate>Tue, 07 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/07/clickhouse-no-writeable-access-storage/</guid>
      <description>I’ve been working with ClickHouse’s access control/account management as part of a video that I created showing how to login to a ClickHouse server with an SSH key, but getting it all setup locally was a bit fiddly. In this blog post, we’ll go through the mistakes I made and how to fix them.
I initially tried starting the ClickHouse server:
./clickhouse server Connecting to it with a client:</description>
    </item>
    
    <item>
      <title>ClickHouse: Convert date or datetime to epoch</title>
      <link>https://www.markhneedham.com/blog/2023/11/06/clickhouse-date-to-epoch/</link>
      <pubDate>Mon, 06 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/06/clickhouse-date-to-epoch/</guid>
      <description>I’ve been working with dates in ClickHouse today and I wanted to convert some values into epoch seconds/milliseconds to use with another tool. We’re going to document how to do that in this blog post, for future me if no one else.
Let’s start an instance of ClickHouse Local:
clickhouse local -m And now we’ll write a query that returns the current date/time:
SELECT now() AS time; Output ┌────────────────time─┐ │ 2023-11-06 14:58:19 │ └─────────────────────┘ If we want to convert this value to epoch seconds, we can use the toUnixTimestamp function.</description>
    </item>
    
    <item>
      <title>ClickHouse: Nested type Array(String) cannot be inside Nullable type (ILLEGAL_TYPE_OF_ARGUMENT)</title>
      <link>https://www.markhneedham.com/blog/2023/11/03/clickhouse-nested-type-cannot-be-inside-nullable-type/</link>
      <pubDate>Fri, 03 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/03/clickhouse-nested-type-cannot-be-inside-nullable-type/</guid>
      <description>I’ve been working with some data that’s in CSV format but has tab-separated values in some columns. In this blog post, we’re going to learn how to process that data in ClickHouse.
The CSV file that we’re working with looks like this:
Table 1. data.csv value foo	bar
We’ll launch ClickHouse Local (clickhouse local) and then run the following:
FROM file(&amp;#39;data.csv&amp;#39;, CSVWithNames) SELECT *; Output ┌─value─────┐ │ foo bar │ └───────────┘ Let’s try to split the value field on tab using the splitByString function:</description>
    </item>
    
    <item>
      <title>Poetry: OSError: Precompiled binaries are not available for the current platform. Please reinstall from source</title>
      <link>https://www.markhneedham.com/blog/2023/11/02/poetry-precompiled-binaries-not-available/</link>
      <pubDate>Thu, 02 Nov 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/11/02/poetry-precompiled-binaries-not-available/</guid>
      <description>I’ve been playing around with the CTransformers library recently and getting it installed via Poetry was a bit fiddly. In this post, we’ll run through what I’ve ended up doing.
If we try to add the library in the usual way:
poetry add ctransformers We’ll get the following error:
Output OSError: Precompiled binaries are not available for the current platform. Please reinstall from source using: pip uninstall ctransformers --yes CT_METAL=1 pip install ctransformers --no-binary ctransformers Instead, we need to call the following command to tell Poetry to install the library from source:</description>
    </item>
    
    <item>
      <title>iPython: How to disable autocomplete</title>
      <link>https://www.markhneedham.com/blog/2023/10/29/ipython-disable-autocomplete/</link>
      <pubDate>Sun, 29 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/29/ipython-disable-autocomplete/</guid>
      <description>I’ve been toying with the idea of using iPython as the Python REPL for videos on @LearnDataWithMark, but I wanted to disable the autocomplete functionality as I find it too distracting. In this blog post, I’ll show how to do it.
First, let’s install iPython:
poetry add ipython And now we’ll launch the iPython REPL:
poetry run ipython Output Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)] Type &amp;#39;copyright&amp;#39;, &amp;#39;credits&amp;#39; or &amp;#39;license&amp;#39; for more information IPython 8.</description>
    </item>
    
    <item>
      <title>Poetry: Install does not contain any element</title>
      <link>https://www.markhneedham.com/blog/2023/10/26/poetry-install-does-not-contain-any-element/</link>
      <pubDate>Thu, 26 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/26/poetry-install-does-not-contain-any-element/</guid>
      <description>I’ve run into an interesting error a few times when using the Poetry package manager over the last few weeks and wanted to document it in case anyone else has the same problem. I’m still not sure how to avoid it in the first place, so if you know, please let me know!
Anyway, let’s get started. Imagine we’re creating a new project and we type the following:
$ poetry init It will pop up the following dialogue and we’ll select the defaults, won’t define anything interactively, and will then have it create the file:</description>
    </item>
    
    <item>
      <title>Ollama: Running GGUF Models from Hugging Face</title>
      <link>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</link>
      <pubDate>Wed, 18 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/</guid>
      <description>GGUF (GPT-Generated Unified Format) has emerged as the de facto standard file format for storing large language models for inference. We are starting to see a lot of models in this format on Hugging Face, many of them uploaded by The Bloke.
One cool thing about GGUF models is that it’s super easy to get them running on your own machine using Ollama. In this blog post, we’re going to look at how to download a GGUF model from Hugging Face and run it locally.</description>
    </item>
    
    <item>
      <title>ClickHouse: Code: 60. DB::Exception: Table does not exist</title>
      <link>https://www.markhneedham.com/blog/2023/10/16/clickhouse-local-table-does-not-exist/</link>
      <pubDate>Mon, 16 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/16/clickhouse-local-table-does-not-exist/</guid>
      <description>I’ve been playing with clickhouse-local again this week and ran into an interesting issue when persisting a table that I thought I’d document for future Mark.
You can install ClickHouse on your machine by running the following command:
curl https://clickhouse.com/ | sh Or you could use HomeBrew if you’re working on a Mac:
brew install clickhouse We can then launch clickhouse-local, which lets you run ClickHouse in what I think of as an embedded mode.</description>
    </item>
    
    <item>
      <title>Apache Superset: Refusing to start due to insecure SECRET_KEY</title>
      <link>https://www.markhneedham.com/blog/2023/10/13/apache-superset-refusing-start-insecure-key/</link>
      <pubDate>Fri, 13 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/13/apache-superset-refusing-start-insecure-key/</guid>
      <description>I’ve been trying to install Apache Superset so that I can use it for a demo and ran into an issue with a secret key when trying to install it. In this blog post, I’ll explain how to work around it.
We’re going to be using Poetry and will follow the installing from scratch guide.
First up is installing the library:
poetry add apache-superset And then after initialising the database with poetry run superset db upgrade, we’ll try to create the admin user:</description>
    </item>
    
    <item>
      <title>Ollama: Experiments with few-shot prompting on Llama2 7B</title>
      <link>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</link>
      <pubDate>Wed, 11 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/11/ollama-few-shot-prompting-experiments-llama2-7b/</guid>
      <description>A problem that I’m currently trying to solve is how to work out whether a given sentence is a question. If there’s a question mark on the end we can assume it is a question, but what about if the question mark’s been left off?
Few-shot prompting is a technique where we provide some examples in our prompt to try to guide the LLM to do what we want. And, this seemed like a good opportunity to try it out on Meta’s Llama2 7B Large Language Model using Ollama.</description>
    </item>
    
    <item>
      <title>Poetry/Dagster: ImportError: cannot import name &#39;appengine&#39; from &#39;requests.packages.urllib3.contrib&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/10/09/dagster-poetry-importerror-cannot-import-appengine-requests/</link>
      <pubDate>Mon, 09 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/09/dagster-poetry-importerror-cannot-import-appengine-requests/</guid>
      <description>I’m taking some tentative steps into the world of batch data pipelines and I’ve been following Dagster’s DuckDB tutorial when I ran into a dependency issue that I had to work around. In this blog post, I’ll share the steps that I took in case you run into the same issue.
I’m using the Poetry dependency management tool, but I think you’d get the same issue even if you used pip directly.</description>
    </item>
    
    <item>
      <title>Poetry: The current project&#39;s Python requirement is not compatible</title>
      <link>https://www.markhneedham.com/blog/2023/10/05/poetry-project-python-not-compatible/</link>
      <pubDate>Thu, 05 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/05/poetry-project-python-not-compatible/</guid>
      <description>A few times this week I’ve run into an interesting problem with Python version requirements when trying to install various packages. In this blog post, we’ll learn what’s going on and how to fix it.
Our story begins with the innocent creation of a Poetry project:
poetry init Next, we’re going to add dlt, the data loading tool:
poetry add dlt Output Creating virtualenv incompatible-blog-Bp2VMsrx-py3.11 in /Users/markhneedham/Library/Caches/pypoetry/virtualenvs Using version ^0.</description>
    </item>
    
    <item>
      <title>Poetry: Updating a package to a new version</title>
      <link>https://www.markhneedham.com/blog/2023/10/04/poetry-package-update/</link>
      <pubDate>Wed, 04 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/04/poetry-package-update/</guid>
      <description>I’m using the Poetry package manager for all my Python projects these days and wanted to upgrade as library that I installed a few weeks ago. I got myself all tangled up and wanted to write down how to do it for future me.
Let’s create a simple project to demonstrate what to do:
poetry init pyproject.toml [tool.poetry] name = &amp;#34;update-blog&amp;#34; version = &amp;#34;0.1.0&amp;#34; description = &amp;#34;&amp;#34; authors = [&amp;#34;Mark Needham &amp;lt;m.</description>
    </item>
    
    <item>
      <title>Running Mistral AI on my machine with Ollama</title>
      <link>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</link>
      <pubDate>Tue, 03 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/03/mistral-ai-own-machine-ollama/</guid>
      <description>Last week Mistral AI announced the release of their first Large Language Model (LLM), trained with 7 billion parameters, and better than Meta’s Llama 2 model with 13 billion parameters. For those keeping track, Mistral AI was founded in the summer of 2023 and raised $113m in their seed round.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>DuckDB: Show a list of views</title>
      <link>https://www.markhneedham.com/blog/2023/10/02/duckdb-list-show-views/</link>
      <pubDate>Mon, 02 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/02/duckdb-list-show-views/</guid>
      <description>I recently wanted to get a list of the views that I’d created in a DuckDB database and it took me a while to figure out how to do it. So this blog post is for future Mark more than anyone else!
We’re going to start with the following CSV file:
data/sales.csv date,product_id,quantity,sales_amount 2021-01-01,101,5,50 2021-01-02,102,3,30 2021-02-01,101,4,40 2021-02-02,103,6,60 And now we’ll create a table from the DuckDB CLI:
CREATE TABLE sales AS SELECT * from &amp;#39;data/sales.</description>
    </item>
    
    <item>
      <title>dbt-duckdb: KeyError: &#34;&#39;winner_seed&#39;&#34;</title>
      <link>https://www.markhneedham.com/blog/2023/10/01/dbt-duckdb-key-error/</link>
      <pubDate>Sun, 01 Oct 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/10/01/dbt-duckdb-key-error/</guid>
      <description>I’ve been building a little demo with dbt and DuckDB to transform CSV files from Jeff Sackmann’s tennis dataset and ran into an error that initially puzzled me. In this blog post, we’ll learn how to deal with it.
But first things first, we’re going to install dbt-duckdb as well as the latest version of DuckDB, which at the time of writing is 0.9.0.
pip install dbt-duckdb duckdb I then cloned Mehdi Ouazza’s demo project and adjusted it to work with my dataset.</description>
    </item>
    
    <item>
      <title>GPT 3.5 Turbo vs GPT 3.5 Turbo Instruct</title>
      <link>https://www.markhneedham.com/blog/2023/09/29/openai-gpt-chat-vs-instruct/</link>
      <pubDate>Fri, 29 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/29/openai-gpt-chat-vs-instruct/</guid>
      <description>Last week OpenAI sent out the following email introducing the gpt-3.5-turbo-instruct large language model:
Figure 1. Open AI announce gpt-3.5-turbo-instruct LLM I’ve never completely understood the difference between the chat and instruct models, so this seemed like a good time to figure it out. In this blog post, we’re going to give the models 5 tasks to do and then we’ll see how they get on.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>FAISS: Exploring Approximate Nearest Neighbours Cell Probe Methods</title>
      <link>https://www.markhneedham.com/blog/2023/09/14/faiss-approximate-nearest-neighbors-cell-probe/</link>
      <pubDate>Thu, 14 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/14/faiss-approximate-nearest-neighbors-cell-probe/</guid>
      <description>I’ve been learning about vector search in recent weeks and I came across FaceBook’s FAISS library. I wanted to learn the simplest way to do approximate nearest neighbours, and that’s what we’ll be exploring in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>kcat: SASL - Java JAAS configuration is not supported</title>
      <link>https://www.markhneedham.com/blog/2023/09/12/kcat-sasl-java-jaas-not-supported/</link>
      <pubDate>Tue, 12 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/12/kcat-sasl-java-jaas-not-supported/</guid>
      <description>I’ve been updating the StarTree Kafka SASL recipe to use Pinot 0.12 and ran into an error while trying to have it use kcat to ingest data into Kafka. In this blog post, we’ll learn how I did this.
The initial recipe was ingesting data into Kafka using kafka-console-consumer.sh, which uses the Java Kafka client. I’m using this Kafka client config file:
kafka-config/kafka_client.conf security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username=&amp;#34;alice&amp;#34; \ password=&amp;#34;alice-secret&amp;#34;; And, we use this script to ingest data from a data generator:</description>
    </item>
    
    <item>
      <title>How to run a Kotlin script</title>
      <link>https://www.markhneedham.com/blog/2023/09/07/how-to-run-kotlin-script/</link>
      <pubDate>Thu, 07 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/07/how-to-run-kotlin-script/</guid>
      <description>I was recently helping Tim get a Pinot data-loading Kotlin script working and it took me a while to figure out the best way to run it. In this blog post, I’ll share the solution we came up with.
But first things first, we need to install Kotlin if it’s not already installed. I used a library called SDKMAN6 for all things JVM, so I’m gonna run the following command:</description>
    </item>
    
    <item>
      <title>Quix Streams: Process certain number of Kafka messages</title>
      <link>https://www.markhneedham.com/blog/2023/09/05/quix-streams-process-n-kafka-messages/</link>
      <pubDate>Tue, 05 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/05/quix-streams-process-n-kafka-messages/</guid>
      <description>In a recent demo, I wanted to use Quix Streams to process a specified number of messages from a Kafka topic, write a message to another stream, and then exit the Quix app. This is an unusual use of Quix Streams, so it took me a while to figure out how to do it.
Let’s assume we have a Kafka broker running. We’ll create a couple of topics using the rpk tool:</description>
    </item>
    
    <item>
      <title>JupyterLab 4.0.5: Scroll output with keyboard shortcut</title>
      <link>https://www.markhneedham.com/blog/2023/09/03/jupyterlab-scroll-output-keyboard-shortcut/</link>
      <pubDate>Sun, 03 Sep 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/09/03/jupyterlab-scroll-output-keyboard-shortcut/</guid>
      <description>In the latest version of Jupyter Notebook/Lab (at least), the output of each cell is shown in full, regardless of how long it is. I wanted to limit the height of the output and then scroll through it within that inner window, ideally by triggering a keyboard shortcut.
I learnt how to do this with the help of Stack Overflow. First, you need to open the settings editor by typing Cmd + , on a Mac or by clicking on that screen from the top menu:</description>
    </item>
    
    <item>
      <title>pyarrow: pyarrow.lib.ArrowNotImplementedError: Filter argument must be boolean type</title>
      <link>https://www.markhneedham.com/blog/2023/08/23/pyarrow-filter-argument-boolean-type/</link>
      <pubDate>Wed, 23 Aug 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/08/23/pyarrow-filter-argument-boolean-type/</guid>
      <description>I wanted to filter a table in pyarrow table recently and ran into troubles when trying to use the filter syntax that I’m used to from DuckDB. In this blog post I’ll explain my mistake and how to fix it.
First, let’s install pyarrow:
pip install pyarrow And now we’re going to create a table that has a few countries and their corresponding continents:
import pyarrow as pa countries = pa.</description>
    </item>
    
    <item>
      <title>Python: TypeError: Instance and class checks can only be used with @runtime_checkable protocols</title>
      <link>https://www.markhneedham.com/blog/2023/08/21/python-typeerrr-instance-class-check-runtime-checkable/</link>
      <pubDate>Mon, 21 Aug 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/08/21/python-typeerrr-instance-class-check-runtime-checkable/</guid>
      <description>I’ve been playing around with ChromaDB and I wanted to programatically get a list of the embedding functions, which was a little trickier thna I expected. In this blog post, we’ll explore how I failed and then succeeded at this task.
But first, let’s install ChromaDB:
pip install chromadb The embedding functions live in the chromadb.utils.embedding_functions module. So my first thought was that I could list all the things defined in that module and then check which ones were a sub class of EmbeddingFunction:</description>
    </item>
    
    <item>
      <title>JupyterLab 4.0.5: Adding execution time to cell</title>
      <link>https://www.markhneedham.com/blog/2023/08/20/jupyterlab-time-cell-execution/</link>
      <pubDate>Sun, 20 Aug 2023 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/08/20/jupyterlab-time-cell-execution/</guid>
      <description>I’ve been using Jupyter Lab notebooks in some of my recent videos on Learn Data with Mark and I wanted to show cell execution timings so that viewers would have an idea of how long things were taking. I thought I’d need to use a custom timer, but it turns out there’s quite a nice plug-in, which we’ll learn about in this blog post.
The plug-in is called jupyterlab-execute-time and it shows a live view of the time that a cell takes to execute, as well as showing the execution time afterward.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Experimenting with the StarTree Index</title>
      <link>https://www.markhneedham.com/blog/2023/07/28/apache-pinot-experimenting-with-startree-index/</link>
      <pubDate>Fri, 28 Jul 2023 11:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/28/apache-pinot-experimenting-with-startree-index/</guid>
      <description>My colleagues Sandeep Dabade and Kulbir Nijjer recently wrote a three part blog post series about the StarTree index, an Apache Pinot indexing technique that dynamically builds a tree structure to maintain aggregates across a group of dimensions. I’ve not used this index before and wanted to give it a try and in this blog post, I’ll share what I learned.
I’ve put all the code in the startreedata/pinot-recipes GitHub repository in case you want to try it out yourself.</description>
    </item>
    
    <item>
      <title>Python/Poetry: Library not loaded: no such file, not in dyld cache</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</link>
      <pubDate>Thu, 27 Jul 2023 11:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/poetry-library-not-loaded-no-such-file-dyld-cache/</guid>
      <description>As I mentioned in a previous blog post, I’ve been using Python’s Poetry library, but today it stopped working! In this blog post, I’ll explain what happened and how I got it working again.
It started off innocent enough, with me trying to create a new project:
poetry init But instead of seeing the usual interactive wizard, I got the following error:
Output dyld[20269]: Library not loaded: /opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/Python Referenced from: &amp;lt;1B2377F9-2187-39A9-AA98-20E438024DE2&amp;gt; /Users/markhneedham/Library/Application Support/pypoetry/venv/bin/python Reason: tried: &amp;#39;/opt/homebrew/Cellar/python@3.</description>
    </item>
    
    <item>
      <title>OpenAI/GPT: Returning consistent/valid JSON from a prompt</title>
      <link>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</link>
      <pubDate>Thu, 27 Jul 2023 01:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/27/return-consistent-predictable-valid-json-openai-gpt/</guid>
      <description>When using OpenAI it can be tricky to get it to return a consistent response for a prompt. In this blog post, we’re going to learn how to use functions to return a consistent JSON format for a basic sentiment analysis prompt.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>How to delete a Kafka topic</title>
      <link>https://www.markhneedham.com/blog/2023/07/26/how-to-delete-kafka-topic/</link>
      <pubDate>Wed, 26 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/26/how-to-delete-kafka-topic/</guid>
      <description>A few years ago I wrote a blog post showing how to delete a Kafka topic when running on Docker and while that approach still works, I think I’ve now got a better way. And that’s what we’re going to learn about in this blog post.
Spin up Kafka Cluster We’re going to spin up Kafka using the following Docker Compose file:
docker-compose.yml version: &amp;#34;3&amp;#34; services: zookeeper: image: zookeeper:3.8.0 hostname: zookeeper container_name: zookeeper-delete ports: - &amp;#34;2181:2181&amp;#34; environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 kafka: image: wurstmeister/kafka:latest restart: unless-stopped container_name: &amp;#34;kafka-delete&amp;#34; ports: - &amp;#34;9092:9092&amp;#34; expose: - &amp;#34;9093&amp;#34; depends_on: - zookeeper environment: KAFKA_ZOOKEEPER_CONNECT: zookeeper-delete:2181/kafka KAFKA_BROKER_ID: 0 KAFKA_ADVERTISED_HOST_NAME: kafka-delete KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-delete:9093,OUTSIDE://localhost:9092 KAFKA_LISTENERS: PLAINTEXT://0.</description>
    </item>
    
    <item>
      <title>Confluent Kafka: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.</title>
      <link>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</link>
      <pubDate>Tue, 25 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/25/confluent-kafka-avroproducer-deprecated-use-avroserializer/</guid>
      <description>I’ve been creating a demo showing how to ingest Avro-encoded data from Apache Kafka into Apache Pinot and ran into a deprecation warning. In this blog post, I’ll show how to update code using the Confluent Kafka Python client to get rid of that warning.
I started by installing the following libraries:
pip install confluent-kafka avro urllib3 requests And then my code to publish an Avro encoded event to Kafka looked like this:</description>
    </item>
    
    <item>
      <title>VSCode: Adding Poetry Python Interpreter</title>
      <link>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</link>
      <pubDate>Mon, 24 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/24/vscode-poetry-python-interpreter/</guid>
      <description>I’ve been trying out Python’s Poetry dependency management tool recently and I really like it, but couldn’t figure out how to get it setup as VSCode’s Python interpreter. In this blog post, we’ll learn how to do that.
One way to add the Python interpreter in VSCode is to press Cmd+Shift+p and then type Python Interpreter. If you select the first result, you’ll see something like the following:
Figure 1.</description>
    </item>
    
    <item>
      <title>Docker: Failed to create network: Error response from daemon: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</title>
      <link>https://www.markhneedham.com/blog/2023/07/20/docker-network-could-not-find-non-overlapping-address-pool/</link>
      <pubDate>Thu, 20 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/20/docker-network-could-not-find-non-overlapping-address-pool/</guid>
      <description>I use Docker for pretty much every demo I create and this sometimes results in me running out of IP addresses to serve all those networks. In this blog post, we’ll learn how to diagnose and solve this issue.
Our story starts with the following command on a new project:
docker compose up Usually this purs along nicely and all our components spin up just fine, but today is not our lucky day and we get the following error:</description>
    </item>
    
    <item>
      <title>Plotly: Visualising a normal distribution given average and standard deviation</title>
      <link>https://www.markhneedham.com/blog/2023/07/19/plotly-normal-distribution-average-stdev/</link>
      <pubDate>Wed, 19 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/19/plotly-normal-distribution-average-stdev/</guid>
      <description>I’ve been playing around with Microsoft’s TrueSkill algorithm, which attempts to quantify the skill of a player using the Bayesian inference algorithm. A rating in this system is a Gaussian distribution that starts with an average of 25 and a confidence of 8.333. I wanted to visualise various ratings using Plotly and that’s what we’ll be doing in this blog post.
To save you from having to install TrueSkill, we’re going to create a named tuple to simulate a TrueSkill Rating object:</description>
    </item>
    
    <item>
      <title>Redpanda: Configure pruning/retention of data</title>
      <link>https://www.markhneedham.com/blog/2023/07/18/redpanda-prune-retention/</link>
      <pubDate>Tue, 18 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/18/redpanda-prune-retention/</guid>
      <description>I wanted to test how Apache Pinot deals with data being truncated from the underlying stream from which it’s consuming, so I’ve been trying to work out how to prune data in Redpanda. In this blog post, I’ll share what I’ve learnt so far.
We’re going to spin up a Redpanda cluster using the following Docker Compose file:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: redpanda: container_name: &amp;#34;redpanda-pruning&amp;#34; image: docker.redpanda.com/vectorized/redpanda:v22.2.2 command: - redpanda start - --smp 1 - --overprovisioned - --node-id 0 - --kafka-addr PLAINTEXT://0.</description>
    </item>
    
    <item>
      <title>Puppeteer: Button click doesn&#39;t work when zoomed in</title>
      <link>https://www.markhneedham.com/blog/2023/07/17/puppeteer-button-click-not-working-after-zoom/</link>
      <pubDate>Mon, 17 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/17/puppeteer-button-click-not-working-after-zoom/</guid>
      <description>I’m still playing around with Puppeteer, a Nodejs library that provides an API to control Chrome/Chromium. I want to load the Pinot UI zoomed to 250% and then write and run some queries.
We can install Puppeteer by running the following command:
npm i puppeteer-core I then created the file drive_pinot.mjs and added the following code, which opens the Pinot query console and then clicks on the &amp;#39;Run Query&amp;#39; button:</description>
    </item>
    
    <item>
      <title>Puppeteer: Unsupported command-line flag: --enabled-blink-features=IdleDetection.</title>
      <link>https://www.markhneedham.com/blog/2023/07/13/puppeteer-unsupported-flag-enabled-blink-features-idledetection/</link>
      <pubDate>Thu, 13 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/13/puppeteer-unsupported-flag-enabled-blink-features-idledetection/</guid>
      <description>In many of the StarTree recipe videos that I’ve worked on, I show how to write queries in the Pinot UI. If I wrote these queries manually there’d be way too many typos, so I drive the UI using a script. I’ve recently been exploring whether I can do this using a Node.js library called Puppeteer and wanted to share a warning message that I ran into early doors.</description>
    </item>
    
    <item>
      <title>Redpanda: Viewing consumer group offsets from __consumer_offsets</title>
      <link>https://www.markhneedham.com/blog/2023/07/12/redpanda-consumer-group-offsets/</link>
      <pubDate>Wed, 12 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/12/redpanda-consumer-group-offsets/</guid>
      <description>Redpanda supports consumer groups, which are sets of consumers that cooperate to consume data from topics. The consumers in a group are assigned a partition and they keep track of the last consumed offset in the __consumer_offsets topic. I wanted to see how many messages had been consumed by a consumer group and that’s what we’ll explore in this post.
My first thought was to query the __consumer_offsets topic using rpk topic consume.</description>
    </item>
    
    <item>
      <title>Quix Streams: Consuming and Producing JSON messages</title>
      <link>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</link>
      <pubDate>Tue, 11 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/11/quix-streams-consume-produce-json-messages/</guid>
      <description>I’ve been meaning to take Quix Streams for a spin for a while and got the chance while building a recent demo. Quix Streams is a library for building streaming applications on time-series data, but I wanted to use it to do some basic consuming and producing of JSON messages. That’s what we’re going to do in this blog post.
We’re going to use Redpanda to store our messages. We’ll launch a Redpanda instance using the following Docker Compose file:</description>
    </item>
    
    <item>
      <title>Python: Re-import module</title>
      <link>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</link>
      <pubDate>Fri, 07 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/07/python-reimport-module/</guid>
      <description>I often write little Python scripts that import code from other local modules and a common problem I have when using the Python REPL is that I update the code in the other modules and then can’t use the new functionality without restarting the REPL and re-importing everything. At least so I thought! It turns out there is a way to refresh those modules and that’s what we’ll be exploring in this blog post.</description>
    </item>
    
    <item>
      <title>ClickHouse: How to unpack or unnest an array</title>
      <link>https://www.markhneedham.com/blog/2023/07/03/clickhouse-unpack-unnest-array/</link>
      <pubDate>Mon, 03 Jul 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/07/03/clickhouse-unpack-unnest-array/</guid>
      <description>I recently came across clickhouse-local via this article in the MotherDuck monthly newsletter and I wanted to give it a try on my expected goals dataset. One of the first things that I wanted to do was unpack an array and in this blog post, we’ll learn how to do that.
I installed Clickhouse by running the following command:
curl https://clickhouse.com/ | sh And then launched the clickhouse-local CLI like this:</description>
    </item>
    
    <item>
      <title>Detecting and splitting scenes in a video</title>
      <link>https://www.markhneedham.com/blog/2023/06/30/detecting-splitting-scenes-video/</link>
      <pubDate>Fri, 30 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/30/detecting-splitting-scenes-video/</guid>
      <description>When editing videos for my YouTube channel, Learn Data with Mark, I spend a bunch of time each week chopping up a screencast into scenes that I then line up with a separately recorded voice-over. I was curious whether I could automate the chopping-up process and that’s what we’re going to explore in this blog post.
I started out by asking ChatGPT the following question:
ChatGPT Prompt I want to chop up a demo for a YouTube video into smaller segments.</description>
    </item>
    
    <item>
      <title>Python: All about the next function</title>
      <link>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</link>
      <pubDate>Wed, 28 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/28/python-next-function-iterator/</guid>
      <description>Yesterday I wrote a blog post about some different ways to take the first element from a Python list. Afterward I was chatting to my new rubber duck, ChatGPT, which suggested the next function on an iterator as an alternative approach. And so that’s what we’re going to explore in this blog post.
The next function gets the first value from an iterator and optionally returns a provided default value if the iterator is empty.</description>
    </item>
    
    <item>
      <title>Python: Get the first item from a collection, ignore the rest</title>
      <link>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</link>
      <pubDate>Tue, 27 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/27/python-get-first-item-collection-ignore-rest/</guid>
      <description>When writing Python scripts, I often find myself wanting to take the first item from a collection and ignore the rest of the values. I usually use something like values[0] to take the first value from the list, but I was curious whether I could do better by using destructuring. That’s what we’re going to explore in this blog post.
We’ll start with a list that contains some names:</description>
    </item>
    
    <item>
      <title>Running a Hugging Face Large Language Model (LLM) locally on my laptop</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</link>
      <pubDate>Fri, 23 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</guid>
      <description>I’ve been playing around with a bunch of Large Language Models (LLMs) on Hugging Face and while the free inference API is cool, it can sometimes be busy, so I wanted to learn how to run the models locally. That’s what we’ll be doing in this blog post.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>LangChain: 1 validation error for LLMChain - value is not a valid dict (type=type_error.dict)</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</link>
      <pubDate>Fri, 23 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/langchain-validation-error-llmchain-value-not-valid-dict/</guid>
      <description>I surely can’t be the first to make the mistake that I’m about to describe and I expect I won’t be the last! I’m still swimming in the LLM waters and I was trying to get GPT4All to play nicely with LangChain.
I wrote the following code to create an LLM chain in LangChain so that every question would use the same prompt template:
from langchain import PromptTemplate, LLMChain from gpt4all import GPT4All llm = GPT4All( model_name=&amp;#34;ggml-gpt4all-j-v1.</description>
    </item>
    
    <item>
      <title>GPT4All/LangChain: Model.__init__() got an unexpected keyword argument &#39;ggml_model&#39; (type=type_error)</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</link>
      <pubDate>Thu, 22 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/gpt4all-langchain-unexpected-keyword-ggml_model/</guid>
      <description>I’m starting to realise that things move insanely fast in the world of LLMs (Large Language Models) and you will run into issues because you aren’t using the latest version of libraries. I say this because I’ve been following Sami Maameri’s blog post which explains how to run an LLM on your own machine and ran into an error, which we’ll explore in this blog post.
Sami’s post is based around a library called GPT4All, but he also uses LangChain to glue things together.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: &#39;NoneType&#39; object has no attribute &#39;info&#39;</title>
      <link>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</link>
      <pubDate>Thu, 22 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/22/chroma-nonetype-object-no-attribute-info/</guid>
      <description>Following on from a blog post that I wrote yesterday about doing similarity search with ChromaDB, I noticed an odd error message being printed as the script was exiting. In this blog post, we’ll explore what was going on.
To recap, I have the following code to find chunks of YouTube transcripts that are most similar to an input query:
test_chroma.py from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma hf_embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;) store = Chroma(collection_name=&amp;#34;transcript&amp;#34;, persist_directory=&amp;#34;db&amp;#34;, embedding_function=hf_embeddings) result = store.</description>
    </item>
    
    <item>
      <title>Chroma/LangChain: Index not found, please create an instance before querying</title>
      <link>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</link>
      <pubDate>Wed, 21 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/21/chroma-index-not-found-create-instance-querying/</guid>
      <description>Somewhat belatedly, I’ve been playing around with LangChain and HuggingFace to spike a tool that lets me ask question about Tim Berglund’s Real-Time Analytics podcast.
I’m using the Chroma database to store vectors of chunks of the transcript so that I can find appropriate sections to feed to the Large Language Model to help with answering my questions. I ran into an initially perplexing error while building this out, which we’re going to explore in this blog post.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Convert string in YYYYmmdd format to Date</title>
      <link>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</link>
      <pubDate>Tue, 20 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/20/duckdb-sql-string-date/</guid>
      <description>I’ve been working with a data set that represents dates as strings in the format &amp;#39;YYYYmmdd&amp;#39; and I wanted to convert those values to Dates in DuckDB. In this blog post, we’ll learn how to do that.
Let’s create a small table with a single column that represents date of births:
create table players (dob VARCHAR); insert into players values(&amp;#39;20080203&amp;#39;), (&amp;#39;20230708&amp;#39;); We can write the following query to return the rows in the table:</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Transpose columns to rows with UNPIVOT</title>
      <link>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</link>
      <pubDate>Tue, 13 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/13/duckdb-sql-transpose-columns-to-rows-unpivot/</guid>
      <description>I’ve been playing around with the Kaggle European Soccer dataset, which contains, amongst other things, players and their stats in the FIFA video game. I wanted to compare the stats of Ronaldo and Messi, which is where this story begins.
I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>GitHub: Get a CSV containing my pull requests (PRs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/12/github-list-pull-requests-csv/</link>
      <pubDate>Mon, 12 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/12/github-list-pull-requests-csv/</guid>
      <description>I wanted to get a list of my GitHub pull requests (PRs) and commits, which was surprisingly difficult to figure out how to do. I’m sure it must be possible to get this data from the API, but it was a lot easier to figure out how to do so with the GitHub CLI.
This blog post explains how to use the GitHub CLI on the Mac OS terminal. If you’re trying to do this on Windows, see Get a CSV of all my pull requests from Github using Github CLI and PowerShell.</description>
    </item>
    
    <item>
      <title>Creating LinkedIn Carousel/Slides</title>
      <link>https://www.markhneedham.com/blog/2023/06/08/linkedin-slides-carousel/</link>
      <pubDate>Thu, 08 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/08/linkedin-slides-carousel/</guid>
      <description>If you’ve been using LinkedIn recently, you’ve likely seen those posts where people post slides in a kind of carousel that you can horizontally scroll. I wanted to create one to explain Apache Pinot’s Upserts feature, but I wasn’t sure how to create one.
Since it’s 2023, I started by asking ChatGPT:
Figure 1. ChatGPT doesn’t know about LinkedIn Carousel Unfortunately ChatGPT doesn’t know how to do it, but Harrison Avisto does and was happy to teach me.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Pivot - 0 if null</title>
      <link>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</link>
      <pubDate>Wed, 07 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/07/duckdb-sql-pivot-0-if-null/</guid>
      <description>I’ve been learning all about the PIVOT function that was recently added in DuckDB and I ran into an issue where lots of the cells in my post PIVOT table were null values. In this blog post, we’ll learn how to replace those nulls with 0s (or indeed any other value).
Setup I’m working with Jeff Sackmann’s tennis dataset, which I loaded by running the following query:
CREATE OR REPLACE TABLE matches AS SELECT * FROM read_csv_auto( list_transform( range(1968, 2023), y -&amp;gt; &amp;#39;https://raw.</description>
    </item>
    
    <item>
      <title>Kafka/Kubernetes: Failed to resolve: nodename nor servname provided, or not known</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/kafka-kubernetes-failed-resolve-nodename-servname-not-known/</guid>
      <description>I’ve been trying out the Running Pinot in Kubernetes tutorial and ran into a problem trying to write data to Kafka. In this blog we’ll explore how I got around that problem.
I’m using Helm with Kubernetes and started a Kafka service by running the following:
helm repo add kafka https://charts.bitnami.com/bitnami helm install -n pinot-quickstart kafka kafka/kafka --set replicas=1,zookeeper.image.tag=latest I waited until the service had started and then ran the following command to port forward the Kafka service’s port 9092 to port 9092 on my host OS:</description>
    </item>
    
    <item>
      <title>Python: Working with tuples in lambda expressions</title>
      <link>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</link>
      <pubDate>Tue, 06 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/06/python-lambda-expression-tuple/</guid>
      <description>I’m still playing around with data returned by Apache Pinot’s HTTP API and I wanted to sort a dictionary of segment names by partition id and index. In this blog post we’re going to look into how to do that.
We’ll start with the following dictionary:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__13__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } As I mentioned above, I want to sort the dictionary’s items by partition id and index, which are embedded inside the key name.</description>
    </item>
    
    <item>
      <title>Python: Padding a string</title>
      <link>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</link>
      <pubDate>Mon, 05 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/05/python-pad-string/</guid>
      <description>I’ve been writing some scripts to parse data from Apache Pinot’s HTTP API and I wanted to format the values stored in a map to make them more readable. In this blog post, we’ll look at some ways that I did that.
I started with a map that looked a bit like this:
segments = { &amp;#34;events3__4__1__20230605T1335Z&amp;#34;: &amp;#34;CONSUMED&amp;#34;, &amp;#34;events3__4__20__20230605T1335Z&amp;#34;: &amp;#34;CONSUMING&amp;#34; } And then I iterated over and printed each item like this:</description>
    </item>
    
    <item>
      <title>DuckDB: Generate dummy data with user defined functions (UDFs)</title>
      <link>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</link>
      <pubDate>Fri, 02 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/02/duckdb-dummy-data-user-defined-functions/</guid>
      <description>In the 0.8 release of DuckDB, they added functionality that lets you add your own functions when using the Python package I wanted to see if I could use it to generate dummy data so that’s what we’re going to do in this blog post.
Note I’ve created a video showing how to do this on my YouTube channel, Learn Data with Mark, so if you prefer to consume content through that medium, I’ve embedded it below:</description>
    </item>
    
    <item>
      <title>Debezium: Capture changes from MySQL</title>
      <link>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</link>
      <pubDate>Wed, 31 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/31/debezium-capture-changes-mysql/</guid>
      <description>I’ve been working on a Real-Time Analytics workshop that I’m going to be presenting at the ODSC Europe conference in June 2023 and I wanted to have Debezium publish records from a MySQL database without including the schema.
I’m using the debezium/connect:2.3 Docker image to run Debezium locally and I have a MySQL database running with the hostname mysql on port 3306. Below is the way that I configured this:</description>
    </item>
    
    <item>
      <title>Node.js: Minifying JSON documents</title>
      <link>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</link>
      <pubDate>Tue, 30 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/30/nodejs-minify-json-documents/</guid>
      <description>I often need to minimise the schema and table config files that you use to configure Apache Pinot so that they don’t take up so much space. After doing this manually for ages, I came across the json-stringify-pretty-compact library, which speeds up the process.
We can install it like this:
npm install json-stringify-pretty-compact And then I have the following script:
minify.mjs import pretty from &amp;#39;json-stringify-pretty-compact&amp;#39;; let inputData = &amp;#39;&amp;#39;; process.</description>
    </item>
    
    <item>
      <title>DuckDB: Ingest a bunch of CSV files from GitHub</title>
      <link>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</link>
      <pubDate>Thu, 25 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/25/duckdb-ingest-csv-files-github/</guid>
      <description>Jeff Sackmann’s tennis_atp repository is one of the best collections of tennis data and I wanted to ingest the ATP Tour singles matches using the DuckDB CLI. In this blog post we’ll learn how to do that.
Usually when I’m ingesting data into DuckDB I’ll specify the files that I want to ingest using the wildcard syntax. In this case that would mean running a query like this:
CREATE OR REPLACE TABLE matches AS SELECT * FROM &amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>DuckDB/SQL: Create a list of numbers</title>
      <link>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</link>
      <pubDate>Wed, 24 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/24/duckdb-sql-create-list-numbers/</guid>
      <description>While in DuckDB land, I wanted to create a list of numbers, just like you can with Cypher’s range function. After a bit of searching that resulted in very complex solutions, I came across the Postgres generate_series function, which does the trick.
We can use it in place of a table, like this:
SELECT * FROM generate_series(1, 10); Table 1. Output generate_series 1
2
3
4
5
6
7</description>
    </item>
    
    <item>
      <title>Arc Browser: Building a plugin (Boost) with help from ChatGPT</title>
      <link>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</link>
      <pubDate>Tue, 23 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/23/arc-browser-creating-boost-plugin-with-chatgpt/</guid>
      <description>I’ve been using the Arc Browser for a couple of months now and one of my favourite things is the simplicity of the plugin (or as they call it, &amp;#39;Boost&amp;#39;) functionality.
I wanted to port over a Chrome bookmark that I use to capture the podcasts that I’ve listened to on Player.FM. In this blog post I’ll show how ChatGPT helped me convert the bookmark code to an Arc Boost.</description>
    </item>
    
    <item>
      <title>Cropping a video using FFMPEG</title>
      <link>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</link>
      <pubDate>Mon, 15 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/15/cropping-video-ffmpeg/</guid>
      <description>I needed to crop a video that I used as part of a video on my YouTube channel, Learn Data With Mark, and Camtasia kept rendering a black screen. So I had to call for FFMPEG!
Cropping the bottom of a video My initial video was 2160 x 3840 but I didn’t need the bottom 1920 pixels because I’m using that part of the screen for a video of me.</description>
    </item>
    
    <item>
      <title>Python: Naming slices</title>
      <link>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</link>
      <pubDate>Sat, 13 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/13/python-naming-slices/</guid>
      <description>Another gem from Fluent Python is that you can name slices. How did I not know that?!
Let’s have a look how it works using an example of a Vehicle Identification Number, which has 17 characters that act as a unique identifier for a vehicle. Different parts of that string mean different things.
So given the following VIN:
vin = &amp;#34;2B3HD46R02H210893&amp;#34; We can extract components like this:
print(f&amp;#34;&amp;#34;&amp;#34; World manufacturer identifier: {vin[0:3]} Vehicle Descriptor: {vin[3:9]} Vehicle Identifier: {vin[9:17]} &amp;#34;&amp;#34;&amp;#34;.</description>
    </item>
    
    <item>
      <title>Python 3.10: Pattern matching with match/case</title>
      <link>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</link>
      <pubDate>Tue, 09 May 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/05/09/python-pattern-matching-match-case/</guid>
      <description>I’ve been reading Fluent Python and learnt about pattern matching with the match/case statement, introduced in Python 3.10. You can use it instead of places where you’d otherwise use if, elif, else statements.
I created a small example to understand how it works. The following function takes in a list where the first argument should be foo, followed by a variable number of arguments, which we print to the console:</description>
    </item>
    
  </channel>
</rss>
