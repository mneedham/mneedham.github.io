<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hugging-face on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/hugging-face/</link>
    <description>Recent content in hugging-face on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jun 2023 04:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/hugging-face/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running a Hugging Face Large Language Model (LLM) locally on my laptop</title>
      <link>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</link>
      <pubDate>Fri, 23 Jun 2023 04:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/</guid>
      <description>I’ve been playing around with a bunch of Large Language Models (LLMs) on Hugging Face and while the free inference API is cool, it can sometimes be busy, so I wanted to learn how to run the models locally. That’s what we’ll be doing in this blog post.
You’ll need to install the following libraries if you want to follow along:
pip install langchain[all] huggingface-hub langchain transformers The first step is to choose a model that you want to download.</description>
    </item>
    
    <item>
      <title>Hugging Face: Using `max_length`&#39;s default (20) to control the generation length. This behaviour is deprecated</title>
      <link>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</link>
      <pubDate>Mon, 19 Jun 2023 02:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2023/06/19/huggingface-max-length-generation-length-deprecated/</guid>
      <description>I’ve been trying out some of the Hugging Face tutorials and came across an interesting warning message while playing around with the google/flan-t5-large model. In this blog post, we’ll learn how to get rid of that warning.
I was running a variation of the getting started example:
from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) model = T5ForConditionalGeneration.from_pretrained(&amp;#34;google/flan-t5-large&amp;#34;) input_text = &amp;#34;Who is the UK Prime Minister? Explain step by step&amp;#34; input_ids = tokenizer(input_text, return_tensors=&amp;#34;pt&amp;#34;).</description>
    </item>
    
  </channel>
</rss>
