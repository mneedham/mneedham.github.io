<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Absplittesting on Mark Needham</title>
    <link>http://markhneedham.com/blog/tag/absplittesting/</link>
    <description>Recent content in Absplittesting on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2013 13:13:49 +0000</lastBuildDate>
    
	<atom:link href="http://markhneedham.com/blog/tag/absplittesting/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A/B Testing: Being pragmatic with statistical significance</title>
      <link>http://markhneedham.com/blog/2013/05/27/ab-testing-pragmatica-statistical-significance/</link>
      <pubDate>Mon, 27 May 2013 13:13:49 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/05/27/ab-testing-pragmatica-statistical-significance/</guid>
      <description>One of the first things that we did before starting any of the A/B tests that I&#39;ve previously written about was to work out how many users we needed to go through before we could be sure that the results we saw were statistically significant.
We used the prop.test function from R to do this and based on our traffic at the time worked out that we&#39;d need to run a test for 6 weeks to achieve statistical significance.</description>
    </item>
    
    <item>
      <title>A/B Testing: User Experience vs Conversion</title>
      <link>http://markhneedham.com/blog/2013/05/18/ab-testing-user-experience-vs-conversion/</link>
      <pubDate>Sat, 18 May 2013 20:18:50 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/05/18/ab-testing-user-experience-vs-conversion/</guid>
      <description>I&#39;ve written a couple of posts over the last few months about my experiences with A/B testing and one conversation we often used to have was around user experience vs conversion rate.
Once you start running an A/B test it encourages you to focus more on the conversion rate of users in different parts of the flow and your inclination is to make changes that increase that conversion rate.
Another one of our drivers is to provide the best user experience that we can to our customers and since sometimes this means that the best thing for them is not to switch it seems that these two must be in conflict.</description>
    </item>
    
    <item>
      <title>A/B Testing: Reporting</title>
      <link>http://markhneedham.com/blog/2013/04/28/ab-testing-reporting/</link>
      <pubDate>Sun, 28 Apr 2013 22:32:38 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/04/28/ab-testing-reporting/</guid>
      <description>A few months ago I wrote about my initial experiences with A/B testing and since then we&#39;ve been working on another one and learnt some things around reporting on these types of tests that I thought was interesting.
Reporting as a first class concern One thing we changed from our previous test after a suggestion by Mike was to start treating the reporting of data related to the test as a first class citizen.</description>
    </item>
    
    <item>
      <title>A/B Testing: Thoughts so far</title>
      <link>http://markhneedham.com/blog/2013/01/27/ab-testing-thoughts-so-far/</link>
      <pubDate>Sun, 27 Jan 2013 13:27:32 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/01/27/ab-testing-thoughts-so-far/</guid>
      <description>I&#39;ve been working at uSwitch for about two months now and for the majority of that time have been working on an A/B test we were running to try and make it easier for users to go through the energy comparison process.
I found the &#39;Practical Guide to Controlled Experiments on the Web&#39; paper useful for explaining how to go about doing an A/B test and there&#39;s also an interesting presentation by Dan McKinley about how etsy do A/B testing.</description>
    </item>
    
  </channel>
</rss>