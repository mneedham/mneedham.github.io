<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llama.cpp on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/llama.cpp/</link>
    <description>Recent content in llama.cpp on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Mar 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/llama.cpp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>llama.cpp - ValueError: Failed to create llama_context - ggml-common.h file not found</title>
      <link>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</link>
      <pubDate>Sun, 31 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</guid>
      <description>I’ve been playing around with the outlines library and needed to install llama.cpp as a result. I ran into trouble when trying to offload model layers to the GPU and in this post, I’ll explain how to install llama.cpp so that you don’t have the same issues.
This was how I installed the library initially:
CMAKE_ARGS=&amp;#34;-DLLAMA_METAL=on&amp;#34; pip install llama-cpp-python And then let’s try to load a GGUF model with some layers offloaded to the GPU:</description>
    </item>
    
  </channel>
</rss>
