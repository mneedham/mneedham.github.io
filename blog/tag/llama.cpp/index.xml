<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llama.cpp on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/tag/llama.cpp/</link>
    <description>Recent content in llama.cpp on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Apr 2024 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/tag/llama.cpp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Semantic Router: Stop LLM chatbots going rogue</title>
      <link>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</link>
      <pubDate>Sun, 14 Apr 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/04/14/semantic-router-stop-llm-chatbot-going-rogue/</guid>
      <description>A tricky problem when deploying LLM-based chatbots is working out how to stop them from talking about topics that you don’t want them to talk about. Even with the cleverest prompts, with enough effort and ingenuity, users will figure a way around the guard rails.
However, I recently came across a library called Semantic Router, which amongst other things, seems to provide a solution to this problem. In this blog post, we’re going to explore Semantic Router and see if we can create a chatbot that only talks about a pre-defined set of topics.</description>
    </item>
    
    <item>
      <title>llama.cpp - ValueError: Failed to create llama_context - ggml-common.h file not found</title>
      <link>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</link>
      <pubDate>Sun, 31 Mar 2024 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2024/03/31/llama-cpp-value-error-llama-context-ggml-common-not-found/</guid>
      <description>I’ve been playing around with the outlines library and needed to install llama.cpp as a result. I ran into trouble when trying to offload model layers to the GPU and in this post, I’ll explain how to install llama.cpp so that you don’t have the same issues.
This was how I installed the library initially:
CMAKE_ARGS=&amp;#34;-DLLAMA_METAL=on&amp;#34; pip install llama-cpp-python And then let’s try to load a GGUF model with some layers offloaded to the GPU:</description>
    </item>
    
  </channel>
</rss>
