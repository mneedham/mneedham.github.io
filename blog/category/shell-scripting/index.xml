<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shell Scripting on Mark Needham</title>
    <link>https://markhneedham.com/blog/category/shell-scripting/</link>
    <description>Recent content in Shell Scripting on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jun 2017 12:26:49 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/category/shell-scripting/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Shell: Create a comma separated string</title>
      <link>https://markhneedham.com/blog/2017/06/23/shell-create-comma-separated-string/</link>
      <pubDate>Fri, 23 Jun 2017 12:26:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/23/shell-create-comma-separated-string/</guid>
      <description>I recently needed to generate a string with comma separated values, based on iterating a range of numbers.  e.g. we should get the following output where n = 3 foo-0,foo-1,foo-2  I only had the shell available to me so I couldn&#39;t shell out into Python or Ruby for example. That means it&#39;s bash scripting time!
 If we want to iterate a range of numbers and print them out on the screen we can write the following code: n=3 for i in $(seq 0 $(($n &amp;gt; 0?</description>
    </item>
    
    <item>
      <title>Unix: Find files greater than date</title>
      <link>https://markhneedham.com/blog/2016/06/24/unix-find-files-greater-than-date/</link>
      <pubDate>Fri, 24 Jun 2016 16:56:17 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/24/unix-find-files-greater-than-date/</guid>
      <description>For the latter part of the week I&#39;ve been running some tests against Neo4j which generate a bunch of log files and I wanted to filter those files based on the time they were created to do some further analysis.  This is an example of what the directory listing looks like: $ ls -alh foo/database-agent-* -rw-r--r-- 1 markneedham wheel 2.5K 23 Jun 14:00 foo/database-agent-mac17f73-1-logs-archive-201606231300176.tar.gz -rw-r--r-- 1 markneedham wheel 8.</description>
    </item>
    
    <item>
      <title>Unix: Find all text below string in a file</title>
      <link>https://markhneedham.com/blog/2016/06/19/unix-find-all-text-below-string-in-a-file/</link>
      <pubDate>Sun, 19 Jun 2016 08:36:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/19/unix-find-all-text-below-string-in-a-file/</guid>
      <description>I recently wanted to parse some text out of a bunch of files so that I could do some sentiment analysis on it. Luckily the text I want is at the end of the file and doesn&#39;t have anything after it but there is text before it that I want to get rid. The files look like this:
# text I don&#39;t care about = Heading of the bit I care about # text I care about  In other words I want to find the line that contains the Heading and then get all the text after that point.</description>
    </item>
    
    <item>
      <title>Unix: Split string using separator</title>
      <link>https://markhneedham.com/blog/2016/06/19/unix-split-string-using-separator/</link>
      <pubDate>Sun, 19 Jun 2016 07:22:57 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/19/unix-split-string-using-separator/</guid>
      <description>I recently found myself needing to iterate over a bunch of &#39;/&#39; separated strings on the command line and extract just the text after the last &#39;/&#39;.  e.g. an example of one of the strings A/B/C  I wanted to write some code that could split on &#39;/&#39; and then pick the 3rd item in the resulting collection. One way of doing this is to echo the string and then pipe it through cut:</description>
    </item>
    
    <item>
      <title>Unix parallel: Populating all the USB sticks</title>
      <link>https://markhneedham.com/blog/2016/06/01/unix-parallel-populating-all-the-usb-sticks/</link>
      <pubDate>Wed, 01 Jun 2016 05:53:38 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/06/01/unix-parallel-populating-all-the-usb-sticks/</guid>
      <description>The day before Graph Connect Europe 2016 we needed to create a bunch of USB sticks containing Neo4j and the training materials and eventually iterated our way to a half decent approach which made use of the GNU parallel command which I&#39;ve always wanted to use!  But first I needed to get a USB hub so I could do lots of them at the same time. I bought the EasyAcc USB 3.</description>
    </item>
    
    <item>
      <title>Unix: Stripping first n bytes in a file / Byte Order Mark (BOM)</title>
      <link>https://markhneedham.com/blog/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:28 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/19/unix-stripping-first-n-bytes-in-a-file-byte-order-mark-bom/</guid>
      <description>I&#39;ve previously written a couple of blog posts showing how to strip out the byte order mark (BOM) from CSV files to make loading them into Neo4j easier and today I came across another way to clean up the file using tail.  The BOM is 3 bytes long at the beginning of the file so if we know that a file contains it then we can strip out those first 3 bytes tail like this: $ time tail -c +4 Casualty7904.</description>
    </item>
    
    <item>
      <title>Unix: Redirecting stderr to stdout</title>
      <link>https://markhneedham.com/blog/2015/08/15/unix-redirecting-stderr-to-stdout/</link>
      <pubDate>Sat, 15 Aug 2015 15:55:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/15/unix-redirecting-stderr-to-stdout/</guid>
      <description>I&#39;ve been trying to optimise some Neo4j import queries over the last couple of days and as part of the script I&#39;ve been executed I wanted to redirect the output of a couple of commands into a file to parse afterwards.  I started with the following script which doesn&#39;t do any explicit redirection of the output: #!/bin/sh ./neo4j-community-2.2.3/bin/neo4j start  Now let&#39;s run that script and redirect the output to a file:</description>
    </item>
    
    <item>
      <title>Sed: Using environment variables</title>
      <link>https://markhneedham.com/blog/2015/08/13/sed-using-environment-variables/</link>
      <pubDate>Thu, 13 Aug 2015 19:30:51 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/13/sed-using-environment-variables/</guid>
      <description>I&#39;ve been playing around with the BBC football data set that I wrote about a couple of months ago and I wanted to write some code that would take the import script and replace all instances of remote URIs with a file system path.  For example the import file contains several lines similar to this: LOAD CSV WITH HEADERS FROM &amp;quot;https://raw.githubusercontent.com/mneedham/neo4j-bbc/master/data/matches.csv&amp;quot; AS row  And I want that to read:</description>
    </item>
    
    <item>
      <title>Neo4j: Using LOAD CSV to help explore CSV files</title>
      <link>https://markhneedham.com/blog/2015/06/11/neo4j-using-load-csv-to-help-explore-csv-files/</link>
      <pubDate>Thu, 11 Jun 2015 23:15:06 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/11/neo4j-using-load-csv-to-help-explore-csv-files/</guid>
      <description>During the Neo4j How I met your mother hackathon that we ran last week one of the attendees noticed that one of the CSV files we were importing wasn&#39;t creating as many records as they expected it to.  This is typically the case when there&#39;s some odd quoting in the CSV file but we decided to look into it. The file in question was one containing references made in HIMYM.</description>
    </item>
    
    <item>
      <title>Mac OS X: GNU sed -  Hex string replacement / replacing new line characters</title>
      <link>https://markhneedham.com/blog/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</link>
      <pubDate>Thu, 11 Jun 2015 21:38:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/11/mac-os-x-gnu-sed-hex-string-replacement-replacing-new-line-characters/</guid>
      <description>Recently I was working with a CSV file which contained both Windows and Unix line endings which was making it difficult to work with.  The actual line endings were HEX &#39;0A0D&#39; i.e. Windows line breaks but there were also HEX &#39;OA&#39; i.e. Unix line breaks within one of the columns.  I wanted to get rid of the Unix line breaks and discovered that you can do HEX sequence replacement using the GNU version of sed - unfortunately the Mac ships with the BSD version which doesn&#39;t have this functionaltiy.</description>
    </item>
    
    <item>
      <title>Unix: Converting a file of values into a comma separated list</title>
      <link>https://markhneedham.com/blog/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</link>
      <pubDate>Mon, 08 Jun 2015 22:23:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/06/08/unix-converting-a-file-of-values-into-a-comma-separated-list/</guid>
      <description>I recently had a bunch of values in a file that I wanted to paste into a Java program which required a comma separated list of strings.  This is what the file looked like: $ cat foo2.txt | head -n 5 1.0 1.0 1.0 1.0 1.0   And the idea is that we would end up with something like this: &amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;,&amp;quot;1.0&amp;quot;   The first thing we need to do is quote each of the values.</description>
    </item>
    
    <item>
      <title>cURL: POST/Upload multi part form</title>
      <link>https://markhneedham.com/blog/2013/09/23/curl-postupload-multi-part-form/</link>
      <pubDate>Mon, 23 Sep 2013 22:16:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/09/23/curl-postupload-multi-part-form/</guid>
      <description>I&#39;ve been doing some work which involved uploading a couple of files from a HTML form and I wanted to check that the server side code was working by executing a cURL command rather than using the browser.
The form looks like this:
&amp;lt;form action=&amp;quot;http://foobar.com&amp;quot; method=&amp;quot;POST&amp;quot; enctype=&amp;quot;multipart/form-data&amp;quot;&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&amp;quot;nodes&amp;quot;&amp;gt;File 1:&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;file1&amp;quot; id=&amp;quot;file1&amp;quot;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;p&amp;gt; &amp;lt;label for=&amp;quot;relationships&amp;quot;&amp;gt;File 2:&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;file2&amp;quot; id=&amp;quot;file2&amp;quot;&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;input type=&amp;quot;submit&amp;quot; name=&amp;quot;submit&amp;quot; value=&amp;quot;Submit&amp;quot;&amp;gt; &amp;lt;/form&amp;gt;  If we convert the POST request from the browser into a cURL equivalent we end up with the following:</description>
    </item>
    
    <item>
      <title>Unix: tar - Extracting, creating and viewing archives</title>
      <link>https://markhneedham.com/blog/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</link>
      <pubDate>Thu, 22 Aug 2013 22:56:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/08/22/unix-tar-extracting-creating-and-viewing-archives/</guid>
      <description>I&#39;ve been playing around with the Unix tar command a bit this week and realised that I&#39;d memorised some of the flag combinations but didn&#39;t actually know what each of them meant.
For example, one of the most common things that I want to do is extract a gripped neo4j archive:
$ wget http://dist.neo4j.org/neo4j-community-1.9.2-unix.tar.gz $ tar -xvf neo4j-community-1.9.2-unix.tar.gz  where:
 -x means extract -v means produce verbose output i.e. print out the names of all the files as you unpack it -f means unpack the file which follows this flag i.</description>
    </item>
    
    <item>
      <title>Unix/awk: Extracting substring using a regular expression with capture groups</title>
      <link>https://markhneedham.com/blog/2013/06/26/unixawk-extracting-substring-using-a-regular-expression-with-capture-groups/</link>
      <pubDate>Wed, 26 Jun 2013 15:23:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/06/26/unixawk-extracting-substring-using-a-regular-expression-with-capture-groups/</guid>
      <description>A couple of years ago I wrote a blog post explaining how I&#39;d used GNU awk to extract story numbers from git commit messages and I wanted to do a similar thing today to extract some node ids from a file.
My eventual solution looked like this:
$ echo &amp;quot;mark #1000&amp;quot; | gawk &#39;{ match($0, /#([0-9]+)/, arr); if(arr[1] != &amp;quot;&amp;quot;) print arr[1] }&#39; 1000  But in the comments an alternative approach was suggested which used the Mac version of awk and the RSTART and RLENGTH global variables which get set when a match is found:</description>
    </item>
    
    <item>
      <title>Unix: find, xargs, zipinfo and the &#39;caution: filename not matched:&#39; error</title>
      <link>https://markhneedham.com/blog/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</link>
      <pubDate>Sun, 09 Jun 2013 23:10:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/06/09/unix-find-xargs-zipinfo-and-the-caution-filename-not-matched-error/</guid>
      <description>As I mentioned in my previous post last week I needed to scan all the jar files included with the neo4j-enterprise gem and I started out by finding out where it&#39;s located on my machine:
$ bundle show neo4j-enterprise /Users/markhneedham/.rbenv/versions/jruby-1.7.1/lib/ruby/gems/shared/gems/neo4j-enterprise-1.8.2-java  I then thought I could get a list of all the jar files using find and pipe it into zipinfo via xargs to get all the file names and then search for HighlyAvailableGraphDatabaseFactory:</description>
    </item>
    
    <item>
      <title>Unix: Working with parts of large files</title>
      <link>https://markhneedham.com/blog/2013/05/19/unix-working-with-parts-of-large-files/</link>
      <pubDate>Sun, 19 May 2013 21:44:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/05/19/unix-working-with-parts-of-large-files/</guid>
      <description>Chris and I were looking at the neo4j log files of a client earlier in the week and wanted to do some processing of the file so we could ask the client to send us some further information.
The log file was over 10,000 lines long but the bit of the file we were interesting in was only a few hundred lines.
I usually use Vim and the &#39;:set number&#39; when I want to refer to line numbers in a file but Chris showed me that we can achieve the same thing with e.</description>
    </item>
    
    <item>
      <title>Unix: Checking for open sockets on nginx</title>
      <link>https://markhneedham.com/blog/2013/04/23/unix-checking-for-open-sockets-on-nginx/</link>
      <pubDate>Tue, 23 Apr 2013 23:59:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/04/23/unix-checking-for-open-sockets-on-nginx/</guid>
      <description>Tim and I were investigating a weird problem we were having with nginx where it was getting in a state where it had exceeded the number of open files allowed on the system and started rejecting requests.
We can find out the maximum number of open files that we&#39;re allowed on a system with the following command:
$ ulimit -n 1024  Our hypothesis was that some socket connections were never being closed and therefore the number of open files was climbing slowly upwards until it exceeded the limit.</description>
    </item>
    
    <item>
      <title>awk: Parsing &#39;free -m&#39; output to get memory usage/consumption</title>
      <link>https://markhneedham.com/blog/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</link>
      <pubDate>Wed, 10 Apr 2013 07:03:15 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/04/10/awk-parsing-free-m-output-to-get-memory-usageconsumption/</guid>
      <description>Although I know this problem is already solved by collectd and New Relic I wanted to write a little shell script that showed me the memory usage on a bunch of VMs by parsing the output of free.
The output I was playing with looks like this:
$ free -m total used free shared buffers cached Mem: 365 360 5 0 59 97 -/+ buffers/cache: 203 161 Swap: 767 13 754  I wanted to find out what % of the memory on the machine was being used and as I understand it the numbers that we would use to calculate this are the &#39;total&#39; value on the &#39;Mem&#39; line and the &#39;used&#39; value on the &#39;buffers/cache&#39; line.</description>
    </item>
    
    <item>
      <title>Sed: Replacing characters with a new line</title>
      <link>https://markhneedham.com/blog/2012/12/29/sed-replacing-characters-with-a-new-line/</link>
      <pubDate>Sat, 29 Dec 2012 17:49:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/12/29/sed-replacing-characters-with-a-new-line/</guid>
      <description>I&#39;ve been playing around with writing some algorithms in both Ruby and Haskell and the latter wasn&#39;t giving the correct result so I wanted to output an intermediate state of the two programs and compare them.
I didn&#39;t do any fancy formatting of the output from either program so I had the raw data structures in text files which I needed to transform so that they were comparable.
The main thing I wanted to do was get each of the elements of the collection onto their own line.</description>
    </item>
    
    <item>
      <title>Unix: Counting the number of commas on a line</title>
      <link>https://markhneedham.com/blog/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</link>
      <pubDate>Sat, 10 Nov 2012 16:30:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/10/unix-counting-the-number-of-commas-on-a-line/</guid>
      <description>A few weeks ago I was playing around with some data stored in a CSV file and wanted to do a simple check on the quality of the data by making sure that each line had the same number of fields.
One way this can be done is with awk:
awk -F &amp;quot;,&amp;quot; &#39; { print NF-1 } &#39; file.csv  Here we&amp;rsquo;re specifying the file separator -F as &amp;lsquo;,&amp;rsquo; and then using the NF (number of fields) variable to print how many commas there are on the line.</description>
    </item>
    
    <item>
      <title>Upstart: Job getting stuck in the start/killed state</title>
      <link>https://markhneedham.com/blog/2012/09/29/upstart-job-getting-stuck-in-the-startkilled-state/</link>
      <pubDate>Sat, 29 Sep 2012 09:56:16 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/29/upstart-job-getting-stuck-in-the-startkilled-state/</guid>
      <description>We&amp;rsquo;re using upstart to handle the processes running on our machines and since the haproxy package only came package with an init.d script we wanted to make it upstartified.
When defining an upstart script you need to specify an expect stanza in which you specify whether or not the process which you&amp;rsquo;re launching is going to fork.
 If you do not specify the expect stanza, Upstart will track the life cycle of the first PID that it executes in the exec or script stanzas.</description>
    </item>
    
    <item>
      <title>Finding ways to use bash command line history shortcuts</title>
      <link>https://markhneedham.com/blog/2012/09/19/finding-ways-to-use-bash-command-line-history-shortcuts/</link>
      <pubDate>Wed, 19 Sep 2012 07:00:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/19/finding-ways-to-use-bash-command-line-history-shortcuts/</guid>
      <description>A couple of months ago I wrote about a bunch of command line history shortcuts that Phil had taught me and after recently coming across Peteris Krumins&amp;rsquo; bash history cheat sheet I thought it&amp;rsquo;d be interesting to find some real ways to use them.
A few weeks ago I wrote about a UTF-8 byte order mark (BOM) that I wanted to remove from a file I was working on and I realised this evening that there were some other files with the same problem.</description>
    </item>
    
    <item>
      <title>zsh: Don&#39;t verify substituted history expansion a.k.a.  disabling histverify</title>
      <link>https://markhneedham.com/blog/2012/09/16/zsh-dont-verify-substituted-history-expansion-a-k-a-disabling-histverify/</link>
      <pubDate>Sun, 16 Sep 2012 13:35:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/16/zsh-dont-verify-substituted-history-expansion-a-k-a-disabling-histverify/</guid>
      <description>I use zsh on my Mac terminal and in general I prefer it to bash but it has an annoying default setting whereby when you try to repeat a command via substituted history expansion it asks you to verify that.
For example let&amp;rsquo;s say by mistake I try to vi into a directory rather than cd&amp;rsquo;ing into it:
vi ~/.oh-my-zsh  If I try to cd into the directory by using &amp;lsquo;!</description>
    </item>
    
    <item>
      <title>cURL and the case of the carriage return</title>
      <link>https://markhneedham.com/blog/2012/09/15/curl-and-the-case-of-the-carriage-return/</link>
      <pubDate>Sat, 15 Sep 2012 09:06:02 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/15/curl-and-the-case-of-the-carriage-return/</guid>
      <description>We were doing some work this week where we needed to make a couple of calls to an API via a shell script and in the first call we wanted to capture one of the lines of the HTTP response headers and use that as in input to the second call.
The way we were doing this was something like the following:
#!/bin/bash # We were actually grabbing a different header but for the sake # of this post we&#39;ll say it was &#39;Set-Cookie&#39; AUTH_HEADER=`curl -I http://www.</description>
    </item>
    
    <item>
      <title>Bash: Piping data into a command using heredocs</title>
      <link>https://markhneedham.com/blog/2012/09/15/bash-piping-data-into-a-command-using-heredocs/</link>
      <pubDate>Sat, 15 Sep 2012 07:54:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/15/bash-piping-data-into-a-command-using-heredocs/</guid>
      <description>I&amp;rsquo;ve been playing around with some data modelled in neo4j recently and one thing I wanted to do is run an adhoc query in the neo4j-shell and grab the results and do some text manipulation on them.
For example I wrote a query which outputted the following to the screen and I wanted to sum together all the values in the 3rd column:
| [&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;] | &amp;quot;3&amp;quot; | 1234567 | | [&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;,&amp;quot;6&amp;quot;] | &amp;quot;6&amp;quot; | 8910112 |  Initially I was pasting the output into a text file and then running the following sequence of commands to work it out:</description>
    </item>
    
    <item>
      <title>Unix: Caught out by shell significant characters</title>
      <link>https://markhneedham.com/blog/2012/09/13/unix-caught-out-by-shell-significant-characters/</link>
      <pubDate>Thu, 13 Sep 2012 00:17:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/13/unix-caught-out-by-shell-significant-characters/</guid>
      <description>One of the applications that Phil and I were deploying today needed a MySQL server and part of our puppet code to provision that node type runs a command to setup the privileges for a database user.
The unevaluated puppet code reads like this:
/usr/bin/mysql -h ${host} -uroot ${rootpassarg} -e &amp;quot;grant all on ${name}.* to ${user}@&#39;${remote_host}&#39; identified by &#39;$password&#39;; flush privileges;&amp;quot;  In the application we were deploying that expanded into something like this:</description>
    </item>
    
    <item>
      <title>While waiting for VMs to provision...</title>
      <link>https://markhneedham.com/blog/2012/09/12/while-waiting-for-vms-to-provision/</link>
      <pubDate>Wed, 12 Sep 2012 22:53:39 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/12/while-waiting-for-vms-to-provision/</guid>
      <description>Phil and I spent part of the day provisioning new virtual machines for some applications that we need to deploy which involves running a provisioning script and then opening another terminal and repeatedly trying to ssh into the box until it succeeds.
Eventually we got bored of doing that so we figured out a nice little one liner to use instead:
while :; do ssh 10.0.0.2; done  The &amp;lsquo;:&amp;rsquo; is a bash noop and is defined like so:</description>
    </item>
    
    <item>
      <title>SSHing onto machines via a jumpbox</title>
      <link>https://markhneedham.com/blog/2012/08/10/sshing-onto-machines-via-a-jumpbox/</link>
      <pubDate>Fri, 10 Aug 2012 00:58:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/08/10/sshing-onto-machines-via-a-jumpbox/</guid>
      <description>We wanted to be able to ssh into some machines which were behind a firewall so we set up a jumpbox which our firewall directed any traffic on port 22 towards.
Initially if we wanted to SSH onto a machine inside the network we&amp;rsquo;d have to do a two step process:
$ ssh jumpbox # now on the jumpbx $ ssh internal-network-machine  That got a bit annoying after a while so Sam showed us a neat way of proxying the second ssh command through the first one by making use of netcat.</description>
    </item>
    
    <item>
      <title>VCloud Guest Customization Script : [: postcustomization: unexpected operator</title>
      <link>https://markhneedham.com/blog/2012/08/06/vcloud-guest-customization-script-postcustomization-unexpected-operator/</link>
      <pubDate>Mon, 06 Aug 2012 21:50:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/08/06/vcloud-guest-customization-script-postcustomization-unexpected-operator/</guid>
      <description>We have been doing some work to automatically provision machines using the VCloud API via fog and one of the things we wanted to do was run a custom script the first time that a node powers on.
The following explains how customization scripts work:  In vCloud Director, when setting a customization script in a virtual machine, the script:  Is called only on initial customization and force recustomization. Is called with the precustomization command line parameter before out-of-box customization begins.</description>
    </item>
    
    <item>
      <title>Unix: tee</title>
      <link>https://markhneedham.com/blog/2012/07/29/unix-tee/</link>
      <pubDate>Sun, 29 Jul 2012 19:11:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/07/29/unix-tee/</guid>
      <description>I&amp;rsquo;ve read about the Unix &amp;lsquo;tee&amp;rsquo; command before but never found a reason to use it until the last few weeks.
One of the things I repeatedly do by mistake is open /etc/hosts without sudo and then try to make changes to it:
$ vi /etc/hosts # Editing it leads to the dreaded &#39;W10: Changing a readonly file&#39;  I always used to close the file and then re-open it with sudo but I recently came across an approach which allows us to use &amp;lsquo;tee&amp;rsquo; to get around the problem.</description>
    </item>
    
    <item>
      <title>Bash Shell: Reusing parts of previous commands</title>
      <link>https://markhneedham.com/blog/2012/07/05/bash-shell-reusing-parts-of-previous-commands/</link>
      <pubDate>Thu, 05 Jul 2012 23:42:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/07/05/bash-shell-reusing-parts-of-previous-commands/</guid>
      <description>I&amp;rsquo;ve paired a few times with my colleague Phil Potter over the last couple of weeks and since he&amp;rsquo;s a bit of a ninja with bash shortcuts/commands I wanted to record some of the things he&amp;rsquo;s shown me so I won&amp;rsquo;t forget them!
Let&amp;rsquo;s say we&amp;rsquo;re in the &amp;lsquo;/tmp&amp;rsquo; directory and want to create a folder a few levels down but forget to pass the &amp;lsquo;-p&amp;rsquo; option to &amp;lsquo;mkdir&amp;rsquo;:
$ mkdir blah/de/blah mkdir: cannot create directory `blah/de/blah&#39;: No such file or directory  One way of fixing that would be to press the up arrow and navigate along the previous command and put in the &amp;lsquo;-p&amp;rsquo; flag but it&amp;rsquo;s a bit fiddly so instead we can do the following:</description>
    </item>
    
    <item>
      <title>Learning Unix find: Searching in/Excluding certain folders</title>
      <link>https://markhneedham.com/blog/2011/10/21/learning-unix-find-searching-inexcluding-certain-folders/</link>
      <pubDate>Fri, 21 Oct 2011 21:25:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/21/learning-unix-find-searching-inexcluding-certain-folders/</guid>
      <description>I love playing around with commands on the Unix shell but one of the ones that I&amp;rsquo;ve found the most difficult to learn beyond the very basics is find.
I think this is partially because I find the find man page quite difficult to read and partially because it&amp;rsquo;s usually quicker to work out how to solve my problem with a command I already know than to learn another one.</description>
    </item>
    
    <item>
      <title>Bash: Reusing previous commands</title>
      <link>https://markhneedham.com/blog/2011/10/13/bash-reusing-previous-commands/</link>
      <pubDate>Thu, 13 Oct 2011 19:46:20 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/13/bash-reusing-previous-commands/</guid>
      <description>A lot of the time when I&amp;rsquo;m using the bash shell I want to re-use commands that I&amp;rsquo;ve previously entered and I&amp;rsquo;ve recently learnt some neat ways to do this from my colleagues Tom and Kief.
If we want to list the history of all the commands we&amp;rsquo;ve entered in a shell session then the following command does the trick:
&amp;gt; history ... 761 sudo port search pdfinfo 762 to_ipad andersen-phd-thesis.</description>
    </item>
    
    <item>
      <title>Unix: Getting the page count of a linearized PDF</title>
      <link>https://markhneedham.com/blog/2011/10/09/unix-getting-the-page-count-of-a-linearized-pdf/</link>
      <pubDate>Sun, 09 Oct 2011 11:34:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/10/09/unix-getting-the-page-count-of-a-linearized-pdf/</guid>
      <description>We were doing some work last week to rasterize a PDF document into a sequence of images and wanted to get a rough idea of how many pages we&amp;rsquo;d be dealing with if we created an image per page.
The PDFs we&amp;rsquo;re dealing with are linearized since they&amp;rsquo;re available for viewing on the web:
 A LINEARIZED PDF FILE is one that has been organized in a special way to enable efﬁcient incremental access in a network environment.</description>
    </item>
    
    <item>
      <title>Unix: Summing the total time from a log file</title>
      <link>https://markhneedham.com/blog/2011/07/27/unix-summing-the-total-time-from-a-log-file/</link>
      <pubDate>Wed, 27 Jul 2011 23:02:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/07/27/unix-summing-the-total-time-from-a-log-file/</guid>
      <description>As I mentioned in my last post we&amp;rsquo;ve been doing some profiling of a data ingestion job and as a result have been putting some logging into our code to try and work out where we need to work on.
We end up with a log file peppered with different statements which looks a bit like the following:
18:50:08.086 [akka:event-driven:dispatcher:global-5] DEBUG - Imported document. /Users/mneedham/foo.xml in: 1298 18:50:09.064 [akka:event-driven:dispatcher:global-1] DEBUG - Imported document.</description>
    </item>
    
    <item>
      <title>mount_smbfs: mount error..File exists</title>
      <link>https://markhneedham.com/blog/2011/01/15/mount_smbfs-mount-error-file-exists/</link>
      <pubDate>Sat, 15 Jan 2011 18:31:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/01/15/mount_smbfs-mount-error-file-exists/</guid>
      <description>I&amp;rsquo;ve been playing around with mounting a Windows file share onto my machine via the terminal because I&amp;rsquo;m getting bored of constantly having to go to Finder and manually mounting it each time!
After a couple of times of mounting and unmounting the drive I ended up with this error:
&amp;gt; mount_smbfs //mneedham@punedc02/shared punedc02_shared/ mount_smbfs: mount error: /Volumes/punedc02_shared: File exists  I originally thought the &amp;lsquo;file exists&amp;rsquo; part of the message was suggesting that I&amp;rsquo;d already mounted a share on &amp;lsquo;punedc02_shared&amp;rsquo; but calling the &amp;lsquo;umount&amp;rsquo; command led to the following error:</description>
    </item>
    
    <item>
      <title>Sed: &#39;sed: 1: invalid command code R&#39; on Mac OS X</title>
      <link>https://markhneedham.com/blog/2011/01/14/sed-sed-1-invalid-command-code-r-on-mac-os-x/</link>
      <pubDate>Fri, 14 Jan 2011 14:15:19 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/01/14/sed-sed-1-invalid-command-code-r-on-mac-os-x/</guid>
      <description>A few days ago I wrote about how we&amp;rsquo;d been using Sed to edit multiple files and while those examples were derived from what we&amp;rsquo;d been using on Ubuntu I realised that they didn&amp;rsquo;t actually work on Mac OS X.
For example, the following command:
sed -i &#39;s/require/include/&#39; Rakefile  Throws this error:
sed: 1: &amp;quot;Rakefile&amp;quot;: invalid command code R  What I hadn&amp;rsquo;t realised is that on the Mac version of sed the &amp;lsquo;-i&amp;rsquo; flag has a mandatory suffix, as described in this post.</description>
    </item>
    
    <item>
      <title>Sed across multiple files</title>
      <link>https://markhneedham.com/blog/2011/01/11/sed-across-multiple-files/</link>
      <pubDate>Tue, 11 Jan 2011 16:43:53 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2011/01/11/sed-across-multiple-files/</guid>
      <description>Pankhuri and I needed to rename a method and change all the places where it was used and decided to see if we could work out how to do it using sed.
We needed to change a method call roughly like this:
home_link(current_user)  To instead read:
homepage_path  For which we need the following sed expression:
sed -i &#39;s/home_link([^)]*)/homepage_path/&#39; [file_name]  Which works pretty well if you know which file you want to change but we wanted to run it over the whole code base.</description>
    </item>
    
    <item>
      <title>A dirty hack to get around aliases not working in a shell script</title>
      <link>https://markhneedham.com/blog/2010/11/24/a-dirty-hack-to-get-around-aliases-not-working-in-a-shell-script/</link>
      <pubDate>Wed, 24 Nov 2010 18:48:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2010/11/24/a-dirty-hack-to-get-around-aliases-not-working-in-a-shell-script/</guid>
      <description>In another script I&amp;rsquo;ve been working on lately I wanted to call &amp;lsquo;mysql&amp;rsquo; but unfortunately on my machine it&amp;rsquo;s &amp;lsquo;mysql5&amp;rsquo; rather than &amp;lsquo;mysql&amp;rsquo;.
I have an alias defined in &amp;lsquo;~/.bash_profile&amp;rsquo; so I can call &amp;lsquo;mysql&amp;rsquo; from the terminal whenever I want to.
alias mysql=mysql5  Unfortunately shell scripts don&amp;rsquo;t seem to have access to this alias and the only suggestion I&amp;rsquo;ve come across while googling this is to source &amp;lsquo;~/.bash_profile&amp;rsquo; inside the script.</description>
    </item>
    
    <item>
      <title>Browsing around the Unix shell more easily</title>
      <link>https://markhneedham.com/blog/2008/10/15/browsing-around-the-unix-shell-more-easily/</link>
      <pubDate>Wed, 15 Oct 2008 22:31:16 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2008/10/15/browsing-around-the-unix-shell-more-easily/</guid>
      <description>Following on from my post about getting the pwd to display on the bash prompt all the time I have learnt a couple of other tricks to make the shell experience more productive.
Aliases are the first new concept I came across and several members of my current team and I now have these setup.
We are primarily using them to provide a shortcut command to get to various locations in the file system.</description>
    </item>
    
    <item>
      <title>Show pwd all the time</title>
      <link>https://markhneedham.com/blog/2008/09/28/show-pwd-all-the-time/</link>
      <pubDate>Sun, 28 Sep 2008 22:50:44 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2008/09/28/show-pwd-all-the-time/</guid>
      <description>Finally back in the world of the shell last week I was constantly typing &amp;lsquo;pwd&amp;rsquo; to work out where exactly I was in the file system until my colleague pointed out that you can adjust your settings to get this to show up automatically for you on the left hand side of the prompt.
To do this you need to create or edit your .bash_profile file by entering the following command:</description>
    </item>
    
  </channel>
</rss>