<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Mark Needham</title>
    <link>http://markhneedham.com/blog/category/r/</link>
    <description>Recent content in R on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Jul 2016 06:36:51 +0000</lastBuildDate>
    
	<atom:link href="http://markhneedham.com/blog/category/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>R: Sentiment analysis of morning pages</title>
      <link>http://markhneedham.com/blog/2016/07/09/r-sentiment-analysis-of-morning-pages/</link>
      <pubDate>Sat, 09 Jul 2016 06:36:51 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2016/07/09/r-sentiment-analysis-of-morning-pages/</guid>
      <description>A couple of months ago I came across a cool blog post by Julia Silge where she runs a sentiment analysis algorithm over her tweet stream to see how her tweet sentiment has varied over time.  I wanted to give it a try but couldn&#39;t figure out how to get a dump of my tweets so I decided to try it out on the text from my morning pages writing which I&#39;ve been experimenting with for a few months.</description>
    </item>
    
    <item>
      <title>R: substr - Getting a vector of positions</title>
      <link>http://markhneedham.com/blog/2016/04/18/r-substr-getting-a-vector-of-positions/</link>
      <pubDate>Mon, 18 Apr 2016 19:49:02 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2016/04/18/r-substr-getting-a-vector-of-positions/</guid>
      <description>I recently found myself writing an R script to extract parts of a string based on a beginning and end index which is reasonably easy using the substr function: &amp;gt; substr(&amp;quot;mark loves graphs&amp;quot;, 0, 4) [1] &amp;quot;mark&amp;quot;   But what if we have a vector of start and end positions? &amp;gt; substr(&amp;quot;mark loves graphs&amp;quot;, c(0, 6), c(4, 10)) [1] &amp;quot;mark&amp;quot;  Hmmm that didn&#39;t work as I expected! It turns out we actually need to use the substring function instead which wasn&#39;t initially obvious to me on reading the documentation:</description>
    </item>
    
    <item>
      <title>R: tm - Unique words/terms per document</title>
      <link>http://markhneedham.com/blog/2016/04/11/r-tm-unique-wordsterms-per-document/</link>
      <pubDate>Mon, 11 Apr 2016 05:40:06 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2016/04/11/r-tm-unique-wordsterms-per-document/</guid>
      <description>I&#39;ve been doing a bit of text mining over the weekend using the R tm package and I wanted to only count a term once per document which isn&#39;t how it works out the box. For example let&#39;s say we&#39;re writing a bit of code to calculate the frequency of terms across some documents. We might write the following code:
library(tm) text = c(&amp;quot;I am Mark I am Mark&amp;quot;, &amp;quot;Neo4j is cool Neo4j is cool&amp;quot;) corpus = VCorpus(VectorSource(text)) tdm = as.</description>
    </item>
    
    <item>
      <title>R: Error in approxfun(x.values.1, y.values.1, method = &#34;constant&#34;, f = 1, :  zero non-NA points</title>
      <link>http://markhneedham.com/blog/2015/12/27/r-error-in-approxfunx-values-1-y-values-1-method-constant-f-1-zero-non-na-points/</link>
      <pubDate>Sun, 27 Dec 2015 12:24:05 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/12/27/r-error-in-approxfunx-values-1-y-values-1-method-constant-f-1-zero-non-na-points/</guid>
      <description>I&#39;ve been following Michy Alice&#39;s logistic regression tutorial to create an attendance model for London dev meetups and ran into an interesting problem while doing so.  Our dataset has a class imbalance i.e. most people RSVP &#39;no&#39; to events which can lead to misleading accuracy score where predicting &#39;no&#39; every time would lead to supposed high accuracy. Source: local data frame [2 x 2] attended n (dbl) (int) 1 0 1541 2 1 53  I sampled the data using caret&#39;s upSample function to avoid this: attended = as.</description>
    </item>
    
    <item>
      <title>R: data.table - Finding the maximum row</title>
      <link>http://markhneedham.com/blog/2015/10/02/r-data-table-finding-the-maximum-row/</link>
      <pubDate>Fri, 02 Oct 2015 18:42:47 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/10/02/r-data-table-finding-the-maximum-row/</guid>
      <description>In my continued playing around with the R data.table package I wanted to find the maximum row based on one of the columns, grouped by another column, and then return back the whole row. We&#39;ll use the following data table to illustrate:
&amp;gt; blogDT = data.table(name = c(&amp;quot;Property 1&amp;quot;,&amp;quot;Property 1&amp;quot;,&amp;quot;Property 1&amp;quot;,&amp;quot;Property 2&amp;quot;,&amp;quot;Property 2&amp;quot;,&amp;quot;Property 2&amp;quot;), price = c(10000, 12500, 18000, 245000, 512000, 1000000), date = c(&amp;quot;Day 1&amp;quot;, &amp;quot;Day 7&amp;quot;, &amp;quot;Day 10&amp;quot;, &amp;quot;Day 3&amp;quot;, &amp;quot;Day 5&amp;quot;, &amp;quot;Day 12&amp;quot;)) &amp;gt; blogDT[, lag.</description>
    </item>
    
    <item>
      <title>R: data.table - Comparing adjacent rows</title>
      <link>http://markhneedham.com/blog/2015/09/27/r-data-table-comparing-adjacent-rows/</link>
      <pubDate>Sun, 27 Sep 2015 22:02:07 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/09/27/r-data-table-comparing-adjacent-rows/</guid>
      <description>As part of my exploration of the Land Registry price paid data set I wanted to compare the difference between consecutive sales of properties.  This means we need to group the sales by a property identifier and then get the previous sale price into a column on each row unless it&#39;s the first sale in which case we&#39;ll have &#39;NA&#39;. We can do this by creating a lag variable.</description>
    </item>
    
    <item>
      <title>R: Querying a 20 million line CSV file - data.table vs data frame</title>
      <link>http://markhneedham.com/blog/2015/09/25/r-querying-a-20-million-line-csv-file-data-table-vs-data-frame/</link>
      <pubDate>Fri, 25 Sep 2015 06:28:29 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/09/25/r-querying-a-20-million-line-csv-file-data-table-vs-data-frame/</guid>
      <description>As I mentioned in a couple of blog posts already, I&#39;ve been exploring the Land Registry price paid data set and although I&#39;ve initially been using SparkR I was curious how easy it would be to explore the data set using plain R.  I thought I&#39;d start out by loading the data into a data frame and run the same queries using deployer.  I&#39;ve come across Hadley Wickham&#39;s readr library before but hadn&#39;t used it and since I needed to load a 20 million line CSV file this seemed the perfect time to give it a try.</description>
    </item>
    
    <item>
      <title>R: Bootstrap confidence intervals</title>
      <link>http://markhneedham.com/blog/2015/07/19/r-bootstrap-confidence-intervals/</link>
      <pubDate>Sun, 19 Jul 2015 19:44:59 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/19/r-bootstrap-confidence-intervals/</guid>
      <description>I recently came across an interesting post on Julia Evans&#39; blog showing how to generate a bigger set of data points by sampling the small set of data points that we actually have using bootstrapping. Julia&#39;s examples are all in Python so I thought it&#39;d be a fun exercise to translate them into R.  We&#39;re doing the bootstrapping to simulate the number of no-shows for a flight so we can work out how many seats we can overbook the plane by.</description>
    </item>
    
    <item>
      <title>R: Blog post frequency anomaly detection</title>
      <link>http://markhneedham.com/blog/2015/07/17/r-blog-post-frequency-anomaly-detection/</link>
      <pubDate>Fri, 17 Jul 2015 23:34:52 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/17/r-blog-post-frequency-anomaly-detection/</guid>
      <description>I came across Twitter&#39;s anomaly detection library last year but haven&#39;t yet had a reason to take it for a test run so having got my blog post frequency data into shape I thought it&#39;d be fun to run it through the algorithm. I wanted to see if it would detect any periods of time when the number of posts differed significantly - I don&#39;t really have an action I&#39;m going to take based on the results, it&#39;s curiosity more than anything else!</description>
    </item>
    
    <item>
      <title>R: I write more in the last week of the month, or do I?</title>
      <link>http://markhneedham.com/blog/2015/07/12/r-i-write-more-in-the-last-week-of-the-month-or-do-i/</link>
      <pubDate>Sun, 12 Jul 2015 09:53:04 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/12/r-i-write-more-in-the-last-week-of-the-month-or-do-i/</guid>
      <description>I&#39;ve been writing on this blog for almost 7 years and have always believed that I write more frequently towards the end of a month. Now that I&#39;ve got all the data I thought it&#39;d be interesting to test that belief.  I started with a data frame containing each post and its publication date and added an extra column which works out how many weeks from the end of the month that post was written: &amp;gt; df %&amp;gt;% sample_n(5) title date 946 Python: Equivalent to flatMap for flattening an array of arrays 2015-03-23 00:45:00 175 Ruby: Hash default value 2010-10-16 14:02:37 375 Java/Scala: Runtime.</description>
    </item>
    
    <item>
      <title>R: Filling in missing dates with 0s</title>
      <link>http://markhneedham.com/blog/2015/07/12/r-filling-in-missing-dates-with-0s/</link>
      <pubDate>Sun, 12 Jul 2015 08:30:40 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/12/r-filling-in-missing-dates-with-0s/</guid>
      <description>I wanted to plot a chart showing the number of blog posts published by month and started with the following code which makes use of zoo&#39;s &#39;as.yearmon&#39; function to add the appropriate column and grouping: &amp;gt; library(zoo) &amp;gt; library(dplyr) &amp;gt; df %&amp;gt;% sample_n(5) title date 888 R: Converting a named vector to a data frame 2014-10-31 23:47:26 144 Rails: Populating a dropdown list using &#39;form_for&#39; 2010-08-31 01:22:14 615 Onboarding: Sketch the landscape 2013-02-15 07:36:06 28 Javascript: The &#39;new&#39; keyword 2010-03-06 15:16:02 1290 Coding Dojo #16: Reading SUnit code 2009-05-28 23:23:19 &amp;gt; posts_by_date = df %&amp;gt;% mutate(year_mon = as.</description>
    </item>
    
    <item>
      <title>R: Date for given week/year</title>
      <link>http://markhneedham.com/blog/2015/07/10/r-date-for-given-weekyear/</link>
      <pubDate>Fri, 10 Jul 2015 22:01:58 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/10/r-date-for-given-weekyear/</guid>
      <description>As I mentioned in my last couple of blog posts I&#39;ve been looking at the data behind this blog and I wanted to plot a chart showing the number of posts per week since the blog started.  I started out with a data frame with posts and publication date: &amp;gt; library(dplyr) &amp;gt; df = read.csv(&amp;quot;posts.csv&amp;quot;) &amp;gt; df$date = ymd_hms(df$date) &amp;gt; df %&amp;gt;% sample_n(10) title date 538 Nygard Big Data Model: The Investigation Stage 2012-10-10 00:00:36 341 The read-only database 2011-08-29 23:32:26 1112 CSS in Internet Explorer - Some lessons learned 2008-10-31 15:24:51 143 Coding: Mutating parameters 2010-08-26 07:47:23 433 Scala: Counting number of inversions (via merge sort) for an unsorted collection 2012-03-20 06:53:18 618 neo4j/cypher: SQL style GROUP BY functionality 2013-02-17 21:05:27 1111 Testing Hibernate mappings: Setting up test data 2008-10-30 13:24:14 462 neo4j: What question do you want to answer?</description>
    </item>
    
    <item>
      <title>R: dplyr - Error: cannot modify grouping variable</title>
      <link>http://markhneedham.com/blog/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</link>
      <pubDate>Thu, 09 Jul 2015 05:55:33 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/09/r-dplyr-error-cannot-modify-grouping-variable/</guid>
      <description>I&#39;ve been doing some exploration of the posts made on this blog and I thought I&#39;d start with answering a simple question - on which dates did I write the most posts?  I started with a data frame containing each post and the date it was published: &amp;gt; library(dplyr) &amp;gt; df %&amp;gt;% sample_n(5) title date 1148 Taiichi Ohno&#39;s Workplace Management: Book Review 2008-12-08 14:14:48 158 Rails: Faking a delete method with &#39;form_for&#39; 2010-09-20 18:52:15 331 Retrospectives: The 4 L&#39;s Retrospective 2011-07-25 21:00:30 1035 msbuild - Use OutputPath instead of OutDir 2008-08-14 18:54:03 1181 The danger of commenting out code 2009-01-17 06:02:33   To find the most popular days for blog posts we can write the following aggregation function: &amp;gt; df %&amp;gt;% mutate(day = as.</description>
    </item>
    
    <item>
      <title>R: Wimbledon - How do the seeds get on?</title>
      <link>http://markhneedham.com/blog/2015/07/05/r-wimbledon-how-do-the-seeds-get-on/</link>
      <pubDate>Sun, 05 Jul 2015 08:38:03 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/05/r-wimbledon-how-do-the-seeds-get-on/</guid>
      <description>Continuing on with the Wimbledon data set I&#39;ve been playing with I wanted to do some exploration on how the seeded players have fared over the years.  Taking the last 10 years worth of data there have always had 32 seeds and with the following function we can feed in a seeding and get back the round they would be expected to reach: expected_round = function(seeding) { if(seeding == 1) { return(&amp;quot;Winner&amp;quot;) } else if(seeding == 2) { return(&amp;quot;Finals&amp;quot;) } else if(seeding &amp;lt;= 4) { return(&amp;quot;Semi-Finals&amp;quot;) } else if(seeding &amp;lt;= 8) { return(&amp;quot;Quarter-Finals&amp;quot;) } else if(seeding &amp;lt;= 16) { return(&amp;quot;Round of 16&amp;quot;) } else { return(&amp;quot;Round of 32&amp;quot;) } } &amp;gt; expected_round(1) [1] &amp;quot;Winner&amp;quot; &amp;gt; expected_round(4) [1] &amp;quot;Semi-Finals&amp;quot;   We can then have a look at each of the Wimbledon tournaments and work out how far they actually got.</description>
    </item>
    
    <item>
      <title>R: Calculating the difference between ordered factor variables</title>
      <link>http://markhneedham.com/blog/2015/07/02/r-calculating-the-difference-between-ordered-factor-variables/</link>
      <pubDate>Thu, 02 Jul 2015 22:55:01 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/07/02/r-calculating-the-difference-between-ordered-factor-variables/</guid>
      <description>In my continued exploration of Wimbledon data I wanted to work out whether a player had done as well as their seeding suggested they should. I therefore wanted to work out the difference between the round they reached and the round they were expected to reach. A &#39;round&#39; in the dataset is an ordered factor variable.  These are all the possible values: rounds = c(&amp;quot;Did not enter&amp;quot;, &amp;quot;Round of 128&amp;quot;, &amp;quot;Round of 64&amp;quot;, &amp;quot;Round of 32&amp;quot;, &amp;quot;Round of 16&amp;quot;, &amp;quot;Quarter-Finals&amp;quot;, &amp;quot;Semi-Finals&amp;quot;, &amp;quot;Finals&amp;quot;, &amp;quot;Winner&amp;quot;)  And if we want to factorise a couple of strings into this factor we would do it like this:</description>
    </item>
    
    <item>
      <title>R: write.csv - unimplemented type &#39;list&#39; in &#39;EncodeElement&#39;</title>
      <link>http://markhneedham.com/blog/2015/06/30/r-write-csv-unimplemented-type-list-in-encodeelement/</link>
      <pubDate>Tue, 30 Jun 2015 22:26:39 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/30/r-write-csv-unimplemented-type-list-in-encodeelement/</guid>
      <description>Everyone now and then I want to serialise an R data frame to a CSV file so I can easily load it up again if my R environment crashes without having to recalculate everything but recently ran into the following error: &amp;gt; write.csv(foo, &amp;quot;/tmp/foo.csv&amp;quot;, row.names = FALSE) Error in .External2(C_writetable, x, file, nrow(x), p, rnames, sep, eol, : unimplemented type &#39;list&#39; in &#39;EncodeElement&#39;  If we take a closer look at the data frame in question it looks ok:</description>
    </item>
    
    <item>
      <title>R: Speeding up the Wimbledon scraping job</title>
      <link>http://markhneedham.com/blog/2015/06/29/r-speeding-up-the-wimbledon-scraping-job/</link>
      <pubDate>Mon, 29 Jun 2015 05:36:22 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/29/r-speeding-up-the-wimbledon-scraping-job/</guid>
      <description>Over the past few days I&#39;ve written a few blog posts about a Wimbledon data set I&#39;ve been building and after running the scripts a few times I noticed that it was taking much longer to run that I expected.  To recap, I started out with the following function which takes in a URI and returns a data frame containing a row for each match: library(rvest) library(dplyr) scrape_matches1 = function(uri) { matches = data.</description>
    </item>
    
    <item>
      <title>R: dplyr - Update rows with earlier/previous rows values</title>
      <link>http://markhneedham.com/blog/2015/06/28/r-dplyr-update-rows-with-earlierprevious-rows-values/</link>
      <pubDate>Sun, 28 Jun 2015 22:30:08 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/28/r-dplyr-update-rows-with-earlierprevious-rows-values/</guid>
      <description>Recently I had a data frame which contained a column which had mostly empty values: &amp;gt; data.frame(col1 = c(1,2,3,4,5), col2 = c(&amp;quot;a&amp;quot;, NA, NA , &amp;quot;b&amp;quot;, NA)) col1 col2 1 1 a 2 2 &amp;lt;NA&amp;gt; 3 3 &amp;lt;NA&amp;gt; 4 4 b 5 5 &amp;lt;NA&amp;gt;   I wanted to fill in the NA values with the last non NA value from that column. So I want the data frame to look like this: 1 1 a 2 2 a 3 3 a 4 4 b 5 5 b   I spent ages searching around before I came across the na.</description>
    </item>
    
    <item>
      <title>R: Command line - Error in GenericTranslator$new : could not find function &#34;loadMethod&#34;</title>
      <link>http://markhneedham.com/blog/2015/06/27/r-command-line-error-in-generictranslatornew-could-not-find-function-loadmethod/</link>
      <pubDate>Sat, 27 Jun 2015 22:47:22 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/27/r-command-line-error-in-generictranslatornew-could-not-find-function-loadmethod/</guid>
      <description>I&#39;ve been reading Text Processing with Ruby over the last week or so and one of the ideas the author describes is setting up your scripts so you can run them directly from the command line.
 I wanted to do this with my Wimbledon R script and wrote the following script which uses the &#39;Rscript&#39; executable so that R doesn&#39;t launch in interactive mode: wimbledon
#!/usr/bin/env Rscript library(rvest) library(dplyr) library(stringr) library(readr) # stuff   Then I tried to run it: $ time .</description>
    </item>
    
    <item>
      <title>R: dplyr - squashing multiple rows per group into one</title>
      <link>http://markhneedham.com/blog/2015/06/27/r-dplyr-squashing-multiple-rows-per-group-into-one/</link>
      <pubDate>Sat, 27 Jun 2015 22:36:50 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/27/r-dplyr-squashing-multiple-rows-per-group-into-one/</guid>
      <description>I spent a bit of the day working on my Wimbledon data set and the next thing I explored is all the people that have beaten Andy Murray in the tournament. The following dplyr query gives us the names of those people and the year the match took place:
library(dplyr) &amp;gt; main_matches %&amp;gt;% filter(loser == &amp;quot;Andy Murray&amp;quot;) %&amp;gt;% select(winner, year) winner year 1 Grigor Dimitrov 2014 2 Roger Federer 2012 3 Rafael Nadal 2011 4 Rafael Nadal 2010 5 Andy Roddick 2009 6 Rafael Nadal 2008 7 Marcos Baghdatis 2006 8 David Nalbandian 2005  As you can see, Rafael Nadal shows up multiple times.</description>
    </item>
    
    <item>
      <title>R: ggplot - Show discrete scale even with no value</title>
      <link>http://markhneedham.com/blog/2015/06/26/r-ggplot-show-discrete-scale-even-with-no-value/</link>
      <pubDate>Fri, 26 Jun 2015 22:48:17 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/26/r-ggplot-show-discrete-scale-even-with-no-value/</guid>
      <description>As I mentioned in a previous blog post, I&#39;ve been scraping data for the Wimbledon tennis tournament, and having got the data for the last ten years I wrote a query using dplyr to find out how players did each year over that period.  I ended up with the following functions to filter my data frame of all the matches: round_reached = function(player, main_matches) { furthest_match = main_matches %&amp;gt;% filter(winner == player | loser == player) %&amp;gt;% arrange(desc(round)) %&amp;gt;% head(1) return(ifelse(furthest_match$winner == player, &amp;quot;Winner&amp;quot;, as.</description>
    </item>
    
    <item>
      <title>R: Scraping Wimbledon draw data</title>
      <link>http://markhneedham.com/blog/2015/06/25/r-scraping-wimbledon-draw-data/</link>
      <pubDate>Thu, 25 Jun 2015 23:14:51 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/25/r-scraping-wimbledon-draw-data/</guid>
      <description>Given Wimbledon starts next week I wanted to find a data set to explore before it gets underway. Having searched around and failed to find one I had to resort to scraping the ATP World Tour&#39;s event page which displays the matches in an easy to access format. We&#39;ll be using the Wimbledon 2013 draw since Andy Murray won that year! This is what the page looks like:
  Each match is in its own row of a table and each column has a class attribute which makes it really easy to scrape.</description>
    </item>
    
    <item>
      <title>R: Scraping the release dates of github projects</title>
      <link>http://markhneedham.com/blog/2015/06/23/r-scraping-the-release-dates-of-github-projects/</link>
      <pubDate>Tue, 23 Jun 2015 22:34:47 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/23/r-scraping-the-release-dates-of-github-projects/</guid>
      <description>Continuing on from my blog post about scraping Neo4j&#39;s release dates I thought it&#39;d be even more interesting to chart the release dates of some github projects.
In theory the release dates should be accessible through the github API but the few that I looked at weren&#39;t returning any data so I scraped the data together.  We&#39;ll be using rvest again and I first wrote the following function to extract the release versions and dates from a single page: library(dplyr) library(rvest) process_page = function(releases, session) { rows = session %&amp;gt;% html_nodes(&amp;quot;ul.</description>
    </item>
    
    <item>
      <title>R: Scraping Neo4j release dates with rvest</title>
      <link>http://markhneedham.com/blog/2015/06/21/r-scraping-neo4j-release-dates-with-rvest/</link>
      <pubDate>Sun, 21 Jun 2015 22:07:49 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/21/r-scraping-neo4j-release-dates-with-rvest/</guid>
      <description>As part of my log analysis I wanted to get the Neo4j release dates which are accessible from the release notes and decided to try out Hadley Wickham&#39;s rvest scraping library which he released at the end of 2014.  rvest is based on Python&#39;s beautifulsoup which has become my scraping library of choice so I didn&#39;t find it too difficult to pick up. To start with we need to download the release notes locally so we don&#39;t have to go over the network when we&#39;re doing our scraping: download.</description>
    </item>
    
    <item>
      <title>R: dplyr - segfault cause &#39;memory not mapped&#39;</title>
      <link>http://markhneedham.com/blog/2015/06/20/r-dplyr-segfault-cause-memory-not-mapped/</link>
      <pubDate>Sat, 20 Jun 2015 22:18:55 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/20/r-dplyr-segfault-cause-memory-not-mapped/</guid>
      <description>In my continued playing around with web logs in R I wanted to process the logs for a day and see what the most popular URIs were.  I first read in all the lines using the read_lines function in readr and put the vector it produced into a data frame so I could process it using dplyr. library(readr) dlines = data.frame(column = read_lines(&amp;quot;~/projects/logs/2015-06-18-22-docs&amp;quot;))   In the previous post I showed some code to extract the URI from a log line.</description>
    </item>
    
    <item>
      <title>R: Regex - capturing multiple matches of the same group</title>
      <link>http://markhneedham.com/blog/2015/06/19/r-regex-capturing-multiple-matches-of-the-same-group/</link>
      <pubDate>Fri, 19 Jun 2015 21:38:47 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/19/r-regex-capturing-multiple-matches-of-the-same-group/</guid>
      <description>I&#39;ve been playing around with some web logs using R and I wanted to extract everything that existed in double quotes within a logged entry.  This is an example of a log entry that I want to parse: log = &#39;2015-06-18-22:277:548311224723746831\t2015-06-18T22:00:11\t2015-06-18T22:00:05Z\t93317114\tip-127-0-0-1\t127.0.0.5\tUser\tNotice\tneo4j.com.access.log\t127.0.0.3 - - [18/Jun/2015:22:00:11 +0000] &amp;quot;GET /docs/stable/query-updating.html HTTP/1.1&amp;quot; 304 0 &amp;quot;http://neo4j.com/docs/stable/cypher-introduction.html&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36&amp;quot;&#39;  And I want to extract these 3 things:</description>
    </item>
    
    <item>
      <title>R: ggplot geom_density - Error in exists(name, envir = env, mode = mode) : argument &#34;env&#34; is missing, with no default</title>
      <link>http://markhneedham.com/blog/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</link>
      <pubDate>Wed, 03 Jun 2015 05:52:08 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/03/r-ggplot-geom_density-error-in-existsname-envir-env-mode-mode-argument-env-is-missing-with-no-default/</guid>
      <description>Continuing on from yesterday&#39;s blog post where I worked out how to clean up the Think Bayes Price is Right data set, the next task was to plot a distribution of the prices of show case items.  To recap, this is what the data frame we&#39;re working with looks like: library(dplyr) df2011 = read.csv(&amp;quot;~/projects/rLearning/showcases.2011.csv&amp;quot;, na.strings = c(&amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;)) df2011 = df2011 %&amp;gt;% na.omit() &amp;gt; df2011 %&amp;gt;% head() X Sep.</description>
    </item>
    
    <item>
      <title>R: dplyr - removing empty rows</title>
      <link>http://markhneedham.com/blog/2015/06/02/r-dplyr-removing-empty-rows/</link>
      <pubDate>Tue, 02 Jun 2015 06:49:10 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/06/02/r-dplyr-removing-empty-rows/</guid>
      <description>I&#39;m still working my way through the exercises in Think Bayes and in Chapter 6 needed to do some cleaning of the data in a CSV file containing information about the Price is Right.  I downloaded the file using wget: wget ￼http://www.greenteapress.com/thinkbayes/showcases.2011.csv￼  And then loaded it into R and explored the first few rows using dplyr
library(dplyr) df2011 = read.csv(&amp;quot;~/projects/rLearning/showcases.2011.csv&amp;quot;) &amp;gt; df2011 %&amp;gt;% head(10) X Sep..19 Sep..20 Sep.</description>
    </item>
    
    <item>
      <title>R: Think Bayes Euro Problem</title>
      <link>http://markhneedham.com/blog/2015/05/31/r-think-bayes-euro-problem/</link>
      <pubDate>Sun, 31 May 2015 23:11:50 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/05/31/r-think-bayes-euro-problem/</guid>
      <description>I&#39;ve got back to working my way through Think Bayes after a month&#39;s break and started out with the one euro coin problem in Chapter 4:  A statistical statement appeared in “The Guardian&#34; on Friday January 4, 2002: When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. ‘It looks very suspicious to me,’ said Barry Blight, a statistics lecturer at the London School of Economics.</description>
    </item>
    
    <item>
      <title>R: ggplot - Displaying multiple charts with a for loop</title>
      <link>http://markhneedham.com/blog/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</link>
      <pubDate>Thu, 14 May 2015 00:17:02 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/05/14/r-ggplot-displaying-multiple-charts-with-a-for-loop/</guid>
      <description>Continuing with my analysis of the Neo4j London user group I wanted to drill into some individual meetups and see the makeup of the people attending those meetups with respect to the cohort they belong to.  I started by writing a function which would take in an event ID and output a bar chart showing the number of people who attended that event from each cohort.   We can work out the cohort that a member belongs to by querying for the first event they attended.</description>
    </item>
    
    <item>
      <title>R: Cohort heatmap of Neo4j London meetup</title>
      <link>http://markhneedham.com/blog/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</link>
      <pubDate>Mon, 11 May 2015 23:16:07 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/05/11/r-cohort-heatmap-of-neo4j-london-meetup/</guid>
      <description>A few months ago I had a go at doing some cohort analysis of the Neo4j London meetup group which was an interesting experiment but unfortunately resulted in a chart that was completely illegible.    I wasn&#39;t sure how to progress from there but a few days ago I came across the cohort heatmap which seemed like a better way of visualising things over time. The underlying idea is still the same - we&#39;ve comparing different cohorts of users against each other to see whether a change or intervention we did at a certain time had any impact.</description>
    </item>
    
    <item>
      <title>R: Neo4j London meetup group - How many events do people come to?</title>
      <link>http://markhneedham.com/blog/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</link>
      <pubDate>Sat, 09 May 2015 22:33:05 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/05/09/r-neo4j-london-meetup-group-how-many-events-do-people-come-to/</guid>
      <description>Earlier this week the number of members in the Neo4j London meetup group creeped over the 2,000 mark and I thought it&#39;d be fun to re-explore the data that I previously imported into Neo4j.  How often do people come to meetups? library(RNeo4j) library(dplyr) graph = startGraph(&amp;quot;http://localhost:7474/db/data/&amp;quot;) query = &amp;quot;MATCH (g:Group {name: &#39;Neo4j - London User Group&#39;})-[:HOSTED_EVENT]-&amp;gt;(event)&amp;lt;-[:TO]-({response: &#39;yes&#39;})&amp;lt;-[:RSVPD]-(profile)-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g) WHERE (event.time + event.utc_offset) &amp;lt; timestamp() RETURN event.id, event.time + event.utc_offset AS eventTime, profile.</description>
    </item>
    
    <item>
      <title>R: dplyr - Error in (list: invalid subscript type &#39;double&#39;</title>
      <link>http://markhneedham.com/blog/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</link>
      <pubDate>Mon, 27 Apr 2015 22:34:43 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/27/r-dplyr-error-in-list-invalid-subscript-type-double/</guid>
      <description>In my continued playing around with R I wanted to find the minimum value for a specified percentile given a data frame representing a cumulative distribution function (CDF).
e.g. imagine we have the following CDF represented in a data frame:
library(dplyr) df = data.frame(score = c(5,7,8,10,12,20), percentile = c(0.05,0.1,0.15,0.20,0.25,0.5))   and we want to find the minimum value for the 0.05 percentile. We can use the filter function to do so: &amp;gt; (df %&amp;gt;% filter(percentile &amp;gt; 0.</description>
    </item>
    
    <item>
      <title>R: Think Bayes Locomotive Problem - Posterior probabilities for different priors</title>
      <link>http://markhneedham.com/blog/2015/04/24/r-think-bayes-locomotive-problem-posterior-probabilities-for-different-priors/</link>
      <pubDate>Fri, 24 Apr 2015 23:53:12 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/24/r-think-bayes-locomotive-problem-posterior-probabilities-for-different-priors/</guid>
      <description>In my continued reading of Think Bayes the next problem to tackle is the Locomotive problem which is defined thus:  A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many loco- motives the railroad has.   The interesting thing about this question is that it initially seems that we don&#39;t have enough information to come up with any sort of answer.</description>
    </item>
    
    <item>
      <title>R: Replacing for loops with data frames</title>
      <link>http://markhneedham.com/blog/2015/04/22/r-replacing-for-loops-with-data-frames/</link>
      <pubDate>Wed, 22 Apr 2015 22:18:00 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/22/r-replacing-for-loops-with-data-frames/</guid>
      <description>In my last blog post I showed how to derive posterior probabilities for the Think Bayes dice problem:  Suppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons &amp; Dragons, you know what I am talking about. Suppose I select a die from the box at random, roll it, and get a 6.</description>
    </item>
    
    <item>
      <title>R: Numeric keys in the nested list/dictionary</title>
      <link>http://markhneedham.com/blog/2015/04/21/r-numeric-keys-in-the-nested-listdictionary/</link>
      <pubDate>Tue, 21 Apr 2015 05:59:24 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/21/r-numeric-keys-in-the-nested-listdictionary/</guid>
      <description>Last week I described how I&#39;ve been creating fake dictionaries in R using lists and I found myself using the same structure while solving the dice problem in Think Bayes.
The dice problem is described as follows:
 Suppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons &amp; Dragons, you know what I am talking about.</description>
    </item>
    
    <item>
      <title>R: non-numeric argument to binary operator</title>
      <link>http://markhneedham.com/blog/2015/04/19/r-non-numeric-argument-to-binary-operator/</link>
      <pubDate>Sun, 19 Apr 2015 23:08:45 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/19/r-non-numeric-argument-to-binary-operator/</guid>
      <description>When debugging R code, given my Java background, I often find myself trying to print out the state of variables along with an appropriate piece of text like this: names = c(1,2,3,4,5,6) &amp;gt; print(&amp;quot;names: &amp;quot; + names) Error in &amp;quot;names: &amp;quot; + names : non-numeric argument to binary operator   We might try this next: &amp;gt; print(&amp;quot;names: &amp;quot;, names) [1] &amp;quot;names: &amp;quot;   which doesn&#39;t actually print the names variable - only the first argument to the print function is printed.</description>
    </item>
    
    <item>
      <title>R: Removing for loops</title>
      <link>http://markhneedham.com/blog/2015/04/18/r-removing-for-loops/</link>
      <pubDate>Sat, 18 Apr 2015 23:53:20 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/18/r-removing-for-loops/</guid>
      <description>In my last blog post I showed the translation of a likelihood function from Think Bayes into R and in my first attempt at this function I used a couple of nested for loops. likelihoods = function(names, mixes, observations) { scores = rep(1, length(names)) names(scores) = names for(name in names) { for(observation in observations) { scores[name] = scores[name] * mixes[[name]][observation] } } return(scores) }  Names = c(&amp;quot;Bowl 1&amp;quot;, &amp;quot;Bowl 2&amp;quot;) bowl1Mix = c(0.</description>
    </item>
    
    <item>
      <title>R: Think Bayes - More posterior probability calculations</title>
      <link>http://markhneedham.com/blog/2015/04/16/r-think-bayes-more-posterior-probability-calculations/</link>
      <pubDate>Thu, 16 Apr 2015 20:57:20 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/16/r-think-bayes-more-posterior-probability-calculations/</guid>
      <description>As I mentioned in a post last week I&#39;ve been reading through Think Bayes and translating some of the examples form Python to R.  After my first post Antonios suggested a more idiomatic way of writing the function in R so I thought I&#39;d give it a try to calculate the probability that combinations of cookies had come from each bowl.  In the simplest case we have this function which takes in the names of the bowls and the likelihood scores: f = function(names,likelihoods) { # Assume each option has an equal prior priors = rep(1, length(names)) / length(names) # create a data frame with all info you have dt = data.</description>
    </item>
    
    <item>
      <title>R: Creating an object with functions to calculate conditional probability</title>
      <link>http://markhneedham.com/blog/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</link>
      <pubDate>Sun, 12 Apr 2015 07:55:29 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/12/r-creating-an-object-with-functions-to-calculate-conditional-probability/</guid>
      <description>I&#39;ve been working through Alan Downey&#39;s Thinking Bayes and I thought it&#39;d be an interesting exercise to translate some of the code from Python to R.  The first example is a simple one about conditional probablity and the author creates a class &#39;PMF&#39; (Probability Mass Function) to solve the following problem:  Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of each.</description>
    </item>
    
    <item>
      <title>R: Snakes and ladders markov chain</title>
      <link>http://markhneedham.com/blog/2015/04/09/r-snakes-and-ladders-markov-chain/</link>
      <pubDate>Thu, 09 Apr 2015 22:02:18 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/09/r-snakes-and-ladders-markov-chain/</guid>
      <description>A few days ago I read a really cool blog post explaining how Markov chains can be used to model the possible state transitions in a game of snakes and ladders, a use of Markov chains I hadn&#39;t even thought of!  While the example is very helpful for understanding the concept, my understanding of the code is that it works off the assumption that any roll of the dice that puts you on a score  100 is a winning roll.</description>
    </item>
    
    <item>
      <title>R: Markov Chain Wikipedia Example</title>
      <link>http://markhneedham.com/blog/2015/04/05/r-markov-chain-wikipedia-example/</link>
      <pubDate>Sun, 05 Apr 2015 10:07:12 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/04/05/r-markov-chain-wikipedia-example/</guid>
      <description>Over the weekend I&#39;ve been reading about Markov Chains and I thought it&#39;d be an interesting exercise for me to translate Wikipedia&#39;s example into R code.  But first a definition:  A Markov chain is a random process that undergoes transitions from one state to another on a state space. It is required to possess a property that is usually characterized as &#34;memoryless&#34;: the probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it.</description>
    </item>
    
    <item>
      <title>R/ggplot: Controlling X axis order</title>
      <link>http://markhneedham.com/blog/2015/02/27/rggplot-controlling-x-axis-order/</link>
      <pubDate>Fri, 27 Feb 2015 00:49:54 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/02/27/rggplot-controlling-x-axis-order/</guid>
      <description>As part of a talk I gave at the Neo4j London meetup earlier this week I wanted to show how you could build a simple chart showing the number of friends that different actors had using the ggplot library. I started out with the following code:
df = read.csv(&amp;quot;/tmp/friends.csv&amp;quot;) top = df %&amp;gt;% head(20) ggplot(aes(x = p.name, y = colleagues), data = top) + geom_bar(fill = &amp;quot;dark blue&amp;quot;, stat = &amp;quot;identity&amp;quot;)  The friends CSV file is available as a gist if you want to reproduce the chart.</description>
    </item>
    
    <item>
      <title>R: Conditionally updating rows of a data frame</title>
      <link>http://markhneedham.com/blog/2015/02/26/r-conditionally-updating-rows-of-a-data-frame/</link>
      <pubDate>Thu, 26 Feb 2015 00:45:42 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/02/26/r-conditionally-updating-rows-of-a-data-frame/</guid>
      <description>In a blog post I wrote a couple of days ago about cohort analysis I had to assign a monthNumber to each row in a data frame and started out with the following code: library(zoo) library(dplyr) monthNumber = function(cohort, date) { cohortAsDate = as.yearmon(cohort) dateAsDate = as.yearmon(date) if(cohortAsDate &amp;gt; dateAsDate) { &amp;quot;NA&amp;quot; } else { paste(round((dateAsDate - cohortAsDate) * 12), sep=&amp;quot;&amp;quot;) } } cohortAttendance %&amp;gt;% group_by(row_number()) %&amp;gt;% mutate(monthNumber = monthNumber(cohort, date)) %&amp;gt;% filter(monthNumber !</description>
    </item>
    
    <item>
      <title>R: Cohort analysis of Neo4j meetup members</title>
      <link>http://markhneedham.com/blog/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</link>
      <pubDate>Tue, 24 Feb 2015 01:19:26 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/02/24/r-cohort-analysis-of-neo4j-meetup-members/</guid>
      <description>A few weeks ago I came across a blog post explaining how to apply cohort analysis to customer retention using R and I thought it&#39;d be a fun exercise to calculate something similar for meetup attendees.  In the customer retention example we track customer purchases on a month by month basis and each customer is put into a cohort or bucket based on the first month they made a purchase in.</description>
    </item>
    
    <item>
      <title>R/dplyr: Extracting data frame column value for filtering with %in%</title>
      <link>http://markhneedham.com/blog/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</link>
      <pubDate>Sun, 22 Feb 2015 08:58:57 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/02/22/rdplyr-extracting-data-frame-column-value-for-filtering-with-in/</guid>
      <description>I&#39;ve been playing around with dplyr over the weekend and wanted to extract the values from a data frame column to use in a later filtering step.  I had a data frame: library(dplyr) df = data.frame(userId = c(1,2,3,4,5), score = c(2,3,4,5,5))  And wanted to extract the userIds of those people who have a score greater than 3. I started with:
highScoringPeople = df %&amp;gt;% filter(score &amp;gt; 3) %&amp;gt;% select(userId) &amp;gt; highScoringPeople userId 1 3 2 4 3 5   And then filtered the data frame expecting to get back those 3 people: &amp;gt; df %&amp;gt;% filter(userId %in% highScoringPeople) [1] userId score &amp;lt;0 rows&amp;gt; (or 0-length row.</description>
    </item>
    
    <item>
      <title>R: Weather vs attendance at NoSQL meetups</title>
      <link>http://markhneedham.com/blog/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</link>
      <pubDate>Wed, 11 Feb 2015 07:09:25 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/02/11/r-weather-vs-attendance-at-nosql-meetups/</guid>
      <description>A few weeks ago I came across a tweet by Sean Taylor asking for a weather data set with a few years worth of recording and I was surprised to learn that R already has such a thing - the weatherData package. Winner is: @UTVilla! &amp;#10;library(weatherData)&amp;#10;df &amp;lt;- getWeatherForYear(&amp;quot;SFO&amp;quot;, 2013)&amp;#10;ggplot(df, aes(x=Date, y = Mean_TemperatureF)) + geom_line()
&amp;mdash; Sean J. Taylor (@seanjtaylor) January 22, 2015 
weatherData provides a thin veneer around the wunderground API and was exactly what I&#39;d been looking for to compare meetup at London&#39;s NoSQL against weather conditions that day.</description>
    </item>
    
    <item>
      <title>R: ggplot2 - Each group consist of only one observation. Do you need to adjust the group aesthetic?</title>
      <link>http://markhneedham.com/blog/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</link>
      <pubDate>Fri, 30 Jan 2015 00:27:53 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2015/01/30/r-ggplot2-each-group-consist-of-only-one-observation-do-you-need-to-adjust-the-group-aesthetic/</guid>
      <description>I&#39;ve been playing around with some weather data over the last couple of days which I aggregated down to the average temperature per month over the last 4 years and stored in a CSV file.
This is what the file looks like:
$ cat /tmp/averageTemperatureByMonth.csv &amp;quot;month&amp;quot;,&amp;quot;aveTemperature&amp;quot; &amp;quot;January&amp;quot;,6.02684563758389 &amp;quot;February&amp;quot;,5.89380530973451 &amp;quot;March&amp;quot;,7.54838709677419 &amp;quot;April&amp;quot;,10.875 &amp;quot;May&amp;quot;,13.3064516129032 &amp;quot;June&amp;quot;,15.9666666666667 &amp;quot;July&amp;quot;,18.8387096774194 &amp;quot;August&amp;quot;,18.3709677419355 &amp;quot;September&amp;quot;,16.2583333333333 &amp;quot;October&amp;quot;,13.4596774193548 &amp;quot;November&amp;quot;,9.19166666666667 &amp;quot;December&amp;quot;,7.01612903225806  I wanted to create a simple line chart which would show the months of the year in ascending order with the appropriate temperature.</description>
    </item>
    
    <item>
      <title>R: Featuring engineering for a linear model</title>
      <link>http://markhneedham.com/blog/2014/12/28/r-featuring-engineering-for-a-linear-model/</link>
      <pubDate>Sun, 28 Dec 2014 21:55:23 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/28/r-featuring-engineering-for-a-linear-model/</guid>
      <description>I previously wrote about a linear model I created to predict how many people would RSVP &#39;yes&#39; to a meetup event and having not found much correlation between any of my independent variables and RSVPs was a bit stuck.  As luck would have it I bumped into Antonios at a meetup a month ago and he offered to take a look at what I&#39;d tried so far and give me some tips on how to progress.</description>
    </item>
    
    <item>
      <title>R: Vectorising all the things</title>
      <link>http://markhneedham.com/blog/2014/12/22/r-vectorising-all-the-things/</link>
      <pubDate>Mon, 22 Dec 2014 11:46:25 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/22/r-vectorising-all-the-things/</guid>
      <description>After my last post about finding the distance a date/time is from the weekend Hadley Wickham suggested I could improve the function by vectorising it...
@markhneedham vectorise with pmin(pmax(dateToLookup - before, 0), pmax(after - dateToLookup, 0)) / dhours(1)
&amp;mdash; Hadley Wickham (@hadleywickham) December 14, 2014 
...so I thought I&#39;d try and vectorise some of the other functions I&#39;ve written recently and show the two versions.
I found the following articles useful for explaining vectorisation and why you might want to do it:</description>
    </item>
    
    <item>
      <title>R: Time to/from the weekend</title>
      <link>http://markhneedham.com/blog/2014/12/13/r-time-tofrom-the-weekend/</link>
      <pubDate>Sat, 13 Dec 2014 20:38:22 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/13/r-time-tofrom-the-weekend/</guid>
      <description>In my last post I showed some examples using R&#39;s lubridate package and another problem it made really easy to solve was working out how close a particular date time was to the weekend.  I wanted to write a function which would return the previous Sunday or upcoming Saturday depending on which was closer.
lubridate&#39;s floor_date and ceiling_date functions make this quite simple. e.g. if we want to round the 18th December down to the beginning of the week and up to the beginning of the next week we could do the following: &amp;gt; library(lubridate) &amp;gt; floor_date(ymd(&amp;quot;2014-12-18&amp;quot;), &amp;quot;week&amp;quot;) [1] &amp;quot;2014-12-14 UTC&amp;quot; &amp;gt; ceiling_date(ymd(&amp;quot;2014-12-18&amp;quot;), &amp;quot;week&amp;quot;) [1] &amp;quot;2014-12-21 UTC&amp;quot;   For the date in the future we actually want to grab the Saturday rather than the Sunday so we&#39;ll subtract one day from that: &amp;gt; ceiling_date(ymd(&amp;quot;2014-12-18&amp;quot;), &amp;quot;week&amp;quot;) - days(1) [1] &amp;quot;2014-12-20 UTC&amp;quot;   Now let&#39;s put that together into a function which finds the closest weekend for a given date: findClosestWeekendDay = function(dateToLookup) { before = floor_date(dateToLookup, &amp;quot;week&amp;quot;) + hours(23) + minutes(59) + seconds(59) after = ceiling_date(dateToLookup, &amp;quot;week&amp;quot;) - days(1) if((dateToLookup - before) &amp;lt; (after - dateToLookup)) { before } else { after } } &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-13 13:33:29&amp;quot;)) [1] &amp;quot;2014-12-13 UTC&amp;quot; &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-14 18:33:29&amp;quot;)) [1] &amp;quot;2014-12-14 23:59:59 UTC&amp;quot; &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-15 18:33:29&amp;quot;)) [1] &amp;quot;2014-12-14 23:59:59 UTC&amp;quot; &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-17 11:33:29&amp;quot;)) [1] &amp;quot;2014-12-14 23:59:59 UTC&amp;quot; &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-17 13:33:29&amp;quot;)) [1] &amp;quot;2014-12-20 UTC&amp;quot; &amp;gt; findClosestWeekendDay(ymd_hms(&amp;quot;2014-12-19 13:33:29&amp;quot;)) [1] &amp;quot;2014-12-20 UTC&amp;quot;   I&#39;ve set the Sunday date at 23:59:59 so that I can use this date in the next step where we want to calculate how how many hours it is from the current date to the nearest weekend.</description>
    </item>
    
    <item>
      <title>R: Numeric representation of date time</title>
      <link>http://markhneedham.com/blog/2014/12/13/r-numeric-representation-of-date-time/</link>
      <pubDate>Sat, 13 Dec 2014 19:58:13 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/13/r-numeric-representation-of-date-time/</guid>
      <description>I&#39;ve been playing around with date times in R recently and I wanted to derive a numeric representation for a given value to make it easier to see the correlation between time and another variable.  e.g. December 13th 2014 17:30 should return 17.5 since it&#39;s 17.5 hours since midnight.  Using the standard R libraries we would write the following code: &amp;gt; december13 = as.POSIXlt(&amp;quot;2014-12-13 17:30:00&amp;quot;) &amp;gt; as.numeric(december13 - trunc(december13, &amp;quot;day&amp;quot;), units=&amp;quot;hours&amp;quot;) [1] 17.</description>
    </item>
    
    <item>
      <title>R: data.table/dplyr/lubridate - Error in wday(date, label = TRUE, abbr = FALSE) :  unused arguments (label = TRUE, abbr = FALSE)</title>
      <link>http://markhneedham.com/blog/2014/12/11/r-data-tabledplyrlubridate-error-in-wdaydate-label-true-abbr-false-unused-arguments-label-true-abbr-false/</link>
      <pubDate>Thu, 11 Dec 2014 19:03:06 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/11/r-data-tabledplyrlubridate-error-in-wdaydate-label-true-abbr-false-unused-arguments-label-true-abbr-false/</guid>
      <description>I spent a couple of hours playing around with data.table this evening and tried changing some code written using a data frame to use a data table instead.  I started off by building a data frame which contains all the weekends between 2010 and 2015... &amp;gt; library(lubridate) &amp;gt; library(dplyr) &amp;gt; dates = data.frame(date = seq( dmy(&amp;quot;01-01-2010&amp;quot;), to=dmy(&amp;quot;01-01-2015&amp;quot;), by=&amp;quot;day&amp;quot; )) &amp;gt; dates = dates %&amp;gt;% filter(wday(date, label = TRUE, abbr = FALSE) %in% c(&amp;quot;Saturday&amp;quot;, &amp;quot;Sunday&amp;quot;))  .</description>
    </item>
    
    <item>
      <title>R: Cleaning up and plotting Google Trends data</title>
      <link>http://markhneedham.com/blog/2014/12/09/r-cleaning-up-plotting-google-trends-data/</link>
      <pubDate>Tue, 09 Dec 2014 18:14:45 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/09/r-cleaning-up-plotting-google-trends-data/</guid>
      <description>I recently came across an excellent article written by Stian Haklev in which he describes things he wishes he&#39;d been told before starting out with R, one being to do all data clean up in code which I thought I&#39;d give a try.  My goal is to leave the raw data completely unchanged, and do all the transformation in code, which can be rerun at any time. While I&#39;m writing the scripts, I&#39;m often jumping around, selectively executing individual lines or code blocks, running commands to inspect the data in the REPL (read-evaluate-print-loop, where each command is executed as soon as you type enter, in the picture above it&#39;s the pane to the right), etc.</description>
    </item>
    
    <item>
      <title>R: dplyr - mutate with strptime (incompatible size/wrong result size)</title>
      <link>http://markhneedham.com/blog/2014/12/08/r-dplyr-mutate-with-strptime-incompatible-sizewrong-result-size/</link>
      <pubDate>Mon, 08 Dec 2014 19:02:46 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/08/r-dplyr-mutate-with-strptime-incompatible-sizewrong-result-size/</guid>
      <description>Having worked out how to translate a string into a date or NA if it wasn&#39;t the appropriate format the next thing I wanted to do was store the result of the transformation in my data frame. I started off with this:
data = data.frame(x = c(&amp;quot;2014-01-01&amp;quot;, &amp;quot;2014-02-01&amp;quot;, &amp;quot;foo&amp;quot;)) &amp;gt; data x 1 2014-01-01 2 2014-02-01 3 foo  And when I tried to do the date translation ran into the following error:</description>
    </item>
    
    <item>
      <title>R: String to Date or NA</title>
      <link>http://markhneedham.com/blog/2014/12/07/r-string-to-date-or-na/</link>
      <pubDate>Sun, 07 Dec 2014 19:29:01 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/07/r-string-to-date-or-na/</guid>
      <description>I&#39;ve been trying to clean up a CSV file which contains some rows with dates and some not - I only want to keep the cells which do have dates so I&#39;ve been trying to work out how to do that.
My first thought was that I&#39;d try and find a function which would convert the contents of the cell into a date if it was in date format and NA if not.</description>
    </item>
    
    <item>
      <title>R: Applying a function to every row of a data frame</title>
      <link>http://markhneedham.com/blog/2014/12/04/r-applying-a-function-to-every-row-of-a-data-frame/</link>
      <pubDate>Thu, 04 Dec 2014 06:31:02 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/12/04/r-applying-a-function-to-every-row-of-a-data-frame/</guid>
      <description>In my continued exploration of London&#39;s meetups I wanted to calculate the distance from meetup venues to a centre point in London.  I&#39;ve created a gist containing the coordinates of some of the venues that host NoSQL meetups in London town if you want to follow along: library(dplyr) # https://gist.github.com/mneedham/7e926a213bf76febf5ed venues = read.csv(&amp;quot;/tmp/venues.csv&amp;quot;) venues %&amp;gt;% head() ## venue lat lon ## 1 Skills Matter 51.52482 -0.099109 ## 2 Skinkers 51.</description>
    </item>
    
    <item>
      <title>R: dplyr - Select &#39;random&#39; rows from a data frame</title>
      <link>http://markhneedham.com/blog/2014/11/26/r-dplyr-select-random-rows-from-a-data-frame/</link>
      <pubDate>Wed, 26 Nov 2014 00:01:12 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/26/r-dplyr-select-random-rows-from-a-data-frame/</guid>
      <description>Frequently I find myself wanting to take a sample of the rows in a data frame where just taking the head isn&#39;t enough.
Let&#39;s say we start with the following data frame:
data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) )  And we&#39;d like to sample 10 rows to see what it contains. We&#39;ll start by generating 10 random numbers to represent row numbers using the runif function:</description>
    </item>
    
    <item>
      <title>R: dplyr - &#34;Variables not shown&#34;</title>
      <link>http://markhneedham.com/blog/2014/11/23/r-dplyr-variables-not-shown/</link>
      <pubDate>Sun, 23 Nov 2014 01:02:06 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/23/r-dplyr-variables-not-shown/</guid>
      <description>I recently ran into a problem where the result of applying some operations to a data frame wasn&#39;t being output the way I wanted. I started with this data frame:
words = function(numberOfWords, lengthOfWord) { w = c(1:numberOfWords) for(i in 1:numberOfWords) { w[i] = paste(sample(letters, lengthOfWord, replace=TRUE), collapse = &amp;quot;&amp;quot;) } w } numberOfRows = 100 df = data.frame(a = sample (1:numberOfRows, 10, replace = TRUE), b = sample (1:numberOfRows, 10, replace = TRUE), name = words(numberOfRows, 10))  I wanted to group the data frame by a and b and output a comma separated list of the associated names.</description>
    </item>
    
    <item>
      <title>R: ggmap - Overlay shapefile with filled polygon of regions</title>
      <link>http://markhneedham.com/blog/2014/11/17/r-ggmap-overlay-shapefile-with-filled-polygon-of-regions/</link>
      <pubDate>Mon, 17 Nov 2014 00:53:11 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/17/r-ggmap-overlay-shapefile-with-filled-polygon-of-regions/</guid>
      <description>I&#39;ve been playing around with plotting maps in R over the last week and got to the point where I wanted to have a google map in the background with a filled polygon on a shapefile in the foreground.
 The first bit is reasonably simple - we can just import the ggmap library and make a call to get_map: &amp;gt; library(ggmap) &amp;gt; sfMap = map = get_map(location = &#39;San Francisco&#39;, zoom = 12)     Next I wanted to show the outlines of the different San Francisco zip codes and came across a blog post by Paul Bidanset on Baltimore neighbourhoods which I was able to adapt.</description>
    </item>
    
    <item>
      <title>R: dplyr - Sum for group_by multiple columns</title>
      <link>http://markhneedham.com/blog/2014/11/11/r-dplyr-sum-for-group_by-multiple-columns/</link>
      <pubDate>Tue, 11 Nov 2014 00:17:32 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/11/r-dplyr-sum-for-group_by-multiple-columns/</guid>
      <description>Over the weekend I was playing around with dplyr and had the following data frame grouped by both columns:
&amp;gt; library(dplyr) &amp;gt; data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) ) &amp;gt; data %&amp;gt;% count(letter, number) %&amp;gt;% head(20) Source: local data frame [20 x 3] Groups: letter letter number n 1 A 1 184 2 A 2 192 3 A 3 183 4 A 4 193 5 A 5 214 6 A 6 172 7 A 7 196 8 A 8 198 9 A 9 174 10 A 10 196 11 B 1 212 12 B 2 198 13 B 3 194 14 B 4 181 15 B 5 203 16 B 6 234 17 B 7 221 18 B 8 179 19 B 9 182 20 B 10 170  I wanted to add an extra column which would show what percentage of the values for that letter each number had.</description>
    </item>
    
    <item>
      <title>R: dplyr - Maximum value row in each group</title>
      <link>http://markhneedham.com/blog/2014/11/10/r-maximum-value-row-in-each-group/</link>
      <pubDate>Mon, 10 Nov 2014 22:06:49 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/10/r-maximum-value-row-in-each-group/</guid>
      <description>In my continued work with R&#39;s dplyr I wanted to be able to group a data frame by some columns and then find the maximum value for each group.
We&#39;ll continue with my favourite dummy data set:
&amp;gt; library(dplyr) &amp;gt; data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) )  I started with the following code to count how many occurrences of each (letter, number) pair there were:</description>
    </item>
    
    <item>
      <title>R: dplyr - Ordering by count after multiple column group_by</title>
      <link>http://markhneedham.com/blog/2014/11/09/r-dplyr-ordering-by-count-after-multiple-column-group_by/</link>
      <pubDate>Sun, 09 Nov 2014 09:30:09 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/09/r-dplyr-ordering-by-count-after-multiple-column-group_by/</guid>
      <description>I was recently trying to group a data frame by two columns and then sort by the count using dplyr but it wasn&#39;t sorting in the way I expecting which was initially very confusing. I started with this data frame:
library(dplyr) data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) )  And I wanted to find out how many occurrences of each (letter, number) pair exist in the data set.</description>
    </item>
    
    <item>
      <title>R: Refactoring to dplyr</title>
      <link>http://markhneedham.com/blog/2014/11/09/r-refactoring-to-dplyr/</link>
      <pubDate>Sun, 09 Nov 2014 00:11:48 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/09/r-refactoring-to-dplyr/</guid>
      <description>I&#39;ve been looking back over some of the early code I wrote using R before I knew about the dplyr library and thought it&#39;d be an interesting exercise to refactor some of the snippets.
We&#39;ll use the following data frame for each of the examples:
library(dplyr) data = data.frame( letter = sample(LETTERS, 50000, replace = TRUE), number = sample (1:10, 50000, replace = TRUE) )  Take {n} rows &amp;gt; data[1:5,] letter number 1 R 7 2 Q 3 3 B 8 4 R 3 5 U 2  becomes:</description>
    </item>
    
    <item>
      <title>R: dplyr - Group by field dynamically (&#39;regroup&#39; is deprecated / no applicable method for &#39;as.lazy&#39; applied to an object of class &#34;list&#34; )</title>
      <link>http://markhneedham.com/blog/2014/11/08/r-dplyr-group-by-field-dynamically-regroup-is-deprecated-no-applicable-method-for-as-lazy-applied-to-an-object-of-class-list/</link>
      <pubDate>Sat, 08 Nov 2014 22:29:01 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/08/r-dplyr-group-by-field-dynamically-regroup-is-deprecated-no-applicable-method-for-as-lazy-applied-to-an-object-of-class-list/</guid>
      <description>A few months ago I wrote a blog explaining how to dynamically/programatically group a data frame by a field using dplyr but that approach has been deprecated in the latest version.
To recap, the original function looked like this:
library(dplyr) groupBy = function(df, field) { df %.% regroup(list(field)) %.% summarise(n = n()) }  And if we execute that with a sample data frame we&#39;ll see the following:
&amp;gt; data = data.</description>
    </item>
    
    <item>
      <title>R: Joining multiple data frames</title>
      <link>http://markhneedham.com/blog/2014/11/07/r-joining-multiple-data-frames/</link>
      <pubDate>Fri, 07 Nov 2014 01:29:09 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/11/07/r-joining-multiple-data-frames/</guid>
      <description>I&#39;ve been looking through the code from Martin Eastwood&#39;s excellent talk &#39;Predicting Football Using R&#39; and was intrigued by the code which reshaped the data into that expected by glm.
The original looks like this:
df &amp;lt;- read.csv(&#39;http://www.football-data.co.uk/mmz4281/1314/E0.csv&#39;) # munge data into format compatible with glm function df &amp;lt;- apply(df, 1, function(row){ data.frame(team=c(row[&#39;HomeTeam&#39;], row[&#39;AwayTeam&#39;]), opponent=c(row[&#39;AwayTeam&#39;], row[&#39;HomeTeam&#39;]), goals=c(row[&#39;FTHG&#39;], row[&#39;FTAG&#39;]), home=c(1, 0)) }) df &amp;lt;- do.call(rbind, df)  The initial data frame looks like this:</description>
    </item>
    
    <item>
      <title>R: Converting a named vector to a data frame</title>
      <link>http://markhneedham.com/blog/2014/10/31/r-converting-a-named-vector-to-a-data-frame/</link>
      <pubDate>Fri, 31 Oct 2014 23:47:26 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/10/31/r-converting-a-named-vector-to-a-data-frame/</guid>
      <description>I&#39;ve been playing around with igraph&#39;s page rank function to see who the most central nodes in the London NoSQL scene are and I wanted to put the result in a data frame to make the data easier to work with.
I started off with a data frame containing pairs of people and the number of events that they&#39;d both RSVP&#39;d &#39;yes&#39; to:
&amp;gt; library(dplyr) &amp;gt; data %&amp;gt;% arrange(desc(times)) %&amp;gt;% head(10) p.</description>
    </item>
    
    <item>
      <title>R: Linear models with the lm function, NA values and Collinearity</title>
      <link>http://markhneedham.com/blog/2014/10/18/r-linear-models-with-the-lm-function-na-values-and-collinearity/</link>
      <pubDate>Sat, 18 Oct 2014 06:35:49 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/10/18/r-linear-models-with-the-lm-function-na-values-and-collinearity/</guid>
      <description>In my continued playing around with R I&#39;ve sometimes noticed &#39;NA&#39; values in the linear regression models I created but hadn&#39;t really thought about what that meant.
On the advice of Peter Huber I recently started working my way through Coursera&#39;s Regression Models which has a whole slide explaining its meaning:
  So in this case &#39;z&#39; doesn&#39;t help us in predicting Fertility since it doesn&#39;t give us any more information that we can&#39;t already get from &#39;Agriculture&#39; and &#39;Education&#39;.</description>
    </item>
    
    <item>
      <title>R: A first attempt at linear regression</title>
      <link>http://markhneedham.com/blog/2014/09/30/r-a-first-attempt-at-linear-regression/</link>
      <pubDate>Tue, 30 Sep 2014 22:20:05 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/30/r-a-first-attempt-at-linear-regression/</guid>
      <description>I&#39;ve been working through the videos that accompany the Introduction to Statistical Learning with Applications in R book and thought it&#39;d be interesting to try out the linear regression algorithm against my meetup data set.
I wanted to see how well a linear regression algorithm could predict how many people were likely to RSVP to a particular event. I started with the following code to build a data frame containing some potential predictors:</description>
    </item>
    
    <item>
      <title>R: Deriving a new data frame column based on containing string</title>
      <link>http://markhneedham.com/blog/2014/09/29/r-deriving-a-new-data-frame-column-based-on-containing-string/</link>
      <pubDate>Mon, 29 Sep 2014 21:37:21 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/29/r-deriving-a-new-data-frame-column-based-on-containing-string/</guid>
      <description>I&#39;ve been playing around with R data frames a bit more and one thing I wanted to do was derive a new column based on the text contained in the existing column.
I started with something like this:
&amp;gt; x = data.frame(name = c(&amp;quot;Java Hackathon&amp;quot;, &amp;quot;Intro to Graphs&amp;quot;, &amp;quot;Hands on Cypher&amp;quot;)) &amp;gt; x name 1 Java Hackathon 2 Intro to Graphs 3 Hands on Cypher  And I wanted to derive a new column based on whether or not the session was a practical one.</description>
    </item>
    
    <item>
      <title>R: Filtering data frames by column type (&#39;x&#39; must be numeric)</title>
      <link>http://markhneedham.com/blog/2014/09/29/r-filtering-data-frames-by-column-type-x-must-be-numeric/</link>
      <pubDate>Mon, 29 Sep 2014 05:46:43 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/29/r-filtering-data-frames-by-column-type-x-must-be-numeric/</guid>
      <description>I&#39;ve been working through the exercises from An Introduction to Statistical Learning and one of them required you to create a pair wise correlation matrix of variables in a data frame.
The exercise uses the &#39;Carseats&#39; data set which can be imported like so:
&amp;gt; install.packages(&amp;quot;ISLR&amp;quot;) &amp;gt; library(ISLR) &amp;gt; head(Carseats) Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 1 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 2 11.</description>
    </item>
    
    <item>
      <title>R: ggplot - Plotting multiple variables on a line chart</title>
      <link>http://markhneedham.com/blog/2014/09/16/r-ggplot-plotting-multiple-variables-on-a-line-chart/</link>
      <pubDate>Tue, 16 Sep 2014 16:59:21 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/16/r-ggplot-plotting-multiple-variables-on-a-line-chart/</guid>
      <description>In my continued playing around with meetup data I wanted to plot the number of members who join the Neo4j group over time. I started off with the variable &#39;byWeek&#39; which shows how many members joined the group each week:
&amp;gt; head(byWeek) Source: local data frame [6 x 2] week n 1 2011-06-02 8 2 2011-06-09 4 3 2011-06-30 2 4 2011-07-14 1 5 2011-07-21 1 6 2011-08-18 1  I wanted to plot the actual count alongside a rolling average for which I created the following data frame:</description>
    </item>
    
    <item>
      <title>R: ggplot - Plotting a single variable line chart (geom_line requires the following missing aesthetics: y)</title>
      <link>http://markhneedham.com/blog/2014/09/13/r-ggplot-plotting-a-single-variable-line-chart-geom_line-requires-the-following-missing-aesthetics-y/</link>
      <pubDate>Sat, 13 Sep 2014 11:41:39 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/13/r-ggplot-plotting-a-single-variable-line-chart-geom_line-requires-the-following-missing-aesthetics-y/</guid>
      <description>I&#39;ve been learning how to do moving averages in R and having done that calculation I wanted to plot these variables on a line chart using ggplot.
The vector of rolling averages looked like this:
&amp;gt; rollmean(byWeek$n, 4) [1] 3.75 2.00 1.25 1.00 1.25 1.25 1.75 1.75 1.75 2.50 2.25 2.75 3.50 2.75 2.75 [16] 2.25 1.50 1.50 2.00 2.00 2.00 2.00 1.25 1.50 2.25 2.50 3.00 3.25 2.75 4.00 [31] 4.</description>
    </item>
    
    <item>
      <title>R: Calculating rolling or moving averages</title>
      <link>http://markhneedham.com/blog/2014/09/13/r-calculating-rolling-or-moving-averages/</link>
      <pubDate>Sat, 13 Sep 2014 08:15:26 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/09/13/r-calculating-rolling-or-moving-averages/</guid>
      <description>I&#39;ve been playing around with some time series data in R and since there&#39;s a bit of variation between consecutive points I wanted to smooth the data out by calculating the moving average.
I struggled to find an in built function to do this but came across Didier Ruedin&#39;s blog post which described the following function to do the job:
mav &amp;lt;- function(x,n=5){filter(x,rep(1/n,n), sides=2)}  I tried plugging in some numbers to understand how it works:</description>
    </item>
    
    <item>
      <title>R: ggplot - Cumulative frequency graphs</title>
      <link>http://markhneedham.com/blog/2014/08/31/r-ggplot-cumulative-frequency-graphs/</link>
      <pubDate>Sun, 31 Aug 2014 22:10:42 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/31/r-ggplot-cumulative-frequency-graphs/</guid>
      <description>In my continued playing around with ggplot I wanted to create a chart showing the cumulative growth of the number of members of the Neo4j London meetup group.
My initial data frame looked like this:
&amp;gt; head(meetupMembers) joinTimestamp joinDate monthYear quarterYear week dayMonthYear 1 1.376572e+12 2013-08-15 13:13:40 2013-08-01 2013-07-01 2013-08-15 2013-08-15 2 1.379491e+12 2013-09-18 07:55:11 2013-09-01 2013-07-01 2013-09-12 2013-09-18 3 1.349454e+12 2012-10-05 16:28:04 2012-10-01 2012-10-01 2012-10-04 2012-10-05 4 1.383127e+12 2013-10-30 09:59:03 2013-10-01 2013-10-01 2013-10-24 2013-10-30 5 1.</description>
    </item>
    
    <item>
      <title>R: dplyr - group_by dynamic or programmatic field / variable (Error: index out of bounds)</title>
      <link>http://markhneedham.com/blog/2014/08/29/r-dplyr-group_by-dynamic-or-programmatic-field-variable-error-index-out-of-bounds/</link>
      <pubDate>Fri, 29 Aug 2014 09:13:00 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/29/r-dplyr-group_by-dynamic-or-programmatic-field-variable-error-index-out-of-bounds/</guid>
      <description>In my last blog post I showed how to group timestamp based data by week, month and quarter and by the end we had the following code samples using dplyr and zoo:
library(RNeo4j) library(zoo) timestampToDate &amp;lt;- function(x) as.POSIXct(x / 1000, origin=&amp;quot;1970-01-01&amp;quot;, tz = &amp;quot;GMT&amp;quot;) query = &amp;quot;MATCH (:Person)-[:HAS_MEETUP_PROFILE]-&amp;gt;()-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g:Group {name: \&amp;quot;Neo4j - London User Group\&amp;quot;}) RETURN membership.joined AS joinTimestamp&amp;quot; meetupMembers = cypher(graph, query) meetupMembers$joinDate &amp;lt;- timestampToDate(meetupMembers$joinTimestamp) meetupMembers$monthYear &amp;lt;- as.Date(as.yearmon(meetupMembers$joinDate)) meetupMembers$quarterYear &amp;lt;- as.</description>
    </item>
    
    <item>
      <title>R: Grouping by week, month, quarter</title>
      <link>http://markhneedham.com/blog/2014/08/29/r-grouping-by-week-month-quarter/</link>
      <pubDate>Fri, 29 Aug 2014 00:25:33 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/29/r-grouping-by-week-month-quarter/</guid>
      <description>In my continued playing around with R and meetup data I wanted to have a look at when people joined the London Neo4j group based on week, month or quarter of the year to see when they were most likely to do so.
I started with the following query to get back the join timestamps:
library(RNeo4j) query = &amp;quot;MATCH (:Person)-[:HAS_MEETUP_PROFILE]-&amp;gt;()-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g:Group {name: \&amp;quot;Neo4j - London User Group\&amp;quot;}) RETURN membership.joined AS joinTimestamp&amp;quot; meetupMembers = cypher(graph, query) &amp;gt; head(meetupMembers) joinTimestamp 1 1.</description>
    </item>
    
    <item>
      <title>R: Rook - Hello world example - &#39;Cannot find a suitable app in file&#39;</title>
      <link>http://markhneedham.com/blog/2014/08/22/r-rook-hello-world-example-cannot-find-a-suitable-app-in-file/</link>
      <pubDate>Fri, 22 Aug 2014 11:05:54 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/22/r-rook-hello-world-example-cannot-find-a-suitable-app-in-file/</guid>
      <description>I&#39;ve been playing around with the Rook library and struggled a bit getting a basic Hello World application up and running so I thought I should document it for future me.
I wanted to spin up a web server using Rook and serve a page with the text &#39;Hello World&#39;. I started with the following code:
library(Rook) s &amp;lt;- Rhttpd$new() s$add(name=&#39;MyApp&#39;,app=&#39;helloworld.R&#39;) s$start() s$browse(&amp;quot;MyApp&amp;quot;)  where helloWorld.R contained the following code:</description>
    </item>
    
    <item>
      <title>Where does r studio install packages/libraries?</title>
      <link>http://markhneedham.com/blog/2014/08/14/where-does-r-studio-install-packageslibraries/</link>
      <pubDate>Thu, 14 Aug 2014 10:24:52 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/14/where-does-r-studio-install-packageslibraries/</guid>
      <description>As a newbie to R I wanted to look at the source code of some of the libraries/packages that I&#39;d installed via R Studio which I initially struggled to do as I wasn&#39;t sure where the packages had been installed.
I eventually came across a StackOverflow post which described the .libPaths function which tells us where that is:
&amp;gt; .libPaths() [1] &amp;quot;/Library/Frameworks/R.framework/Versions/3.1/Resources/library&amp;quot;  If we want to see which libraries are installed we can use the list.</description>
    </item>
    
    <item>
      <title>R: Grouping by two variables</title>
      <link>http://markhneedham.com/blog/2014/08/11/r-grouping-by-two-variables/</link>
      <pubDate>Mon, 11 Aug 2014 16:47:35 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/08/11/r-grouping-by-two-variables/</guid>
      <description>In my continued playing around with R and meetup data I wanted to group a data table by two variables - day and event - so I could see the most popular day of the week for meetups and which events we&#39;d held on those days.
I started off with the following data table:
&amp;gt; head(eventsOf2014, 20) eventTime event.name rsvps datetime day monthYear 16 1.393351e+12 Intro to Graphs 38 2014-02-25 18:00:00 Tuesday 02-2014 17 1.</description>
    </item>
    
    <item>
      <title>R: ggplot  - Plotting back to back charts using facet_wrap</title>
      <link>http://markhneedham.com/blog/2014/07/25/r-ggplot-plotting-back-to-back-charts-using-facet_wrap/</link>
      <pubDate>Fri, 25 Jul 2014 21:57:24 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/25/r-ggplot-plotting-back-to-back-charts-using-facet_wrap/</guid>
      <description>Earlier in the week I showed a way to plot back to back charts using R&#39;s ggplot library but looking back on the code it felt like it was a bit hacky to &#39;glue&#39; two charts together using a grid.
I wanted to find a better way.
To recap, I came up with the following charts showing the RSVPs to Neo4j London meetup events using this code:
  The first thing we need to do to simplify chart generation is to return &#39;yes&#39; and &#39;no&#39; responses in the same cypher query, like so:</description>
    </item>
    
    <item>
      <title>R: ggplot - Plotting back to back bar charts</title>
      <link>http://markhneedham.com/blog/2014/07/20/r-ggplot-plotting-back-to-back-bar-charts/</link>
      <pubDate>Sun, 20 Jul 2014 16:50:55 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/20/r-ggplot-plotting-back-to-back-bar-charts/</guid>
      <description>I&#39;ve been playing around with R&#39;s ggplot library to explore the Neo4j London meetup and the next thing I wanted to do was plot back to back bar charts showing &#39;yes&#39; and &#39;no&#39; RSVPs.
I&#39;d already done the &#39;yes&#39; bar chart using the following code:
query = &amp;quot;MATCH (e:Event)&amp;lt;-[:TO]-(response {response: &#39;yes&#39;}) RETURN response.time AS time, e.time + e.utc_offset AS eventTime&amp;quot; allYesRSVPs = cypher(graph, query) allYesRSVPs$time = timestampToDate(allYesRSVPs$time) allYesRSVPs$eventTime = timestampToDate(allYesRSVPs$eventTime) allYesRSVPs$difference = as.</description>
    </item>
    
    <item>
      <title>R: ggplot - Don&#39;t know how to automatically pick scale for object of type difftime - Discrete value supplied to continuous scale</title>
      <link>http://markhneedham.com/blog/2014/07/20/r-ggplot-dont-know-how-to-automatically-pick-scale-for-object-of-type-difftime-discrete-value-supplied-to-continuous-scale/</link>
      <pubDate>Sun, 20 Jul 2014 00:21:17 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/20/r-ggplot-dont-know-how-to-automatically-pick-scale-for-object-of-type-difftime-discrete-value-supplied-to-continuous-scale/</guid>
      <description>While reading &#39;Why The R Programming Language Is Good For Business&#39; I came across Udacity&#39;s &#39;Data Analysis with R&#39; courses - part of which focuses exploring data sets using visualisations, something I haven&#39;t done much of yet.
I thought it&#39;d be interesting to create some visualisations around the times that people RSVP &#39;yes&#39; to the various Neo4j events that we run in London.
I started off with the following query which returns the date time that people replied &#39;Yes&#39; to an event and the date time of the event:</description>
    </item>
    
    <item>
      <title>R: Apply a custom function across multiple lists</title>
      <link>http://markhneedham.com/blog/2014/07/16/r-apply-a-custom-function-across-multiple-lists/</link>
      <pubDate>Wed, 16 Jul 2014 05:04:46 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/16/r-apply-a-custom-function-across-multiple-lists/</guid>
      <description>In my continued playing around with R I wanted to map a custom function over two lists comparing each item with its corresponding items.
If we just want to use a built in function such as subtraction between two lists it&#39;s quite easy to do:
&amp;gt; c(10,9,8,7,6,5,4,3,2,1) - c(5,4,3,4,3,2,2,1,2,1) [1] 5 5 5 3 3 3 2 2 0 0  I wanted to do a slight variation on that where instead of returning the difference I wanted to return a text value representing the difference e.</description>
    </item>
    
    <item>
      <title>R/plyr: ddply - Error in vector(type, length) : vector: cannot make a vector of mode &#39;closure&#39;.</title>
      <link>http://markhneedham.com/blog/2014/07/07/rplyr-ddply-error-in-vectortype-length-vector-cannot-make-a-vector-of-mode-closure/</link>
      <pubDate>Mon, 07 Jul 2014 06:07:29 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/07/rplyr-ddply-error-in-vectortype-length-vector-cannot-make-a-vector-of-mode-closure/</guid>
      <description>In my continued playing around with plyr&#39;s ddply function I was trying to group a data frame by one of its columns and return a count of the number of rows with specific values and ran into a strange (to me) error message.
I had a data frame:
n = c(2, 3, 5) s = c(&amp;quot;aa&amp;quot;, &amp;quot;bb&amp;quot;, &amp;quot;cc&amp;quot;) b = c(TRUE, FALSE, TRUE) df = data.frame(n, s, b)  And wanted to group and count on column &#39;b&#39; so I&#39;d get back a count of 2 for TRUE and 1 for FALSE.</description>
    </item>
    
    <item>
      <title>R/plyr: ddply - Renaming the grouping/generated column when grouping by date</title>
      <link>http://markhneedham.com/blog/2014/07/02/rplyr-ddply-renaming-the-groupinggenerate-column-when-grouping-by-date/</link>
      <pubDate>Wed, 02 Jul 2014 06:30:50 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/07/02/rplyr-ddply-renaming-the-groupinggenerate-column-when-grouping-by-date/</guid>
      <description>On Nicole&#39;s recommendation I&#39;ve been having a look at R&#39;s plyr package to see if I could simplify my meetup analysis and I started by translating my code that grouped meetup join dates by day of the week.
To refresh, the code without plyr looked like this:
library(Rneo4j) timestampToDate &amp;lt;- function(x) as.POSIXct(x / 1000, origin=&amp;quot;1970-01-01&amp;quot;) query = &amp;quot;MATCH (:Person)-[:HAS_MEETUP_PROFILE]-&amp;gt;()-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g:Group {name: \&amp;quot;Neo4j - London User Group\&amp;quot;}) RETURN membership.joined AS joinDate&amp;quot; meetupMembers = cypher(graph, query) meetupMembers$joined &amp;lt;- timestampToDate(meetupMembers$joinDate) dd = aggregate(meetupMembers$joined, by=list(format(meetupMembers$joined, &amp;quot;%A&amp;quot;)), function(x) length(x)) colnames(dd) = c(&amp;quot;dayOfWeek&amp;quot;, &amp;quot;count&amp;quot;)  which returns the following:</description>
    </item>
    
    <item>
      <title>R: Aggregate by different functions and join results into one data frame</title>
      <link>http://markhneedham.com/blog/2014/06/30/r-aggregate-by-different-functions-and-join-results-into-one-data-frame/</link>
      <pubDate>Mon, 30 Jun 2014 22:47:44 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/06/30/r-aggregate-by-different-functions-and-join-results-into-one-data-frame/</guid>
      <description>In continuing my analysis of the London Neo4j meetup group using R I wanted to see which days of the week we organise meetups and how many people RSVP affirmatively by the day.
I started out with this query which returns each event and the number of &#39;yes&#39; RSVPS:
library(Rneo4j) timestampToDate &amp;lt;- function(x) as.POSIXct(x / 1000, origin=&amp;quot;1970-01-01&amp;quot;) query = &amp;quot;MATCH (g:Group {name: \&amp;quot;Neo4j - London User Group\&amp;quot;})-[:HOSTED_EVENT]-&amp;gt;(event)&amp;lt;-[:TO]-({response: &#39;yes&#39;})&amp;lt;-[:RSVPD]-() WHERE (event.time + event.</description>
    </item>
    
    <item>
      <title>R: Order by data frame column and take top 10 rows</title>
      <link>http://markhneedham.com/blog/2014/06/30/r-order-by-data-frame-column-and-take-top-10-rows/</link>
      <pubDate>Mon, 30 Jun 2014 21:44:14 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/06/30/r-order-by-data-frame-column-and-take-top-10-rows/</guid>
      <description>I&#39;ve been doing some ad-hoc analysis of the Neo4j London meetup group using R and Neo4j and having worked out how to group by certain keys the next step was to order the rows of the data frame.
I wanted to drill into the days on which people join the group and see whether they join it at a specific time of day. My feeling was that most people would join on a Monday morning.</description>
    </item>
    
    <item>
      <title>Neo4j/R: Grouping meetup members by join timestamp</title>
      <link>http://markhneedham.com/blog/2014/06/30/neo4jr-grouping-meetup-members-by-join-timestamp/</link>
      <pubDate>Mon, 30 Jun 2014 00:06:54 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/06/30/neo4jr-grouping-meetup-members-by-join-timestamp/</guid>
      <description>I wanted to do some ad-hoc analysis on the join date of members of the Neo4j London meetup group and since cypher doesn&#39;t yet have functions for dealings with dates I thought I&#39;d give R a try.
I started off by executing a cypher query which returned the join timestamp of all the group members using Nicole White&#39;s RNeo4j package:
&amp;gt; library(Rneo4j) &amp;gt; query = &amp;quot;match (:Person)-[:HAS_MEETUP_PROFILE]-&amp;gt;()-[:HAS_MEMBERSHIP]-&amp;gt;(membership)-[:OF_GROUP]-&amp;gt;(g:Group {name: \&amp;quot;Neo4j - London User Group\&amp;quot;}) RETURN membership.</description>
    </item>
    
    <item>
      <title>Neo4j/R: Analysing London NoSQL meetup membership</title>
      <link>http://markhneedham.com/blog/2014/05/31/neo4jr-analysing-london-nosql-meetup-membership/</link>
      <pubDate>Sat, 31 May 2014 21:32:24 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2014/05/31/neo4jr-analysing-london-nosql-meetup-membership/</guid>
      <description>In my spare time I&#39;ve been working on a Neo4j application that runs on tops of meetup.com&#39;s API and Nicole recently showed me how I could wire up some of the queries to use her Rneo4j library: @markhneedham pic.twitter.com/8014jckEUl
&amp;mdash; Nicole White (@_nicolemargaret) May 31, 2014 
The query used in that visualisation shows the number of members that overlap between each pair of groups but a more interesting query is the one which shows the % overlap between groups based on the unique members across the groups.</description>
    </item>
    
    <item>
      <title>R: Building up a data frame row by row</title>
      <link>http://markhneedham.com/blog/2013/02/10/r-building-up-a-data-frame-row-by-row/</link>
      <pubDate>Sun, 10 Feb 2013 13:29:32 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/02/10/r-building-up-a-data-frame-row-by-row/</guid>
      <description>Jen and I recently started working on the Kaggle Titanic problem and we thought it&#39;d probably be useful to start with some exploratory data analysis to get a feel for the data set.
For this problem you are given a selection of different features describing the passengers on board the Titanic and you have to predict whether or not they would have survived or died based on those features.
I thought an interesting first thing to look at would be the survival rate for passengers of different socio economic status as you would imagine that people of a higher status were more likely to have survived.</description>
    </item>
    
    <item>
      <title>R: Modelling a conversion rate with a binomial distribution</title>
      <link>http://markhneedham.com/blog/2013/02/07/r-modelling-a-conversion-rate-with-a-binomial-distribution/</link>
      <pubDate>Thu, 07 Feb 2013 01:26:12 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/02/07/r-modelling-a-conversion-rate-with-a-binomial-distribution/</guid>
      <description>As part of some work Sid and I were doing last week we wanted to simulate the conversion rate for an A/B testing we were planning.
We started with the following function which returns the simulated conversion rate for a given conversion rate of 12%:
generateConversionRates &amp;lt;- function(sampleSize) { sample_a &amp;lt;- rbinom(seq(0, sampleSize), 1, 0.12) conversion_a &amp;lt;- length(sample_a[sample_a == 1]) / sampleSize sample_b &amp;lt;- rbinom(seq(0, sampleSize), 1, 0.12) conversion_b &amp;lt;- length(sample_b[sample_b == 1]) / sampleSize c(conversion_a, conversion_b) }  If we call it:</description>
    </item>
    
    <item>
      <title>R: Mapping over a list of lists</title>
      <link>http://markhneedham.com/blog/2013/02/03/r-mapping-over-a-list-of-lists/</link>
      <pubDate>Sun, 03 Feb 2013 10:40:48 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/02/03/r-mapping-over-a-list-of-lists/</guid>
      <description>As part of the coursera Data Analysis course I had the following code to download and then read in a file:
&amp;gt; file &amp;lt;- &amp;quot;https://dl.dropbox.com/u/7710864/data/csv_hid/ss06hid.csv&amp;quot; &amp;gt; download.file(file, destfile=&amp;quot;americancommunity.csv&amp;quot;, method=&amp;quot;curl&amp;quot;) &amp;gt; acomm &amp;lt;- read.csv(&amp;quot;americancommunity.csv&amp;quot;)  We then had to filter the data based on the values in a couple of columns and work out how many rows were returned in each case:
&amp;gt; one &amp;lt;- acomm[acomm$RMS == 4 &amp;amp; !is.na(acomm$RMS) &amp;amp; acomm$BDS == 3 &amp;amp; !</description>
    </item>
    
    <item>
      <title>R: Ordering rows in a data frame by multiple columns</title>
      <link>http://markhneedham.com/blog/2013/01/23/r-ordering-rows-in-a-data-frame-by-multiple-columns/</link>
      <pubDate>Wed, 23 Jan 2013 23:09:28 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/01/23/r-ordering-rows-in-a-data-frame-by-multiple-columns/</guid>
      <description>In one of the assignments of Computing for Data Analysis we needed to sort a data frame based on the values in two of the columns and then return the top value.
The initial data frame looked a bit like this:
&amp;gt; names &amp;lt;- c(&amp;quot;paul&amp;quot;, &amp;quot;mark&amp;quot;, &amp;quot;dave&amp;quot;, &amp;quot;will&amp;quot;, &amp;quot;john&amp;quot;) &amp;gt; values &amp;lt;- c(1,4,1,2,1) &amp;gt; smallData &amp;lt;- data.frame(name = names, value = values) &amp;gt; smallData name value 1 paul 1 2 mark 4 3 dave 1 4 will 2 5 john 1  I want to be able to sort the data frame by value and name both in ascending order so the final result should look like this:</description>
    </item>
    
    <item>
      <title>R: Filter a data frame based on values in two columns</title>
      <link>http://markhneedham.com/blog/2013/01/23/r-filter-a-data-frame-based-on-values-in-two-columns/</link>
      <pubDate>Wed, 23 Jan 2013 22:34:01 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/01/23/r-filter-a-data-frame-based-on-values-in-two-columns/</guid>
      <description>In the most recent assignment of the Computing for Data Analysis course we had to filter a data frame which contained N/A values in two columns to only return rows which had no N/A&#39;s.
I started with a data frame that looked like this:
&amp;gt; data &amp;lt;- read.csv(&amp;quot;specdata/002.csv&amp;quot;) &amp;gt; # we&#39;ll just use a few rows to make it easier to see what&#39;s going on &amp;gt; data[2494:2500,] Date sulfate nitrate ID 2494 2007-10-30 3.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>http://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>I&#39;ve written previously about our attempts at the Kaggle Digit Recogniser problem and our approach so far has been to use the data provided and plug it into different algorithms and see what we end up with.
From browsing through the forums we saw others mentioning feature extraction - an approach where we transform the data into another format , the thinking being that we can train a better classifier with better data.</description>
    </item>
    
    <item>
      <title>R: Mapping a function over a collection of values</title>
      <link>http://markhneedham.com/blog/2012/07/23/r-mapping-a-function-over-a-collection-of-values/</link>
      <pubDate>Mon, 23 Jul 2012 23:25:00 +0000</pubDate>
      
      <guid>http://markhneedham.com/blog/2012/07/23/r-mapping-a-function-over-a-collection-of-values/</guid>
      <description>I spent a bit of Sunday playing around with R and one thing I wanted to do was map a function over a collection of values and transform each value slightly.
I loaded my data set using the &amp;lsquo;Import Dataset&amp;rsquo; option in R Studio (suggested to me by Rob) which gets converted to the following function call:
&amp;gt; data &amp;lt;- read.csv(&amp;quot;~/data.csv&amp;quot;, header=T, encoding=&amp;quot;ISO-8859&amp;quot;) &amp;gt; data Column1 InterestingColumn 1 Mark 12.50 2 Dave 100.</description>
    </item>
    
  </channel>
</rss>