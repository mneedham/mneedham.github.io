<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Mark Needham</title>
    <link>https://markhneedham.com/blog/category/spark/</link>
    <description>Recent content in Spark on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Oct 2015 23:10:47 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/category/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark: MatchError (of class org.apache.spark.sql.catalyst.expressions.GenericRow) spark</title>
      <link>https://markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</link>
      <pubDate>Tue, 27 Oct 2015 23:10:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</guid>
      <description>I&#39;ve been using Spark again lately to do some pre-processing on the Land Registry data set and ran into an initially confusing problem when trying to parse the CSV file.  I&#39;m using the Databricks CSV parsing library and wrote the following script to go over each row, collect up the address components and then derive a &#39;fullAddress&#39; field.  To refresh, this is what the CSV file looks like: $ head -n5 pp-complete.</description>
    </item>
    
    <item>
      <title>SparkR: Add new column to data frame by concatenating other columns</title>
      <link>https://markhneedham.com/blog/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</link>
      <pubDate>Mon, 21 Sep 2015 22:30:51 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</guid>
      <description>Continuing with my exploration of the Land Registry open data set using SparkR I wanted to see which road in the UK has had the most property sales over the last 20 years. To recap, this is what the data frame looks like:
./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0 &amp;gt; sales &amp;lt;- read.df(sqlContext, &amp;quot;pp-complete.csv&amp;quot;, &amp;quot;com.databricks.spark.csv&amp;quot;, header=&amp;quot;false&amp;quot;) &amp;gt; head(sales) C0 C1 C2 C3 C4 C5 1 {0C7ADEF5-878D-4066-B785-0000003ED74A} 163000 2003-02-21 00:00 UB5 4PJ T N 2 {35F67271-ABD4-40DA-AB09-00000085B9D3} 247500 2005-07-15 00:00 TA19 9DD D N 3 {B20B1C74-E8E1-4137-AB3E-0000011DF342} 320000 2010-09-10 00:00 W4 1DZ F N 4 {7D6B0915-C56B-4275-AF9B-00000156BCE7} 104000 1997-08-27 00:00 NE61 2BH D N 5 {47B60101-B64C-413D-8F60-000002F1692D} 147995 2003-05-02 00:00 PE33 0RU D N 6 {51F797CA-7BEB-4958-821F-000003E464AE} 110000 2013-03-22 00:00 NR35 2SF T N C6 C7 C8 C9 C10 C11 1 F 106 READING ROAD NORTHOLT NORTHOLT 2 F 58 ADAMS MEADOW ILMINSTER ILMINSTER 3 L 58 WHELLOCK ROAD LONDON 4 F 17 WESTGATE MORPETH MORPETH 5 F 4 MASON GARDENS WEST WINCH KING&#39;S LYNN 6 F 5 WILD FLOWER WAY DITCHINGHAM BUNGAY C12 C13 C14 1 EALING GREATER LONDON A 2 SOUTH SOMERSET SOMERSET A 3 EALING GREATER LONDON A 4 CASTLE MORPETH NORTHUMBERLAND A 5 KING&#39;S LYNN AND WEST NORFOLK NORFOLK A 6 SOUTH NORFOLK NORFOLK A   This document explains the data stored in each field and for this particular query we&#39;re interested in fields C9-C12.</description>
    </item>
    
    <item>
      <title>SparkR: Error in invokeJava(isStatic = TRUE, className, methodName, ...) :  java.lang.ClassNotFoundException: Failed to load class for data source: csv.</title>
      <link>https://markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</link>
      <pubDate>Mon, 21 Sep 2015 22:06:44 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</guid>
      <description>I&#39;ve been wanting to play around with SparkR for a while and over the weekend deciding to explore a large Land Registry CSV file containing all the sales of properties in the UK over the last 20 years. First I started up the SparkR shell with the CSV package loaded in:
./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0   Next I tried to read the CSV file into a Spark data frame by modifying one of the examples from the tutorial: &amp;gt; sales &amp;lt;- read.</description>
    </item>
    
    <item>
      <title>Spark: Convert RDD to DataFrame</title>
      <link>https://markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</link>
      <pubDate>Thu, 06 Aug 2015 21:11:44 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</guid>
      <description>As I mentioned in a previous blog post I&#39;ve been playing around with the Databricks Spark CSV library and wanted to take a CSV file, clean it up and then write out a new CSV file containing some of the columns.  I started by processing the CSV file and writing it into a temporary table: import org.apache.spark.sql.{SQLContext, Row, DataFrame} val sqlContext = new SQLContext(sc) val crimeFile = &amp;quot;Crimes_-_2001_to_present.csv&amp;quot; sqlContext.</description>
    </item>
    
    <item>
      <title>Spark: pyspark/Hadoop - py4j.protocol.Py4JJavaError: An error occurred while calling o23.load.: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>https://markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Tue, 04 Aug 2015 06:35:40 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>I&#39;ve been playing around with pyspark - Spark&#39;s Python library - and I wanted to execute the following job which takes a file from my local HDFS and then counts how many times each FBI code appears using Spark SQL: from pyspark import SparkContext from pyspark.sql import SQLContext sc = SparkContext(&amp;quot;local&amp;quot;, &amp;quot;Simple App&amp;quot;) sqlContext = SQLContext(sc) file = &amp;quot;hdfs://localhost:9000/user/markneedham/Crimes_-_2001_to_present.csv&amp;quot; sqlContext.load(source=&amp;quot;com.databricks.spark.csv&amp;quot;, header=&amp;quot;true&amp;quot;, path = file).registerTempTable(&amp;quot;crimes&amp;quot;) rows = sqlContext.sql(&amp;quot;select `FBI Code` AS fbiCode, COUNT(*) AS times FROM crimes GROUP BY `FBI Code` ORDER BY times DESC&amp;quot;).</description>
    </item>
    
    <item>
      <title>Spark: Processing CSV files using Databricks Spark CSV Library</title>
      <link>https://markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</link>
      <pubDate>Sun, 02 Aug 2015 18:08:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</guid>
      <description>Last year I wrote about exploring the Chicago crime data set using Spark and the OpenCSV parser and while this worked well, a few months ago I noticed that there&#39;s now a spark-csv library which I should probably use instead.  I thought it&#39;d be a fun exercise to translate my code to use it.  So to recap our goal: we want to count how many times each type of crime has been committed.</description>
    </item>
    
    <item>
      <title>Spark: Generating CSV files to import into Neo4j</title>
      <link>https://markhneedham.com/blog/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</link>
      <pubDate>Tue, 14 Apr 2015 22:56:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</guid>
      <description>About a year ago Ian pointed me at a Chicago Crime data set which seemed like a good fit for Neo4j and after much procrastination I&#39;ve finally got around to importing it.  The data set covers crimes committed from 2001 until now. It contains around 4 million crimes and meta data around those crimes such as the location, type of crime and year to name a few.  The contents of the file follow this structure: $ head -n 10 ~/Downloads/Crimes_-_2001_to_present.</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file with header using saveAsFile</title>
      <link>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</link>
      <pubDate>Sun, 30 Nov 2014 08:21:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</guid>
      <description>In my last blog post I showed how to write to a single CSV file using Spark and Hadoop and the next thing I wanted to do was add a header row to the resulting row.
 Hadoop&#39;s FileUtil#copyMerge function does take a String parameter but it adds this text to the end of each partition file which isn&#39;t quite what we want. However, if we copy that function into our own FileUtil class we can restructure it to do what we want:</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file</title>
      <link>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</link>
      <pubDate>Sun, 30 Nov 2014 07:40:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</guid>
      <description>A couple of weeks ago I wrote how I&#39;d been using Spark to explore a City of Chicago Crime data set and having worked out how many of each crime had been committed I wanted to write that to a CSV file.  Spark provides a saveAsTextFile function which allows us to save RDD&#39;s so I refactored my code into the following format to allow me to use that: import au.</description>
    </item>
    
    <item>
      <title>Spark: Parse CSV file and group by column value</title>
      <link>https://markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</link>
      <pubDate>Sun, 16 Nov 2014 22:53:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</guid>
      <description>I&#39;ve found myself working with large CSV files quite frequently and realising that my existing toolset didn&#39;t let me explore them quickly I thought I&#39;d spend a bit of time looking at Spark to see if it could help.
I&#39;m working with a crime data set released by the City of Chicago: it&#39;s 1GB in size and contains details of 4 million crimes:
$ ls -alh ~/Downloads/Crimes_-_2001_to_present.csv -rw-r--r--@ 1 markneedham staff 1.</description>
    </item>
    
  </channel>
</rss>