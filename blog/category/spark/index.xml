<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/category/spark/</link>
    <description>Recent content in Spark on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2019 09:00:00 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/category/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>pyspark: Py4JJavaError: An error occurred while calling o138.loadClass.: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI</title>
      <link>https://www.markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</link>
      <pubDate>Wed, 17 Apr 2019 09:00:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</guid>
      <description>I’ve been building a Docker Container that has support for Jupyter, Spark, GraphFrames, and Neo4j, and ran into a problem that had me pulling my (metaphorical) hair out!
The pyspark-notebook container gets us most of the way there, but it doesn’t have GraphFrames or Neo4j support. Adding Neo4j is as simple as pulling in the Python Driver from Conda Forge, which leaves us with GraphFrames.
When I’m using GraphFrames with pyspark locally I would pull it in via the --packages config parameter, like this:</description>
    </item>
    
    <item>
      <title>Spark: MatchError (of class org.apache.spark.sql.catalyst.expressions.GenericRow) spark</title>
      <link>https://www.markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</link>
      <pubDate>Tue, 27 Oct 2015 23:10:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/10/27/spark-matcherror-of-class-org-apache-spark-sql-catalyst-expressions-genericrow-spark/</guid>
      <description>I’ve been using Spark again lately to do some pre-processing on the Land Registry data set and ran into an initially confusing problem when trying to parse the CSV file.
I’m using the Databricks CSV parsing library and wrote the following script to go over each row, collect up the address components and then derive a &amp;#39;fullAddress&amp;#39; field.
To refresh, this is what the CSV file looks like:
$ head -n5 pp-complete.</description>
    </item>
    
    <item>
      <title>SparkR: Add new column to data frame by concatenating other columns</title>
      <link>https://www.markhneedham.com/blog/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</link>
      <pubDate>Mon, 21 Sep 2015 22:30:51 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/09/21/sparkr-add-new-column-to-data-frame-by-concatenating-other-columns/</guid>
      <description>Continuing with my exploration of the Land Registry open data set using SparkR I wanted to see which road in the UK has had the most property sales over the last 20 years.
To recap, this is what the data frame looks like:
./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0 &amp;gt; sales &amp;lt;- read.df(sqlContext, &amp;#34;pp-complete.csv&amp;#34;, &amp;#34;com.databricks.spark.csv&amp;#34;, header=&amp;#34;false&amp;#34;) &amp;gt; head(sales) C0 C1 C2 C3 C4 C5 1 {0C7ADEF5-878D-4066-B785-0000003ED74A} 163000 2003-02-21 00:00 UB5 4PJ T N 2 {35F67271-ABD4-40DA-AB09-00000085B9D3} 247500 2005-07-15 00:00 TA19 9DD D N 3 {B20B1C74-E8E1-4137-AB3E-0000011DF342} 320000 2010-09-10 00:00 W4 1DZ F N 4 {7D6B0915-C56B-4275-AF9B-00000156BCE7} 104000 1997-08-27 00:00 NE61 2BH D N 5 {47B60101-B64C-413D-8F60-000002F1692D} 147995 2003-05-02 00:00 PE33 0RU D N 6 {51F797CA-7BEB-4958-821F-000003E464AE} 110000 2013-03-22 00:00 NR35 2SF T N C6 C7 C8 C9 C10 C11 1 F 106 READING ROAD NORTHOLT NORTHOLT 2 F 58 ADAMS MEADOW ILMINSTER ILMINSTER 3 L 58 WHELLOCK ROAD LONDON 4 F 17 WESTGATE MORPETH MORPETH 5 F 4 MASON GARDENS WEST WINCH KING&amp;#39;S LYNN 6 F 5 WILD FLOWER WAY DITCHINGHAM BUNGAY C12 C13 C14 1 EALING GREATER LONDON A 2 SOUTH SOMERSET SOMERSET A 3 EALING GREATER LONDON A 4 CASTLE MORPETH NORTHUMBERLAND A 5 KING&amp;#39;S LYNN AND WEST NORFOLK NORFOLK A 6 SOUTH NORFOLK NORFOLK A This document explains the data stored in each field and for this particular query we’re interested in fields C9-C12.</description>
    </item>
    
    <item>
      <title>SparkR: Error in invokeJava(isStatic = TRUE, className, methodName, ...) :  java.lang.ClassNotFoundException: Failed to load class for data source: csv.</title>
      <link>https://www.markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</link>
      <pubDate>Mon, 21 Sep 2015 22:06:44 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/09/21/sparkr-error-in-invokejavaisstatic-true-classname-methodname-java-lang-classnotfoundexception-failed-to-load-class-for-data-source-csv/</guid>
      <description>I’ve been wanting to play around with SparkR for a while and over the weekend deciding to explore a large Land Registry CSV file containing all the sales of properties in the UK over the last 20 years.
First I started up the SparkR shell with the CSV package loaded in: ~bash ./spark-1.5.0-bin-hadoop2.6/bin/sparkR --packages com.databricks:spark-csv_2.11:1.2.0 ~
Next I tried to read the CSV file into a Spark data frame by modifying one of the examples from the tutorial: ~bash &amp;gt; sales &amp;lt;- read.</description>
    </item>
    
    <item>
      <title>Spark: Convert RDD to DataFrame</title>
      <link>https://www.markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</link>
      <pubDate>Thu, 06 Aug 2015 21:11:44 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/08/06/spark-convert-rdd-to-dataframe/</guid>
      <description>As I mentioned in a previous blog post I’ve been playing around with the Databricks Spark CSV library and wanted to take a CSV file, clean it up and then write out a new CSV file containing some of the columns.
I started by processing the CSV file and writing it into a temporary table:
import org.apache.spark.sql.{SQLContext, Row, DataFrame} val sqlContext = new SQLContext(sc) val crimeFile = &amp;#34;Crimes_-_2001_to_present.csv&amp;#34; sqlContext.load(&amp;#34;com.databricks.spark.csv&amp;#34;, Map(&amp;#34;path&amp;#34; -&amp;gt; crimeFile, &amp;#34;header&amp;#34; -&amp;gt; &amp;#34;true&amp;#34;)).</description>
    </item>
    
    <item>
      <title>Spark: pyspark/Hadoop - py4j.protocol.Py4JJavaError: An error occurred while calling o23.load.: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4</title>
      <link>https://www.markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</link>
      <pubDate>Tue, 04 Aug 2015 06:35:40 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/08/04/spark-pysparkhadoop-py4j-protocol-py4jjavaerror-an-error-occurred-while-calling-o23-load-org-apache-hadoop-ipc-remoteexception-server-ipc-version-9-cannot-communicate-with-client-version-4/</guid>
      <description>I’ve been playing around with pyspark - Spark’s Python library - and I wanted to execute the following job which takes a file from my local HDFS and then counts how many times each FBI code appears using Spark SQL:
from pyspark import SparkContext from pyspark.sql import SQLContext sc = SparkContext(&amp;#34;local&amp;#34;, &amp;#34;Simple App&amp;#34;) sqlContext = SQLContext(sc) file = &amp;#34;hdfs://localhost:9000/user/markneedham/Crimes_-_2001_to_present.csv&amp;#34; sqlContext.load(source=&amp;#34;com.databricks.spark.csv&amp;#34;, header=&amp;#34;true&amp;#34;, path = file).registerTempTable(&amp;#34;crimes&amp;#34;) rows = sqlContext.sql(&amp;#34;select `FBI Code` AS fbiCode, COUNT(*) AS times FROM crimes GROUP BY `FBI Code` ORDER BY times DESC&amp;#34;).</description>
    </item>
    
    <item>
      <title>Spark: Processing CSV files using Databricks Spark CSV Library</title>
      <link>https://www.markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</link>
      <pubDate>Sun, 02 Aug 2015 18:08:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/08/02/spark-processing-csv-files-using-databricks-spark-csv-library/</guid>
      <description>Last year I wrote about exploring the Chicago crime data set using Spark and the OpenCSV parser and while this worked well, a few months ago I noticed that there’s now a spark-csv library which I should probably use instead.
I thought it’d be a fun exercise to translate my code to use it.
So to recap our goal: we want to count how many times each type of crime has been committed.</description>
    </item>
    
    <item>
      <title>Spark: Generating CSV files to import into Neo4j</title>
      <link>https://www.markhneedham.com/blog/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</link>
      <pubDate>Tue, 14 Apr 2015 22:56:35 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/04/14/spark-generating-csv-files-to-import-into-neo4j/</guid>
      <description>About a year ago Ian pointed me at a Chicago Crime data set which seemed like a good fit for Neo4j and after much procrastination I’ve finally got around to importing it.
The data set covers crimes committed from 2001 until now. It contains around 4 million crimes and meta data around those crimes such as the location, type of crime and year to name a few.
The contents of the file follow this structure:</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file with header using saveAsFile</title>
      <link>https://www.markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</link>
      <pubDate>Sun, 30 Nov 2014 08:21:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/11/30/spark-write-to-csv-file-with-header-using-saveasfile/</guid>
      <description>In my last blog post I showed how to write to a single CSV file using Spark and Hadoop and the next thing I wanted to do was add a header row to the resulting row.
Hadoop’s https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileUtil.html#copyMerge(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration, java.lang.String) function does take a String parameter but it adds this text to the end of each partition file which isn’t quite what we want.
However, if we copy that function into our own FileUtil class we can restructure it to do what we want:</description>
    </item>
    
    <item>
      <title>Spark: Write to CSV file</title>
      <link>https://www.markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</link>
      <pubDate>Sun, 30 Nov 2014 07:40:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/11/30/spark-write-to-csv-file/</guid>
      <description>A couple of weeks ago I wrote how I’d been using Spark to explore a City of Chicago Crime data set and having worked out how many of each crime had been committed I wanted to write that to a CSV file.
Spark provides a saveAsTextFile function which allows us to save RDD’s so I refactored my code into the following format to allow me to use that:
import au.</description>
    </item>
    
    <item>
      <title>Spark: Parse CSV file and group by column value</title>
      <link>https://www.markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</link>
      <pubDate>Sun, 16 Nov 2014 22:53:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/11/16/spark-parse-csv-file-and-group-by-column-value/</guid>
      <description>I’ve found myself working with large CSV files quite frequently and realising that my existing toolset didn’t let me explore them quickly I thought I’d spend a bit of time looking at Spark to see if it could help.
I’m working with a crime data set released by the City of Chicago: it’s 1GB in size and contains details of 4 million crimes:
$ ls -alh ~/Downloads/Crimes_-_2001_to_present.csv -rw-r--r--@ 1 markneedham staff 1.</description>
    </item>
    
  </channel>
</rss>
