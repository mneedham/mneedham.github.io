<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/category/machine-learning/</link>
    <description>Recent content in Machine Learning on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 May 2018 08:12:21 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/category/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Predicting movie genres with node2Vec and Tensorflow</title>
      <link>https://www.markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</link>
      <pubDate>Fri, 11 May 2018 08:12:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</guid>
      <description>In my previous post we looked at how to get up and running with the node2Vec algorithm, and in this post we’ll learn how we can feed graph embeddings into a simple Tensorflow model.
Recall that node2Vec takes in a list of edges (or relationships) and gives us back an embedding (array of numbers) for each node.
This time we’re going to run the algorithm over a movies recommendation dataset from the Neo4j Sandbox.</description>
    </item>
    
    <item>
      <title>Exploring node2vec - a graph embedding algorithm</title>
      <link>https://www.markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</link>
      <pubDate>Fri, 11 May 2018 08:08:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</guid>
      <description>In my explorations of graph based machine learning, one algorithm I came across is called node2Vec. The paper describes it as &amp;#34;an algorithmic framework for learning continuous feature representations for nodes in networks&amp;#34;.
So what does the algorithm do? From the website:
The node2vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective. The objective is flexible, and the algorithm accommodates for various definitions of network neighborhoods by simulating biased random walks.</description>
    </item>
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://www.markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week I’ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiri’s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
I’ll use the same data as Sidath, so if you want to follow along you’ll need to download these files:
iris_training.csv
iris_test.csv
Loading data The way we load data will remain exactly the same - we’ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://www.markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>I’ve been playing around with some Tensorflow tutorials recently and wanted to see if I could create a submission for Kaggle’s Spooky Author Identification competition that I’ve written about recently.
My model is based on one from the text classification tutorial. The tutorial shows how to create custom Estimators which we can learn more about in a post on the Google Developers blog.
Imports Let’s get started. First, our imports:</description>
    </item>
    
    <item>
      <title>scikit-learn: First steps with log_loss</title>
      <link>https://www.markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</link>
      <pubDate>Wed, 14 Sep 2016 05:33:38 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</guid>
      <description>Over the last week I’ve spent a little bit of time playing around with the data in the Kaggle TalkingData Mobile User Demographics competition, and came across a notebook written by dune_dweller showing how to run a logistic regression algorithm on the dataset.
The metric used to evaluate the output in this competition is multi class logarithmic loss, which is implemented by the http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html function in the scikit-learn library.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:
Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).</description>
    </item>
    
    <item>
      <title>scikit-learn: Trying to find clusters of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</link>
      <pubDate>Thu, 25 Aug 2016 22:07:25 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</guid>
      <description>In my last post I showed how to find similar Game of Thrones episodes based on the characters that appear in different episodes. This allowed us to find similar episodes on an episode by episode basis, but I was curious whether there were groups of similar episodes that we could identify.
scikit-learn provides several clustering algorithms that can run over our episode vectors and hopefully find clusters of similar episodes.</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>A couple of months ago Praveena and I created a Game of Thrones dataset to use in a workshop and I thought it’d be fun to run it through some machine learning algorithms and hopefully find some interesting insights.
The dataset is available as CSV files but for this analysis I’m assuming that it’s already been imported into neo4j. If you want to import the data you can run the tutorial by typing the following into the query bar of the neo4j browser:</description>
    </item>
    
    <item>
      <title>How I met your mother: Story arcs</title>
      <link>https://www.markhneedham.com/blog/2015/04/03/how-i-met-your-mother-story-arcs/</link>
      <pubDate>Fri, 03 Apr 2015 23:31:33 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/04/03/how-i-met-your-mother-story-arcs/</guid>
      <description>After weeks of playing around with various algorithms to extract story arcs in How I met your mother I’ve come to the conclusion that I don’t yet have the skills to completely automate this process so I’m going to change my approach.
The new plan is to treat the outputs of the algorithms as suggestions for possible themes but then have a manual step where I extract what I think are interesting themes in the series.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>In my continued exploration of topic modelling I came across The Programming Historian blog and a post showing how to derive topics from a corpus using the Java library mallet.
The instructions on the blog make it very easy to get up and running but as with other libraries I’ve used, you have to specify how many topics the corpus consists of. I’m never sure what value to select but the authors make the following suggestion:</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>Following on from Rik van Bruggen’s blog post on a QCon graph he’s created ahead of this week’s conference, I was curious whether we could extract any interesting relationships between talks based on their abstracts.
Talks are already grouped by their hosting track but there’s likely to be some overlap in topics even for talks on different tracks. I therefore wanted to extract topics and connect each talk to the topic that describes it best.</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>https://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>Nathan and I have been looking at Kaggle’s Titanic problem and while working through the Python tutorial Nathan pointed out that we could greatly simplify the code if we used pandas instead.
The problem we had with numpy is that you use integers to reference columns. We spent a lot of time being thoroughly confused as to why something wasn’t working only to realise we were using the wrong column.</description>
    </item>
    
    <item>
      <title>Feature Extraction/Selection - What I&#39;ve learnt so far</title>
      <link>https://www.markhneedham.com/blog/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</link>
      <pubDate>Sun, 10 Feb 2013 15:42:07 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</guid>
      <description>A couple of weeks ago I wrote about some feature extraction work that I’d done on the Kaggle Digit Recognizer data set and having realised that I had no idea what I was doing I thought I should probably learn a bit more.
I came across Dunja Mladenic’s &amp;#39;Dimensionality Reduction by Feature Selection in Machine Learning&amp;#39; presentation in which she sweeps across the landscape of feature selection and explains how everything fits together.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>https://www.markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I’ve written a few blog posts about our attempts at the Kaggle Digit Recogniser problem and one thing we haven’t yet tried is feature extraction.
Feature extraction in this context means that we’d generate some other features to train a classifier with rather than relying on just the pixel values we were provided.
Every week Jen would try and persuade me that we should try it out but it wasn’t until I was flicking through the notes from the Columbia Data Science class that it struck home:</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>https://www.markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>I’ve written previously about our attempts at the Kaggle Digit Recogniser problem and our approach so far has been to use the data provided and plug it into different algorithms and see what we end up with.
From browsing through the forums we saw others mentioning feature extraction - an approach where we transform the data into another format , the thinking being that we can train a better classifier with better data.</description>
    </item>
    
    <item>
      <title>Mahout: Parallelising the creation of DecisionTrees</title>
      <link>https://www.markhneedham.com/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</link>
      <pubDate>Thu, 27 Dec 2012 00:08:01 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</guid>
      <description>A couple of months ago I wrote a blog post describing our use of Mahout random forests for the Kaggle Digit Recogniser Problem and after seeing how long it took to create forests with 500+ trees I wanted to see if this could be sped up by parallelising the process.
From looking at the https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java it seemed like it should be possible to create lots of small forests and then combine them together.</description>
    </item>
    
    <item>
      <title>Weka: Saving and loading classifiers</title>
      <link>https://www.markhneedham.com/blog/2012/12/12/weka-saving-and-loading-classifiers/</link>
      <pubDate>Wed, 12 Dec 2012 00:04:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/12/12/weka-saving-and-loading-classifiers/</guid>
      <description>In our continued machine learning travels Jen and I have been building some classifiers using Weka and one thing we wanted to do was save the classifier and then reuse it later.
There is documentation for how to do this from the command line but we’re doing everything programatically and wanted to be able to save our classifiers from Java code.
As it turns out it’s not too tricky when you know which classes to call and saving a classifier to a file is as simple as this:</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://www.markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle’s Digit Recognizer Jen and I decided to try out boosting on our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
We couldn’t find any documentation that it was possible to apply boosting to Mahout’s random forest algorithm but we knew it was possible with Weka so we decided to use that instead!
As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it’s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>A first failed attempt at Natural Language Processing</title>
      <link>https://www.markhneedham.com/blog/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</link>
      <pubDate>Sat, 24 Nov 2012 19:43:32 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</guid>
      <description>One of the things I find fascinating about dating websites is that the profiles of people are almost identical so I thought it would be an interesting exercise to grab some of the free text that people write about themselves and prove the similarity.
I’d been talking to Matt Biddulph about some Natural Language Processing (NLP) stuff he’d been working on and he wrote up a bunch of libraries, articles and books that he’d found useful.</description>
    </item>
    
    <item>
      <title>Learning: Switching between theory and practice</title>
      <link>https://www.markhneedham.com/blog/2012/11/19/learning-switching-between-theory-and-practice/</link>
      <pubDate>Mon, 19 Nov 2012 13:31:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/11/19/learning-switching-between-theory-and-practice/</guid>
      <description>In one of my first ever blog posts I wrote about the differences I’d experienced in learning the theory about a topic and then seeing it in practice.
The way I remember learning at school and university was that you learn all the theory first and then put it into practice but I typically don’t find myself doing this whenever I learn something new.
I spent a bit of time over the weekend learning more about neural networks as my colleague Jen Smith suggested this might be a more effective technique for getting a higher accuracy score on the Kaggle Digit Recogniser problem.</description>
    </item>
    
    <item>
      <title>Mahout: Using a saved Random Forest/DecisionTree</title>
      <link>https://www.markhneedham.com/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</link>
      <pubDate>Sat, 27 Oct 2012 22:03:30 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</guid>
      <description>One of the things that I wanted to do while playing around with random forests using Mahout was to save the random forest and then use use it again which is something Mahout does cater for.
It was actually much easier to do this than I’d expected and assuming that we already have a https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java built we’d just need the following code to save it to disc:
int numberOfTrees = 1; Data data = loadData(.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Mahout Random Forest attempt</title>
      <link>https://www.markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 20:24:48 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</guid>
      <description>I’ve written previously about the K-means approach that Jen and I took when trying to solve Kaggle’s Digit Recognizer and having stalled at about 80% accuracy we decided to try one of the algorithms suggested in the tutorials section - the random forest!
We initially used a clojure random forests library but struggled to build the random forest from the training set data in a reasonable amount of time so we switched to Mahout’s version which is based on Leo Breiman’s random forests paper.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://www.markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jen and I used the K-means algorithm to classify digits in Kaggle’s Digit Recognizer problem and one of the things we’d read was that with this algorithm you often end up with situations where it’s difficult to classify a new item because if falls between two labels.
We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A K-means attempt</title>
      <link>https://www.markhneedham.com/blog/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</link>
      <pubDate>Tue, 23 Oct 2012 19:04:20 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</guid>
      <description>Over the past couple of months Jen and I have been playing around with the Kaggle Digit Recognizer problem - a &amp;#39;competition&amp;#39; created to introduce people to Machine Learning.
The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.
You are given an input file which contains multiple rows each containing 784 pixel values representing a 28x28 pixel image as well as a label indicating which number that image actually represents.</description>
    </item>
    
  </channel>
</rss>
