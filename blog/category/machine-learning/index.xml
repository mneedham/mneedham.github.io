<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Mark Needham</title>
    <link>https://markhneedham.com/blog/category/machine-learning/</link>
    <description>Recent content in Machine Learning on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 May 2018 08:12:21 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/category/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predicting movie genres with node2Vec and Tensorflow</title>
      <link>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</link>
      <pubDate>Fri, 11 May 2018 08:12:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/node2vec-tensorflow/</guid>
      <description>In my previous post we looked at how to get up and running with the node2Vec algorithm, and in this post we&amp;#8217;ll learn how we can feed graph embeddings into a simple Tensorflow model.
 Recall that node2Vec takes in a list of edges (or relationships) and gives us back an embedding (array of numbers) for each node.
 This time we&amp;#8217;re going to run the algorithm over a movies recommendation dataset from the Neo4j Sandbox.</description>
    </item>
    
    <item>
      <title>Exploring node2vec - a graph embedding algorithm</title>
      <link>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</link>
      <pubDate>Fri, 11 May 2018 08:08:21 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/11/exploring-node2vec-graph-embedding-algorithm/</guid>
      <description>In my explorations of graph based machine learning, one algorithm I came across is called node2Vec. The paper describes it as &#34;an algorithmic framework for learning continuous feature representations for nodes in networks&#34;.
 So what does the algorithm do? From the website:
  The node2vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective. The objective is flexible, and the algorithm accommodates for various definitions of network neighborhoods by simulating biased random walks.</description>
    </item>
    
    <item>
      <title>Tensorflow 1.8: Hello World using the Estimator API</title>
      <link>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</link>
      <pubDate>Sat, 05 May 2018 00:31:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/05/05/tensorflow-18-hello-world-using-estimator-api/</guid>
      <description>Over the last week I&amp;#8217;ve been going over various Tensorflow tutorials and one of the best ones when getting started is Sidath Asiri&amp;#8217;s Hello World in TensorFlow, which shows how to build a simple linear classifier on the Iris dataset.
 I&amp;#8217;ll use the same data as Sidath, so if you want to follow along you&amp;#8217;ll need to download these files:
   iris_training.csv
  iris_test.csv
   Loading data The way we load data will remain exactly the same - we&amp;#8217;ll still be reading it into a Pandas dataframe:</description>
    </item>
    
    <item>
      <title>Tensorflow: Kaggle Spooky Authors Bag of Words Model</title>
      <link>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</link>
      <pubDate>Mon, 29 Jan 2018 06:51:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2018/01/29/tensorflow-kaggle-spooky-authors-bag-words-model/</guid>
      <description>from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import pandas as pd import tensorflow as tf from sklearn import preprocessing from sklearn.model_selection import train_test_split EMBEDDING_SIZE = 50 MAX_LABEL = 3 WORDS_FEATURE = &amp;#39;words&amp;#39; # Name of the input words feature. def bag_of_words_model(features, labels, mode): bow_column = tf.feature_column.categorical_column_with_identity(WORDS_FEATURE, num_buckets=n_words) bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=EMBEDDING_SIZE) bow = tf.feature_column.input_layer(features, feature_columns=[bow_embedding_column]) logits = tf.layers.dense(bow, MAX_LABEL, activation=None) return create_estimator_spec(logits=logits, labels=labels, mode=mode) def create_estimator_spec(logits, labels, mode): predicted_classes = tf.</description>
    </item>
    
    <item>
      <title>scikit-learn: First steps with log_loss</title>
      <link>https://markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</link>
      <pubDate>Wed, 14 Sep 2016 05:33:38 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</guid>
      <description>&amp;gt;&amp;gt;&amp;gt; actual_labels = [&amp;#34;bam&amp;#34;, &amp;#34;ham&amp;#34;, &amp;#34;spam&amp;#34;] &amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import log_loss &amp;gt;&amp;gt;&amp;gt; log_loss(actual_labels, [[1, 0, 0], [0, 1, 0], [0, 0, 1]]) 2.1094237467877998e-15 &amp;gt;&amp;gt;&amp;gt; log_loss(actual_labels, [[0, 0, 1], [1, 0, 0], [0, 1, 0]]) 34.538776394910684 This means that the predicted probability for that given class would be less than exp(-1) or around 0.368.
So, seeing a log loss greater than one can be expected in the cass that that your model only gives less than a 36% probability estimate for the correct class.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).
Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.
from sklearn.metrics.pairwise import cosine_similarity import numpy as np def distances(a, b): return np.linalg.norm(a-b), cosine_similarity([a, b])[0][1] def mixed(n_zeros, n_ones): return np.concatenate((np.repeat([1], n_ones), np.repeat([0], n_zeros)), axis=0) def ones(n_ones): return np.repeat([1], n_ones) print distances(mixed(2, 2), ones(4)) print distances(mixed(3, 3), ones(6)) print distances(mixed(50, 50), ones(100)) print distances(mixed(300, 300), ones(600)) (1.</description>
    </item>
    
    <item>
      <title>scikit-learn: Trying to find clusters of Game of Thrones episodes</title>
      <link>https://markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</link>
      <pubDate>Thu, 25 Aug 2016 22:07:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</guid>
      <description>&amp;gt;&amp;gt;&amp;gt; all.shape (60, 638) &amp;gt;&amp;gt;&amp;gt; all array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) &amp;gt;&amp;gt;&amp;gt; from sklearn.cluster import KMeans &amp;gt;&amp;gt;&amp;gt; n_clusters = 3 &amp;gt;&amp;gt;&amp;gt; km = KMeans(n_clusters=n_clusters, init=&amp;#39;k-means++&amp;#39;, max_iter=100, n_init=1) &amp;gt;&amp;gt;&amp;gt; cluster_labels = km.fit_predict(all) &amp;gt;&amp;gt;&amp;gt; cluster_labels array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32) &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>:play http://guides.neo4j.com/got Episode 1 = [1, 1, 0] Episode 2 = [0, 1, 1] &amp;gt;&amp;gt;&amp;gt; from sklearn.metrics.pairwise import cosine_similarity &amp;gt;&amp;gt;&amp;gt; one = [1,1,0] &amp;gt;&amp;gt;&amp;gt; two = [0,1,1] &amp;gt;&amp;gt;&amp;gt; cosine_similarity([one, two]) array([[ 1. , 0.5], [ 0.5, 1. ]]) from neo4j.v1 import GraphDatabase, basic_auth driver = GraphDatabase.driver(&amp;#34;bolt://localhost&amp;#34;, auth=basic_auth(&amp;#34;neo4j&amp;#34;, &amp;#34;neo&amp;#34;)) session = driver.session() rows = session.run(&amp;#34;&amp;#34;&amp;#34; MATCH (c:Character), (e:Episode) OPTIONAL MATCH (c)-[appearance:APPEARED_IN]-&amp;gt;(e) RETURN e, c, appearance ORDER BY e.id, c.id&amp;#34;&amp;#34;&amp;#34;) &amp;gt;&amp;gt;&amp;gt; for row in rows: print row &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5415 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Addam Marbrand&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Addam_Marbrand&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5882 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Adrack Humble&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Adrack_Humble&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=6747 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Aegon V Targaryen&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Aegon_V_Targaryen&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5750 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Aemon&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Aemon&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5928 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Aeron Greyjoy&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Aeron_Greyjoy&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5503 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Aerys II Targaryen&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Aerys_II_Targaryen&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=6753 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Alannys Greyjoy&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Alannys_Greyjoy&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=6750 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Alerie Tyrell&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Alerie_Tyrell&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5753 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Alliser Thorne&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Alliser_Thorne&amp;#39;}&amp;gt; appearance=None&amp;gt; &amp;lt;Record e=&amp;lt;Node id=6780 labels=set([u&amp;#39;Episode&amp;#39;]) properties={u&amp;#39;season&amp;#39;: 1, u&amp;#39;number&amp;#39;: 1, u&amp;#39;id&amp;#39;: 1, u&amp;#39;title&amp;#39;: u&amp;#39;Winter Is Coming&amp;#39;}&amp;gt; c=&amp;lt;Node id=5858 labels=set([u&amp;#39;Character&amp;#39;]) properties={u&amp;#39;name&amp;#39;: u&amp;#39;Alton Lannister&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;/wiki/Alton_Lannister&amp;#39;}&amp;gt; appearance=None&amp;gt; episodes = {} for row in rows: if episodes.</description>
    </item>
    
    <item>
      <title>How I met your mother: Story arcs</title>
      <link>https://markhneedham.com/blog/2015/04/03/how-i-met-your-mother-story-arcs/</link>
      <pubDate>Fri, 03 Apr 2015 23:31:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/04/03/how-i-met-your-mother-story-arcs/</guid>
      <description>#!/bin/bash find_term() { arc=${1} searchTerm=${2} episodes=$(grep --color -iE &amp;#34;${searchTerm}&amp;#34; data/import/sentences.csv | awk -F&amp;#34;,&amp;#34; &amp;#39;{ print $2 }&amp;#39; | sort | uniq) for episode in ${episodes}; do echo ${arc},${episode} done } find_term &amp;#34;Bro Code&amp;#34; &amp;#34;bro code&amp;#34; find_term &amp;#34;Legendary&amp;#34; &amp;#34;legen(.*)ary&amp;#34; find_term &amp;#34;Slutty Pumpkin&amp;#34; &amp;#34;slutty pumpkin&amp;#34; find_term &amp;#34;Magician&amp;#39;s Code&amp;#34; &amp;#34;magician&amp;#39;s code&amp;#34; find_term &amp;#34;Thanksgiving&amp;#34; &amp;#34;thanksgiving&amp;#34; find_term &amp;#34;The Playbook&amp;#34; &amp;#34;playbook&amp;#34; find_term &amp;#34;Slap Bet&amp;#34; &amp;#34;slap bet&amp;#34; find_term &amp;#34;Wrestlers and Robots&amp;#34; &amp;#34;wrestlers&amp;#34; find_term &amp;#34;Robin Sparkles&amp;#34; &amp;#34;sparkles&amp;#34; find_term &amp;#34;Blue French Horn&amp;#34; &amp;#34;blue french horn&amp;#34; find_term &amp;#34;Olive Theory&amp;#34; &amp;#34;olive&amp;#34; find_term &amp;#34;Thank You Linus&amp;#34; &amp;#34;thank you, linus&amp;#34; find_term &amp;#34;Have you met.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>There are computational ways of searching for this, including using MALLETs hlda command, but for the reader of this tutorial, it is probably just quicker to cycle through a number of iterations (but for more see Griffiths, T. L., &amp;amp; Steyvers, M. (2004). Finding scientific topics. Proceedings of the National Academy of Science, 101, 5228-5235).
As I understand it, the idea is to try and get a uniform spread of topics -&amp;gt; documents i.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>import csv from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import NMF from collections import defaultdict from bs4 import BeautifulSoup, NavigableString from soupselect import select def uri_to_file_name(uri): return uri.replace(&amp;#34;/&amp;#34;, &amp;#34;-&amp;#34;) sessions = {} with open(&amp;#34;data/sessions.csv&amp;#34;, &amp;#34;r&amp;#34;) as sessions_file: reader = csv.reader(sessions_file, delimiter = &amp;#34;,&amp;#34;) reader.next() # header for row in reader: session_id = int(row[0]) filename = &amp;#34;data/sessions/&amp;#34; + uri_to_file_name(row[4]) page = open(filename).read() soup = BeautifulSoup(page) abstract = select(soup, &amp;#34;div.brenham-main-content p&amp;#34;) if abstract: sessions[session_id] = {&amp;#34;abstract&amp;#34; : abstract[0].</description>
    </item>
    
    <item>
      <title>Kaggle Titanic: Python pandas attempt</title>
      <link>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</link>
      <pubDate>Wed, 30 Oct 2013 07:26:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</guid>
      <description>import pandas as pd def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) def fare_in_bucket(fare, fare_bracket_size, bucket): return (fare &amp;gt; bucket * fare_bracket_size) &amp;amp; (fare &amp;lt;= ((bucket+1) * fare_bracket_size)) def build_survival_table(training_file): fare_ceiling = 40 train_df = pd.read_csv(training_file) train_df[train_df[&amp;#39;Fare&amp;#39;] &amp;gt;= 39.0] = 39.0 fare_bracket_size = 10 number_of_price_brackets = fare_ceiling / fare_bracket_size number_of_classes = 3 #There were 1st, 2nd and 3rd classes on board  survival_table = pd.DataFrame(columns=[&amp;#39;Sex&amp;#39;, &amp;#39;Pclass&amp;#39;, &amp;#39;PriceDist&amp;#39;, &amp;#39;Survived&amp;#39;, &amp;#39;NumberOfPeople&amp;#39;]) for pclass in range(1, number_of_classes + 1): # add 1 to handle 0 start for bucket in range(0, number_of_price_brackets): for sex in [&amp;#39;female&amp;#39;, &amp;#39;male&amp;#39;]: survival = train_df[(train_df[&amp;#39;Sex&amp;#39;] == sex) &amp;amp; (train_df[&amp;#39;Pclass&amp;#39;] == pclass) &amp;amp; fare_in_bucket(train_df[&amp;#34;Fare&amp;#34;], fare_bracket_size, bucket)] row = [dict(Pclass=pclass, Sex=sex, PriceDist = bucket, Survived = round(survival[&amp;#39;Survived&amp;#39;].</description>
    </item>
    
    <item>
      <title>Feature Extraction/Selection - What I&#39;ve learnt so far</title>
      <link>https://markhneedham.com/blog/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</link>
      <pubDate>Sun, 10 Feb 2013 15:42:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/02/10/feature-extractionselection-what-ive-learnt-so-far/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A feature extraction #fail</title>
      <link>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</link>
      <pubDate>Thu, 31 Jan 2013 23:24:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/31/kaggle-digit-recognizer-a-feature-extraction-fail/</guid>
      <description>I&amp;rsquo;ve written a few blog postsabout our attempts at the Kaggle Digit Recogniserproblem and one thing we haven&amp;rsquo;t yet tried is feature extraction.
Many people go straight from a data set to applying an algorithm. But there’s a huge space in between of important stuff. It’s easy to run a piece of code that predicts or classifies. That’s not the hard part. The hard part is doing it well.
One needs to conduct exploratory data analysis as I’ve emphasized; and conduct feature selection as Will Cukierski emphasized.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Finding pixels with no variance using R</title>
      <link>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</link>
      <pubDate>Tue, 08 Jan 2013 00:48:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/01/08/kaggle-digit-recognizer-finding-pixels-with-no-variance-using-r/</guid>
      <description>Many people go straight from a data set to applying an algorithm. But there’s a huge space in between of important stuff. It’s easy to run a piece of code that predicts or classifies. That’s not the hard part. The hard part is doing it well.
initial &amp;lt;- read.csv(&amp;#34;train.csv&amp;#34;, nrows=10000, header = TRUE) # take a sample of 1000 rows of the input  sampleSet &amp;lt;- initial[sample(1:nrow(initial), 1000), ] # get all the labels sampleSet.</description>
    </item>
    
    <item>
      <title>Mahout: Parallelising the creation of DecisionTrees</title>
      <link>https://markhneedham.com/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</link>
      <pubDate>Thu, 27 Dec 2012 00:08:01 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/12/27/mahout-parallelising-the-creation-of-decisiontrees/</guid>
      <description>List&amp;lt;Node&amp;gt; trees = new ArrayList&amp;lt;Node&amp;gt;(); MultiDecisionForest forest = MultiDecisionForest.load(new Configuration(), new Path(&amp;#34;/path/to/mahout-tree&amp;#34;)); trees.addAll(forest.getTrees()); MultiDecisionForest forest = new MultiDecisionForest(trees); deb http://ppa.launchpad.net/ieltonf/ppa/ubuntu oneiric main deb-src http://ppa.launchpad.net/ieltonf/ppa/ubuntu oneiric main parallelise-forests.sh#!/bin/bash start=`date` startTime=`date &amp;#39;+%s&amp;#39;` numberOfRuns=$1 seq 1 ${numberOfRuns} | parallel -P 8 &amp;#34;./build-forest.sh&amp;#34; end=`date` endTime=`date &amp;#39;+%s&amp;#39;` echo &amp;#34;Started: ${start}&amp;#34; echo &amp;#34;Finished: ${end}&amp;#34; echo &amp;#34;Took: &amp;#34; $(expr $endTime - $startTime) build-forest.sh#!/bin/bash java -Xmx1024m -cp target/machinenursery-1.0.0-SNAPSHOT-standalone.jar main.java.MahoutPlaybox </description>
    </item>
    
    <item>
      <title>Weka: Saving and loading classifiers</title>
      <link>https://markhneedham.com/blog/2012/12/12/weka-saving-and-loading-classifiers/</link>
      <pubDate>Wed, 12 Dec 2012 00:04:42 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/12/12/weka-saving-and-loading-classifiers/</guid>
      <description>MultilayerPerceptron classifier = new MultilayerPerceptron(); classifier.buildClassifier(instances); // instances gets passed in from elsewhere  Debug.saveToFile(&amp;#34;/path/to/weka-neural-network&amp;#34;, classifier); SerializedClassifier classifier = new SerializedClassifier(); classifier.setModelFile(new File(&amp;#34;/path/to/weka-neural-network&amp;#34;)); </description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Weka AdaBoost attempt</title>
      <link>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</link>
      <pubDate>Thu, 29 Nov 2012 17:09:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/29/kaggle-digit-recognizer-weka-adaboost-attempt/</guid>
      <description>In our latest attempt at Kaggle&amp;rsquo;s Digit RecognizerJenand I decided to try out boostingon our random forest algorithm, an approach that Jen had come across in a talk at the Clojure Conj.
We couldn&amp;rsquo;t find any documentation that it was possible to apply boosting to Mahout&amp;rsquo;s random forest algorithm but we knew it was possible with Wekaso we decided to use that instead!
As I understand it the way that boosting works in the context of random forests is that each of the trees in the forest will be assigned a weight based on how accurately it&amp;rsquo;s able to classify the data set and these weights are then used in the voting stage.</description>
    </item>
    
    <item>
      <title>A first failed attempt at Natural Language Processing</title>
      <link>https://markhneedham.com/blog/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</link>
      <pubDate>Sat, 24 Nov 2012 19:43:32 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/24/a-first-failed-attempt-at-natural-language-processing/</guid>
      <description>One of the things I find fascinating about dating websites is that the profiles of people are almost identical so I thought it would be an interesting exercise to grab some of the free text that people write about themselves and prove the similarity.
I&amp;rsquo;d been talking to Matt Biddulphabout some Natural Language Processing (NLP) stuff he&amp;rsquo;d been working on and he wrote up a bunch of libraries, articles and books that he&amp;rsquo;d found useful.</description>
    </item>
    
    <item>
      <title>Learning: Switching between theory and practice</title>
      <link>https://markhneedham.com/blog/2012/11/19/learning-switching-between-theory-and-practice/</link>
      <pubDate>Mon, 19 Nov 2012 13:31:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/11/19/learning-switching-between-theory-and-practice/</guid>
      <description>In one of my first ever blog posts I wrote about the differences I&amp;rsquo;d experienced in learning the theory about a topic and then seeing it in practice.
The way I remember learning at school and university was that you learn all the theory first and then put it into practice but I typically don&amp;rsquo;t find myself doing this whenever I learn something new.
I spent a bit of time over the weekend learning more about neural networks as my colleague Jen Smithsuggested this might be a more effective technique for getting a higher accuracy scoreon the Kaggle Digit Recogniserproblem.</description>
    </item>
    
    <item>
      <title>Mahout: Using a saved Random Forest/DecisionTree</title>
      <link>https://markhneedham.com/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</link>
      <pubDate>Sat, 27 Oct 2012 22:03:30 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/27/mahout-using-a-saved-random-forestdecisiontree/</guid>
      <description>One of the things that I wanted to do while playing around with random forestsusing Mahoutwas to save the random forest and then use use it again which is something Mahout does cater for.
It was actually much easier to do this than I&amp;rsquo;d expected and assuming that we already have a DecisionForestbuilt we&amp;rsquo;d just need the following code to save it to disc:
int numberOfTrees = 1; Data data = loadData(.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: Mahout Random Forest attempt</title>
      <link>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 20:24:48 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-mahout-random-forest-attempt/</guid>
      <description>I&amp;rsquo;ve written previously about the K-meansapproachthat Jenand I took when trying to solve Kaggle&amp;rsquo;s Digit Recognizerand having stalled at about 80% accuracy we decided to try one of the algorithms suggested in the tutorials section- the random forest!
We initially used a clojure random forests librarybut struggled to build the random forest from the training set data in a reasonable amount of time so we switched to Mahout&amp;rsquo;s versionwhich is based on Leo Breiman&amp;rsquo;s random forestspaper.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: K-means optimisation attempt</title>
      <link>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</link>
      <pubDate>Sat, 27 Oct 2012 12:27:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/27/kaggle-digit-recognizer-k-means-optimisation-attempt/</guid>
      <description>I recently wrote a blog post explaining how Jenand I used the K-means algorithmto classify digits in Kaggle&amp;rsquo;s Digit Recognizer problemand one of the things we&amp;rsquo;d read was that with this algorithm you often end up with situations where it&amp;rsquo;s difficult to classify a new item because if falls between two labels.
We decided to have a look at the output of our classifier function to see whether or not that was the case.</description>
    </item>
    
    <item>
      <title>Kaggle Digit Recognizer: A K-means attempt</title>
      <link>https://markhneedham.com/blog/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</link>
      <pubDate>Tue, 23 Oct 2012 19:04:20 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/23/kaggle-digit-recognizer-a-k-means-attempt/</guid>
      <description>Over the past couple of months Jen and I have been playing around with the Kaggle Digit Recognizer problem- a &amp;lsquo;competition&amp;rsquo; created to introduce people to Machine Learning.
You are given an input file which contains multiple rows each containing 784 pixel values representing a 28x28 pixel image as well as a label indicating which number that image actually represents.
One of the algorithms that we tried out for this problem was a variation on the k-means clusteringone whereby we took the values at each pixel location for each of the labels and came up with an average value for each pixel.</description>
    </item>
    
  </channel>
</rss>