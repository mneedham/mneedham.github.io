<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Mark Needham</title>
    <link>https://markhneedham.com/blog/category/data-science/</link>
    <description>Recent content in Data Science on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jul 2017 21:41:55 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/category/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pandas: ValueError: The truth value of a Series is ambiguous.</title>
      <link>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Wed, 26 Jul 2017 21:41:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>import pandas as pd &amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame({&amp;#34;a&amp;#34;: [1,2,3,4,5], &amp;#34;b&amp;#34;: [2,3,4,5,6]}) &amp;gt;&amp;gt;&amp;gt; df a b 0 5 2 1 6 6 2 0 8 3 3 2 4 1 6 &amp;gt;&amp;gt;&amp;gt; divmod(df[&amp;#34;a&amp;#34;], 2)[1] &amp;gt; 0 0 True 1 False 2 True 3 False 4 True Name: a, dtype: bool &amp;gt;&amp;gt;&amp;gt; divmod(df[&amp;#34;b&amp;#34;], 2)[1] &amp;gt; 0 0 False 1 True 2 False 3 True 4 False Name: b, dtype: bool &amp;gt;&amp;gt;&amp;gt; df[&amp;#34;anyOdd&amp;#34;] = (divmod(df[&amp;#34;a&amp;#34;], 2)[1] &amp;gt; 0) or (divmod(df[&amp;#34;b&amp;#34;], 2)[1] &amp;gt; 0) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/Users/markneedham/projects/kaggle/house-prices/a/lib/python3.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # We&amp;#39;ll use this library to make the display pretty from tabulate import tabulate train = pd.read_csv(&amp;#39;train.csv&amp;#39;) # the model can only handle numeric values so filter out the rest data = train.select_dtypes(include=[np.number]).interpolate().dropna() y = train.SalePrice X = data.drop([&amp;#34;SalePrice&amp;#34;, &amp;#34;Id&amp;#34;], axis=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33) clf = RandomForestRegressor(n_jobs=2, n_estimators=1000) model = clf.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>import pandas as pd train = pd.read_csv(&amp;#39;train.csv&amp;#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64 cols = [col for col in train.columns if col.startswith(&amp;#34;Lot&amp;#34;)] missing_frontage = train[cols][train[&amp;#34;LotFrontage&amp;#34;].isnull()] &amp;gt;&amp;gt;&amp;gt; print(missing_frontage.head()) LotFrontage LotArea LotShape LotConfig 7 NaN 10382 IR1 Corner 12 NaN 12968 IR2 Inside 14 NaN 10920 IR1 Corner 16 NaN 11241 IR1 CulDSac 24 NaN 8246 IR1 Inside sub_train = train[train.</description>
    </item>
    
    <item>
      <title>Exploring (potential) data entry errors in the Land Registry data set</title>
      <link>https://markhneedham.com/blog/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</link>
      <pubDate>Sun, 18 Oct 2015 10:03:57 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/10/18/exploring-potential-data-entry-errors-in-the-land-registry-data-set/</guid>
      <description>&amp;gt; library(data.table) &amp;gt; dt = fread(&amp;#34;pp-complete.csv&amp;#34;, header = FALSE) &amp;gt; dt[1:5] V1 V2 V3 V4 V5 1: {0C7ADEF5-878D-4066-B785-0000003ED74A} 163000 2003-02-21 00:00 UB5 4PJ T 2: {35F67271-ABD4-40DA-AB09-00000085B9D3} 247500 2005-07-15 00:00 TA19 9DD D 3: {B20B1C74-E8E1-4137-AB3E-0000011DF342} 320000 2010-09-10 00:00 W4 1DZ F 4: {7D6B0915-C56B-4275-AF9B-00000156BCE7} 104000 1997-08-27 00:00 NE61 2BH D 5: {47B60101-B64C-413D-8F60-000002F1692D} 147995 2003-05-02 00:00 PE33 0RU D V6 V7 V8 V9 V10 V11 V12 1: N F 106 READING ROAD NORTHOLT NORTHOLT 2: N F 58 ADAMS MEADOW ILMINSTER ILMINSTER 3: N L 58 WHELLOCK ROAD LONDON 4: N F 17 WESTGATE MORPETH MORPETH 5: N F 4 MASON GARDENS WEST WINCH KING&amp;#39;S LYNN V13 V14 V15 1: EALING GREATER LONDON A 2: SOUTH SOMERSET SOMERSET A 3: EALING GREATER LONDON A 4: CASTLE MORPETH NORTHUMBERLAND A 5: KING&amp;#39;S LYNN AND WEST NORFOLK NORFOLK A &amp;gt; dt = dt[, V2:= as.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn - Training a classifier with non numeric features</title>
      <link>https://markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</link>
      <pubDate>Mon, 02 Mar 2015 07:48:24 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</guid>
      <description>import json import nltk import collections from himymutil.ml import pos_features from sklearn import tree from sklearn.cross_validation import train_test_split with open(&amp;#34;data/import/trained_sentences.json&amp;#34;, &amp;#34;r&amp;#34;) as json_file: json_data = json.load(json_file) tagged_sents = [] for sentence in json_data: tagged_sents.append([(word[&amp;#34;word&amp;#34;], word[&amp;#34;speaker&amp;#34;]) for word in sentence[&amp;#34;words&amp;#34;]]) featuresets = [] for tagged_sent in tagged_sents: untagged_sent = nltk.tag.untag(tagged_sent) sentence_pos = nltk.pos_tag(untagged_sent) for i, (word, tag) in enumerate(tagged_sent): featuresets.append((pos_features(untagged_sent, sentence_pos, i), tag) ) clf = tree.DecisionTreeClassifier() train_data, test_data = train_test_split(featuresets, test_size=0.</description>
    </item>
    
    <item>
      <title>Data Science: Mo&#39; Data Mo&#39; Problems</title>
      <link>https://markhneedham.com/blog/2014/06/28/data-science-mo-data-mo-problems/</link>
      <pubDate>Sat, 28 Jun 2014 23:35:25 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2014/06/28/data-science-mo-data-mo-problems/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Science: Don&#39;t build a crawler (if you can avoid it!)</title>
      <link>https://markhneedham.com/blog/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</link>
      <pubDate>Thu, 19 Sep 2013 06:55:19 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/09/19/data-science-dont-build-a-crawler-if-you-can-avoid-it/</guid>
      <description>$ head -n 5 uris.txt https://www.some-made-up-place.com/page1.html https://www.some-made-up-place.com/page2.html https://www.some-made-up-place.com/page3.html https://www.some-made-up-place.com/page4.html https://www.some-made-up-place.com/page5.html $ cat uris.txt | time xargs wget ... Total wall clock time: 3.7s Downloaded: 60 files, 625K in 0.7s (870 KB/s) 3.73 real 0.03 user 0.09 sys cat uris.txt | time xargs -n1 -P10 wget 1.65 real 0.20 user 0.21 sys </description>
    </item>
    
    <item>
      <title>Micro Services Style Data Work Flow</title>
      <link>https://markhneedham.com/blog/2013/02/18/micro-services-style-data-work-flow/</link>
      <pubDate>Mon, 18 Feb 2013 22:16:39 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/02/18/micro-services-style-data-work-flow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Science: Don&#39;t filter data prematurely</title>
      <link>https://markhneedham.com/blog/2013/02/17/data-science-dont-filter-data-prematurely/</link>
      <pubDate>Sun, 17 Feb 2013 20:02:31 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2013/02/17/data-science-dont-filter-data-prematurely/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Science: Discovery work</title>
      <link>https://markhneedham.com/blog/2012/12/09/data-science-discovery-work/</link>
      <pubDate>Sun, 09 Dec 2012 10:36:39 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/12/09/data-science-discovery-work/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nygard Big Data Model: The Investigation Stage</title>
      <link>https://markhneedham.com/blog/2012/10/10/nygard-big-data-model-the-investigation-stage/</link>
      <pubDate>Wed, 10 Oct 2012 00:00:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/10/nygard-big-data-model-the-investigation-stage/</guid>
      <description>Earlier this year Michael Nygard wrote an extremely detailed post about his experiences in the world of big data projectsand included in the post was the following diagram which I&amp;rsquo;ve found very useful.
Ashokand I have been doing some work in this area helping one of our clients make sense of and visualise some of their data and we realised retrospectively that we were very acting very much in the investigation stage of the model.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 2 Wrap Up</title>
      <link>https://markhneedham.com/blog/2012/10/03/strata-conf-london-day-2-wrap-up/</link>
      <pubDate>Wed, 03 Oct 2012 06:46:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/03/strata-conf-london-day-2-wrap-up/</guid>
      <description>Yesterday I attended the second day of Strata Conf Londonand these are the some of the things I learned from the talks I attended:
British Rail were trying to solve a graph problem when people didn&amp;rsquo;t know about graphs and Dijkstra&amp;rsquo;s algorithm hadn&amp;rsquo;t been inventedand it was effectively invented on this project but never publicised. John&amp;rsquo;s suggestion here was that we need to share the stuff that we&amp;rsquo;re doing so that people don&amp;rsquo;t re-invent the wheel.</description>
    </item>
    
    <item>
      <title>Strata Conf London: Day 1 Wrap Up</title>
      <link>https://markhneedham.com/blog/2012/10/02/strata-conf-london-day-1-wrap-up/</link>
      <pubDate>Tue, 02 Oct 2012 23:42:58 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/10/02/strata-conf-london-day-1-wrap-up/</guid>
      <description>For the past couple of days I attended the first Strata Conf to be held in London- a conference which seems to bring together people from the data science and big data worldsto talk about the stuff they&amp;rsquo;re doing.
Since I&amp;rsquo;ve been playing around with a couple of different things in this area over the last 4/5 months I thought it&amp;rsquo;d be interesting to come along and see what people much more experienced in this area had to say!</description>
    </item>
    
    <item>
      <title>Data Science: Making sense of the data</title>
      <link>https://markhneedham.com/blog/2012/09/30/data-science-making-sense-of-the-data/</link>
      <pubDate>Sun, 30 Sep 2012 14:58:11 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/30/data-science-making-sense-of-the-data/</guid>
      <description>Over the past month or so Ashokand I have been helping one of our clients explore and visualise some of their data and one of the first things we needed to do was make sense of the data that was available.
Ashok suggested that we work with a subset of our eventual data setso that we could get a feel for the data and quickly see whether what we were planning to do made sense.</description>
    </item>
    
    <item>
      <title>Data Science: Scrapping the data together</title>
      <link>https://markhneedham.com/blog/2012/09/30/data-science-scrapping-the-data-together/</link>
      <pubDate>Sun, 30 Sep 2012 13:44:18 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2012/09/30/data-science-scrapping-the-data-together/</guid>
      <description>On Friday Martin, Darrenand I were discussing the ThoughtWorks graph that I was working on earlier in the year and Martin pointed out that an interesting aspect of this type of work is that the data you want to work with isn&amp;rsquo;t easily available.
You therefore need to find a way to scrap the data together to make some headway and then maybe at a later stage once some progress has been made it will become easier to replace that with a cleaner solution.</description>
    </item>
    
  </channel>
</rss>