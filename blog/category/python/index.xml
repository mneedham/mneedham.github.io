<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/category/python/</link>
    <description>Recent content in Python on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 May 2021 00:44:37 +0000</lastBuildDate>
    
	<atom:link href="https://www.markhneedham.com/blog/category/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pandas: Add row to DataFrame</title>
      <link>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</link>
      <pubDate>Thu, 13 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</guid>
      <description>Usually when I&amp;#8217;m working with Pandas DataFrames I want to add new columns of data, but I recently wanted to add a row to an existing DataFrame. It turns out there are more than one ways to do that, which we&amp;#8217;ll explore in this blog post.
 Let&amp;#8217;s start by importing Pandas into our Python script:
 import pandas as pd   We&amp;#8217;ll start from a DataFrame that has two rows and the columns name and age:</description>
    </item>
    
    <item>
      <title>Altair/Pandas: TypeError: Cannot interpret &#39;Float64Dtype()&#39; as a data type</title>
      <link>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</link>
      <pubDate>Wed, 28 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</guid>
      <description>I ran into an interesting problem when trying to use Altair to visualise a Pandas DataFrame containing vaccination rates of different parts of England. In this blog post we&amp;#8217;ll look at how to work around this issue.
 First, let&amp;#8217;s install Pandas, numpy, and altair:
 pip install pandas altair numpy   And now we&amp;#8217;ll import those modules into a Python script or Jupyter notebook:
 import pandas as pd import altair as alt import numpy as np   Next, we&amp;#8217;ll create a DataFrame containing the vaccinations rates of a couple of regions:</description>
    </item>
    
    <item>
      <title>Pandas: Compare values in DataFrame to previous days</title>
      <link>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</link>
      <pubDate>Wed, 21 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</guid>
      <description>I&amp;#8217;m still playing around with Covid vaccine data, this time exploring how the number of doses varies week by week. I want to know how many more (or less) vaccines have been done on a given day compared to that same day last week.
 We&amp;#8217;ll be using Pandas in this blog post, so let&amp;#8217;s first install that library and import it:
 Install Pandas pip install pandas   Import module import pandas as pd   And now let&amp;#8217;s create a DataFrame containing a subset of the data that I&amp;#8217;m working with:</description>
    </item>
    
    <item>
      <title>Vaccinating England: The Data (cleanup)</title>
      <link>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</link>
      <pubDate>Sat, 17 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</guid>
      <description>Over the last 13 months I&amp;#8217;ve spent countless hours looking at dashboards that showed Coronavirus infection rates, death rates, and numbers of people vaccinated. The UK government host a dashboard at coronavirus.data.gov.uk, which contains charts and tables showing all of the above.
 One thing I haven&amp;#8217;t been able to find, however, is a drill down of vaccinations by local area and age group. So I&amp;#8217;m going to try to build my own!</description>
    </item>
    
    <item>
      <title>Pandas - Format DataFrame numbers with commas and control decimal places</title>
      <link>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</link>
      <pubDate>Sun, 11 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</guid>
      <description>I&amp;#8217;m still playing around with the UK&amp;#8217;s COVID-19 vaccination data and in this blog post we&amp;#8217;ll learn how to format a DataFrame that contains a mix of string and numeric values.
 We&amp;#8217;ll be using Pandas&#39; styling functionality, which generates CSS and HTML, so if you want to follow along you&amp;#8217;ll need to install Pandas and Jupyter:
 pip install pandas jupyter   Next, launch Jupyter and create a notebook:</description>
    </item>
    
    <item>
      <title>Pandas - Dividing two DataFrames (TypeError: unsupported operand type(s) for /: &#39;str&#39; and &#39;str&#39;)</title>
      <link>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</link>
      <pubDate>Thu, 08 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</guid>
      <description>I&amp;#8217;ve been doing some more exploration of the UK Coronavirus vaccine data, this time looking at the number of people vaccinated by Local Tier Local Authority. The government publish data showing the number of people vaccinated in each authority by age group, as well as population estimates for each cohort.
 Having loaded that data into two Pandas DataFrames, I wanted to work out the % of people vaccinated per age group per local area.</description>
    </item>
    
    <item>
      <title>Altair - Remove margin/padding on discrete X axis</title>
      <link>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</link>
      <pubDate>Fri, 02 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</guid>
      <description>One of the Altair charts on my Covid Vaccine Dashboards Streamlit app shows the % of first doses, but when I first created it there was some padding on the X axis that I wanted to remove. In this blog post we&amp;#8217;ll learn how to do that.
 Pre requisites Let&amp;#8217;s start by installing the following libraries:
 pip install pandas altair altair_viewer   Next let&amp;#8217;s import them, as shown below:</description>
    </item>
    
    <item>
      <title>Pandas: Filter column value in array/list - ValueError: The truth value of a Series is ambiguous</title>
      <link>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</link>
      <pubDate>Sun, 28 Mar 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</guid>
      <description>The UK government publishes Coronavirus vaccinations data on coronavirus.data.gov.uk, but I wanted to create some different visualisations so I downloaded the data and have been playing with it in the mneedham/covid-vaccines GitHub repository.
 I massaged the data so that I have rows in a Pandas DataFrame representing the numbers of first doses, second doses, and total doses done each day. I then wanted to filter this DataFrame based on the type of dose, but initially got a bit stuck.</description>
    </item>
    
    <item>
      <title>pipenv: ImportError: No module named &#39;virtualenv.seed.via_app_data&#39;</title>
      <link>https://www.markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</link>
      <pubDate>Fri, 07 Aug 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/08/07/pipenv-import-file-no-module-named-virtualenv/</guid>
      <description>I&amp;#8217;ve been trying to install pipenv on a new computer and ran into a frustrating issue. After installing pipenv using pip, I tried to run the command below:
 $ /home/markhneedham/.local/bin/pipenv shell Creating a virtualenv for this project… Pipfile: /tmp/Pipfile Using /usr/bin/python3.8 (3.8.2) to create virtualenv… ⠙ Creating virtual environment...ModuleNotFoundError: No module named &#39;virtualenv.seed.via_app_data&#39; ✘ Failed creating virtual environment [pipenv.exceptions.VirtualenvCreationException]: Failed to create virtual environment.   Hmmm, for some reason it&amp;#8217;s unable to find one of the virtualenv modules.</description>
    </item>
    
    <item>
      <title>Python: Select keys from map/dictionary</title>
      <link>https://www.markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</link>
      <pubDate>Mon, 27 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/04/27/python-select-keys-from-map-dictionary/</guid>
      <description>In this post we&amp;#8217;re going to learn how to filter a Python map/dictionary to return a subset of keys or values. I needed to do this recently while logging some maps that had a lot of keys that I wasn&amp;#8217;t interested in.
 We&amp;#8217;ll start with the following map:
 x = {&#34;a&#34;: 1, &#34;b&#34;: 2, &#34;c&#34;: 3, &#34;d&#34;: 4} {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4}   We want to filter this map so that we only have the keys a and c.</description>
    </item>
    
    <item>
      <title>Python: Find the starting Sunday for all the weeks in a month</title>
      <link>https://www.markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</link>
      <pubDate>Sat, 18 Apr 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/04/18/python-starting-sundays-in-a-month/</guid>
      <description>In this post we&amp;#8217;re going to learn how to find the dates of all the Sundays in a given month, as well as the Sunday immediately preceding the 1st day in the month, assuming that day isn&amp;#8217;t a Sunday.
 Let&amp;#8217;s start by importing some libraries that we&amp;#8217;re going to use in this blog post:
 from dateutil import parser import datetime import calendar   Next we need to find the first day of the current month, which we can do with the following code:</description>
    </item>
    
    <item>
      <title>Python: Altair - Setting the range of Date values for an axis</title>
      <link>https://www.markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</link>
      <pubDate>Tue, 14 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/14/altair-range-values-dates-axis/</guid>
      <description>In my continued experiments with the Altair visualisation library, I wanted to set a custom range of data values on the x axis of a chart. In this blog post we&amp;#8217;ll learn how to do that.
 We&amp;#8217;ll start where we left off in the last blog post, with the following code that renders a scatterplot containing the chart position of a song on a certain date:
 import altair as alt import pandas as pd import datetime df = pd.</description>
    </item>
    
    <item>
      <title>Python: Altair - TypeError: Object of type date is not JSON serializable</title>
      <link>https://www.markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</link>
      <pubDate>Fri, 10 Jan 2020 00:21:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2020/01/10/altair-typeerror-object-type-date-not-json-serializable/</guid>
      <description>I&amp;#8217;ve been playing with the Altair statistical visualisation library and recently ran into an error while trying to render a DataFrame that contained dates.
 I was trying to render a scatterplot containing the chart position of a song on a certain date, as seen in the code below:
 # pip install altair pandas import altair as alt import pandas as pd import datetime df = pd.DataFrame( [ {&#34;position&#34;: 2, &#34;</description>
    </item>
    
    <item>
      <title>Python: Click - Handling Date Parameter</title>
      <link>https://www.markhneedham.com/blog/2019/07/29/python-click-date-parameter-type/</link>
      <pubDate>Mon, 29 Jul 2019 11:08:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/07/29/python-click-date-parameter-type/</guid>
      <description>I&amp;#8217;ve been building a little CLI application using the Python Click Library, and I wanted to pass in a Date as a parameter. There&amp;#8217;s more than one way to do this.
 Let&amp;#8217;s first install the Click library:
 pip install click   And now we&amp;#8217;ll import our required libraries:
 from datetime import date import click   Now we&amp;#8217;ll create a sub command that takes two parameters: date-start and date-end.</description>
    </item>
    
    <item>
      <title>Jupyter: RuntimeError: This event loop is already running</title>
      <link>https://www.markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</link>
      <pubDate>Fri, 10 May 2019 23:00:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</guid>
      <description>I&amp;#8217;ve been using the twint library to explore the Neo4j twitter community, and ran into an initially confusing error when I moved the code I&amp;#8217;d written into a Jupyter notebook.
 The first three cells of my notebook contain the following code:
 Cell 1:
 ! pip install twint   Cell 2:
 import json import twint   Cell 3:
 users = [&#34;vikatakavi11&#34;, &#34;tee_mars3&#34;] for username in users[:10]: c = twint.</description>
    </item>
    
    <item>
      <title>Python: Getting GitHub download count from the GraphQL API using requests</title>
      <link>https://www.markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</link>
      <pubDate>Sun, 07 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</guid>
      <description>I was recently trying to use some code I shared just over a year ago to compute GitHub Project download numbers from the GraphQL API, and wanted to automate this in a Python script.
 It was more fiddly than I expected, so I thought I&amp;#8217;d share the code for the benefit of future me more than anything else!
 Pre requisites We&amp;#8217;re going to use the popular requests library to query the API, so we need to import that.</description>
    </item>
    
    <item>
      <title>Finding famous MPs based on their Wikipedia Page Views</title>
      <link>https://www.markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</link>
      <pubDate>Mon, 01 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</guid>
      <description>As part of the Graphing Brexit series of blog posts, I wanted to work out who were the most important Members of the UK parliament, and after a bit of Googling I realised that views of their Wikipedia pages would do the trick.
 I initially found my way to tools.wmflabs.org, which is great for exploring the popularity of an individual MP, but not so good if you want to extract the data for 600 of them.</description>
    </item>
    
    <item>
      <title>Python: Add query parameters to a URL</title>
      <link>https://www.markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</link>
      <pubDate>Fri, 11 Jan 2019 09:42:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</guid>
      <description>I was recently trying to automate adding a query parameter to a bunch of URLS and came across a neat approach a long way down this StackOverflow answer, that uses the PreparedRequest class from the requests library.
 Let&amp;#8217;s first get the class imported:
 from requests.models import PreparedRequest req = PreparedRequest()   And now let&amp;#8217;s use use this class to add a query parameter to a URL. We can do this with the following code:</description>
    </item>
    
    <item>
      <title>Python: Pandas - DataFrame plotting ignoring figure</title>
      <link>https://www.markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</link>
      <pubDate>Tue, 25 Dec 2018 21:09:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/25/python-pandas-dataframe-plot-figure/</guid>
      <description>In my continued use of matplotlib I wanted to change the size of the chart I was plotting and struggled a bit to start with. We&amp;#8217;ll use the same DataFrame as before:
 df = pd.DataFrame({ &#34;name&#34;: [&#34;Mark&#34;, &#34;Arya&#34;, &#34;Praveena&#34;], &#34;age&#34;: [34, 1, 31] }) df   In my last blog post I showed how we can create a bar chart by executing the following code:
 df.plot.bar(x=&#34;name&#34;) plt.tight_layout() plt.</description>
    </item>
    
    <item>
      <title>Pandas: Create matplotlib plot with x-axis label not index</title>
      <link>https://www.markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</link>
      <pubDate>Fri, 21 Dec 2018 16:57:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/21/pandas-plot-x-axis-index/</guid>
      <description>I&amp;#8217;ve been using matplotlib a bit recently, and wanted to share a lesson I learnt about choosing the label of the x-axis. Let&amp;#8217;s first import the libraries we&amp;#8217;ll use in this post:
 import pandas as pd import matplotlib.pyplot as plt   And now we&amp;#8217;ll create a DataFrame of values that we want to chart:
 df = pd.DataFrame({ &#34;name&#34;: [&#34;Mark&#34;, &#34;Arya&#34;, &#34;Praveena&#34;], &#34;age&#34;: [34, 1, 31] }) df   This is what our DataFrame looks like:</description>
    </item>
    
    <item>
      <title>PySpark: Creating DataFrame with one column - TypeError: Can not infer schema for type: &lt;type &#39;int&#39;&gt;</title>
      <link>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</link>
      <pubDate>Sun, 09 Dec 2018 10:25:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/12/09/pyspark-creating-dataframe-one-column/</guid>
      <description>I&amp;#8217;ve been playing with PySpark recently, and wanted to create a DataFrame containing only one column. I tried to do this by writing the following code:
 spark.createDataFrame([(1)], [&#34;count&#34;])   If we run that code we&amp;#8217;ll get the following error message:
 Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; File &#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&#34;, line 748, in createDataFrame rdd, schema = self._createFromLocal(map(prepare, data), schema) File &#34;/home/markhneedham/projects/graph-algorithms/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py&#34;, line 416, in _createFromLocal struct = self.</description>
    </item>
    
    <item>
      <title>Neo4j Graph Algorithms: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2018/09/28/neo4j-graph-algorithms-cosine-game-of-thrones/</link>
      <pubDate>Fri, 28 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/28/neo4j-graph-algorithms-cosine-game-of-thrones/</guid>
      <description>A couple of years ago I wrote a blog post showing how to calculate cosine similarity on Game of Thrones episodes using scikit-learn, and with the release of Similarity Algorithms in the Neo4j Graph Algorithms library I thought it was a good time to revisit that post.
 The dataset contains characters and episodes, and we want to calculate episode similarity based on the characters that appear in each episode. Before we run any algorithms we need to get the data into Neo4j.</description>
    </item>
    
    <item>
      <title>matplotlib - Create a histogram/bar chart for ratings/full numbers</title>
      <link>https://www.markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</link>
      <pubDate>Mon, 24 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/24/matplotlib-histogram-bar-chart-ratings-full-values/</guid>
      <description>In my continued work with matplotlib I wanted to plot a histogram (or bar chart) for a bunch of star ratings to see how they were distributed.
 Before we do anything let&amp;#8217;s import matplotlib as well as pandas:
 import random import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.pyplot as plt plt.style.use(&#39;fivethirtyeight&#39;)   Next we&amp;#8217;ll create an array of randomly chosen star ratings between 1 and 5:</description>
    </item>
    
    <item>
      <title>matplotlib - MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.</title>
      <link>https://www.markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</link>
      <pubDate>Tue, 18 Sep 2018 07:56:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/18/matplotlib-matplotlib-deprecation-adding-axes/</guid>
      <description>In my last post I showed how to remove axes legends from a matplotlib chart, and while writing the post I actually had the change the code I used as my initial approach is now deprecated.
 As in the previous post, we&amp;#8217;ll first import pandas and matplotlib:
 import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.pyplot as plt plt.style.use(&#39;fivethirtyeight&#39;)   And we&amp;#8217;ll still use this DataFrame:
 df = pd.</description>
    </item>
    
    <item>
      <title>matplotlib - Remove axis legend</title>
      <link>https://www.markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</link>
      <pubDate>Tue, 18 Sep 2018 07:55:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/09/18/matplotlib-remove-axis-legend/</guid>
      <description>I&amp;#8217;ve been working with matplotlib a bit recently, and I wanted to remove all axis legends from my chart. It took me a bit longer than I expected to figure it out so I thought I&amp;#8217;d write it up.
 Before we do anything let&amp;#8217;s import matplotlib as well as pandas, since we&amp;#8217;re going to plot data from a pandas DataFrame.
 import pandas as pd import matplotlib matplotlib.use(&#39;TkAgg&#39;) import matplotlib.</description>
    </item>
    
    <item>
      <title>QuickGraph #1: Analysing Python Dependency Graph with PageRank, Closeness Centrality, and Betweenness Centrality</title>
      <link>https://www.markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</link>
      <pubDate>Mon, 16 Jul 2018 05:25:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/07/16/quick-graph-python-dependency-graph/</guid>
      <description>I&amp;#8217;ve always wanted to build a dependency graph of libraries in the Python ecosytem but I never quite got around to it&amp;#8230;&amp;#8203;until now! I thought I might be able to get a dump of all the libraries and their dependencies, but while searching I came across this article which does a good job of explaining why that&amp;#8217;s not possible.
 Finding Python Dependencies The best we can do is generate a dependency graph of our locally installed packages using the excellent pipdeptree tool.</description>
    </item>
    
    <item>
      <title>Python: Parallel download files using requests</title>
      <link>https://www.markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</link>
      <pubDate>Sun, 15 Jul 2018 15:10:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/</guid>
      <description>I often find myself downloading web pages with Python&amp;#8217;s requests library to do some local scrapping when building datasets but I&amp;#8217;ve never come up with a good way for downloading those pages in parallel.
 Below is the code that I use. First we&amp;#8217;ll import the required libraries:
 import os import requests from time import time as timer   And now a function that streams a response into a local file:</description>
    </item>
    
    <item>
      <title>Interpreting Word2vec or GloVe embeddings using scikit-learn and Neo4j graph algorithms</title>
      <link>https://www.markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</link>
      <pubDate>Sat, 19 May 2018 09:47:21 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/</guid>
      <description>A couple of weeks I came across a paper titled Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings via Abigail See&#39;s blog post about ACL 2017.
  The paper explains an algorithm that helps to make sense of word embeddings generated by algorithms such as Word2vec and GloVe.
 I&amp;#8217;m fascinated by how graphs can be used to interpret seemingly black box data, so I was immediately intrigued and wanted to try and reproduce their findings using Neo4j.</description>
    </item>
    
    <item>
      <title>Python via virtualenv on Mac OS X: RuntimeError: Python is not installed as a framework.</title>
      <link>https://www.markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</link>
      <pubDate>Fri, 04 May 2018 22:03:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/</guid>
      <description>I&amp;#8217;ve previously written a couple of blog posts about my troubles getting matplotlib to play nicely and I run into a slightly different variant today while following Sidath Asiri&amp;#8217;s Hello World in TensorFlow tutorial.
 When I ran the script using a version of Python installed via virtualenv I got the following exception:
 Traceback (most recent call last): File &#34;iris.py&#34;, line 4, in &amp;lt;module&amp;gt; from matplotlib import pyplot as plt File &#34;</description>
    </item>
    
    <item>
      <title>PyData London 2018 Conference Experience Report</title>
      <link>https://www.markhneedham.com/blog/2018/04/29/pydata-london-2018/</link>
      <pubDate>Sun, 29 Apr 2018 11:54:02 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/29/pydata-london-2018/</guid>
      <description>Over the last few days I attended PyData London 2018 and wanted to share my experience. The PyData series of conferences aim to bring together users and developers of data analysis tools to share ideas and learn from each other. I presented a talk on building a recommendation with Python and Neo4j at the 2016 version but didn&amp;#8217;t attend last year.
 The organisers said there were ~ 550 attendees spread over 1 day of tutorials and 2 days of talks.</description>
    </item>
    
    <item>
      <title>Python: Serialize and Deserialize Numpy 2D arrays</title>
      <link>https://www.markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</link>
      <pubDate>Sat, 07 Apr 2018 19:38:36 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/07/python-serialize-deserialize-numpy-2d-arrays/</guid>
      <description>I&amp;#8217;ve been playing around with saving and loading scikit-learn models and needed to serialize and deserialize Numpy arrays as part of the process.
 I could use pickle but that seems a bit overkill so I decided instead to save the byte representation of the array. We can get that representation by calling the tobytes method on a Numpy array:
 import numpy as np &amp;gt;&amp;gt;&amp;gt; np.array([ [1,2,3], [4,5,6], [7,8,9] ]) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>Python 3: Converting a list to a dictionary with dictionary comprehensions</title>
      <link>https://www.markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</link>
      <pubDate>Mon, 02 Apr 2018 04:20:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/04/02/python-list-to-dictionary-comprehensions/</guid>
      <description>When coding in Python I often find myself with lists containing key/value pairs that I want to convert to a dictionary.
 In a recent example I had the following code:
 values = [{&#39;key&#39;: &#39;name&#39;, &#39;value&#39;: &#39;Mark&#39;}, {&#39;key&#39;: &#39;age&#39;, &#39;value&#39;: 34}]   And I wanted to create a dictionary that had the keys name and age and their respective values. The easiest way to convert this list to a dictionary is to iterate over the list and construct the dictionary key by key:</description>
    </item>
    
    <item>
      <title>Yelp: Reverse geocoding businesses to extract detailed location information</title>
      <link>https://www.markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</link>
      <pubDate>Wed, 14 Mar 2018 08:53:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2018/03/14/yelp-reverse-geocoding-businesses-extract-detailed-location-information/</guid>
      <description>I&amp;#8217;ve been playing around with the Yelp Open Dataset and wanted to extract more detailed location information for each business.
 This is an example of the JSON representation of one business:
 $ cat dataset/business.json | head -n1 | jq { &#34;business_id&#34;: &#34;FYWN1wneV18bWNgQjJ2GNg&#34;, &#34;name&#34;: &#34;Dental by Design&#34;, &#34;neighborhood&#34;: &#34;&#34;, &#34;address&#34;: &#34;4855 E Warner Rd, Ste B9&#34;, &#34;city&#34;: &#34;Ahwatukee&#34;, &#34;state&#34;: &#34;AZ&#34;, &#34;postal_code&#34;: &#34;85044&#34;, &#34;latitude&#34;: 33.3306902, &#34;longitude&#34;: -111.9785992, &#34;stars&#34;: 4, &#34;review_count&#34;: 22, &#34;</description>
    </item>
    
    <item>
      <title>scikit-learn: Using GridSearch to tune the hyper-parameters of VotingClassifier</title>
      <link>https://www.markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</link>
      <pubDate>Sun, 10 Dec 2017 07:55:43 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</guid>
      <description>In my last blog post I showed how to create a multi class classification ensemble using scikit-learn&amp;#8217;s http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier and finished mentioning that I didn&amp;#8217;t know which classifiers should be part of the ensemble.
 We need to get a better score with each of the classifiers in the ensemble otherwise they can be excluded.
 We have a TF/IDF based classifier as well as well as the classifiers I wrote about in the last post.</description>
    </item>
    
    <item>
      <title>scikit-learn: Building a multi class classification ensemble</title>
      <link>https://www.markhneedham.com/blog/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</link>
      <pubDate>Tue, 05 Dec 2017 22:19:34 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</guid>
      <description>For the Kaggle Spooky Author Identification I wanted to combine multiple classifiers together into an ensemble and found the VotingClassifier that does exactly that.
 We need to predict the probability that a sentence is written by one of three authors so the VotingClassifier needs to make a &#39;soft&#39; prediction. If we only needed to know the most likely author we could have it make a &#39;hard&#39; prediction instead.
 We start with three classifiers which generate different n-gram based features.</description>
    </item>
    
    <item>
      <title>Python: Combinations of values on and off</title>
      <link>https://www.markhneedham.com/blog/2017/12/03/python-combinations-values-off/</link>
      <pubDate>Sun, 03 Dec 2017 17:23:14 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/03/python-combinations-values-off/</guid>
      <description>In my continued exploration of Kaggle&amp;#8217;s Spooky Authors competition, I wanted to run a GridSearch turning on and off different classifiers to work out the best combination.
 I therefore needed to generate combinations of 1s and 0s enabling different classifiers.
 e.g. if we had 3 classifiers we&amp;#8217;d generate these combinations
 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1   where.</description>
    </item>
    
    <item>
      <title>Python: Learning about defaultdict&#39;s handling of missing keys</title>
      <link>https://www.markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</link>
      <pubDate>Fri, 01 Dec 2017 15:26:36 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</guid>
      <description>While reading the scikit-learn code I came across a bit of code that I didn&amp;#8217;t understand for a while but in retrospect is quite neat.
 This is the code snippet that intrigued me:
 vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__   Let&amp;#8217;s quickly see how it works by adapting an example from scikit-learn:
 &amp;gt;&amp;gt;&amp;gt; from collections import defaultdict &amp;gt;&amp;gt;&amp;gt; vocabulary = defaultdict() &amp;gt;&amp;gt;&amp;gt; vocabulary.default_factory = vocabulary.__len__ &amp;gt;&amp;gt;&amp;gt; vocabulary[&#34;</description>
    </item>
    
    <item>
      <title>scikit-learn: Creating a matrix of named entity counts</title>
      <link>https://www.markhneedham.com/blog/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</link>
      <pubDate>Wed, 29 Nov 2017 23:01:38 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</guid>
      <description>I&amp;#8217;ve been trying to improve my score on Kaggle&amp;#8217;s Spooky Author Identification competition, and my latest idea was building a model which used named entities extracted using the polyglot NLP library.
 We&amp;#8217;ll start by learning how to extract entities form a sentence using polyglot which isn&amp;#8217;t too tricky:
 &amp;gt;&amp;gt;&amp;gt; from polyglot.text import Text &amp;gt;&amp;gt;&amp;gt; doc = &#34;My name is David Beckham. Hello from London, England&#34; &amp;gt;&amp;gt;&amp;gt; Text(doc, hint_language_code=&#34;en&#34;).entities [I-PER([&#39;David&#39;, &#39;Beckham&#39;]), I-LOC([&#39;London&#39;]), I-LOC([&#39;England&#39;])]   This sentence contains three entities.</description>
    </item>
    
    <item>
      <title>Python: polyglot - ModuleNotFoundError: No module named &#39;icu&#39;</title>
      <link>https://www.markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</link>
      <pubDate>Tue, 28 Nov 2017 19:52:13 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</guid>
      <description>I wanted to use the polyglot NLP library that my colleague Will Lyon mentioned in his analysis of Russian Twitter Trolls but had installation problems which I thought I&amp;#8217;d share in case anyone else experiences the same issues.
 I started by trying to install polyglot:
 $ pip install polyglot ImportError: No module named &#39;icu&#39;   Hmmm I&amp;#8217;m not sure what icu is but luckily there&amp;#8217;s a GitHub issue covering this problem.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: unsupported format string passed to numpy.ndarray.*format*</title>
      <link>https://www.markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</link>
      <pubDate>Sun, 19 Nov 2017 07:16:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</guid>
      <description>This post explains how to work around a change in how Python string formatting works for numpy arrays between Python 2 and Python 3.
 I&amp;#8217;ve been going through Kevin Markham&#39;s scikit-learn Jupyter notebooks and ran into a problem on the Cross Validation one, which was throwing this error when attempting to print the KFold example:
 Iteration Training set observations Testing set observations --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-28-007cbab507e3&amp;gt; in &amp;lt;module&amp;gt;() 6 print(&#39;{} {:^61} {}&#39;.</description>
    </item>
    
    <item>
      <title>Python 3: Create sparklines using matplotlib</title>
      <link>https://www.markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</link>
      <pubDate>Sat, 23 Sep 2017 06:51:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</guid>
      <description>I recently wanted to create sparklines to show how some values were changing over time. In addition, I wanted to generate them as images on the server rather than introducing a JavaScript library.
 Chris Seymour&amp;#8217;s excellent gist which shows how to create sparklines inside a Pandas dataframe got me most of the way there, but I had to tweak his code a bit to get it to play nicely with Python 3.</description>
    </item>
    
    <item>
      <title>PHP vs Python: Generating a HMAC</title>
      <link>https://www.markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</link>
      <pubDate>Wed, 02 Aug 2017 06:09:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</guid>
      <description>I&amp;#8217;ve been writing a bit of code to integrate with a ClassMarker webhook, and you&amp;#8217;re required to check that an incoming request actually came from ClassMarker by checking the value of a base64 hash using HMAC SHA256.
 The example in the documentation is written in PHP which I haven&amp;#8217;t done for about 10 years so I had to figure out how to do the same thing in Python.
 This is the PHP version:</description>
    </item>
    
    <item>
      <title>Pandas/scikit-learn: get_dummies test/train sets - ValueError: shapes not aligned</title>
      <link>https://www.markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</link>
      <pubDate>Wed, 05 Jul 2017 15:42:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</guid>
      <description>I&amp;#8217;ve been using panda&amp;#8217;s https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html function to generate dummy columns for categorical variables to use with scikit-learn, but noticed that it sometimes doesn&amp;#8217;t work as I expect.
 Prerequisites import pandas as pd import numpy as np from sklearn import linear_model   Let&amp;#8217;s say we have the following training and test sets:
   Training set train = pd.DataFrame({&#34;letter&#34;:[&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;], &#34;value&#34;: [1, 2, 3, 4]}) X_train = train.</description>
    </item>
    
    <item>
      <title>Pandas: Find rows where column/field is null</title>
      <link>https://www.markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</link>
      <pubDate>Wed, 05 Jul 2017 14:31:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</guid>
      <description>In my continued playing around with the Kaggle house prices dataset I wanted to find any columns/fields that have null values in.
 If we want to get a count of the number of null fields by column we can use the following code, adapted from Poonam Ligade&amp;#8217;s kernel:
 Prerequisites import pandas as pd     Count the null columns train = pd.read_csv(&#34;train.csv&#34;) null_columns=train.columns[train.isnull().any()] train[null_columns].isnull().sum()   LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   So there are lots of different columns containing null values.</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://www.markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, I&amp;#8217;ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.
 Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant I&amp;#8217;d overfitted the model.
 I wasn&amp;#8217;t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albon&amp;#8217;s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://www.markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>I&amp;#8217;ve been playing around with the data in Kaggle&amp;#8217;s House Prices: Advanced Regression Techniques and while replicating Poonam Ligade&amp;#8217;s exploratory analysis I wanted to see if I could create a model to fill in some of the missing values.
 Poonam wrote the following code to identify which columns in the dataset had the most missing values:
 import pandas as pd train = pd.read_csv(&#39;train.csv&#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   The one that I&amp;#8217;m most interested in is LotFrontage, which describes &#39;Linear feet of street connected to property&#39;.</description>
    </item>
    
    <item>
      <title>Luigi: Defining dynamic requirements (on output files)</title>
      <link>https://www.markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</link>
      <pubDate>Tue, 28 Mar 2017 05:39:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</guid>
      <description>In my last blog post I showed how to convert a JSON document containing meetup groups into a CSV file using Luigi, the Python library for building data pipelines. As well as creating that CSV file I wanted to go back to the meetup.com API and download all the members of those groups.
 This was a rough flow of what i wanted to do:
   Take JSON document containing all groups</description>
    </item>
    
    <item>
      <title>Luigi: An ExternalProgramTask example - Converting JSON to CSV</title>
      <link>https://www.markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</link>
      <pubDate>Sat, 25 Mar 2017 14:09:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</guid>
      <description>I&amp;#8217;ve been playing around with the Python library Luigi which is used to build pipelines of batch jobs and I struggled to find an example of an ExternalProgramTask so this is my attempt at filling that void.
   I&amp;#8217;m building a little data pipeline to get data from the meetup.com API and put it into CSV files that can be loaded into Neo4j using the LOAD CSV command.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: Object of type &#39;dict_values&#39; is not JSON serializable</title>
      <link>https://www.markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</link>
      <pubDate>Sun, 19 Mar 2017 16:40:03 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</guid>
      <description>I&amp;#8217;ve recently upgraded to Python 3 (I know, took me a while!) and realised that one of my scripts that writes JSON to a file no longer works!
 This is a simplified version of what I&amp;#8217;m doing:
 &amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; x = {&#34;mark&#34;: {&#34;name&#34;: &#34;Mark&#34;}, &#34;michael&#34;: {&#34;name&#34;: &#34;Michael&#34;} } &amp;gt;&amp;gt;&amp;gt; json.dumps(x.values()) Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; File &#34;/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py&#34;, line 231, in dumps return _default_encoder.</description>
    </item>
    
    <item>
      <title>Go vs Python: Parsing a JSON response from a HTTP API</title>
      <link>https://www.markhneedham.com/blog/2017/01/21/go-vs-python-parsing-a-json-response-from-a-http-api/</link>
      <pubDate>Sat, 21 Jan 2017 10:49:46 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2017/01/21/go-vs-python-parsing-a-json-response-from-a-http-api/</guid>
      <description>As part of a recommendations with Neo4j talk that I&amp;#8217;ve presented a few times over the last year I have a set of scripts that download some data from the meetup.com API.
 They&amp;#8217;re all written in Python but I thought it&amp;#8217;d be a fun exercise to see what they&amp;#8217;d look like in Go. My eventual goal is to try and parallelise the API calls.
 This is the Python version of the script:</description>
    </item>
    
    <item>
      <title>scikit-learn: First steps with log_loss</title>
      <link>https://www.markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</link>
      <pubDate>Wed, 14 Sep 2016 05:33:38 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/09/14/scikit-learn-first-steps-with-log_loss/</guid>
      <description>Over the last week I&amp;#8217;ve spent a little bit of time playing around with the data in the Kaggle TalkingData Mobile User Demographics competition, and came across a notebook written by dune_dweller showing how to run a logistic regression algorithm on the dataset.
 The metric used to evaluate the output in this competition is multi class logarithmic loss, which is implemented by the http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html function in the scikit-learn library.</description>
    </item>
    
    <item>
      <title>scikit-learn: Clustering and the curse of dimensionality</title>
      <link>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</link>
      <pubDate>Sat, 27 Aug 2016 20:32:09 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/</guid>
      <description>In my last post I attempted to cluster Game of Thrones episodes based on character appearances without much success. After I wrote that post I was flicking through the scikit-learn clustering documentation and noticed the following section which describes some of the weaknesses of the K-means clustering algorithm:
  Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;#8220;curse of dimensionality&amp;#8221;).</description>
    </item>
    
    <item>
      <title>scikit-learn: Trying to find clusters of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</link>
      <pubDate>Thu, 25 Aug 2016 22:07:25 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/25/scikit-learn-trying-to-find-clusters-of-game-of-thrones-episodes/</guid>
      <description>In my last post I showed how to find similar Game of Thrones episodes based on the characters that appear in different episodes. This allowed us to find similar episodes on an episode by episode basis, but I was curious whether there were groups of similar episodes that we could identify.
 scikit-learn provides several clustering algorithms that can run over our episode vectors and hopefully find clusters of similar episodes.</description>
    </item>
    
    <item>
      <title>Neo4j/scikit-learn: Calculating the cosine similarity of Game of Thrones episodes</title>
      <link>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</link>
      <pubDate>Mon, 22 Aug 2016 21:12:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/22/neo4jscikit-learn-calculating-the-cosine-similarity-of-game-of-thrones-episodes/</guid>
      <description>A couple of months ago Praveena and I created a Game of Thrones dataset to use in a workshop and I thought it&amp;#8217;d be fun to run it through some machine learning algorithms and hopefully find some interesting insights.
 The dataset is available as CSV files but for this analysis I&amp;#8217;m assuming that it&amp;#8217;s already been imported into neo4j. If you want to import the data you can run the tutorial by typing the following into the query bar of the neo4j browser:</description>
    </item>
    
    <item>
      <title>Python: matplotlib, seaborn, virtualenv - Python is not installed as a framework</title>
      <link>https://www.markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</link>
      <pubDate>Sun, 14 Aug 2016 18:56:35 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/08/14/python-matplotlibseabornvirtualenv-python-is-not-installed-as-a-framework/</guid>
      <description>Over the weekend I was following The Marketing Technologist&amp;#8217;s content based recommender tutorial but ran into the following exception when trying to import the seaborn library:
 $ python 5_content_based_recommender/run.py Traceback (most recent call last): File &#34;5_content_based_recommender/run.py&#34;, line 14, in &amp;lt;module&amp;gt; import seaborn as sns File &#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/__init__.py&#34;, line 6, in &amp;lt;module&amp;gt; from .rcmod import * File &#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/rcmod.py&#34;, line 8, in &amp;lt;module&amp;gt; from . import palettes, _orig_rc_params File &#34;/Users/markneedham/projects/themarketingtechnologist/tmt/lib/python2.7/site-packages/seaborn/palettes.py&#34;, line 12, in &amp;lt;module&amp;gt; from .</description>
    </item>
    
    <item>
      <title>scikit-learn: TF/IDF and cosine similarity for computer science papers</title>
      <link>https://www.markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</link>
      <pubDate>Wed, 27 Jul 2016 02:45:28 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/</guid>
      <description>A couple of months ago I downloaded the meta data for a few thousand computer science papers so that I could try and write a mini recommendation engine to tell me what paper I should read next.
 Since I don&amp;#8217;t have any data on which people read each paper a collaborative filtering approach is ruled out, so instead I thought I could try content based filtering instead.
 Let&amp;#8217;s quickly check the Wikipedia definition of content based filtering:</description>
    </item>
    
    <item>
      <title>Python: Scraping elements relative to each other with BeautifulSoup</title>
      <link>https://www.markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</link>
      <pubDate>Mon, 11 Jul 2016 06:01:22 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/07/11/python-scraping-elements-relative-to-each-other-with-beautifulsoup/</guid>
      <description>Last week we hosted a Game of Thrones based intro to Cypher at the Women Who Code London meetup and in preparation had to scrape the wiki to build a dataset.
 I&amp;#8217;ve built lots of datasets this way and it&amp;#8217;s a painless experience as long as the pages make liberal use of CSS classes and/or IDs.
 Unfortunately the Game of Thrones wiki doesn&amp;#8217;t really do that so I had to find another way to extract the data I wanted - extracting elements based on their position to more prominent elements on the page.</description>
    </item>
    
    <item>
      <title>Python: BeautifulSoup - Insert tag</title>
      <link>https://www.markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</link>
      <pubDate>Thu, 30 Jun 2016 21:28:35 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/06/30/python-beautifulsoup-insert-tag/</guid>
      <description>I&amp;#8217;ve been scraping the Game of Thrones wiki in preparation for a meetup at Women Who Code next week and while attempting to extract character allegiances I wanted to insert missing line breaks to separate different allegiances.
 I initially tried creating a line break like this:
 &amp;gt;&amp;gt;&amp;gt; from bs4 import BeautifulSoup &amp;gt;&amp;gt;&amp;gt; tag = BeautifulSoup(&#34;&amp;lt;br /&amp;gt;&#34;, &#34;html.parser&#34;) &amp;gt;&amp;gt;&amp;gt; tag &amp;lt;br/&amp;gt;   It looks like it should work but later on in my script I check the &#39;name&#39; attribute to work out whether I&amp;#8217;ve got a line break and it doesn&amp;#8217;t return the value I expected it to:</description>
    </item>
    
    <item>
      <title>Python: Regex - matching foreign characters/unicode letters</title>
      <link>https://www.markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</link>
      <pubDate>Sat, 18 Jun 2016 07:38:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2016/06/18/python-regex-matching-foreign-charactersunicode-letters/</guid>
      <description>I&amp;#8217;ve been back in the land of screen scrapping this week extracting data from the Game of Thrones wiki and needed to write a regular expression to pull out characters and actors.
 Here are some examples of the format of the data: ~text Peter Dinklage as Tyrion Lannister Daniel Naprous as Oznak zo Pahl(credited as Stunt Performer) Filip Lozić as Young Nobleman Morgan C. Jones as a Braavosi captain Adewale Akinnuoye-Agbaje as Malko ~</description>
    </item>
    
    <item>
      <title>Python: Squashing &#39;duplicate&#39; pairs together</title>
      <link>https://www.markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</link>
      <pubDate>Sun, 20 Dec 2015 12:12:46 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/12/20/python-squashing-duplicate-pairs-together/</guid>
      <description>As part of a data cleaning pipeline I had pairs of ids of duplicate addresses that I wanted to group together.
 I couldn&amp;#8217;t work out how to solve the problem immediately so I simplified the problem into pairs of letters i.e.
 A	B	(A is the same as B) B	C	(B is the same as C) C	D	... E	F	(E is the same as F) F	G	.</description>
    </item>
    
    <item>
      <title>Python: Parsing a JSON HTTP chunking stream</title>
      <link>https://www.markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</link>
      <pubDate>Sat, 28 Nov 2015 13:56:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/11/28/python-parsing-a-json-http-chunking-stream/</guid>
      <description>I&amp;#8217;ve been playing around with meetup.com&amp;#8217;s API again and this time wanted to consume the chunked HTTP RSVP stream and filter RSVPs for events I&amp;#8217;m interested in.
 I use Python for most of my hacking these days and if HTTP requests are required the requests library is my first port of call.
 I started out with the following script
 import requests import json def stream_meetup_initial(): uri = &#34;</description>
    </item>
    
    <item>
      <title>Python: Extracting Excel spreadsheet into CSV files</title>
      <link>https://www.markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</link>
      <pubDate>Wed, 19 Aug 2015 23:27:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/08/19/python-extracting-excel-spreadsheet-into-csv-files/</guid>
      <description>I&amp;#8217;ve been playing around with the Road Safety open data set and the download comes with several CSV files and an excel spreadsheet containing the legend.
 There are 45 sheets in total and each of them looks like this:
   I wanted to create a CSV file for each sheet so that I can import the data set into Neo4j using the LOAD CSV command.
 I came across the Python Excel website which pointed me at the xlrd library since I&amp;#8217;m working with a pre 2010 Excel file.</description>
    </item>
    
    <item>
      <title>Python: Difference between two datetimes in milliseconds</title>
      <link>https://www.markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</link>
      <pubDate>Tue, 28 Jul 2015 20:05:47 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/28/python-difference-between-two-datetimes-in-milliseconds/</guid>
      <description>I&amp;#8217;ve been doing a bit of adhoc measurement of some cypher queries executed via py2neo and wanted to work out how many milliseconds each query was taking end to end.
 I thought there&amp;#8217;d be an obvious way of doing this but if there is it&amp;#8217;s evaded me so far and I ended up calculating the different between two datetime objects which gave me the following timedelta object: ~python &amp;gt;&amp;gt;&amp;gt; import datetime &amp;gt;&amp;gt;&amp;gt; start = datetime.</description>
    </item>
    
    <item>
      <title>Python: UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 0: ordinal not in range(128)</title>
      <link>https://www.markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</link>
      <pubDate>Wed, 15 Jul 2015 06:20:07 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/15/python-unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-0-ordinal-not-in-range128/</guid>
      <description>I was recently doing some text scrubbing and had difficulty working out how to remove the &#39;†&#39; character from strings.
 e.g. I had a string like this:
 &amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39; u&#39;foo \u2020&#39;   I wanted to get rid of the &#39;†&#39; character and then strip any trailing spaces so I&amp;#8217;d end up with the string &#39;foo&#39;. I tried to do this in one call to &#39;replace&#39;:
 &amp;gt;&amp;gt;&amp;gt; u&#39;foo †&#39;.</description>
    </item>
    
    <item>
      <title>Python: Converting WordPress posts in CSV format</title>
      <link>https://www.markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</link>
      <pubDate>Tue, 07 Jul 2015 06:28:01 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/07/07/python-converting-wordpress-posts-in-csv-format/</guid>
      <description>Over the weekend I wanted to look into the Wordpress data behind this blog (very meta!) and wanted to get the data in CSV format so I could do some analysis in R.
   I found a couple of WordPress CSV plugins but unfortunately I couldn&amp;#8217;t get any of them to work and ended up working with the raw XML data that WordPress produces when you &#39;export&#39; a blog.</description>
    </item>
    
    <item>
      <title>Python: CSV writing - TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;*getitem*&#39;</title>
      <link>https://www.markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</link>
      <pubDate>Sun, 31 May 2015 22:33:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/31/python-csv-writing-typeerror-builtin_function_or_method-object-has-no-attribute-__getitem__/</guid>
      <description>When I&amp;#8217;m working in Python I often find myself writing to CSV files using the in built library and every now and then make a mistake when calling writerow:
 import csv writer = csv.writer(file, delimiter=&#34;,&#34;) writer.writerow[&#34;player&#34;, &#34;team&#34;]   This results in the following error message:
 TypeError: &#39;builtin_function_or_method&#39; object has no attribute &#39;__getitem__&#39;   The error message is a bit weird at first but it&amp;#8217;s basically saying that I&amp;#8217;ve tried to do an associative lookup on an object which doesn&amp;#8217;t support that operation.</description>
    </item>
    
    <item>
      <title>Python: Look ahead multiple elements in an iterator/generator</title>
      <link>https://www.markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</link>
      <pubDate>Thu, 28 May 2015 20:56:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/28/python-look-ahead-multiple-elements-in-an-iteratorgenerator/</guid>
      <description>As part of the BBC live text scraping code I&amp;#8217;ve been working on I needed to take an iterator of raw events created by a generator and transform this into an iterator of cards shown in a match.
 The structure of the raw events I&amp;#8217;m interested in is as follows:
   Line 1: Player booked
  Line 2: Player fouled
  Line 3: Information about the foul</description>
    </item>
    
    <item>
      <title>Python: Joining multiple generators/iterators</title>
      <link>https://www.markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</link>
      <pubDate>Sun, 24 May 2015 23:51:25 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/24/python-joining-multiple-generatorsiterators/</guid>
      <description>In my previous blog post I described how I&amp;#8217;d refactored some scraping code I&amp;#8217;ve been working on to use iterators and ended up with a function which returned a generator containing all the events for one BBC live text match:
 match_id = &#34;32683310&#34; events = extract_events(&#34;data/raw/%s&#34; % (match_id)) &amp;gt;&amp;gt;&amp;gt; print type(events) &amp;lt;type &#39;generator&#39;&amp;gt;   The next thing I wanted to do is get the events for multiple matches which meant I needed to glue together multiple generators into one big generator.</description>
    </item>
    
    <item>
      <title>Python: Refactoring to iterator</title>
      <link>https://www.markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</link>
      <pubDate>Sat, 23 May 2015 10:14:38 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/23/python-refactoring-to-iterator/</guid>
      <description>Over the last week I&amp;#8217;ve been building a set of scripts to scrape the events from the Bayern Munich/Barcelona game and I&amp;#8217;ve ended up with a few hundred lines of nested for statements, if statements and mutated lists. I thought it was about time I did a bit of refactoring.
 The following is a function which takes in a match file and spits out a collection of maps containing times &amp;amp; events.</description>
    </item>
    
    <item>
      <title>Python: UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\xfc&#39; in position 11: ordinal not in range(128)</title>
      <link>https://www.markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</link>
      <pubDate>Thu, 21 May 2015 06:14:32 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/</guid>
      <description>I&amp;#8217;ve been trying to write some Python code to extract the players and the team they represented in the Bayern Munich/Barcelona match into a CSV file and had much more difficulty than I expected.
 I have some scraping code (which is beyond the scope of this article) which gives me a list of (player, team) pairs that I want to write to disk. The contents of the list is as follows:</description>
    </item>
    
    <item>
      <title>Python: Selecting certain indexes in an array</title>
      <link>https://www.markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</link>
      <pubDate>Tue, 05 May 2015 21:39:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/05/05/python-selecting-certain-indexes-in-an-array/</guid>
      <description>A couple of days ago I was scrapping the UK parliament constituencies from Wikipedia in preparation for the Graph Connect hackathon and had got to the point where I had an array with one entry per column in the table.
   import requests from bs4 import BeautifulSoup from soupselect import select page = open(&#34;constituencies.html&#34;, &#39;r&#39;) soup = BeautifulSoup(page.read()) for row in select(soup, &#34;table.wikitable tr&#34;): if select(row, &#34;th&#34;): print [cell.</description>
    </item>
    
    <item>
      <title>Python: Creating a skewed random discrete distribution</title>
      <link>https://www.markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</link>
      <pubDate>Mon, 30 Mar 2015 22:28:23 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/30/python-creating-a-skewed-random-discrete-distribution/</guid>
      <description>I&amp;#8217;m planning to write a variant of the TF/IDF algorithm over the HIMYM corpus which weights in favour of term that appear in a medium number of documents and as a prerequisite needed a function that when given a number of documents would return a weighting.
 It should return a higher value when a term appears in a medium number of documents i.e. if I pass in 10 I should get back a higher value than 200 as a term that appears in 10 episodes is likely to be more interesting than one which appears in almost every episode.</description>
    </item>
    
    <item>
      <title>Python: matplotlib hangs and shows nothing (Mac OS X)</title>
      <link>https://www.markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</link>
      <pubDate>Thu, 26 Mar 2015 00:02:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/26/python-matplotlib-hangs-and-shows-nothing-mac-os-x/</guid>
      <description>I&amp;#8217;ve been playing around with some of the matplotlib demos recently and discovered that simply copying one of the examples didn&amp;#8217;t actually work for me.
 I was following the bar chart example and had the following code:
 import numpy as np import matplotlib.pyplot as plt N = 5 ind = np.arange(N) fig, ax = plt.subplots() menMeans = (20, 35, 30, 35, 27) menStd = (2, 3, 4, 1, 2) width = 0.</description>
    </item>
    
    <item>
      <title>Topic Modelling: Working out the optimal number of topics</title>
      <link>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</link>
      <pubDate>Tue, 24 Mar 2015 22:33:42 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/24/topic-modelling-working-out-the-optimal-number-of-topics/</guid>
      <description>In my continued exploration of topic modelling I came across The Programming Historian blog and a post showing how to derive topics from a corpus using the Java library mallet.
 The instructions on the blog make it very easy to get up and running but as with other libraries I&amp;#8217;ve used, you have to specify how many topics the corpus consists of. I&amp;#8217;m never sure what value to select but the authors make the following suggestion:</description>
    </item>
    
    <item>
      <title>Python: Equivalent to flatMap for flattening an array of arrays</title>
      <link>https://www.markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</link>
      <pubDate>Mon, 23 Mar 2015 00:45:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/</guid>
      <description>I found myself wanting to flatten an array of arrays while writing some Python code earlier this afternoon and being lazy my first attempt involved building the flattened array manually:
 episodes = [ {&#34;id&#34;: 1, &#34;topics&#34;: [1,2,3]}, {&#34;id&#34;: 2, &#34;topics&#34;: [4,5,6]} ] flattened_episodes = [] for episode in episodes: for topic in episode[&#34;topics&#34;]: flattened_episodes.append({&#34;id&#34;: episode[&#34;id&#34;], &#34;topic&#34;: topic}) for episode in flattened_episodes: print episode   If we run that we&amp;#8217;ll see this output:</description>
    </item>
    
    <item>
      <title>Python: Simplifying the creation of a stop word list with defaultdict</title>
      <link>https://www.markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</link>
      <pubDate>Sun, 22 Mar 2015 01:51:52 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/22/python-simplifying-the-creation-of-a-stop-word-list-with-defaultdict/</guid>
      <description>I&amp;#8217;ve been playing around with topics models again and recently read a paper by David Mimno which suggested the following heuristic for working out which words should go onto the stop list:
  A good heuristic for identifying such words is to remove those that occur in more than 5-10% of documents (most common) and those that occur fewer than 5-10 times in the entire corpus (least common).
   I decided to try this out on the HIMYM dataset that I&amp;#8217;ve been working on over the last couple of months.</description>
    </item>
    
    <item>
      <title>Python: Forgetting to use enumerate</title>
      <link>https://www.markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</link>
      <pubDate>Sun, 22 Mar 2015 01:28:33 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/22/python-forgetting-to-use-enumerate/</guid>
      <description>Earlier this evening I found myself writing the equivalent of the following Python code while building a stop list for a topic model...
 words = [&#34;mark&#34;, &#34;neo4j&#34;, &#34;michael&#34;] word_position = 0 for word in words: print word_position, word word_position +=1   ...which is very foolish given that there&amp;#8217;s already a function that makes it really easy to grab the position of an item in a list:
 for word_position, word in enumerate(words): print word_position, word   Python does make things extremely easy at times - you&amp;#8217;re welcome future Mark!</description>
    </item>
    
    <item>
      <title>Python: Transforming Twitter datetime string to timestamp (z&#39; is a bad directive in format)</title>
      <link>https://www.markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</link>
      <pubDate>Sun, 15 Mar 2015 22:43:17 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/15/python-transforming-twitter-datetime-string-to-timestamp-z-is-a-bad-directive-in-format/</guid>
      <description>I&amp;#8217;ve been playing around with importing Twitter data into Neo4j and since Neo4j can&amp;#8217;t store dates natively just yet I needed to convert a date string to timestamp.
 I started with the following which unfortunately throws an exception:
 from datetime import datetime date = &#34;Sat Mar 14 18:43:19 +0000 2015&#34; &amp;gt;&amp;gt;&amp;gt; datetime.strptime(date, &#34;%a %b %d %H:%M:%S %z %Y&#34;) Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; File &#34;</description>
    </item>
    
    <item>
      <title>Python: Checking any value in a list exists in a line of text</title>
      <link>https://www.markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</link>
      <pubDate>Sat, 14 Mar 2015 02:52:02 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/14/python-checking-any-value-in-a-list-exists-in-a-line-of-text/</guid>
      <description>I&amp;#8217;ve been doing some log file analysis to see what cypher queries were being run on a Neo4j instance and I wanted to narrow down the lines I looked at to only contain ones which had mutating operations i.e. those containing the words MERGE, DELETE, SET or CREATE
 Here&amp;#8217;s an example of the text file I was parsing:
 $ cat blog.txt MATCH n RETURN n MERGE (n:Person {name: &#34;</description>
    </item>
    
    <item>
      <title>Python/Neo4j: Finding interesting computer sciency people to follow on Twitter</title>
      <link>https://www.markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</link>
      <pubDate>Wed, 11 Mar 2015 21:13:26 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/11/pythonneo4j-finding-interesting-computer-sciency-people-to-follow-on-twitter/</guid>
      <description>At the beginning of this year I moved from Neo4j&amp;#8217;s field team to dev team and since the code we write there is much lower level than I&amp;#8217;m used to I thought I should find some people to follow on twitter whom I can learn from.
 My technique for finding some of those people was to pick a person from the Neo4j kernel team who&amp;#8217;s very good at systems programming and uses twitter which led me to Mr Chris Vest.</description>
    </item>
    
    <item>
      <title>Python: Streaming/Appending to a file</title>
      <link>https://www.markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</link>
      <pubDate>Mon, 09 Mar 2015 23:00:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/09/python-streamingappending-to-a-file/</guid>
      <description>I&amp;#8217;ve been playing around with Twitter&amp;#8217;s API (via the tweepy library) and due to the rate limiting it imposes I wanted to stream results to a CSV file rather than waiting until my whole program had finished.
 I wrote the following program to simulate what I was trying to do:
 import csv import time with open(&#34;rows.csv&#34;, &#34;a&#34;) as file: writer = csv.writer(file, delimiter = &#34;,&#34;) end = time.time() + 10 while True: if time.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn/lda: Extracting topics from QCon talk abstracts</title>
      <link>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</link>
      <pubDate>Thu, 05 Mar 2015 08:52:22 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/05/python-scikit-learnlda-extracting-topics-from-qcon-talk-abstracts/</guid>
      <description>Following on from Rik van Bruggen&amp;#8217;s blog post on a QCon graph he&amp;#8217;s created ahead of this week&amp;#8217;s conference, I was curious whether we could extract any interesting relationships between talks based on their abstracts.
 Talks are already grouped by their hosting track but there&amp;#8217;s likely to be some overlap in topics even for talks on different tracks. I therefore wanted to extract topics and connect each talk to the topic that describes it best.</description>
    </item>
    
    <item>
      <title>Python: scikit-learn - Training a classifier with non numeric features</title>
      <link>https://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</link>
      <pubDate>Mon, 02 Mar 2015 07:48:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/</guid>
      <description>Following on from my previous posts on training a classifier to pick out the speaker in sentences of HIMYM transcripts the next thing to do was train a random forest of decision trees to see how that fared.
 I&amp;#8217;ve used scikit-learn for this before so I decided to use that. However, before building a random forest I wanted to check that I could build an equivalent decision tree.
 I initially thought that scikit-learn&amp;#8217;s DecisionTree classifier would take in data in the same format as nltk&amp;#8217;s so I started out with the following code:</description>
    </item>
    
    <item>
      <title>Python: Detecting the speaker in HIMYM using Parts of Speech (POS) tagging</title>
      <link>https://www.markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</link>
      <pubDate>Sun, 01 Mar 2015 02:36:06 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/03/01/python-detecting-the-speaker-in-himym-using-parts-of-speech-pos-tagging/</guid>
      <description>Over the last couple of weeks I&amp;#8217;ve been experimenting with different classifiers to detect speakers in HIMYM transcripts and in all my attempts so far the only features I&amp;#8217;ve used have been words.
 This led to classifiers that were overfitted to the training data so I wanted to generalise them by introducing parts of speech of the words in sentences which are more generic.
 First I changed the function which generates the features for each word to also contain the parts of speech of the previous and next words as well as the word itself:</description>
    </item>
    
    <item>
      <title>Python/nltk: Naive vs Naive Bayes vs Decision Tree</title>
      <link>https://www.markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</link>
      <pubDate>Tue, 24 Feb 2015 22:39:49 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/</guid>
      <description>Last week I wrote a blog post describing a decision tree I&amp;#8217;d trained to detect the speakers in a How I met your mother transcript and after writing the post I wondered whether a simple classifier would do the job.
 The simple classifier will work on the assumption that any word followed by a &#34;:&#34; is a speaker and anything else isn&amp;#8217;t. Here&amp;#8217;s the definition of a NaiveClassifier:
 import nltk from nltk import ClassifierI class NaiveClassifier(ClassifierI): def classify(self, featureset): if featureset[&#39;next-word&#39;] == &#34;</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Detecting which sentences in a transcript contain a speaker</title>
      <link>https://www.markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</link>
      <pubDate>Fri, 20 Feb 2015 22:42:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/20/pythonscikit-learn-detecting-which-sentences-in-a-transcript-contain-a-speaker/</guid>
      <description>Over the past couple of months I&amp;#8217;ve been playing around with How I met your mother transcripts and the most recent thing I&amp;#8217;ve been working on is how to extract the speaker for a particular sentence.
 This initially seemed like a really simple problem as most of the initial sentences I looked at weere structured like this:
 &amp;lt;speaker&amp;gt;: &amp;lt;sentence&amp;gt;   If there were all in that format then we could write a simple regular expression and then move on but unfortunately they aren&amp;#8217;t.</description>
    </item>
    
    <item>
      <title>Python/pandas: Column value in list (ValueError: The truth value of a Series is ambiguous.)</title>
      <link>https://www.markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Mon, 16 Feb 2015 21:39:16 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/16/pythonpandas-column-value-in-list-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>I&amp;#8217;ve been using Python&amp;#8217;s pandas library while exploring some CSV files and although for the most part I&amp;#8217;ve found it intuitive to use, I had trouble filtering a data frame based on checking whether a column value was in a list.
 A subset of one of the CSV files I&amp;#8217;ve been working with looks like this:
 $ cat foo.csv &#34;Foo&#34; 1 2 3 4 5 6 7 8 9 10   Loading it into a pandas data frame is reasonably simple:</description>
    </item>
    
    <item>
      <title>Python/scikit-learn: Calculating TF/IDF on How I met your mother transcripts</title>
      <link>https://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</link>
      <pubDate>Sun, 15 Feb 2015 15:56:09 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/</guid>
      <description>Over the past few weeks I&amp;#8217;ve been playing around with various NLP techniques to find interesting insights into How I met your mother from its transcripts and one technique that kept coming up is TF/IDF.
 The Wikipedia definition reads like this:
  tf&amp;#8212;&amp;#8203;idf, short for term frequency&amp;#8212;&amp;#8203;inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</description>
    </item>
    
    <item>
      <title>Neo4j: Building a topic graph with Prismatic Interest Graph API</title>
      <link>https://www.markhneedham.com/blog/2015/02/13/neo4j-building-a-topic-graph-with-prismatic-interest-graph-api/</link>
      <pubDate>Fri, 13 Feb 2015 23:38:43 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/13/neo4j-building-a-topic-graph-with-prismatic-interest-graph-api/</guid>
      <description>Over the last few weeks I&amp;#8217;ve been using various NLP libraries to derive topics for my corpus of How I met your mother episodes without success and was therefore enthused to see the release of Prismatic&amp;#8217;s Interest Graph API
 The Interest Graph API exposes a web service to which you feed a block of text and get back a set of topics and associated score.
 It has been trained over the last few years with millions of articles that people share on their social media accounts and in my experience using Prismatic the topics have been very useful for finding new material to read.</description>
    </item>
    
    <item>
      <title>Python/gensim: Creating bigrams over How I met your mother transcripts</title>
      <link>https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</link>
      <pubDate>Thu, 12 Feb 2015 23:45:03 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</guid>
      <description>As part of my continued playing around with How I met your mother transcripts I wanted to identify plot arcs and as a first step I wrote some code using the gensim and nltk libraries to identify bigrams (two word phrases).
 There&amp;#8217;s an easy to follow tutorial in the gensim docs showing how to go about this but I needed to do a couple of extra steps to get my text data from a CSV file into the structure gensim expects.</description>
    </item>
    
    <item>
      <title>Python/matpotlib: Plotting occurrences of the main characters in How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</link>
      <pubDate>Fri, 30 Jan 2015 21:29:00 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/30/pythonmatpotlib-plotting-occurrences-of-the-main-characters-in-how-i-met-your-mother/</guid>
      <description>Normally when I&amp;#8217;m playing around with data sets in R I get out ggplot2 to plot some charts to get a feel for the data but having spent quite a bit of time with Python and How I met your mother transcripts I haven&amp;#8217;t created a single plot. I thought I&amp;#8217;d better change change that.
 After a bit of searching around it seems that matplotlib is the go to library for this job and I thought an interesting thing to plot would be how often each of the main characters appear in each episode across the show.</description>
    </item>
    
    <item>
      <title>Python: Find the highest value in a group</title>
      <link>https://www.markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</link>
      <pubDate>Sun, 25 Jan 2015 12:47:01 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/25/python-find-the-highest-value-in-a-group/</guid>
      <description>In my continued playing around with a How I met your mother data set I needed to find out the last episode that happened in a season so that I could use it in a chart I wanted to plot.
 I had this CSV file containing each of the episodes:
 $ head -n 10 data/import/episodes.csv NumberOverall,NumberInSeason,Episode,Season,DateAired,Timestamp 1,1,/wiki/Pilot,1,&#34;September 19, 2005&#34;,1127084400 2,2,/wiki/Purple_Giraffe,1,&#34;September 26, 2005&#34;,1127689200 3,3,/wiki/Sweet_Taste_of_Liberty,1,&#34;October 3, 2005&#34;,1128294000 4,4,/wiki/Return_of_the_Shirt,1,&#34;October 10, 2005&#34;,1128898800 5,5,/wiki/Okay_Awesome,1,&#34;</description>
    </item>
    
    <item>
      <title>Python/pdfquery: Scraping the FIFA World Player of the Year votes PDF into shape</title>
      <link>https://www.markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</link>
      <pubDate>Thu, 22 Jan 2015 00:25:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/22/pythonpdfquery-scraping-the-fifa-world-player-of-the-year-votes-pdf-into-shape/</guid>
      <description>Last week the FIFA Ballon d&amp;#8217;Or 2014 was announced and along with the announcement of the winner the individual votes were also made available.
 Unfortunately they weren&amp;#8217;t made open in a way that Ben Wellington (of IQuantNY fame) would approve of - the choice of format for the data is a PDF file!
 I wanted to extract this data to play around with it but I wanted to automate the extraction as I&amp;#8217;d done when working with Google Trends data.</description>
    </item>
    
    <item>
      <title>Python/NLTK: Finding the most common phrases in How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</link>
      <pubDate>Mon, 19 Jan 2015 00:24:23 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/19/pythonnltk-finding-the-most-common-phrases-in-how-i-met-your-mother/</guid>
      <description>Following on from last week&amp;#8217;s blog post where I found the most popular words in How I met your mother transcripts, in this post we&amp;#8217;ll have a look at how we can pull out sentences and then phrases from our corpus.
 The first thing I did was tweak the scraping script to pull out the sentences spoken by characters in the transcripts.&amp;lt;/p&amp;gt;
 Each dialogue is separated by two line breaks so we use that as our separator.</description>
    </item>
    
    <item>
      <title>Python: Counter - ValueError: too many values to unpack</title>
      <link>https://www.markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</link>
      <pubDate>Mon, 12 Jan 2015 23:16:58 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/12/python-counter-valueerror-too-many-values-to-unpack/</guid>
      <description>I recently came across Python&amp;#8217;s Counter tool which makes it really easy to count the number of occurrences of items in a list.
 In my case I was trying to work out how many times words occurred in a corpus so I had something like the following:
 &amp;gt;&amp;gt; from collections import Counter &amp;gt;&amp;gt; counter = Counter([&#34;word1&#34;, &#34;word2&#34;, &#34;word3&#34;, &#34;word1&#34;]) &amp;gt;&amp;gt; print counter Counter({&#39;word1&#39;: 2, &#39;word3&#39;: 1, &#39;word2&#39;: 1})   I wanted to write a for loop to iterate over the counter and print the (key, value) pairs and started with the following:</description>
    </item>
    
    <item>
      <title>Python: scikit-learn: ImportError: cannot import name __check_build</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</link>
      <pubDate>Sat, 10 Jan 2015 08:48:04 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-scikit-learn-importerror-cannot-import-name-__check_build/</guid>
      <description>In part 3 of Kaggle&amp;#8217;s series on text analytics I needed to install scikit-learn and having done so ran into the following error when trying to use one of its classes:
 &amp;gt;&amp;gt;&amp;gt; from sklearn.feature_extraction.text import CountVectorizer Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; File &#34;/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/sklearn/__init__.py&#34;, line 37, in &amp;lt;module&amp;gt; from . import __check_build ImportError: cannot import name __check_build   This error doesn&amp;#8217;t reveal very much but I found that when I exited the REPL and tried the same command again I got a different error which was a bit more useful:</description>
    </item>
    
    <item>
      <title>Python: gensim - clang: error: unknown argument: &#39;-mno-fused-madd&#39; [-Wunused-command-line-argument-hard-error-in-future]</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</link>
      <pubDate>Sat, 10 Jan 2015 08:39:15 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-gensim-clang-error-unknown-argument-mno-fused-madd-wunused-command-line-argument-hard-error-in-future/</guid>
      <description>While working through part 2 of Kaggle&amp;#8217;s bag of words tutorial I needed to install the gensim library and initially ran into the following error:
 $ pip install gensim ... cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/Users/markneedham/projects/neo4j-himym/himym/build/gensim/gensim/models -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/Users/markneedham/projects/neo4j-himym/himym/lib/python2.7/site-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.</description>
    </item>
    
    <item>
      <title>Python NLTK/Neo4j: Analysing the transcripts of How I Met Your Mother</title>
      <link>https://www.markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</link>
      <pubDate>Sat, 10 Jan 2015 01:22:56 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2015/01/10/python-nltkneo4j-analysing-the-transcripts-of-how-i-met-your-mother/</guid>
      <description>After reading Emil&amp;#8217;s blog post about dark data a few weeks ago I became intrigued about trying to find some structure in free text data and I thought How I met your mother&amp;#8217;s transcripts would be a good place to start.
 I found a website which has the transcripts for all the episodes and then having manually downloaded the two pages which listed all the episodes, wrote a script to grab each of the transcripts so I could use them on my machine.</description>
    </item>
    
    <item>
      <title>Python: Converting a date string to timestamp</title>
      <link>https://www.markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</link>
      <pubDate>Mon, 20 Oct 2014 15:53:51 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/10/20/python-converting-a-date-string-to-timestamp/</guid>
      <description>I&amp;#8217;ve been playing around with Python over the last few days while cleaning up a data set and one thing I wanted to do was translate date strings into a timestamp.
 I started with a date in this format:
 date_text = &#34;13SEP2014&#34;   So the first step is to translate that into a Python date - the strftime section of the documentation is useful for figuring out which format code is needed:</description>
    </item>
    
    <item>
      <title>Jython/Neo4j: java.lang.ExceptionInInitializerError: java.lang.ExceptionInInitializerError</title>
      <link>https://www.markhneedham.com/blog/2014/02/05/jythonneo4j-java-lang-exceptionininitializererror-java-lang-exceptionininitializererror/</link>
      <pubDate>Wed, 05 Feb 2014 12:21:30 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2014/02/05/jythonneo4j-java-lang-exceptionininitializererror-java-lang-exceptionininitializererror/</guid>
      <description>I&amp;#8217;ve been playing around with calling Neo4j&amp;#8217;s Java API from Python via Jython and immediately ran into the following exception when trying to create an embedded instance:
 $ jython -Dpython.path /path/to/neo4j.jar Jython 2.5.3 (2.5:c56500f08d34+, Aug 13 2012, 14:48:36) [Java HotSpot(TM) 64-Bit Server VM (Oracle Corporation)] on java1.7.0_45 Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information. &amp;gt;&amp;gt;&amp;gt; import org.neo4j.graphdb.factory &amp;gt;&amp;gt;&amp;gt; org.neo4j.graphdb.factory.GraphDatabaseFactory().newEmbeddedDatabase(&#34;/tmp/foo&#34;) Traceback (most recent call last): File &#34;&amp;lt;stdin&amp;gt;&#34;, line 1, in &amp;lt;module&amp;gt; at org.</description>
    </item>
    
    <item>
      <title>Python: Making scikit-learn and pandas play nice</title>
      <link>https://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</link>
      <pubDate>Sat, 09 Nov 2013 13:58:54 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/</guid>
      <description>In the last post I wrote about Nathan and my http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/[attempts at the http://www.kaggle.com/c/titanic-gettingStarted[Kaggle Titanic Problem\] I mentioned that we our next step was to try out http://scikit-learn.org/stable/tutorial/[scikit-learn\] so I thought I should summarise where we&amp;#8217;ve got up to.&amp;lt;/p&amp;gt;
 We needed to write a classification algorithm to work out whether a person onboard the Titanic survived and luckily scikit-learn has http://scikit-learn.org/stable/supervised_learning.html#supervised-learning[extensive documentation on each of the algorithms\].
 Unfortunately almost all those examples use http://www.</description>
    </item>
    
    <item>
      <title>Python: Scoping variables to use with timeit</title>
      <link>https://www.markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</link>
      <pubDate>Sat, 09 Nov 2013 11:01:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/09/python-scoping-variables-to-use-with-timeit/</guid>
      <description>I&amp;#8217;ve been playing around with Python&amp;#8217;s timeit library to help benchmark some Neo4j cypher queries but I ran into some problems when trying to give it accessible to variables in my program.
 I had the following python script which I would call from the terminal using python top-away-scorers.py:
 import query_profiler as qp attempts = [ {&#34;query&#34;: &#39;&#39;&#39;MATCH (player:Player)-[:played]-&amp;gt;stats-[:in]-&amp;gt;game, stats-[:for]-&amp;gt;team WHERE game&amp;lt;-[:away_team]-team RETURN player.name, SUM(stats.goals) AS goals ORDER BY goals DESC LIMIT 10&#39;&#39;&#39;} ] qp.</description>
    </item>
    
    <item>
      <title>Python: Generate all combinations of a list</title>
      <link>https://www.markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</link>
      <pubDate>Wed, 06 Nov 2013 07:25:24 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/06/python-generate-all-combinations-of-a-list/</guid>
      <description>Nathan and I have been playing around with different scikit-learn machine learning classifiers and we wanted to run different combinations of features through each one and work out which gave the best result.
 We started with a list of features:
 all_columns = [&#34;Fare&#34;, &#34;Sex&#34;, &#34;Pclass&#34;, &#39;Embarked&#39;]   itertools#combinations allows us to create combinations with a length of our choice:
 &amp;gt;&amp;gt;&amp;gt; import itertools as it &amp;gt;&amp;gt;&amp;gt; list(it.combinations(all_columns, 3)) [(&#39;Fare&#39;, &#39;Sex&#39;, &#39;Pclass&#39;), (&#39;Fare&#39;, &#39;Sex&#39;, &#39;Embarked&#39;), (&#39;Fare&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;), (&#39;Sex&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;)]   We wanted to create combinations of arbitrary length so we wanted to combine a few invocations of that functions like this:</description>
    </item>
    
    <item>
      <title>Python: matplotlib -  Import error ft2font Symbol not found: _FT_Attach_File (Mac OS X 10.8.3/Mountain Lion)</title>
      <link>https://www.markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</link>
      <pubDate>Sun, 03 Nov 2013 11:14:48 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/11/03/python-matplotlib-import-error-ft2font-symbol-not-found-_ft_attach_file-mac-os-x-10-8-3mountain-lion/</guid>
      <description>As I mentioned at the end of my last post about the Titanic Kaggle problem our next step was to do some proper machine learning&amp;trade; using scikit-learn so I started by looking at the Decision Tree example.
 Unfortunately I ended up on the mother of all yak shaving missions while trying to execute the code which draws a chart using matplotlib.
 I ran the following line from the tutorial:</description>
    </item>
    
    <item>
      <title>pandas: Adding a column to a DataFrame (based on another DataFrame)</title>
      <link>https://www.markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</link>
      <pubDate>Wed, 30 Oct 2013 06:12:08 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/10/30/pandas-adding-a-column-to-a-dataframe-based-on-another-dataframe/</guid>
      <description>Nathan and I have been working on the Titanic Kaggle problem using the pandas data analysis library and one thing we wanted to do was add a column to a DataFrame indicating if someone survived.
 We had the following (simplified) DataFrame containing some information about customers on board the Titanic:
 def addrow(df, row): return df.append(pd.DataFrame(row), ignore_index=True) customers = pd.DataFrame(columns=[&#39;PassengerId&#39;,&#39;Pclass&#39;,&#39;Name&#39;,&#39;Sex&#39;,&#39;Fare&#39;]) customers = addrow(customers, [dict(PassengerId=892, Pclass=3, Name=&#34;Kelly, Mr. James&#34;, Sex=&#34;male&#34;, Fare=7.</description>
    </item>
    
    <item>
      <title>Python: for/list comprehensions and dictionaries</title>
      <link>https://www.markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</link>
      <pubDate>Tue, 13 Aug 2013 22:59:52 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/08/13/python-forlist-comprehensions-and-dictionaries/</guid>
      <description>I&amp;#8217;ve been working through Coursera&amp;#8217;s Linear Algebra course and since all of the exercises are in Python I&amp;#8217;ve been playing around with it again.
 One interesting thing I learnt is that you can construct dictionaries using a list comprehension type syntax.
 For example, if we start with the following dictionaries:
 &amp;gt;&amp;gt;&amp;gt; x = { &#34;a&#34;: 1, &#34;b&#34;:2 } &amp;gt;&amp;gt;&amp;gt; y = {1: &#34;mark&#34;, 2: &#34;will&#34;} &amp;gt;&amp;gt;&amp;gt; x {&#39;a&#39;: 1, &#39;b&#39;: 2} &amp;gt;&amp;gt;&amp;gt; y {1: &#39;mark&#39;, 2: &#39;will&#39;}   We might want to create a new dictionary which links from the keys in x to the values in y.</description>
    </item>
    
    <item>
      <title>Ruby/Python: Constructing a taxonomy from an array using zip</title>
      <link>https://www.markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</link>
      <pubDate>Sun, 19 May 2013 22:44:40 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/05/19/rubypython-constructing-a-taxonomy-from-an-array-using-zip/</guid>
      <description>As I mentioned in my previous blog post I&amp;#8217;ve been hacking on a product taxonomy and I wanted to create a &#39;CHILD&#39; relationship between a collection of categories.
 For example, I had the following array and I wanted to transform it into an array of &#39;SubCategory, Category&#39; pairs:
 taxonomy = [&#34;Cat&#34;, &#34;SubCat&#34;, &#34;SubSubCat&#34;] # I wanted this to become [(&#34;Cat&#34;, &#34;SubCat&#34;), (&#34;SubCat&#34;, &#34;SubSubCat&#34;)   In order to do this we need to zip the first 2 items with the last which I found reasonably easy to do using Python:</description>
    </item>
    
    <item>
      <title>Python: Reading a JSON file</title>
      <link>https://www.markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</link>
      <pubDate>Tue, 09 Apr 2013 07:23:59 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/04/09/python-reading-a-json-file/</guid>
      <description>I&amp;#8217;ve been playing around with some code to spin up AWS instances using Fabric and Boto and one thing that I wanted to do was define a bunch of default properties in a JSON file and then load this into a script.
 I found it harder to work out how to do this than I expected to so I thought I&amp;#8217;d document it for future me!
 My JSON file looks like this:</description>
    </item>
    
    <item>
      <title>Python: (Conceptually) removing an item from a tuple</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</link>
      <pubDate>Sun, 27 Jan 2013 02:30:05 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/python-conceptually-removing-an-item-from-a-tuple/</guid>
      <description>As part of some code I&amp;#8217;ve been playing around I wanted to remove an item from a tuple which wasn&amp;#8217;t particularly easy because Python&amp;#8217;s tuple data structure is immutable.
 I therefore needed to create a new tuple excluding the value which I wanted to remove.
 I ended up writing the following function to do this but I imagine there might be an easier way because it&amp;#8217;s quite verbose:</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting values by multiple indices</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</link>
      <pubDate>Sun, 27 Jan 2013 02:21:39 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-values-by-multiple-indices/</guid>
      <description>As I mentioned in my previous post I&amp;#8217;ve been playing around with numpy and I wanted to get the values of a collection of different indices in a 2D array.
 If we had a 2D array that looked like this:
 &amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])   I knew that it was possible to retrieve the first 3 rows by using the following code:</description>
    </item>
    
    <item>
      <title>Python/numpy: Selecting specific column in 2D array</title>
      <link>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</link>
      <pubDate>Sun, 27 Jan 2013 02:10:10 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2013/01/27/pythonnumpy-selecting-specific-column-in-2d-array/</guid>
      <description>I&amp;#8217;ve been playing around with numpy this evening in an attempt to improve the performance of a Travelling Salesman Problem implementation and I wanted to get every value in a specific column of a 2D array.
 The array looked something like this:
 &amp;gt;&amp;gt;&amp;gt; x = arange(20).reshape(4,5) &amp;gt;&amp;gt;&amp;gt; x array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])   I wanted to get the values for the 2nd column of each row which would return an array containing 1, 6, 11 and 16.</description>
    </item>
    
  </channel>
</rss>