<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019s on Mark Needham</title>
    <link>https://markhneedham.com/blog/2019/</link>
    <description>Recent content in 2019s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2019 06:10:00 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/2019/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Docker: Find the network for a container</title>
      <link>https://markhneedham.com/blog/2019/05/24/docker-find-network-for-container/</link>
      <pubDate>Fri, 24 May 2019 06:10:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/24/docker-find-network-for-container/</guid>
      <description>If we want two Docker containers to communicate with each other they need to belong to the same network. In this post we&amp;#8217;ll learn how to find out the network of existing containers so that we can attach new containers to that network.
 All the containers mentioned in this post can be launched locally from Docker compose, using the following command:
 git clone git@github.com:mneedham/ksql-kafka-neo4j-streams.git &amp;amp;&amp;amp; cd ksql-kafka-neo4j-streams docker-compose-up   Running this command will create four containers:</description>
    </item>
    
    <item>
      <title>Processing Neo4j Transaction Events with KSQL and Kafka Streams</title>
      <link>https://markhneedham.com/blog/2019/05/23/processing-neo4j-transaction-events-ksql-kafka-streams/</link>
      <pubDate>Thu, 23 May 2019 12:46:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/23/processing-neo4j-transaction-events-ksql-kafka-streams/</guid>
      <description>The Neo4j Streams Library lets users send transaction events to a Kafka topic, and in this post we&amp;#8217;re going to learn how to explore these events using the KSQL streaming SQL Engine.
 All the infrastructure used in this post can be launched locally from Docker compose, using the following command:
 git clone git@github.com:mneedham/ksql-kafka-neo4j-streams.git &amp;amp;&amp;amp; cd ksql-kafka-neo4j-streams docker-compose-up   Running this command will create four containers:
 Starting zookeeper-blog .</description>
    </item>
    
    <item>
      <title>Deleting Kafka Topics on Docker</title>
      <link>https://markhneedham.com/blog/2019/05/23/deleting-kafka-topics-on-docker/</link>
      <pubDate>Thu, 23 May 2019 07:58:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/23/deleting-kafka-topics-on-docker/</guid>
      <description>In this post we&amp;#8217;re going to learn how to delete a Kafka Topic when running a Kafka Broker on Docker.
 We&amp;#8217;ll spin up a local Kafka environment using the Docker Compose template from the Kafka Basic Tutorial blog post that I wrote last week. Let&amp;#8217;s open a terminal window and run the following commands to set up our environment:
 git clone git@github.com:mneedham/basic-kafka-tutorial.git &amp;amp;&amp;amp; cd basic-kafka-tutorial docker-compose up   On another terminal window, run the following command to see the list of Docker containers that are running:</description>
    </item>
    
    <item>
      <title>KSQL: Create Stream - extraneous input &#39;properties&#39;</title>
      <link>https://markhneedham.com/blog/2019/05/20/kql-create-stream-extraneous-input/</link>
      <pubDate>Mon, 20 May 2019 11:43:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/20/kql-create-stream-extraneous-input/</guid>
      <description>In my continued playing with the KSQL streaming engine for Kafka, I came across another interesting error while trying to put a stream on top of a topic generated by the Neo4j Streams Library.
 We&amp;#8217;ll simplify the events being posted on the topic for this blog post, so this is what the events on the topic look like:
 { &#34;id&#34;:&#34;ABCDEFGHI&#34;, &#34;properties&#34;: { &#34;name&#34;:&#34;Mark&#34;, &#34;location&#34;:&#34;London&#34; } }   We then create a stream on that topic:</description>
    </item>
    
    <item>
      <title>KSQL: Create Stream - Failed to prepare statement: name is null</title>
      <link>https://markhneedham.com/blog/2019/05/19/ksql-create-stream-failed-to-prepare-statement-name-is-null/</link>
      <pubDate>Sun, 19 May 2019 19:21:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/19/ksql-create-stream-failed-to-prepare-statement-name-is-null/</guid>
      <description>I&amp;#8217;ve been playing with KSQL over the weekend and ran into a basic error message that took me a little while to solve.
 I was trying to create a stream over a topic dummy1, which is the simplest possible thing you can do with KSQL. The events posted to dummy1 are JSON messages containing only an id key. Below is an example of a message posted to the topic:</description>
    </item>
    
    <item>
      <title>Kafka: A basic tutorial</title>
      <link>https://markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</link>
      <pubDate>Thu, 16 May 2019 10:02:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/16/kafka-basic-tutorial/</guid>
      <description>In this post we&amp;#8217;re going to learn how to launch Kafka locally and write to and read from a topic using one of the Python drivers.
 To make things easy for myself, I&amp;#8217;ve created a Docker Compose template that launches 3 containers:
   broker - our Kafka broker
  zookeeper - used by Kafka for leader election
  jupyter - notebooks for connecting to our Kafka broker</description>
    </item>
    
    <item>
      <title>Neo4j: keep/filter keys in a map using APOC</title>
      <link>https://markhneedham.com/blog/2019/05/12/neo4j-keep-filter-keys-map-apoc/</link>
      <pubDate>Sun, 12 May 2019 17:58:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/12/neo4j-keep-filter-keys-map-apoc/</guid>
      <description>In this post we&amp;#8217;ll learn how to write a Cypher query to create a node in Neo4j containing some of the keys from a map. This post assumes that the APOC library is installed.
 We&amp;#8217;ll start by creating a map that contains data from my twitter profile:
 :param document =&amp;gt; { id: 14707949, name: &#34;Mark Needham&#34;, username: &#34;markhneedham&#34;, bio: &#34;Developer Relations @neo4j&#34;, location: &#34;London, United Kingdom&#34;, url: &#34;http://www.markhneedham.com&#34;, join_date: &#34;</description>
    </item>
    
    <item>
      <title>Jupyter: RuntimeError: This event loop is already running</title>
      <link>https://markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</link>
      <pubDate>Fri, 10 May 2019 23:00:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/</guid>
      <description>I&amp;#8217;ve been using the twint library to explore the Neo4j twitter community, and ran into an initially confusing error when I moved the code I&amp;#8217;d written into a Jupyter notebook.
 The first three cells of my notebook contain the following code:
 Cell 1:
 ! pip install twint   Cell 2:
 import json import twint   Cell 3:
 users = [&#34;vikatakavi11&#34;, &#34;tee_mars3&#34;] for username in users[:10]: c = twint.</description>
    </item>
    
    <item>
      <title>pyspark: Py4JJavaError: An error occurred while calling o138.loadClass.: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI</title>
      <link>https://markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</link>
      <pubDate>Wed, 17 Apr 2019 09:00:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/04/17/pyspark-class-not-found-exception-org-graphframes-graphframepythonapi/</guid>
      <description>I&amp;#8217;ve been building a Docker Container that has support for Jupyter, Spark, GraphFrames, and Neo4j, and ran into a problem that had me pulling my (metaphorical) hair out!
 The pyspark-notebook container gets us most of the way there, but it doesn&amp;#8217;t have GraphFrames or Neo4j support. Adding Neo4j is as simple as pulling in the Python Driver from Conda Forge, which leaves us with GraphFrames.
 When I&amp;#8217;m using GraphFrames with pyspark locally I would pull it in via the --packages config parameter, like this:</description>
    </item>
    
    <item>
      <title>Neo4j: Delete all nodes</title>
      <link>https://markhneedham.com/blog/2019/04/14/neo4j-delete-all-nodes/</link>
      <pubDate>Sun, 14 Apr 2019 12:52:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/04/14/neo4j-delete-all-nodes/</guid>
      <description>When experimenting with a new database, at some stage we&amp;#8217;ll probably want to delete all our data and start again. I was trying to do this with Neo4j over the weekend and it didn&amp;#8217;t work as I expected, so I thought I&amp;#8217;d write the lessons I learned.
 We&amp;#8217;ll be using Neo4j via the Neo4j Desktop with the default settings. This means that we have a maximum heap size of 1GB.</description>
    </item>
    
    <item>
      <title>Python: Getting GitHub download count from the GraphQL API using requests</title>
      <link>https://markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</link>
      <pubDate>Sun, 07 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/04/07/python-github-download-count-graphql-requests/</guid>
      <description>I was recently trying to use some code I shared just over a year ago to compute GitHub Project download numbers from the GraphQL API, and wanted to automate this in a Python script.
 It was more fiddly than I expected, so I thought I&amp;#8217;d share the code for the benefit of future me more than anything else!
 Pre requisites We&amp;#8217;re going to use the popular requests library to query the API, so we need to import that.</description>
    </item>
    
    <item>
      <title>Finding famous MPs based on their Wikipedia Page Views</title>
      <link>https://markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</link>
      <pubDate>Mon, 01 Apr 2019 05:03:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/04/01/famous-mps-wikipedia-pageviews/</guid>
      <description>As part of the Graphing Brexit series of blog posts, I wanted to work out who were the most important Members of the UK parliament, and after a bit of Googling I realised that views of their Wikipedia pages would do the trick.
 I initially found my way to tools.wmflabs.org, which is great for exploring the popularity of an individual MP, but not so good if you want to extract the data for 600 of them.</description>
    </item>
    
    <item>
      <title>Neo4j: From Graph Model to Neo4j Import</title>
      <link>https://markhneedham.com/blog/2019/03/27/from-graph-model-to-neo4j-import/</link>
      <pubDate>Wed, 27 Mar 2019 06:42:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/03/27/from-graph-model-to-neo4j-import/</guid>
      <description>In this post we&amp;#8217;re going to learn how to import the DBLP citation network into Neo4j using the Neo4j Import Tool.
 In case you haven&amp;#8217;t come across this dataset before, Tomaz Bratanic has a great blog post explaining it.
 The tl;dr is that we have articles, authors, and venues. Authors can write articles, articles can reference other articles, and articles are presented at a venue. Below is the graph model for this dataset:</description>
    </item>
    
    <item>
      <title>Neo4j: Delete/Remove dynamic properties</title>
      <link>https://markhneedham.com/blog/2019/03/14/neo4j-delete-dynamic-properties/</link>
      <pubDate>Thu, 14 Mar 2019 06:42:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/03/14/neo4j-delete-dynamic-properties/</guid>
      <description>Irfan and I were playing with a dataset earlier today, and having run a bunch of graph algorithms, we had a lot of properties that we wanted to clear out.
 The following Cypher query puts Neo4j into the state that we were dealing with.
 CREATE (:Node {name: &#34;Mark&#34;, pagerank: 2.302, louvain: 1, lpa: 4 }) CREATE (:Node {name: &#34;Michael&#34;, degree: 23, triangles: 12, betweeness: 48.70 }) CREATE (:Node {name: &#34;</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Date ranges</title>
      <link>https://markhneedham.com/blog/2019/01/13/neo4j-cypher-date-ranges/</link>
      <pubDate>Sun, 13 Jan 2019 06:42:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/01/13/neo4j-cypher-date-ranges/</guid>
      <description>As part of a dataset I&amp;#8217;ve been working with this week, I wanted to generate a collection of a range of dates using the Cypher query language.
 I&amp;#8217;ve previously used the duration function, which lets you add (or subtract) from a specific date, so I thought I&amp;#8217;d start from there. If we want to find the day after 1st January 2019, we could write the following query:
 neo4j&amp;gt; WITH date(&#34;</description>
    </item>
    
    <item>
      <title>Neo4j: APOC - Caused by: java.io.RuntimeException: Can&#39;t read url or key file (No such file or directory)</title>
      <link>https://markhneedham.com/blog/2019/01/12/neo4j-apoc-file-not-found-exception-no-such-file-directory/</link>
      <pubDate>Sat, 12 Jan 2019 19:05:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/01/12/neo4j-apoc-file-not-found-exception-no-such-file-directory/</guid>
      <description>I&amp;#8217;ve been using Neo4j&amp;#8217;s APOC library to load some local JSON files this week, and ran into an interesting problem.
 The LOAD CSV tool assumes that any files you load locally are in the import directory, so I&amp;#8217;ve got into the habit of putting my data there. Let&amp;#8217;s check what I&amp;#8217;m trying to import by opening the import directory:
   What&amp;#8217;s in there?
   Just the one JSON file needs processing.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Remove consecutive duplicates from a list</title>
      <link>https://markhneedham.com/blog/2019/01/12/neo4j-cypher-remove-consecutive-duplicates/</link>
      <pubDate>Sat, 12 Jan 2019 04:32:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/01/12/neo4j-cypher-remove-consecutive-duplicates/</guid>
      <description>I was playing with a dataset this week and wanted to share how I removes duplicate consecutive elements from a list using the Cypher query language.
 For simplicity&amp;#8217;s sake, imagine that we have this list:
 neo4j&amp;gt; return [1,2,3,3,4,4,4,5,3] AS values; +-----------------------------+ | values | +-----------------------------+ | [1, 2, 3, 3, 4, 4, 4, 5, 3] | +-----------------------------+   We want to remove the duplicate 3&amp;#8217;s and 4&amp;#8217;s, such that our end result should be:</description>
    </item>
    
    <item>
      <title>Python: Add query parameters to a URL</title>
      <link>https://markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</link>
      <pubDate>Fri, 11 Jan 2019 09:42:00 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2019/01/11/python-add-query-parameters-url/</guid>
      <description>I was recently trying to automate adding a query parameter to a bunch of URLS and came across a neat approach a long way down this StackOverflow answer, that uses the PreparedRequest class from the requests library.
 Let&amp;#8217;s first get the class imported:
 from requests.models import PreparedRequest req = PreparedRequest()   And now let&amp;#8217;s use use this class to add a query parameter to a URL. We can do this with the following code:</description>
    </item>
    
  </channel>
</rss>