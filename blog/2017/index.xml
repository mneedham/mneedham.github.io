<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2017s on Mark Needham</title>
    <link>https://markhneedham.com/blog/2017/</link>
    <description>Recent content in 2017s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Dec 2017 17:35:03 +0000</lastBuildDate>
    
	<atom:link href="https://markhneedham.com/blog/2017/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Leaflet: Fit polyline in view</title>
      <link>https://markhneedham.com/blog/2017/12/31/leaflet-fit-polyline-view/</link>
      <pubDate>Sun, 31 Dec 2017 17:35:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/31/leaflet-fit-polyline-view/</guid>
      <description>I&#39;ve been playing with the Leaflet.js library over the Christmas holidays to visualise running routes drawn onto the map using a Polyline and I wanted to zoom the map the right amount to see all the points. Pre requisites  We have the following HTML to define the div that will contain the map. &amp;lt;div id=&amp;quot;container&amp;quot;&amp;gt; &amp;lt;div id=&amp;quot;map&amp;quot; style=&amp;quot;width: 100%; height: 100%&amp;quot;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;   We also need to import the following Javascript and CSS files: &amp;lt;script src=&amp;quot;http://cdn.</description>
    </item>
    
    <item>
      <title>Ethereum Hello World Example using solc and web3</title>
      <link>https://markhneedham.com/blog/2017/12/28/ethereum-hello-world-example-using-solc-and-web3/</link>
      <pubDate>Thu, 28 Dec 2017 11:03:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/28/ethereum-hello-world-example-using-solc-and-web3/</guid>
      <description>I&#39;ve been trying to find an Ethereum Hello World example and came across Thomas Conté&#39;s excellent post that shows how to compile and deploy an Ethereum smart contract with solc and web3.  In the latest version of web3 the API has changed to be based on promises so I decided to translate Thomas&#39; example. Let&#39;s get started.
Install npm libraries  We need to install these libraries before we start: npm install web3 npm install abi-decoder npm install ethereumjs-testrpc  What do these libraries do?</description>
    </item>
    
    <item>
      <title>Morning Pages: What should I write about?</title>
      <link>https://markhneedham.com/blog/2017/12/27/morning-pages-write/</link>
      <pubDate>Wed, 27 Dec 2017 23:28:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/27/morning-pages-write/</guid>
      <description>I&#39;ve been journalling for almost 2 years now but some days I get stuck and can&#39;t think of anything to write about.
 I did a bit of searching to see if anybody had advice on solving this problem and found a few different articles:  The Productive Benefits of Journaling (plus 11 ideas for making the habit stick) Read This If You Want To Keep A Journal But Don’t Know How Turn Pain to Joy: 11 Tips for a Powerful Gratitude Journal Tips for Keeping a Gratitude Journal WTF Is A Bullet Journal And Why Should You Start One?</description>
    </item>
    
    <item>
      <title>scikit-learn: Using GridSearch to tune the hyper-parameters of VotingClassifier</title>
      <link>https://markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</link>
      <pubDate>Sun, 10 Dec 2017 07:55:43 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/</guid>
      <description>In my last blog post I showed how to create a multi class classification ensemble using scikit-learn&#39;s VotingClassifier and finished mentioning that I didn&#39;t know which classifiers should be part of the ensemble.  We need to get a better score with each of the classifiers in the ensemble otherwise they can be excluded.  We have a TF/IDF based classifier as well as well as the classifiers I wrote about in the last post.</description>
    </item>
    
    <item>
      <title>scikit-learn: Building a multi class classification ensemble</title>
      <link>https://markhneedham.com/blog/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</link>
      <pubDate>Tue, 05 Dec 2017 22:19:34 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/05/scikit-learn-building-multi-class-classification-ensemble/</guid>
      <description>For the Kaggle Spooky Author Identification I wanted to combine multiple classifiers together into an ensemble and found the VotingClassifier that does exactly that.  We need to predict the probability that a sentence is written by one of three authors so the VotingClassifier needs to make a &#39;soft&#39; prediction. If we only needed to know the most likely author we could have it make a &#39;hard&#39; prediction instead.  We start with three classifiers which generate different n-gram based features.</description>
    </item>
    
    <item>
      <title>Python: Combinations of values on and off</title>
      <link>https://markhneedham.com/blog/2017/12/03/python-combinations-values-off/</link>
      <pubDate>Sun, 03 Dec 2017 17:23:14 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/03/python-combinations-values-off/</guid>
      <description>In my continued exploration of Kaggle&#39;s Spooky Authors competition, I wanted to run a GridSearch turning on and off different classifiers to work out the best combination. I therefore needed to generate combinations of 1s and 0s enabling different classifiers. e.g. if we had 3 classifiers we&#39;d generate these combinations
0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1   where.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Property values can only be of primitive types or arrays thereof.</title>
      <link>https://markhneedham.com/blog/2017/12/01/neo4j-cypher-property-values-can-primitive-types-arrays-thereof/</link>
      <pubDate>Fri, 01 Dec 2017 22:09:17 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/01/neo4j-cypher-property-values-can-primitive-types-arrays-thereof/</guid>
      <description>I ran into an interesting Cypher error message earlier this week while trying to create an array property on a node which I thought I&#39;d share.  This was the Cypher query I wrote: CREATE (:Person {id: [1, &amp;quot;mark&amp;quot;, 2.0]})  which results in this error:
Neo.ClientError.Statement.TypeError Property values can only be of primitive types or arrays thereof.   We actually are storing an array of primitives but we have a mix of different types which isn&#39;t allowed.</description>
    </item>
    
    <item>
      <title>Python: Learning about defaultdict&#39;s handling of missing keys</title>
      <link>https://markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</link>
      <pubDate>Fri, 01 Dec 2017 15:26:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/12/01/python-learning-defaultdicts-handling-missing-keys/</guid>
      <description>While reading the scikit-learn code I came across a bit of code that I didn&#39;t understand for a while but in retrospect is quite neat.  This is the code snippet that intrigued me: vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__   Let&#39;s quickly see how it works by adapting an example from scikit-learn: &amp;gt;&amp;gt;&amp;gt; from collections import defaultdict &amp;gt;&amp;gt;&amp;gt; vocabulary = defaultdict() &amp;gt;&amp;gt;&amp;gt; vocabulary.default_factory = vocabulary.__len__ &amp;gt;&amp;gt;&amp;gt; vocabulary[&amp;quot;foo&amp;quot;] 0 &amp;gt;&amp;gt;&amp;gt; vocabulary.</description>
    </item>
    
    <item>
      <title>scikit-learn: Creating a matrix of named entity counts</title>
      <link>https://markhneedham.com/blog/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</link>
      <pubDate>Wed, 29 Nov 2017 23:01:38 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/29/scikit-learn-creating-a-matrix-of-named-entity-counts/</guid>
      <description>I&#39;ve been trying to improve my score on Kaggle&#39;s Spooky Author Identification competition, and my latest idea was building a model which used named entities extracted using the polyglot NLP library.  We&#39;ll start by learning how to extract entities form a sentence using polyglot which isn&#39;t too tricky: &amp;gt;&amp;gt;&amp;gt; from polyglot.text import Text &amp;gt;&amp;gt;&amp;gt; doc = &amp;quot;My name is David Beckham. Hello from London, England&amp;quot; &amp;gt;&amp;gt;&amp;gt; Text(doc, hint_language_code=&amp;quot;en&amp;quot;).entities [I-PER([&#39;David&#39;, &#39;Beckham&#39;]), I-LOC([&#39;London&#39;]), I-LOC([&#39;England&#39;])]   This sentence contains three entities.</description>
    </item>
    
    <item>
      <title>Python: polyglot - ModuleNotFoundError: No module named &#39;icu&#39;</title>
      <link>https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</link>
      <pubDate>Tue, 28 Nov 2017 19:52:13 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/</guid>
      <description>I wanted to use the polyglot NLP library that my colleague Will Lyon mentioned in his analysis of Russian Twitter Trolls but had installation problems which I thought I&#39;d share in case anyone else experiences the same issues.  I started by trying to install polyglot: $ pip install polyglot ImportError: No module named &#39;icu&#39;   Hmmm I&#39;m not sure what icu is but luckily there&#39;s a GitHub issue covering this problem.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: unsupported format string passed to numpy.ndarray.__format__</title>
      <link>https://markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</link>
      <pubDate>Sun, 19 Nov 2017 07:16:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/19/python-3-typeerror-unsupported-format-string-passed-to-numpy-ndarray-__format__/</guid>
      <description>This post explains how to work around a change in how Python string formatting works for numpy arrays between Python 2 and Python 3.  I&#39;ve been going through Kevin Markham&#39;s scikit-learn Jupyter notebooks and ran into a problem on the Cross Validation one, which was throwing this error when attempting to print the KFold example: Iteration Training set observations Testing set observations --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-28-007cbab507e3&amp;gt; in &amp;lt;module&amp;gt;() 6 print(&#39;{} {:^61} {}&#39;.</description>
    </item>
    
    <item>
      <title>Kubernetes: Copy a dataset to a StatefulSet&#39;s PersistentVolume</title>
      <link>https://markhneedham.com/blog/2017/11/18/kubernetes-copy-a-dataset-to-a-statefulsets-persistentvolume/</link>
      <pubDate>Sat, 18 Nov 2017 12:44:37 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/18/kubernetes-copy-a-dataset-to-a-statefulsets-persistentvolume/</guid>
      <description>In this post we&#39;ll learn how to copy an existing dataset to the PersistentVolumes used by a Neo4j cluster running on Kubernetes. Neo4j Clusters on Kubernetes  This posts assumes that we&#39;re familiar with deploying Neo4j on Kubernetes. I wrote an article on the Neo4j blog explaining this in more detail.  The StatefulSet we create for our core servers require persistent storage, achieved via the PersistentVolumeClaim (PVC) primitive. A Neo4j cluster containing 3 core servers would have the following PVCs: $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE datadir-neo-helm-neo4j-core-0 Bound pvc-043efa91-cc54-11e7-bfa5-080027ab9eac 10Gi RWO standard 45s datadir-neo-helm-neo4j-core-1 Bound pvc-1737755a-cc54-11e7-bfa5-080027ab9eac 10Gi RWO standard 13s datadir-neo-helm-neo4j-core-2 Bound pvc-18696bfd-cc54-11e7-bfa5-080027ab9eac 10Gi RWO standard 11s   Each of the PVCs has a corresponding PersistentVolume (PV) that satisifies it: $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-043efa91-cc54-11e7-bfa5-080027ab9eac 10Gi RWO Delete Bound default/datadir-neo-helm-neo4j-core-0 standard 41m pvc-1737755a-cc54-11e7-bfa5-080027ab9eac 10Gi RWO Delete Bound default/datadir-neo-helm-neo4j-core-1 standard 40m pvc-18696bfd-cc54-11e7-bfa5-080027ab9eac 10Gi RWO Delete Bound default/datadir-neo-helm-neo4j-core-2 standard 40m   The PVCs and PVs are usually created at the same time that we deploy our StatefulSet.</description>
    </item>
    
    <item>
      <title>Kubernetes 1.8: Using Cronjobs to take Neo4j backups</title>
      <link>https://markhneedham.com/blog/2017/11/17/kubernetes-1-8-using-cronjobs-take-neo4j-backups/</link>
      <pubDate>Fri, 17 Nov 2017 18:10:28 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/17/kubernetes-1-8-using-cronjobs-take-neo4j-backups/</guid>
      <description>With the release of Kubernetes 1.8 Cronjobs have graduated to beta, which means we can now more easily run Neo4j backup jobs against Kubernetes clusters. Before we learn how to write a Cronjob let&#39;s first create a local Kubernetes cluster and deploy Neo4j.
Spinup Kubernetes &amp; Helm minikube start --memory 8192 helm init &amp;amp;&amp;amp; kubectl rollout status -w deployment/tiller-deploy --namespace=kube-system  Deploy a Neo4j cluster helm repo add incubator https://kubernetes-charts-incubator.</description>
    </item>
    
    <item>
      <title>Neo4j Browser: Expected entity id to be an integral value</title>
      <link>https://markhneedham.com/blog/2017/11/06/neo4j-browser-expected-entity-id-integral-value/</link>
      <pubDate>Mon, 06 Nov 2017 16:17:35 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/11/06/neo4j-browser-expected-entity-id-integral-value/</guid>
      <description>I came across an interesting error while writing a Cypher query that used parameters in the Neo4j browser which I thought I should document for future me.  We&#39;ll start with a graph that has 1,000 people: unwind range(0,1000) AS id create (:Person {id: id})   Now we&#39;ll try and retrieve some of those people via a parameter lookup: :param ids: [0]  match (p:Person) where p.id in {ids} return p ╒════════╕ │&amp;quot;p&amp;quot; │ ╞════════╡ │{&amp;quot;id&amp;quot;:0}│ └────────┘   All good so far.</description>
    </item>
    
    <item>
      <title>Neo4j: Traversal query timeout</title>
      <link>https://markhneedham.com/blog/2017/10/31/neo4j-traversal-query-timeout/</link>
      <pubDate>Tue, 31 Oct 2017 21:43:17 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/10/31/neo4j-traversal-query-timeout/</guid>
      <description>I&#39;ve been spending some of my spare time over the last few weeks creating an application that generates running routes from Open Roads data - transformed and imported into Neo4j of course!  I&#39;ve created a user defined procedure which combines several shortest path queries, but I wanted to exit any of these shortest path searches if they were taking too long. My code without a timeout looks like this: StandardExpander orderedExpander = new OrderedByTypeExpander() .</description>
    </item>
    
    <item>
      <title>Kubernetes: Simple example of pod running</title>
      <link>https://markhneedham.com/blog/2017/10/21/kubernetes-simple-example-pod-running/</link>
      <pubDate>Sat, 21 Oct 2017 10:06:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/10/21/kubernetes-simple-example-pod-running/</guid>
      <description>I recently needed to create a Kubernetes pod that would &#39;just sit there&#39; while I used kube cp to copy some files to a persistent volume to which it was bound. I started out with this naive pod spec:
 pod_no_while.yaml kind: Pod apiVersion: v1 metadata: name: marks-dummy-pod spec: containers: - name: marks-dummy-pod image: ubuntu restartPolicy: Never  Let&#39;s apply that template:
$ kubectl apply -f pod_no_while.yaml pod &amp;quot;marks-dummy-pod&amp;quot; created   And let&#39;s check if we have any running pods: $ kubectl get pods No resources found, use --show-all to see completed objects.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Deleting duplicate nodes</title>
      <link>https://markhneedham.com/blog/2017/10/06/neo4j-cypher-deleting-duplicate-nodes/</link>
      <pubDate>Fri, 06 Oct 2017 16:13:33 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/10/06/neo4j-cypher-deleting-duplicate-nodes/</guid>
      <description>I had a problem on a graph I was working on recently where I&#39;d managed to create duplicate nodes because I hadn&#39;t applied any unique constraints.  I wanted to remove the duplicates, and came across Jimmy Ruts&#39; excellent post which shows some ways to do this.  Let&#39;s first create a graph with some duplicate nodes to play with: UNWIND range(0, 100) AS id CREATE (p1:Person {id: toInteger(rand() * id)}) MERGE (p2:Person {id: toInteger(rand() * id)}) MERGE (p3:Person {id: toInteger(rand() * id)}) MERGE (p4:Person {id: toInteger(rand() * id)}) CREATE (p1)-[:KNOWS]-&amp;gt;(p2) CREATE (p1)-[:KNOWS]-&amp;gt;(p3) CREATE (p1)-[:KNOWS]-&amp;gt;(p4) Added 173 labels, created 173 nodes, set 173 properties, created 5829 relationships, completed after 408 ms.</description>
    </item>
    
    <item>
      <title>AWS: Spinning up a Neo4j instance with APOC installed</title>
      <link>https://markhneedham.com/blog/2017/09/30/aws-spinning-up-a-neo4j-instance-with-apoc-installed/</link>
      <pubDate>Sat, 30 Sep 2017 21:23:11 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/30/aws-spinning-up-a-neo4j-instance-with-apoc-installed/</guid>
      <description>One of the first things I do after installing Neo4j is install the APOC library, but I find it&#39;s a bit of a manual process when spinning up a server on AWS so I wanted to simplify it a bit.  There&#39;s already a Neo4j AMI which installs Neo4j 3.2.0 and my colleague Michael pointed out that we could download APOC into the correct folder by writing a script and sending it as UserData.</description>
    </item>
    
    <item>
      <title>Serverless: Building a mini producer/consumer data pipeline with AWS SNS</title>
      <link>https://markhneedham.com/blog/2017/09/30/serverless-building-mini-producerconsumer-data-pipeline-aws-sns/</link>
      <pubDate>Sat, 30 Sep 2017 07:51:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/30/serverless-building-mini-producerconsumer-data-pipeline-aws-sns/</guid>
      <description>I wanted to create a little data pipeline with Serverless whose main use would be to run once a day, call an API, and load that data into a database.  It&#39;s mostly used to pull in recent data from that API, but I also wanted to be able to invoke it manually and specify a date range.  I created the following pair of lambdas that communicate with each other via an SNS topic.</description>
    </item>
    
    <item>
      <title>Serverless: S3 - S3BucketPermissions - Action does not apply to any resource(s) in statement</title>
      <link>https://markhneedham.com/blog/2017/09/29/serverless-s3-s3bucketpermissions-action-does-not-apply-to-any-resources-in-statement/</link>
      <pubDate>Fri, 29 Sep 2017 06:09:58 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/29/serverless-s3-s3bucketpermissions-action-does-not-apply-to-any-resources-in-statement/</guid>
      <description>I&#39;ve been playing around with S3 buckets with Serverless, and recently wrote the following code to create an S3 bucket and put a file into that bucket: const AWS = require(&amp;quot;aws-sdk&amp;quot;); let regionParams = { &#39;region&#39;: &#39;us-east-1&#39; } let s3 = new AWS.S3(regionParams); let s3BucketName = &amp;quot;marks-blog-bucket&amp;quot;; console.log(&amp;quot;Creating bucket: &amp;quot; + s3BucketName); let bucketParams = { Bucket: s3BucketName, ACL: &amp;quot;public-read&amp;quot; }; s3.createBucket(bucketParams).promise() .then(console.log) .catch(console.error); var putObjectParams = { Body: &amp;quot;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello blog!</description>
    </item>
    
    <item>
      <title>Python 3: Create sparklines using matplotlib</title>
      <link>https://markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</link>
      <pubDate>Sat, 23 Sep 2017 06:51:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/23/python-3-create-sparklines-using-matplotlib/</guid>
      <description>I recently wanted to create sparklines to show how some values were changing over time. In addition, I wanted to generate them as images on the server rather than introducing a JavaScript library.  Chris Seymour&#39;s excellent gist which shows how to create sparklines inside a Pandas dataframe got me most of the way there, but I had to tweak his code a bit to get it to play nicely with Python 3.</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Create Cypher map with dynamic keys</title>
      <link>https://markhneedham.com/blog/2017/09/19/neo4j-cypher-create-cypher-map-with-dynamic-keys/</link>
      <pubDate>Tue, 19 Sep 2017 19:30:09 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/09/19/neo4j-cypher-create-cypher-map-with-dynamic-keys/</guid>
      <description>I was recently trying to create a map in a Cypher query but wanted to have dynamic keys in that map. I started off with this query: WITH &amp;quot;a&amp;quot; as dynamicKey, &amp;quot;b&amp;quot; as dynamicValue RETURN { dynamicKey: dynamicValue } AS map ╒══════════════════╕ │&amp;quot;map&amp;quot; │ ╞══════════════════╡ │{&amp;quot;dynamicKey&amp;quot;:&amp;quot;b&amp;quot;}│ └──────────────────┘   Not quite what we want! We want dynamicKey to be evaluated rather than treated as a literal. As usual, APOC comes to the rescue!</description>
    </item>
    
    <item>
      <title>Neo4j: Cypher - Rounding of floating point numbers/BigDecimals</title>
      <link>https://markhneedham.com/blog/2017/08/13/neo4j-cypher-rounding-of-floating-point-numbersbigdecimals/</link>
      <pubDate>Sun, 13 Aug 2017 07:23:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/13/neo4j-cypher-rounding-of-floating-point-numbersbigdecimals/</guid>
      <description>I was doing some data cleaning a few days ago and wanting to multiply a value by 1 million. My Cypher code to do this looked like this: with &amp;quot;8.37&amp;quot; as rawNumeric RETURN toFloat(rawNumeric) * 1000000 AS numeric ╒═════════════════╕ │&amp;quot;numeric&amp;quot; │ ╞═════════════════╡ │8369999.999999999│ └─────────────────┘   Unfortunately that suffers from the classic rounding error when working with floating point numbers. I couldn&#39;t figure out a way to solve it using pure Cypher, but there tends to be an APOC function to solve every problem and this was no exception.</description>
    </item>
    
    <item>
      <title>Serverless: AWS HTTP Gateway - 502 Bad Gateway</title>
      <link>https://markhneedham.com/blog/2017/08/11/serverless-aws-http-gateway-502-bad-gateway/</link>
      <pubDate>Fri, 11 Aug 2017 16:01:50 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/11/serverless-aws-http-gateway-502-bad-gateway/</guid>
      <description>In my continued work with Serverless and AWS Lambda I ran into a problem when trying to call a HTTP gateway.  My project looked like this:  serverless.yaml service: http-gateway frameworkVersion: &amp;quot;&amp;gt;=1.2.0 &amp;lt;2.0.0&amp;quot; provider: name: aws runtime: python3.6 timeout: 180 functions: no-op: name: NoOp handler: handler.noop events: - http: POST noOp   handler.py def noop(event, context): return &amp;quot;hello&amp;quot;   Let&#39;s deploy to AWS: $ serverless deploy Serverless: Packaging service.</description>
    </item>
    
    <item>
      <title>Serverless: Python - virtualenv - { &#34;errorMessage&#34;: &#34;Unable to import module &#39;handler&#39;&#34; }</title>
      <link>https://markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</link>
      <pubDate>Sun, 06 Aug 2017 19:03:30 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/06/serverless-python-virtualenv-errormessage-unable-import-module-handler/</guid>
      <description>I&#39;ve been using the Serverless library to deploy and run some Python functions on AWS lambda recently and was initially confused about how to handle my dependencies.  I tend to create a new virtualenv for each of my project so let&#39;s get that setup first: Prerequisites $ npm install serverless  $ virtualenv -p python3 a $ . a/bin/activate   Now let&#39;s create our Serverless project. I&#39;m going to install the requests library so that I can use it in my function.</description>
    </item>
    
    <item>
      <title>AWS Lambda: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory&#39;</title>
      <link>https://markhneedham.com/blog/2017/08/03/aws-lambda-libld-linux-2-bad-elf-interpreter-no-file-directory/</link>
      <pubDate>Thu, 03 Aug 2017 17:24:16 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/03/aws-lambda-libld-linux-2-bad-elf-interpreter-no-file-directory/</guid>
      <description>I&#39;ve been working on an AWS lambda job to convert a HTML page to PDF using a Python wrapper around the wkhtmltopdf library but ended up with the following error when I tried to execute it: b&#39;/bin/sh: ./binary/wkhtmltopdf: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory\n&#39;: Exception Traceback (most recent call last): File &amp;quot;/var/task/handler.py&amp;quot;, line 33, in generate_certificate wkhtmltopdf(local_html_file_name, local_pdf_file_name) File &amp;quot;/var/task/lib/wkhtmltopdf.py&amp;quot;, line 64, in wkhtmltopdf wkhp.render() File &amp;quot;/var/task/lib/wkhtmltopdf.</description>
    </item>
    
    <item>
      <title>PHP vs Python: Generating a HMAC</title>
      <link>https://markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</link>
      <pubDate>Wed, 02 Aug 2017 06:09:10 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/08/02/php-vs-python-generating-a-hmac/</guid>
      <description>I&#39;ve been writing a bit of code to integrate with a ClassMarker webhook, and you&#39;re required to check that an incoming request actually came from ClassMarker by checking the value of a base64 hash using HMAC SHA256.  The example in the documentation is written in PHP which I haven&#39;t done for about 10 years so I had to figure out how to do the same thing in Python.  This is the PHP version: $ php -a php &amp;gt; echo base64_encode(hash_hmac(&amp;quot;sha256&amp;quot;, &amp;quot;my data&amp;quot;, &amp;quot;my_secret&amp;quot;, true)); vyniKpNSlxu4AfTgSJImt+j+pRx7v6m+YBobfKsoGhE=   The Python equivalent is a bit more code but it&#39;s not too bad.</description>
    </item>
    
    <item>
      <title>Docker: Building custom Neo4j images on Mac OS X</title>
      <link>https://markhneedham.com/blog/2017/07/26/docker-building-custom-neo4j-images-on-mac-os-x/</link>
      <pubDate>Wed, 26 Jul 2017 22:20:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/26/docker-building-custom-neo4j-images-on-mac-os-x/</guid>
      <description>I sometimes needs to create custom Neo4j Docker images to try things out and wanted to share my work flow, mostly for future Mark but also in case it&amp;rsquo;s useful to someone else.
There&amp;rsquo;s already a docker-neo4j repository so we&amp;rsquo;ll just tweak the files in there to achieve what we want.
$ git clone git@github.com:neo4j/docker-neo4j.git $ cd docker-neo4j  If we want to build a Docker image for Neo4j Enterprise Edition we can run the following build target:</description>
    </item>
    
    <item>
      <title>Pandas: ValueError: The truth value of a Series is ambiguous.</title>
      <link>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</link>
      <pubDate>Wed, 26 Jul 2017 21:41:55 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/26/pandas-valueerror-the-truth-value-of-a-series-is-ambiguous/</guid>
      <description>I&#39;ve been playing around with Kaggle in my spare time over the last few weeks and came across an unexpected behaviour when trying to add a column to a dataframe.  First let&#39;s get Panda&#39;s into our program scope: Prerequisites import pandas as pd   Now we&#39;ll create a data frame to play with for the duration of this post: &amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame({&amp;quot;a&amp;quot;: [1,2,3,4,5], &amp;quot;b&amp;quot;: [2,3,4,5,6]}) &amp;gt;&amp;gt;&amp;gt; df a b 0 5 2 1 6 6 2 0 8 3 3 2 4 1 6   Let&#39;s say we want to create a new column which returns True if either of the numbers are odd.</description>
    </item>
    
    <item>
      <title>Pandas/scikit-learn: get_dummies test/train sets - ValueError: shapes not aligned</title>
      <link>https://markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</link>
      <pubDate>Wed, 05 Jul 2017 15:42:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/05/pandasscikit-learn-get_dummies-testtrain-sets-valueerror-shapes-not-aligned/</guid>
      <description>I&#39;ve been using panda&#39;s get_dummies function to generate dummy columns for categorical variables to use with scikit-learn, but noticed that it sometimes doesn&#39;t work as I expect.
Prerequisites import pandas as pd import numpy as np from sklearn import linear_model  Let&#39;s say we have the following training and test sets:
Training set train = pd.DataFrame({&amp;quot;letter&amp;quot;:[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;], &amp;quot;value&amp;quot;: [1, 2, 3, 4]}) X_train = train.drop([&amp;quot;value&amp;quot;], axis=1) X_train = pd.</description>
    </item>
    
    <item>
      <title>Pandas: Find rows where column/field is null</title>
      <link>https://markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</link>
      <pubDate>Wed, 05 Jul 2017 14:31:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/07/05/pandas-find-rows-where-columnfield-is-null/</guid>
      <description>In my continued playing around with the Kaggle house prices dataset I wanted to find any columns/fields that have null values in.  If we want to get a count of the number of null fields by column we can use the following code, adapted from Poonam Ligade&#39;s kernel: Prerequisites import pandas as pd  Count the null columns train = pd.read_csv(&amp;quot;train.csv&amp;quot;) null_columns=train.columns[train.isnull().any()] train[null_columns].isnull().sum()  LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   So there are lots of different columns containing null values.</description>
    </item>
    
    <item>
      <title>Shell: Create a comma separated string</title>
      <link>https://markhneedham.com/blog/2017/06/23/shell-create-comma-separated-string/</link>
      <pubDate>Fri, 23 Jun 2017 12:26:49 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/23/shell-create-comma-separated-string/</guid>
      <description>I recently needed to generate a string with comma separated values, based on iterating a range of numbers.  e.g. we should get the following output where n = 3 foo-0,foo-1,foo-2  I only had the shell available to me so I couldn&#39;t shell out into Python or Ruby for example. That means it&#39;s bash scripting time!
 If we want to iterate a range of numbers and print them out on the screen we can write the following code: n=3 for i in $(seq 0 $(($n &amp;gt; 0?</description>
    </item>
    
    <item>
      <title>scikit-learn: Random forests - Feature Importance</title>
      <link>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</link>
      <pubDate>Fri, 16 Jun 2017 05:55:29 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/16/scikit-learn-random-forests-feature-importance/</guid>
      <description>As I mentioned in a blog post a couple of weeks ago, I&#39;ve been playing around with the Kaggle House Prices competition and the most recent thing I tried was training a random forest regressor.  Unfortunately, although it gave me better results locally it got a worse score on the unseen data, which I figured meant I&#39;d overfitted the model.  I wasn&#39;t really sure how to work out if that theory was true or not, but by chance I was reading Chris Albon&#39;s blog and found a post where he explains how to inspect the importance of every feature in a random forest.</description>
    </item>
    
    <item>
      <title>Kubernetes: Which node is a pod on?</title>
      <link>https://markhneedham.com/blog/2017/06/14/kubernetes-node-pod/</link>
      <pubDate>Wed, 14 Jun 2017 08:49:06 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/14/kubernetes-node-pod/</guid>
      <description>When running Kubernetes on a cloud provider, rather than locally using minikube, it&#39;s useful to know which node a pod is running on.  The normal command to list pods doesn&#39;t contain this information: $ kubectl get pod NAME READY STATUS RESTARTS AGE neo4j-core-0 1/1 Running 0 6m neo4j-core-1 1/1 Running 0 6m neo4j-core-2 1/1 Running 0 2m   I spent a while searching for a command that I could use before I came across Ta-Ching Chen&#39;s blog post while looking for something else.</description>
    </item>
    
    <item>
      <title>Kaggle: House Prices: Advanced Regression Techniques - Trying to fill in missing values</title>
      <link>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</link>
      <pubDate>Sun, 04 Jun 2017 09:22:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/06/04/kaggle-house-prices-advanced-regression-techniques-trying-fill-missing-values/</guid>
      <description>I&#39;ve been playing around with the data in Kaggle&#39;s House Prices: Advanced Regression Techniques and while replicating Poonam Ligade&#39;s exploratory analysis I wanted to see if I could create a model to fill in some of the missing values.  Poonam wrote the following code to identify which columns in the dataset had the most missing values: import pandas as pd train = pd.read_csv(&#39;train.csv&#39;) null_columns=train.columns[train.isnull().any()] &amp;gt;&amp;gt;&amp;gt; print(train[null_columns].isnull().sum()) LotFrontage 259 Alley 1369 MasVnrType 8 MasVnrArea 8 BsmtQual 37 BsmtCond 37 BsmtExposure 38 BsmtFinType1 37 BsmtFinType2 38 Electrical 1 FireplaceQu 690 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 PoolQC 1453 Fence 1179 MiscFeature 1406 dtype: int64   The one that I&#39;m most interested in is LotFrontage, which describes &#39;Linear feet of street connected to property&#39;.</description>
    </item>
    
    <item>
      <title>GraphQL-Europe: A trip to Berlin</title>
      <link>https://markhneedham.com/blog/2017/05/27/graphql-europe-trip-berlin/</link>
      <pubDate>Sat, 27 May 2017 11:31:08 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/05/27/graphql-europe-trip-berlin/</guid>
      <description>Last weekend my colleagues Will, Michael, Oskar, and I went to Berlin to spend Sunday at the GraphQL Europe conference in Berlin.   Neo4j sponsored the conference as we&#39;ve been experimenting with building a GraphQL to Neo4j integration and wanted to get some feedback from the community as well as learn what&#39;s going on in GraphQL land.
Will and Michael have written about their experience where they talk more about the hackathon we hosted so I&#39;ll cover it more from a personal perspective.</description>
    </item>
    
    <item>
      <title>PostgreSQL: ERROR:  argument of WHERE must not return a set</title>
      <link>https://markhneedham.com/blog/2017/05/01/postgresql-error-argument-must-not-return-set/</link>
      <pubDate>Mon, 01 May 2017 20:42:07 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/05/01/postgresql-error-argument-must-not-return-set/</guid>
      <description>In my last post I showed how to load and query data from the Strava API in PostgreSQL and after executing some simple queries my next task was to query more complex part of the JSON structure.   Strava allows users to create segments, which are edited portions of road or trail where athletes can compete for time.  I wanted to write a query to find all the times that I&#39;d run a particular segment.</description>
    </item>
    
    <item>
      <title>Loading and analysing Strava runs using PostgreSQL JSON data type</title>
      <link>https://markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</link>
      <pubDate>Mon, 01 May 2017 19:11:54 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/05/01/loading-and-analysing-strava-runs-using-postgresql-json-data-type/</guid>
      <description>In my last post I showed how to map Strava runs using data that I&#39;d extracted from their /activities API, but the API returns a lot of other data that I discarded because I wasn&#39;t sure what I should keep.  The API returns a nested JSON structure so the easiest solution would be to save each run as an individual file but I&#39;ve always wanted to try out PostgreSQL&#39;s JSON data type and this seemed like a good opportunity.</description>
    </item>
    
    <item>
      <title>Leaflet: Mapping Strava runs/polylines on Open Street Map</title>
      <link>https://markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</link>
      <pubDate>Sat, 29 Apr 2017 15:36:36 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/29/leaflet-strava-polylines-osm/</guid>
      <description>I&#39;m a big Strava user and spent a bit of time last weekend playing around with their API to work out how to map all my runs.  Strava API and polylines  This is a two step process:   Call the /athlete/activities/ endpoint to get a list of all my activities For each of those activities call /activities/[activityId] endpoint to get more detailed information for each activity   That second API returns a &#39;polyline&#39; property which the documentation describes as follows:  Activity and segment API requests may include summary polylines of their respective routes.</description>
    </item>
    
    <item>
      <title>Python: Flask - Generating a static HTML page</title>
      <link>https://markhneedham.com/blog/2017/04/27/python-flask-generating-a-static-html-page/</link>
      <pubDate>Thu, 27 Apr 2017 20:59:56 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/27/python-flask-generating-a-static-html-page/</guid>
      <description>Whenever I need to quickly spin up a web application Python&#39;s Flask library is my go to tool but I recently found myself wanting to generate a static HTML to upload to S3 and wondered if I could use it for that as well.
 It&#39;s actually not too tricky. If we&#39;re in the scope of the app context then we have access to the template rendering that we&#39;d normally use when serving the response to a web request.</description>
    </item>
    
    <item>
      <title>AWS Lambda: Programmatically scheduling a CloudWatchEvent</title>
      <link>https://markhneedham.com/blog/2017/04/05/aws-lambda-programatically-scheduling-a-cloudwatchevent/</link>
      <pubDate>Wed, 05 Apr 2017 23:49:45 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/05/aws-lambda-programatically-scheduling-a-cloudwatchevent/</guid>
      <description>I recently wrote a blog post showing how to create a Python &#39;Hello World&#39; AWS lambda function and manually invoke it, but what I really wanted to do was have it run automatically every hour.  To achieve that in AWS Lambda land we need to create a CloudWatch Event. The documentation describes them as follows:  Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</description>
    </item>
    
    <item>
      <title>AWS Lambda: Encrypted environment variables</title>
      <link>https://markhneedham.com/blog/2017/04/03/aws-lambda-encrypted-environment-variables/</link>
      <pubDate>Mon, 03 Apr 2017 05:49:53 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/03/aws-lambda-encrypted-environment-variables/</guid>
      <description>Continuing on from my post showing how to create a &#39;Hello World&#39; AWS lambda function I wanted to pass encrypted environment variables to my function.
The following function takes in both an encrypted and unencrypted variable and prints them out.
 Don&#39;t print out encrypted variables in a real function, this is just so we can see the example working! import boto3 import os from base64 import b64decode def lambda_handler(event, context): encrypted = os.</description>
    </item>
    
    <item>
      <title>AWS Lambda: Programatically create a Python &#39;Hello World&#39; function</title>
      <link>https://markhneedham.com/blog/2017/04/02/aws-lambda-programatically-create-a-python-hello-world-function/</link>
      <pubDate>Sun, 02 Apr 2017 22:11:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/04/02/aws-lambda-programatically-create-a-python-hello-world-function/</guid>
      <description>I&#39;ve been playing around with AWS Lambda over the last couple of weeks and I wanted to automate the creation of these functions and all their surrounding config. Let&#39;s say we have the following Hello World function: ~~~python def lambda_handler(event, context): print(&#34;Hello world&#34;) ~~~  To upload it to AWS we need to put it inside a zip file so let&#39;s do that: $ zip HelloWorld.zip HelloWorld.py  $ unzip -l HelloWorld.</description>
    </item>
    
    <item>
      <title>My top 10 technology podcasts</title>
      <link>https://markhneedham.com/blog/2017/03/30/top-10-technology-podcasts/</link>
      <pubDate>Thu, 30 Mar 2017 22:38:47 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/30/top-10-technology-podcasts/</guid>
      <description>For the last six months I&amp;#8217;ve been listening to 2 or 3 technology podcasts every day while out running and on my commute and I thought it&amp;#8217;d be cool to share some of my favourites.
I listen to all of these on the Podbean android app which seems pretty good. It can&amp;#8217;t read the RSS feeds of some podcasts but other than that it&amp;#8217;s worked well.
Anyway, on with the podcasts:</description>
    </item>
    
    <item>
      <title>Luigi: Defining dynamic requirements (on output files)</title>
      <link>https://markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</link>
      <pubDate>Tue, 28 Mar 2017 05:39:04 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/28/luigi-defining-dynamic-requirements-on-output-files/</guid>
      <description>In my last blog post I showed how to convert a JSON document containing meetup groups into a CSV file using Luigi, the Python library for building data pipelines. As well as creating that CSV file I wanted to go back to the meetup.com API and download all the members of those groups.
This was a rough flow of what i wanted to do:
  Take JSON document containing all groups   Parse that document and for each group:    Call the /members endpoint   Save each one of those files as a JSON file    Iterate over all those JSON files and create a members CSV file   In the previous post we created the GroupsToJSON task which calls the /groups endpoint on the meetup API and creates the file /tmp/groups.</description>
    </item>
    
    <item>
      <title>Luigi: An ExternalProgramTask example - Converting JSON to CSV</title>
      <link>https://markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</link>
      <pubDate>Sat, 25 Mar 2017 14:09:59 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/25/luigi-externalprogramtask-example-converting-json-csv/</guid>
      <description>I&#39;ve been playing around with the Python library Luigi which is used to build pipelines of batch jobs and I struggled to find an example of an ExternalProgramTask so this is my attempt at filling that void.   I&#39;m building a little data pipeline to get data from the meetup.com API and put it into CSV files that can be loaded into Neo4j using the LOAD CSV command.  The first task I created calls the /groups endpoint and saves the result into a JSON file: import luigi import requests import json from collections import Counter class GroupsToJSON(luigi.</description>
    </item>
    
    <item>
      <title>Python 3: TypeError: Object of type &#39;dict_values&#39; is not JSON serializable</title>
      <link>https://markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</link>
      <pubDate>Sun, 19 Mar 2017 16:40:03 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/19/python-3-typeerror-object-type-dict_values-not-json-serializable/</guid>
      <description>I&#39;ve recently upgraded to Python 3 (I know, took me a while!) and realised that one of my scripts that writes JSON to a file no longer works!  This is a simplified version of what I&#39;m doing: &amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; x = {&amp;quot;mark&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Mark&amp;quot;}, &amp;quot;michael&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Michael&amp;quot;} } &amp;gt;&amp;gt;&amp;gt; json.dumps(x.values()) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py&amp;quot;, line 231, in dumps return _default_encoder.</description>
    </item>
    
    <item>
      <title>Neo4j: apoc.date.parse - java.lang.IllegalArgumentException: Illegal pattern character &#39;T&#39; / java.text.ParseException: Unparseable date: &#34;2012-11-12T08:46:15Z&#34;</title>
      <link>https://markhneedham.com/blog/2017/03/06/neo4j-apoc-date-parse-java-lang-illegalargumentexception-illegal-pattern-character-t-java-text-parseexception-unparseable-date-2012-11-12t084615z/</link>
      <pubDate>Mon, 06 Mar 2017 20:52:01 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/03/06/neo4j-apoc-date-parse-java-lang-illegalargumentexception-illegal-pattern-character-t-java-text-parseexception-unparseable-date-2012-11-12t084615z/</guid>
      <description>I often find myself wanting to convert date strings into Unix timestamps using Neo4j&#39;s APOC library and unfortunately some sources don&#39;t use the format that apoc.date.parse expects. e.g.
return apoc.date.parse(&amp;quot;2012-11-12T08:46:15Z&amp;quot;,&#39;s&#39;) AS ts Failed to invoke function `apoc.date.parse`: Caused by: java.lang.IllegalArgumentException: java.text.ParseException: Unparseable date: &amp;quot;2012-11-12T08:46:15Z&amp;quot;  We need to define the format explicitly so the SimpleDataFormat documentation comes in handy. I tried the following: return apoc.date.parse(&amp;quot;2012-11-12T08:46:15Z&amp;quot;,&#39;s&#39;,&amp;quot;yyyy-MM-ddTHH:mm:ssZ&amp;quot;) AS ts Failed to invoke function `apoc.</description>
    </item>
    
    <item>
      <title>Neo4j: Graphing the &#39;My name is...I work&#39; Twitter meme</title>
      <link>https://markhneedham.com/blog/2017/02/28/neo4j-graphing-name-work-twitter-meme/</link>
      <pubDate>Tue, 28 Feb 2017 15:50:27 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/02/28/neo4j-graphing-name-work-twitter-meme/</guid>
      <description>Over the last few days I&#39;ve been watching the chain of &#39;My name is...&#39; tweets kicked off by DHH with interest. As I understand it, the idea is to show that coding interview riddles/hard tasks on a whiteboard are ridiculous. Hello, my name is David. I would fail to write bubble sort on a whiteboard. I look code up on the internet all the time. I don&amp;#39;t do riddles.</description>
    </item>
    
    <item>
      <title>Neo4j: How do null values even work?</title>
      <link>https://markhneedham.com/blog/2017/02/22/neo4j-null-values-even-work/</link>
      <pubDate>Wed, 22 Feb 2017 23:28:23 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/02/22/neo4j-null-values-even-work/</guid>
      <description>Every now and then I find myself wanting to import a CSV file into Neo4j and I always get confused with how to handle the various null values that can lurk within.  Let&#39;s start with an example that doesn&#39;t have a CSV file in sight. Consider the following list and my attempt to only return null values: WITH [null, &amp;quot;null&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;Mark&amp;quot;] AS values UNWIND values AS value WITH value WHERE value = null RETURN value (no changes, no records)   Hmm that&#39;s weird.</description>
    </item>
    
    <item>
      <title>Neo4j: Analysing a CSV file using LOAD CSV and Cypher</title>
      <link>https://markhneedham.com/blog/2017/02/19/neo4j-analysing-csv-file-using-load-csv-cypher/</link>
      <pubDate>Sun, 19 Feb 2017 22:39:05 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/02/19/neo4j-analysing-csv-file-using-load-csv-cypher/</guid>
      <description>Last week we ran our first online meetup for several years and I wanted to wanted to analyse the stats that YouTube lets you download for an event.  The file I downloaded looked like this: $ cat ~/Downloads/youtube_stats_pW9boJoUxO0.csv Video IDs:, pW9boJoUxO0, Start time:, Wed Feb 15 08:57:55 2017, End time:, Wed Feb 15 10:03:10 2017 Playbacks, Peak concurrent viewers, Total view time (hours), Average session length (minutes) 348, 112, 97.</description>
    </item>
    
    <item>
      <title>ReactJS/Material-UI: Cannot resolve module &#39;material-ui/lib/&#39;</title>
      <link>https://markhneedham.com/blog/2017/02/12/reactjsmaterial-ui-cannot-resolve-module-material-uilib/</link>
      <pubDate>Sun, 12 Feb 2017 22:43:53 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/02/12/reactjsmaterial-ui-cannot-resolve-module-material-uilib/</guid>
      <description>I&#39;ve been playing around with ReactJS and the Material-UI library over the weekend and ran into this error while trying to follow one of the example from the demo application: ERROR in ./src/app/modules/Foo.js Module not found: Error: Cannot resolve module &#39;material-ui/lib/Subheader&#39; in /Users/markneedham/neo/reactjs-test/src/app/modules @ ./src/app/modules/Foo.js 13:17-53 webpack: Failed to compile.  This was the component code:
import React from &#39;react&#39; import Subheader from &#39;material-ui/lib/Subheader&#39; export default React.createClass({ render() { return &amp;lt;div&amp;gt; &amp;lt;Subheader&amp;gt;Some Text&amp;lt;/Subheader&amp;gt; &amp;lt;/div&amp;gt; } })   which is then rendered like this: import Foo from &#39;.</description>
    </item>
    
    <item>
      <title>Go: Multi-threaded writing to a CSV file</title>
      <link>https://markhneedham.com/blog/2017/01/31/go-multi-threaded-writing-csv-file/</link>
      <pubDate>Tue, 31 Jan 2017 05:57:11 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/01/31/go-multi-threaded-writing-csv-file/</guid>
      <description>As part of a Go script I&amp;rsquo;ve been working on I wanted to write to a CSV file from multiple Go routines, but realised that the built in CSV Writer isn&amp;rsquo;t thread safe.
My first attempt at writing to the CSV file looked like this:
package main import ( &amp;quot;encoding/csv&amp;quot; &amp;quot;os&amp;quot; &amp;quot;log&amp;quot; &amp;quot;strconv&amp;quot; ) func main() { csvFile, err := os.Create(&amp;quot;/tmp/foo.csv&amp;quot;) if err != nil { log.Panic(err) } w := csv.</description>
    </item>
    
    <item>
      <title>Go vs Python: Parsing a JSON response from a HTTP API</title>
      <link>https://markhneedham.com/blog/2017/01/21/go-vs-python-parsing-a-json-response-from-a-http-api/</link>
      <pubDate>Sat, 21 Jan 2017 10:49:46 +0000</pubDate>
      
      <guid>https://markhneedham.com/blog/2017/01/21/go-vs-python-parsing-a-json-response-from-a-http-api/</guid>
      <description>As part of a recommendations with Neo4j talk that I&amp;rsquo;ve presented a few times over the last year I have a set of scripts that download some data from the meetup.com API.
They&amp;rsquo;re all written in Python but I thought it&amp;rsquo;d be a fun exercise to see what they&amp;rsquo;d look like in Go. My eventual goal is to try and parallelise the API calls.
This is the Python version of the script:</description>
    </item>
    
  </channel>
</rss>