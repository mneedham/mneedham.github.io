<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2021s on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/2021/</link>
    <description>Recent content in 2021s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jul 2021 00:44:37 +0000</lastBuildDate>
    
	<atom:link href="https://www.markhneedham.com/blog/2021/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Apache Pinot: BadQueryRequestException - Cannot convert value to type: LONG</title>
      <link>https://www.markhneedham.com/blog/2021/07/16/pinot-bad-query-request-exception-cannot-convert-value-long/</link>
      <pubDate>Fri, 16 Jul 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/07/16/pinot-bad-query-request-exception-cannot-convert-value-long/</guid>
      <description>In my continued exploration of Apache Pinot I&amp;#8217;ve been trying out the GitHub events recipe , which imports data from the GitHub events stream into Pinot. In this blog post I want to show how I worked around an exception I was getting when trying to filter the data by one of the timestamp&amp;#8217;s column.
 Setup We&amp;#8217;re going to spin up a local instance of Pinot using the following Docker compose config:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Analysing England&#39;s Covid case data</title>
      <link>https://www.markhneedham.com/blog/2021/06/22/pinot-analysing-england-covid-cases/</link>
      <pubDate>Tue, 22 Jun 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/06/22/pinot-analysing-england-covid-cases/</guid>
      <description>As I mentioned in my last blog post, I&amp;#8217;ve been playing around with Apache Pinot, a data store that&amp;#8217;s optimised for user facing analytical workloads.
   My understanding is that Pinot is a really good fit for datasets where:
  The query patterns are of an analytical nature e.g. slicing and dicing on any columns.
  We&amp;#8217;re ingesting the data in real time from a stream of events.</description>
    </item>
    
    <item>
      <title>Apache Pinot: {&#39;errorCode&#39;: 410, &#39;message&#39;: &#39;BrokerResourceMissingError&#39;}</title>
      <link>https://www.markhneedham.com/blog/2021/06/21/pinot-broker-resource-missing/</link>
      <pubDate>Mon, 21 Jun 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/06/21/pinot-broker-resource-missing/</guid>
      <description>I&amp;#8217;ve recently been playing around with Apache Pinot, a realtime analytical data store that&amp;#8217;s used for user facing analytics use cases. In this blog post I want to walk through some challenges I had connecting to Pinot using the Python driver and how I got things working.
 I&amp;#8217;m running Pinot locally using the Docker image, which I setup in a Docker compose file:
 docker-compose.yml version: &#39;3.7&#39; services: pinot: image: apachepinot/pinot:0.</description>
    </item>
    
    <item>
      <title>jq: Select multiple keys</title>
      <link>https://www.markhneedham.com/blog/2021/05/19/jq-select-multiple-keys/</link>
      <pubDate>Wed, 19 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/19/jq-select-multiple-keys/</guid>
      <description>I recently started a new job, working for a FinTech company called Finbourne, who build a data platform for investment data. It&amp;#8217;s an API first product that publishes a Swagger API JSON file that I&amp;#8217;ve been trying to parse to get a list of the end points and their operation ids. In this blog post I&amp;#8217;ll show how I&amp;#8217;ve been parsing that file using jq, my favourite tool for parsing JSON files.</description>
    </item>
    
    <item>
      <title>Pandas: Add row to DataFrame</title>
      <link>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</link>
      <pubDate>Thu, 13 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</guid>
      <description>Usually when I&amp;#8217;m working with Pandas DataFrames I want to add new columns of data, but I recently wanted to add a row to an existing DataFrame. It turns out there are more than one ways to do that, which we&amp;#8217;ll explore in this blog post.
 Let&amp;#8217;s start by importing Pandas into our Python script:
 import pandas as pd   We&amp;#8217;ll start from a DataFrame that has two rows and the columns name and age:</description>
    </item>
    
    <item>
      <title>Altair/Pandas: TypeError: Cannot interpret &#39;Float64Dtype()&#39; as a data type</title>
      <link>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</link>
      <pubDate>Wed, 28 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</guid>
      <description>I ran into an interesting problem when trying to use Altair to visualise a Pandas DataFrame containing vaccination rates of different parts of England. In this blog post we&amp;#8217;ll look at how to work around this issue.
 First, let&amp;#8217;s install Pandas, numpy, and altair:
 pip install pandas altair numpy   And now we&amp;#8217;ll import those modules into a Python script or Jupyter notebook:
 import pandas as pd import altair as alt import numpy as np   Next, we&amp;#8217;ll create a DataFrame containing the vaccinations rates of a couple of regions:</description>
    </item>
    
    <item>
      <title>Pandas: Compare values in DataFrame to previous days</title>
      <link>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</link>
      <pubDate>Wed, 21 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</guid>
      <description>I&amp;#8217;m still playing around with Covid vaccine data, this time exploring how the number of doses varies week by week. I want to know how many more (or less) vaccines have been done on a given day compared to that same day last week.
 We&amp;#8217;ll be using Pandas in this blog post, so let&amp;#8217;s first install that library and import it:
 Install Pandas pip install pandas   Import module import pandas as pd   And now let&amp;#8217;s create a DataFrame containing a subset of the data that I&amp;#8217;m working with:</description>
    </item>
    
    <item>
      <title>Vaccinating England: The Data (cleanup)</title>
      <link>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</link>
      <pubDate>Sat, 17 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</guid>
      <description>Over the last 13 months I&amp;#8217;ve spent countless hours looking at dashboards that showed Coronavirus infection rates, death rates, and numbers of people vaccinated. The UK government host a dashboard at coronavirus.data.gov.uk, which contains charts and tables showing all of the above.
 One thing I haven&amp;#8217;t been able to find, however, is a drill down of vaccinations by local area and age group. So I&amp;#8217;m going to try to build my own!</description>
    </item>
    
    <item>
      <title>Pandas - Format DataFrame numbers with commas and control decimal places</title>
      <link>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</link>
      <pubDate>Sun, 11 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</guid>
      <description>I&amp;#8217;m still playing around with the UK&amp;#8217;s COVID-19 vaccination data and in this blog post we&amp;#8217;ll learn how to format a DataFrame that contains a mix of string and numeric values.
 We&amp;#8217;ll be using Pandas&#39; styling functionality, which generates CSS and HTML, so if you want to follow along you&amp;#8217;ll need to install Pandas and Jupyter:
 pip install pandas jupyter   Next, launch Jupyter and create a notebook:</description>
    </item>
    
    <item>
      <title>Pandas - Dividing two DataFrames (TypeError: unsupported operand type(s) for /: &#39;str&#39; and &#39;str&#39;)</title>
      <link>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</link>
      <pubDate>Thu, 08 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</guid>
      <description>I&amp;#8217;ve been doing some more exploration of the UK Coronavirus vaccine data, this time looking at the number of people vaccinated by Local Tier Local Authority. The government publish data showing the number of people vaccinated in each authority by age group, as well as population estimates for each cohort.
 Having loaded that data into two Pandas DataFrames, I wanted to work out the % of people vaccinated per age group per local area.</description>
    </item>
    
    <item>
      <title>Altair - Remove margin/padding on discrete X axis</title>
      <link>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</link>
      <pubDate>Fri, 02 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</guid>
      <description>One of the Altair charts on my Covid Vaccine Dashboards Streamlit app shows the % of first doses, but when I first created it there was some padding on the X axis that I wanted to remove. In this blog post we&amp;#8217;ll learn how to do that.
 Pre requisites Let&amp;#8217;s start by installing the following libraries:
 pip install pandas altair altair_viewer   Next let&amp;#8217;s import them, as shown below:</description>
    </item>
    
    <item>
      <title>Pandas: Filter column value in array/list - ValueError: The truth value of a Series is ambiguous</title>
      <link>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</link>
      <pubDate>Sun, 28 Mar 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</guid>
      <description>The UK government publishes Coronavirus vaccinations data on coronavirus.data.gov.uk, but I wanted to create some different visualisations so I downloaded the data and have been playing with it in the mneedham/covid-vaccines GitHub repository.
 I massaged the data so that I have rows in a Pandas DataFrame representing the numbers of first doses, second doses, and total doses done each day. I then wanted to filter this DataFrame based on the type of dose, but initially got a bit stuck.</description>
    </item>
    
    <item>
      <title>Neo4j Graph Data Science 1.5: Exploring the Speaker-Listener LPA Overlapping Community Detection Algorithm</title>
      <link>https://www.markhneedham.com/blog/2021/02/08/neo4j-gdsl-overlapping-community-detection-sllpa/</link>
      <pubDate>Mon, 08 Feb 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/02/08/neo4j-gdsl-overlapping-community-detection-sllpa/</guid>
      <description>The Neo4j Graph Data Science Library provides efficiently implemented, parallel versions of common graph algorithms for Neo4j, exposed as Cypher procedures. It recently published version 1.5, which introduces some fun new algorithms.
   In this blog post, we&amp;#8217;re going to explore the newly added Speaker-Listener Label Propagation algorithm with the help of a twitter dataset.
 Launching Neo4j We&amp;#8217;re going to run Neo4j with the Graph Data Science Library using the following Docker Compose configuration:</description>
    </item>
    
    <item>
      <title>Neo4j Graph Data Science 1.5: Exploring the HITS Algorithm</title>
      <link>https://www.markhneedham.com/blog/2021/02/03/neo4j-gdsl-hits-algorithm/</link>
      <pubDate>Wed, 03 Feb 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/02/03/neo4j-gdsl-hits-algorithm/</guid>
      <description>The Neo4j Graph Data Science Library provides efficiently implemented, parallel versions of common graph algorithms for Neo4j, exposed as Cypher procedures. It recently published version 1.5, which has lots of goodies to play with.
   In this blog post, we&amp;#8217;re going to explore the newly added HITS algorithm with the help of a citations dataset.
 Launching Neo4j We&amp;#8217;re going to run Neo4j with the Graph Data Science Library using the following Docker Compose configuration:</description>
    </item>
    
    <item>
      <title>Materialize: Creating multiple views on one source</title>
      <link>https://www.markhneedham.com/blog/2021/01/28/materialize-multiple-views-one-source/</link>
      <pubDate>Thu, 28 Jan 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/01/28/materialize-multiple-views-one-source/</guid>
      <description>This is another post describing my exploration of Materialize, a SQL streaming database. In this post we&amp;#8217;re going to learn how to create multiple views on top of the same underlying source.
   We&amp;#8217;re still going to be using data extracted from Strava, an app that I use to record my runs, but this time we have more detailed information about each run. As in the previous blog posts, each run is represented as JSON document and store in the activities-detailed-all.</description>
    </item>
    
  </channel>
</rss>