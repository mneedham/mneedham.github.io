<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2021s on Mark Needham</title>
    <link>https://www.markhneedham.com/blog/2021/</link>
    <description>Recent content in 2021s on Mark Needham</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Dec 2021 00:44:37 +0000</lastBuildDate><atom:link href="https://www.markhneedham.com/blog/2021/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Apache Pinot: Exploring range queries</title>
      <link>https://www.markhneedham.com/blog/2021/12/07/apache-pinot-exploring-range-queries/</link>
      <pubDate>Tue, 07 Dec 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/12/07/apache-pinot-exploring-range-queries/</guid>
      <description>In our last post about the Chicago Crimes dataset and Apache Pinot, we learnt how to use various indexes to filter columns by exact values. In this post we’re going to learn how to write range queries against the dataset.
Figure 1. Apache Pinot - Range Queries Recap To recap, the Chicago Crimes dataset contains more than 7 million crimes committed in Chicago from 2001 until today. For each crime we have various identifiers, a timestamp, location, and codes reprsenting the type of crime that’s been committed.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Copying a segment to a new table</title>
      <link>https://www.markhneedham.com/blog/2021/12/06/apache-pinot-copy-segment-new-table/</link>
      <pubDate>Mon, 06 Dec 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/12/06/apache-pinot-copy-segment-new-table/</guid>
      <description>In this post we’ll learn how to use the same Pinot segment in multiple tables.
Figure 1. Apache Pinot - Copy segment to another table Setup First, we’re going to spin up a local instance of Pinot using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: zookeeper: image: zookeeper:3.5.6 hostname: zookeeper container_name: manual-zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 pinot-controller: image: apachepinot/pinot:0.9.0 command: &amp;#34;StartController -zkAddress manual-zookeeper:2181&amp;#34; container_name: &amp;#34;manual-pinot-controller&amp;#34; volumes: - .</description>
    </item>
    
    <item>
      <title>Apache Pinot: Convert DateTime string to Timestamp - IllegalArgumentException: Invalid timestamp</title>
      <link>https://www.markhneedham.com/blog/2021/12/03/apache-pinot-convert-datetime-string-timestamp-invalid-timestamp/</link>
      <pubDate>Fri, 03 Dec 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/12/03/apache-pinot-convert-datetime-string-timestamp-invalid-timestamp/</guid>
      <description>In this post we’ll learn how to deal with a field that contains DateTime strings when importing a CSV file into Apache Pinot. We’ll also cover some of the error messages that you’ll see if you do it the wrong way.
Figure 1. Apache Pinot - Convert DateTime string to Timestamp Setup We’re going to spin up a local instance of Pinot using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Exploring indexing techniques on Chicago Crimes</title>
      <link>https://www.markhneedham.com/blog/2021/11/30/apache-pinot-exploring-index-chicago-crimes/</link>
      <pubDate>Tue, 30 Nov 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/11/30/apache-pinot-exploring-index-chicago-crimes/</guid>
      <description>In Neha Pawar’s recent blog post, What Makes Apache Pinot fast?, she summarises it with the following sentence:
At the heart of the system, Pinot is a columnar store with several smart optimizations that can be applied at various stages of the query by the different Pinot components. Some of the most commonly used and impactful optimizations are data partitioning strategies, segment assignment strategies, smart query routing techniques, a rich set of indexes for filter optimizations, and aggregation optimization techniques.</description>
    </item>
    
    <item>
      <title>Apache Pinot: Importing CSV files with columns containing spaces</title>
      <link>https://www.markhneedham.com/blog/2021/11/25/apache-pinot-csv-columns-spaces/</link>
      <pubDate>Thu, 25 Nov 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/11/25/apache-pinot-csv-columns-spaces/</guid>
      <description>I’ve been playing around with one of my favourite datasets from the Chicago Data Portal and spent a while figuring out how to import columns that contain spaces into Apache Pinot. In this blog post we’ll learn how to do that using a subset of the data.
Setup We’re going to spin up a local instance of Pinot using the following Docker compose config:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: zookeeper: image: zookeeper:3.</description>
    </item>
    
    <item>
      <title>Apache Pinot: org.apache.helix.HelixException: Cluster structure is not set up for cluster: PinotCluster</title>
      <link>https://www.markhneedham.com/blog/2021/11/23/apache-pinot-helix-exception-cluster-structure-not-set-up/</link>
      <pubDate>Tue, 23 Nov 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/11/23/apache-pinot-helix-exception-cluster-structure-not-set-up/</guid>
      <description>In my continued exploration of Apache Pinot, I wanted to spin up all the components individually rather than relying on one of the QuickStarts that takes care of that for me. In doing so I came across an interesting error that we’ll explore in this post.
Setup We’re going to spin up a local instance of Pinot using the following Docker compose config:
version: &amp;#39;3.7&amp;#39; services: zookeeper: image: zookeeper:3.5.6 hostname: zookeeper container_name: manual-zookeeper ports: - &amp;#34;2181:2181&amp;#34; environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 pinot-controller: image: apachepinot/pinot:0.</description>
    </item>
    
    <item>
      <title>Apache Pinot: BadQueryRequestException - Cannot convert value to type: LONG</title>
      <link>https://www.markhneedham.com/blog/2021/07/16/pinot-bad-query-request-exception-cannot-convert-value-long/</link>
      <pubDate>Fri, 16 Jul 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/07/16/pinot-bad-query-request-exception-cannot-convert-value-long/</guid>
      <description>In my continued exploration of Apache Pinot I’ve been trying out the GitHub events recipe , which imports data from the GitHub events stream into Pinot. In this blog post I want to show how I worked around an exception I was getting when trying to filter the data by one of the timestamp’s column.
Setup We’re going to spin up a local instance of Pinot using the following Docker compose config:</description>
    </item>
    
    <item>
      <title>Apache Pinot: Analysing England&#39;s Covid case data</title>
      <link>https://www.markhneedham.com/blog/2021/06/22/pinot-analysing-england-covid-cases/</link>
      <pubDate>Tue, 22 Jun 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/06/22/pinot-analysing-england-covid-cases/</guid>
      <description>As I mentioned in my last blog post, I’ve been playing around with Apache Pinot, a data store that’s optimised for user facing analytical workloads.
My understanding is that Pinot is a really good fit for datasets where:
The query patterns are of an analytical nature e.g. slicing and dicing on any columns.
We’re ingesting the data in real time from a stream of events. Kenny Bastani has some cool blog posts showing how to do this with Wikipedia and GitHub, and Jackie Jiang showed how to analyse Meetup’s RSVP stream in last week’s Pinot meeetup.</description>
    </item>
    
    <item>
      <title>Apache Pinot: {&#39;errorCode&#39;: 410, &#39;message&#39;: &#39;BrokerResourceMissingError&#39;}</title>
      <link>https://www.markhneedham.com/blog/2021/06/21/pinot-broker-resource-missing/</link>
      <pubDate>Mon, 21 Jun 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/06/21/pinot-broker-resource-missing/</guid>
      <description>I’ve recently been playing around with Apache Pinot, a realtime analytical data store that’s used for user facing analytics use cases. In this blog post I want to walk through some challenges I had connecting to Pinot using the Python driver and how I got things working.
I’m running Pinot locally using the Docker image, which I setup in a Docker compose file:
docker-compose.yml version: &amp;#39;3.7&amp;#39; services: pinot: image: apachepinot/pinot:0.</description>
    </item>
    
    <item>
      <title>jq: Select multiple keys</title>
      <link>https://www.markhneedham.com/blog/2021/05/19/jq-select-multiple-keys/</link>
      <pubDate>Wed, 19 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/19/jq-select-multiple-keys/</guid>
      <description>I recently started a new job, working for a FinTech company called Finbourne, who build a data platform for investment data. It’s an API first product that publishes a Swagger API JSON file that I’ve been trying to parse to get a list of the end points and their operation ids. In this blog post I’ll show how I’ve been parsing that file using jq, my favourite tool for parsing JSON files.</description>
    </item>
    
    <item>
      <title>Pandas: Add row to DataFrame</title>
      <link>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</link>
      <pubDate>Thu, 13 May 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/05/13/pandas-add-row-to-dataframe-with-index/</guid>
      <description>Usually when I’m working with Pandas DataFrames I want to add new columns of data, but I recently wanted to add a row to an existing DataFrame. It turns out there are more than one ways to do that, which we’ll explore in this blog post.
Let’s start by importing Pandas into our Python script:
import pandas as pd We’ll start from a DataFrame that has two rows and the columns name and age:</description>
    </item>
    
    <item>
      <title>Altair/Pandas: TypeError: Cannot interpret &#39;Float64Dtype()&#39; as a data type</title>
      <link>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</link>
      <pubDate>Wed, 28 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/28/altair-pandas-cannot-interpret-float64dtype-as-data-type/</guid>
      <description>I ran into an interesting problem when trying to use Altair to visualise a Pandas DataFrame containing vaccination rates of different parts of England. In this blog post we’ll look at how to work around this issue.
First, let’s install Pandas, numpy, and altair:
pip install pandas altair numpy And now we’ll import those modules into a Python script or Jupyter notebook:
import pandas as pd import altair as alt import numpy as np Next, we’ll create a DataFrame containing the vaccinations rates of a couple of regions:</description>
    </item>
    
    <item>
      <title>Pandas: Compare values in DataFrame to previous days</title>
      <link>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</link>
      <pubDate>Wed, 21 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/21/pandas-compare-dataframe-to-previous-days/</guid>
      <description>I’m still playing around with Covid vaccine data, this time exploring how the number of doses varies week by week. I want to know how many more (or less) vaccines have been done on a given day compared to that same day last week.
We’ll be using Pandas in this blog post, so let’s first install that library and import it:
Install Pandas pip install pandas Import module import pandas as pd And now let’s create a DataFrame containing a subset of the data that I’m working with:</description>
    </item>
    
    <item>
      <title>Vaccinating England: The Data (cleanup)</title>
      <link>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</link>
      <pubDate>Sat, 17 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/17/england-covid-vaccination-rates-the-data/</guid>
      <description>Over the last 13 months I’ve spent countless hours looking at dashboards that showed Coronavirus infection rates, death rates, and numbers of people vaccinated. The UK government host a dashboard at coronavirus.data.gov.uk, which contains charts and tables showing all of the above.
One thing I haven’t been able to find, however, is a drill down of vaccinations by local area and age group. So I’m going to try to build my own!</description>
    </item>
    
    <item>
      <title>Pandas - Format DataFrame numbers with commas and control decimal places</title>
      <link>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</link>
      <pubDate>Sun, 11 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/11/pandas-format-dataframe-numbers-commas-decimals/</guid>
      <description>I’m still playing around with the UK’s COVID-19 vaccination data and in this blog post we’ll learn how to format a DataFrame that contains a mix of string and numeric values.
We’ll be using Pandas&amp;#39; styling functionality, which generates CSS and HTML, so if you want to follow along you’ll need to install Pandas and Jupyter:
pip install pandas jupyter Next, launch Jupyter and create a notebook:
jupyter notebook Let’s start by importing Pandas:</description>
    </item>
    
    <item>
      <title>Pandas - Dividing two DataFrames (TypeError: unsupported operand type(s) for /: &#39;str&#39; and &#39;str&#39;)</title>
      <link>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</link>
      <pubDate>Thu, 08 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/08/pandas-divide-dataframes-unsupported-operand-type-str/</guid>
      <description>I’ve been doing some more exploration of the UK Coronavirus vaccine data, this time looking at the number of people vaccinated by Local Tier Local Authority. The government publish data showing the number of people vaccinated in each authority by age group, as well as population estimates for each cohort.
Having loaded that data into two Pandas DataFrames, I wanted to work out the % of people vaccinated per age group per local area.</description>
    </item>
    
    <item>
      <title>Altair - Remove margin/padding on discrete X axis</title>
      <link>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</link>
      <pubDate>Fri, 02 Apr 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/04/02/altair-discrete-x-axis-margin-padding/</guid>
      <description>One of the Altair charts on my Covid Vaccine Dashboards Streamlit app shows the % of first doses, but when I first created it there was some padding on the X axis that I wanted to remove. In this blog post we’ll learn how to do that.
Pre requisites Let’s start by installing the following libraries:
pip install pandas altair altair_viewer Next let’s import them, as shown below:
import pandas as pd import altair as alt Visualising % of first doses Now we’re going to create a DataFrame that contains two columns - one contains the year and week number, the other the percentage of 1st doses administered.</description>
    </item>
    
    <item>
      <title>Pandas: Filter column value in array/list - ValueError: The truth value of a Series is ambiguous</title>
      <link>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</link>
      <pubDate>Sun, 28 Mar 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/03/28/pandas-column-value-in-array-list-truth-value-ambiguous/</guid>
      <description>The UK government publishes Coronavirus vaccinations data on coronavirus.data.gov.uk, but I wanted to create some different visualisations so I downloaded the data and have been playing with it in the mneedham/covid-vaccines GitHub repository.
I massaged the data so that I have rows in a Pandas DataFrame representing the numbers of first doses, second doses, and total doses done each day. I then wanted to filter this DataFrame based on the type of dose, but initially got a bit stuck.</description>
    </item>
    
    <item>
      <title>Neo4j Graph Data Science 1.5: Exploring the Speaker-Listener LPA Overlapping Community Detection Algorithm</title>
      <link>https://www.markhneedham.com/blog/2021/02/08/neo4j-gdsl-overlapping-community-detection-sllpa/</link>
      <pubDate>Mon, 08 Feb 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/02/08/neo4j-gdsl-overlapping-community-detection-sllpa/</guid>
      <description>The Neo4j Graph Data Science Library provides efficiently implemented, parallel versions of common graph algorithms for Neo4j, exposed as Cypher procedures. It recently published version 1.5, which introduces some fun new algorithms.
In this blog post, we’re going to explore the newly added Speaker-Listener Label Propagation algorithm with the help of a twitter dataset.
Launching Neo4j We’re going to run Neo4j with the Graph Data Science Library using the following Docker Compose configuration:</description>
    </item>
    
    <item>
      <title>Neo4j Graph Data Science 1.5: Exploring the HITS Algorithm</title>
      <link>https://www.markhneedham.com/blog/2021/02/03/neo4j-gdsl-hits-algorithm/</link>
      <pubDate>Wed, 03 Feb 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/02/03/neo4j-gdsl-hits-algorithm/</guid>
      <description>The Neo4j Graph Data Science Library provides efficiently implemented, parallel versions of common graph algorithms for Neo4j, exposed as Cypher procedures. It recently published version 1.5, which has lots of goodies to play with.
In this blog post, we’re going to explore the newly added HITS algorithm with the help of a citations dataset.
Launching Neo4j We’re going to run Neo4j with the Graph Data Science Library using the following Docker Compose configuration:</description>
    </item>
    
    <item>
      <title>Materialize: Creating multiple views on one source</title>
      <link>https://www.markhneedham.com/blog/2021/01/28/materialize-multiple-views-one-source/</link>
      <pubDate>Thu, 28 Jan 2021 00:44:37 +0000</pubDate>
      
      <guid>https://www.markhneedham.com/blog/2021/01/28/materialize-multiple-views-one-source/</guid>
      <description>This is another post describing my exploration of Materialize, a SQL streaming database. In this post we’re going to learn how to create multiple views on top of the same underlying source.
We’re still going to be using data extracted from Strava, an app that I use to record my runs, but this time we have more detailed information about each run. As in the previous blog posts, each run is represented as JSON document and store in the activities-detailed-all.</description>
    </item>
    
  </channel>
</rss>
